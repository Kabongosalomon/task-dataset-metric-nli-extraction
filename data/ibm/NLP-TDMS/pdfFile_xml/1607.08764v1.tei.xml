<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SwiDeN : Convolutional Neural Networks For Depiction Invariant Object Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Kiran Sarvadevabhatla</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computational and Data Sciences</orgName>
								<orgName type="laboratory">Video Analytics Lab</orgName>
								<orgName type="institution">Indian Institute of Science Bangalore</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiv</forename><surname>Surya</surname></persName>
							<email>shiv.surya314@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computational and Data Sciences</orgName>
								<orgName type="laboratory">Video Analytics Lab</orgName>
								<orgName type="institution">Indian Institute of Science Bangalore</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivas</forename><forename type="middle">S S</forename><surname>Kruthiventi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computational and Data Sciences</orgName>
								<orgName type="laboratory">Video Analytics Lab</orgName>
								<orgName type="institution">Indian Institute of Science Bangalore</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatesh</forename><surname>Babu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computational and Data Sciences</orgName>
								<orgName type="laboratory">Video Analytics Lab</orgName>
								<orgName type="institution">Indian Institute of Science Bangalore</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computational and Data Sciences</orgName>
								<orgName type="laboratory">Video Analytics Lab</orgName>
								<orgName type="institution">Indian Institute of Science Bangalore</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SwiDeN : Convolutional Neural Networks For Depiction Invariant Object Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>object category recognition</term>
					<term>convolutional neural networks</term>
					<term>deep learning</term>
					<term>depiction-invariance</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Current state of the art object recognition architectures achieve impressive performance but are typically specialized for a single depictive style (e.g. photos only, sketches only). In this paper, we present SwiDeN: our Convolutional Neural Network (CNN) architecture which recognizes objects regardless of how they are visually depicted (line drawing, realistic shaded drawing, photograph etc.). In SwiDeN, we utilize a novel 'deep' depictive style-based switching mechanism which appropriately addresses the depiction-specific and depiction-invariant aspects of the problem. We compare SwiDeN with alternative architectures and prior work on a 50-category Photo-Art dataset containing objects depicted in multiple styles. Experimental results show that SwiDeN outperforms other approaches for the depiction-invariant object recognition problem.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Depiction-invariant object recognition is the ability to determine an object's category regardless of how the object is visually depicted (line drawing, realistic shaded drawing, photograph etc.). Given the varying level of abstraction and complexity in depiction (See <ref type="figure" target="#fig_0">Figure 1</ref>), this is a challenging task. Human beings easily accomplish depiction-invariant recognition but machine-based systems are nowhere close to a similar level of performance. Current state-of-the-art object recognition architectures do achieve good performance but they are specialized for a single depiction style (e.g. photos <ref type="bibr" target="#b7">[8]</ref>, sketches <ref type="bibr" target="#b19">[19]</ref>). Therefore, designing architectures which recognize objects regardless of depiction style can facilitate progress towards matching human-level abilities. Moreover, the associated performance scores can also aid in quantitatively determining the semantic gap between human and machine capabilities <ref type="bibr" target="#b14">[15]</ref>.</p><p>Surprisingly, not much work exists for depiction-invariant object recognition. To address this gap, we propose a Convolutional Neural Network (CNN) architecture for depictioninvariant object category recognition which we call SwiDeN (Section 3). A novel aspect of our architecture is a 'deep' dynamic switching mechanism between two parallel CNN * Equal contributor as the first author sub-architectures (Section 3.1.1). Our switch-based design not only reduces the overall burden of the generalized object recognition task but also enables the system to address depiction-specific and depiction-invariant aspects of the problem. We compare our approach with baselines, alternative architectures (Section 4.2) and previous work on a 50-category Photo-Art dataset containing multiple depictions of objects (Section 4). Experimental results show that our architecture outperforms other architectures, especially for non-photo object depictions (Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Object class (category) recognition, albeit restricted to photographic depictions, has been studied extensively by researchers <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12]</ref>. However, little previous work exists for truly general multi-depiction object recognition. Wu et al. <ref type="bibr" target="#b15">[16]</ref> construct multi-attribute part-graphs for object categories and use graph matching for classification on the same dataset we use. However, their evaluation procedure, also used by Cai et al. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, induces an unreasonable amount of category bias which makes comparison difficult. We present an alternative evaluation procedure which is more principled (See Section 4.1). Xiao et al. <ref type="bibr" target="#b17">[17]</ref> present a graphbased object modelling approach and evaluate it on 10 augmented classes of Caltech-256. Shrivastava et al <ref type="bibr" target="#b12">[13]</ref> utilize a depiction-invariant method for image matching. Domain adaption approaches have been also been tried <ref type="bibr" target="#b3">[4]</ref>. However, when the domain-specific identifiers (e.g. target domain labels) are available as in our case, a domain-adaptation proce- </p><formula xml:id="formula_0">M-P | 3X3/2 Input -Art/Photo SWITCH C-1a | 2 C | 1 M-P C-2a | 2 C | 1 M-P C-3a | 4 C | 1 M-P C-4a | 4 C | 1 M-P C-5a | 4 C | 1 M-P C-1p | 2 C | 1 M-P C-2p | 2 C | 1 M-P C-3p | 4 C | 1 M-P C-4p | 4 C | 1 M-P C-5p | 4 C | 1 M-P</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Switch Layer</head><p>Depiction Style Labels  <ref type="figure">Figure 2</ref>(a) is the baseline architecture. <ref type="figure">Figure 2</ref>(b) (GRN) is a modification of architecture proposed by Ganin et al <ref type="bibr" target="#b6">[7]</ref>. VGG-19 <ref type="bibr" target="#b13">[14]</ref> is used as the base network for all architectures.</p><p>dure unnecessarily makes the overall problem harder since the objective in domain-adaptation is typically to "forget" the source domain. All the approaches mentioned above utilize multiple handcrafted modules in the recognition pipeline. To the best of our knowledge, ours is the first end-to-end deep learning approach for depiction-invariant recognition of object categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">OUR FRAMEWORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Motivation</head><p>Instead of learning from scratch, a common paradigm is to utilize pre-trained CNNs as a starting point while constructing deep networks of interest. We follow a similar paradigm in our approach.</p><p>In an effort to represent the sheer variety seen in image content, the convolutional layers in a CNN typically contain a large number of learnable filters. However, the filters are only sufficient to the extent that the depiction style remains unchanged (e.g. photographs). To accommodate the increase in variety when images from additional depiction styles need to be recognized, a naïve strategy would be to add additional learnable filters for each convolution layer of a pre-trained network 1 and perform fine-tuning. However, this strategy results in an unbalanced learning regime since convolutional layers now contain a mixture of learnt and non-learnt filters. In addition, the added filters necessitate an ad-hoc grouping of filter layers to ensure operational consistency which further complicates the overall framework.</p><p>An alternative design would be to learn the filters for each depictive style separately. In this design, a set of shallow layer sub-networks exist for each depictive style (see <ref type="figure">Figure</ref> 2(c)). Since our final objective is to achieve depictioninvariant recognition, we require our network to learn a depiction-invariant feature representation. This is achieved by having a final set of layers. To serve as a relay mechanism between the initial depiction-specific sub-network branches and the shared, deeper depiction-invariant fully-connected layers, we employ a custom-designed "switch" (Section 3.1.1). The switch is trained such that given an image, it determines its depictive style and selects the corresponding depictionspecific sub-network for processing the image. The output of this sub-network is then processed by the depiction-invariant layers of the network. The network culminates in a typical softmax-based classification layer which determines the image category, regardless of its depictive style <ref type="figure">(Figure 2(c)</ref>).</p><p>Next, we describe the depiction style-based switching mechanism. Subsequently, we delve into the architectural details of the main network pipeline which we dub SwiDeN (Switching Deep Network).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Switch</head><p>To realize the switching mechanism mentioned in Section 3.1, we design and train a switch network (see <ref type="figure">Figure 2</ref> (c)), henceforth referred to as Switch, that determines the depiction style of the input image and passes the image to corresponding depiction sub-network (Photo or Art). The Switch has two convolution layers which capture depictiondiscriminative features such as edges, textures, corners, colors and their conjunctions <ref type="bibr" target="#b20">[20]</ref>. The first convolution layer is initialized from AlexNet <ref type="bibr" target="#b9">[10]</ref>. The features from the first layer are max pooled while the features from the second convolution layer are average pooled globally <ref type="bibr" target="#b10">[11]</ref>. The pooled features are processed by two fully connected layers and passed to a classifier layer which determines the depiction style of the input image as 'Art' or 'Photo'. For better generalization, we use dropout for fully connected layers with 0.5 as the dropout value. We trained Switch using stochastic gradient descent (SGD) <ref type="bibr" target="#b9">[10]</ref> with a base learning rate α = 10 −2 and momentum µ = 0.9. Overall, Switch achieves an average accuracy of 83.7% (80.6% for 'Art' and 86.8% for 'Photo').</p><p>Switch's inability to achieve 100% accuracy can be attributed to the fact that some photo images have a predominantly artistic quality and vice-versa (see supplementary material). While this may seem like a liability, in practice, all we require is that Switch achieve a reasonably high accuracy which ensures an overall burden reduction for the filter learning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Switching Deep Network (SwiDeN)</head><p>The initial portion of SwiDeN consists of two separate subnetworks, one each for photo and art depiction style. During training, Switch (Section 3.1.1) selects the sub-network branch through which the input image is passed in the forward pass and ensures that the corresponding network loss is backpropagated through the branch selected during the forward pass. The layers after Switch are shared layers, designed to learn depiction-style invariant representations.</p><p>For our problem, we build SwiDeN using VGG-19 deep network <ref type="bibr" target="#b13">[14]</ref> layers. We select a subset of initial convolutional layers of VGG-19 and utilize them as the sub-networks for each depictive style. The rest of the VGG-19 layers (except the final classification layer) are used as the shared layers of SwiDeN. <ref type="figure">Figure 2</ref>(c) illustrates a SwiDeN architecture where the first four convolutional layers of VGG-19 are used for the depictive style sub-networks (C1a -C4a for 'Art' and C1p -C4p for 'Photo') and the rest of VGG-19 layers (C-5,FC-6,FC-7) form the shared portion.</p><p>In our experiments, we systematically examined the effect on recognition performance when the first k, 1 ≤ k ≤ 5 convolutional layers of VGG-19 are used as depiction-style subnetworks (Section 5). For the rest of the paper, we refer to the corresponding architectures as C1-S,C2-S,C3-S,C4-S and C5-S. Thus, the architecture in <ref type="figure">Figure 2</ref>(c) is C4-S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>We evaluate the classification performance on the Photo-Art-50 dataset <ref type="bibr" target="#b2">[3]</ref>. This dataset contains 50 classes and 90 to 138 images in each class with approximately half photo and half art images. The authors also provide train-test splits for comparative evaluation. However, the splits are unbalanced and do not include a validation split, thus inducing significant class bias during evaluation. To avoid this issue, we create our own train, validation and test splits. We create five random splits, each containing 60 images from each category for training (30 art and 30 photo) and 20 images for testing (10 art and 10 photo). The remaining images from each category are used for validation. We augment the dataset by taking 5 crops of size 224 × 224 (four corner crops and the center crop) after rescaling the smallest side of the image to 256. For training images, the center crop alone is centered around the bounding box. Multiple objects of same class in a single image are ignored. We plan to release our balanced splits to the public.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison architectures</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Baseline</head><p>As a natural baseline, we fine-tune VGG-19 using the training data for the 50 classes described in Section 4.1. For training, we used a stochastic gradient descent (SGD) method with a base learning rate α = 10 −5 and momentum µ = 0.9 to learn the weights. The learning rate was stepped down by a factor of 10 when the validation accuracy plateaued.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Gradient Reversal Network</head><p>Ganin et al. <ref type="bibr" target="#b6">[7]</ref> propose a deep network-based domainadaptation framework. The authors aim to maximize the target domain accuracy by simultaneously minimizing the target-domain label loss function and maximizing the loss for domain type (target or source) classification. To achieve this, they introduce a gradient reversal layer which not only assists domain-adaptation but also helps learn a domaininvariant representation (FC-8 in <ref type="figure">Figure 2(b)</ref>). Intrigued by this domain-invariance feature, we wished to examine the architecture's suitability for our cross-depiction problem by viewing depictive styles as domains. However, in their original formulation, Ganin et al. maximize the accuracy for a single domain (depictive style). Therefore, we modify their formulation such that the overall network loss for both the domains ('Art' and 'Photo') is minimized. In addition, we replace Alexnet used by Ganin et al. with VGG-19. For the rest of the paper, we shall refer to this modified formulation as Gradient Reversal Network (GRN).</p><p>We initialize GRN with VGG-19 model weights and performed training using SGD with base learning rate of α = 10 −5 and momentum µ = 0.9. A uniform learning rate was maintained throughout training. For the gradient reversal layer's scaling factor λ (see <ref type="bibr" target="#b6">[7]</ref> for details), we tried values of 1, 2, 3, 5, 10 and found that λ = 2 gave the best result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">SwiDeN: training</head><p>The same training procedure and hyperparameters as in the baseline were used for training SwiDeN architectures C1-S,C2-S. . .C5-S (Section 3.1.2) with the exception of the the learning rates for the depictive style sub-networks. For the 'Art' sub-network, we used a learning rate scaled by a factor of 4 since the base network (VGG) is primarily Arch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overall</head><p>Acc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Art</head><p>Acc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Photo</head><p>Acc. trained for non-Art images. The learning rate was stepped down by a factor of 10 when the validation accuracy plateaued.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation</head><p>For evaluation, we determined the final label by pooling the results for five crops of the test image (four corner crops and one center crop) for all the architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Implementation</head><p>We used Caffe <ref type="bibr" target="#b8">[9]</ref> for all experiments on the baseline. For SwiDeN, we integrated the switch layer from a branch of Caffe <ref type="bibr" target="#b1">[2]</ref> into the master branch <ref type="bibr" target="#b0">[1]</ref> and customized it for our experiments involving SwiDeN. For experiments on GRN, we used the Caffe version provided by Ganin et al. <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RESULTS</head><p>For each architecture (baseline, GRN and SwiDeN (C4-S), we computed the average test set accuracy across all the classes and all the splits. We collate the results into three groups -accuracy regardless of depictive style ('Overall') and style-wise accuracies for 'Photo' (i.e. accuracy on photographic test images only) and 'Art'). The results can be seen in seen in <ref type="table" target="#tab_0">Table 1</ref>. Our SwiDeN architecture outperforms the other two architectures overall and for 'Art' while remaining competitive for 'Photo'. In SwiDeN, the depiction-style Switch guided sub-network learning reduces the overall burden for the deeper shared layers in learning a robust depiction-invariant representation, which in turn contributes to SwiDeN's performance.</p><p>GRN performs worse than the baseline and SwiDeN. Similar to SwiDeN, GRN also utilizes feedback from a depictionstyle classifier. However, the feedback is provided coarsely and indirectly (in terms of loss). Moreover, the feedback is provided at a layer situated deep in the network. This hinders the fine-tuning of shallower (convolutional) filters to learn 'Art'-specific filters, thus affecting the performance on 'Art' in particular and overall performance in general.</p><p>We also observe that the baseline performs slightly better than other architectures for 'Photo' style. This is to be expected since the original filters are highly-tuned for photos. However, its performance for 'Art' is relatively lower compared to SwiDeN. This shows that the complexity involved in cross-depiction recognition cannot be addressed merely by employing typical transfer learning approaches such as fine-tuning.</p><p>In spite of the class-bias induced by the splits provided by Cai et al. <ref type="bibr" target="#b2">[3]</ref>, we compared the performance of our C4-S SwiDeN architecture against that of the multi-attribute part-graph model proposed by Wu et al. <ref type="bibr" target="#b15">[16]</ref>. To aid training, we augment the training set by performing RGB jittering, horizontal flip on all images and morphological operations for 'Art' images. As <ref type="table">Table 2</ref> shows, SwiDeN achieves Arch.</p><p>Overall Acc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Art</head><p>Acc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Photo</head><p>Acc.</p><p>Wu et al. <ref type="bibr" target="#b15">[16]</ref> 89.67% 89.06% 90.29% SwiDeN (Ours) 93.02% 88.47% 97.56% <ref type="table">Table 2</ref>: Classification accuracy on train-test splits by Cai et al. <ref type="bibr" target="#b2">[3]</ref>.</p><p>state-of-the-art results , outperforming the result of Wu et al. <ref type="bibr" target="#b15">[16]</ref> overall and for 'Photo' images while remaining competitive for 'Art'. <ref type="table" target="#tab_2">Table 3</ref> summarizes the performance of different SwiDeN architectures. As can be seen, C4-S outperforms other SwiDeN architectures. As an interesting observation, the trends in overall accuracy and 'Art' accuracy as the depth of depictive-style sub-networks increases resemble the patterns observed by Yosinski et al. <ref type="bibr" target="#b18">[18]</ref> for deep networks but in the context of transfer learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SwiDeN Arch.</head><p>Overall Acc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Art</head><p>Acc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Photo</head><p>Acc.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>In this paper, we have described SwiDeN, our end-to-end deep learning framework for recognizing objects regardless of depiction. A key aspect of SwiDeN is the 'deep' depictive style-based switching mechanism which judiciously addresses depiction-specific and depiction-invariant aspects of the problem. Addressing these aspects enables us to achieve state-of-the-art results on a challenging dataset containing 'Photo' and 'Art' style object depictions. In future, we plan to explore unsupervised network learning approaches. Our code and pre-trained models can be accessed at https://github.com/val-iisc/swiden.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Sample images from the Photo-Art-50 dataset grouped by category. For each category, one image each from 'Art'(left in the pair) and 'Photo' depictive style are shown. Given the extreme changes in appearance, recognizing such images regardless of depiction is extremely challenging.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>C</head><label></label><figDesc>arXiv:1607.08764v1 [cs.CV] 29 Jul 2016</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FC- 6 | 4096 FCFigure 2 :</head><label>640962</label><figDesc>Our proposed architecture SwiDeN is shown in 2(c). The depictive style of the cartoon-ish horse image is determined as 'Art' by Switch (purple block). An associated switch layer relays it to the 'Art' sub-network (green block). The latter's output is passed via a series of shared layers and finally, a softmax classifier generates the label Horse.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Baseline 93.80% 89.80% 97.80% GRN 92.64% 88.52% 96.76% SwiDeN(Ours) 94.42% 91.12% 97.72% Classification accuracy for different architectures.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Classification accuracy for different SwiDeN architectures C1-S-C5-S(see Section 3.1.2).</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In this case, the network could be one pre-trained for a particular depiction style (e.g. photographs).</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Branch</surname></persName>
		</author>
		<ptr target="https://github.com/BVLC/caffe" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Caffe switch layer branch</title>
		<ptr target="https://github.com/sguada/caffe-public/tree/switchlayer/src/caffe/layers" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The cross-depiction problem: Computer vision algorithms for recognising objects in artwork and in photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Corradi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hall</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00110</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Beyond photo-domain object recognition: Benchmarks for the cross-depiction problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Object class recognition by unsupervised scale-invariant learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 2003 IEEE Computer Society Conference on</title>
		<meeting>2003 IEEE Computer Society Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">264</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Towards scalable representations of object categories: Learning a hierarchy of parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, 2007. CVPR&apos;07. IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 32nd International Conference on Machine Learning</title>
		<meeting>The 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<title level="m">Network in network</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multiscale categorical object recognition using contour fragments. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1270" to="1281" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Data-driven visual similarity for cross-domain image matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGGRAPH ASIA)</title>
		<meeting>ACM SIGGRAPH ASIA)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Atoms of recognition in human and computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Assif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Harari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2744" to="2749" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning graphs to model visual objects across different depictive styles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014</title>
		<imprint>
			<biblScope unit="page" from="313" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning invariant structure for object identification by using graph methods. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi-Zhe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="1023" to="1031" />
		</imprint>
	</monogr>
	<note>Special issue on Graph-Based Representations in Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
		<title level="m">Sketch-a-net that beats humans. BMVC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer vision-ECCV 2014</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
