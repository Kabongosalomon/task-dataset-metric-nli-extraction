<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fast-SCNN: Fast Semantic Segmentation Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudra</forename><forename type="middle">Pk</forename><surname>Poudel</surname></persName>
							<email>rudra.poudel@crl.toshiba.co.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">Toshiba Research Europe</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Liwicki</surname></persName>
							<email>stephan.liwicki@crl.toshiba.co.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">Toshiba Research Europe</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Cambridge University</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fast-SCNN: Fast Semantic Segmentation Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The encoder-decoder framework is state-of-the-art for offline semantic image segmentation. Since the rise in autonomous systems, real-time computation is increasingly desirable. In this paper, we introduce fast segmentation convolutional neural network (Fast-SCNN), an above realtime semantic segmentation model on high resolution image data (1024 × 2048px) suited to efficient computation on embedded devices with low memory. Building on existing two-branch methods for fast segmentation, we introduce our 'learning to downsample' module which computes lowlevel features for multiple resolution branches simultaneously. Our network combines spatial detail at high resolution with deep features extracted at lower resolution, yielding an accuracy of 68.0% mean intersection over union at 123.5 frames per second on Cityscapes. We also show that large scale pre-training is unnecessary. We thoroughly validate our metric in experiments with ImageNet pre-training and the coarse labeled data of Cityscapes. Finally, we show even faster computation with competitive results on subsampled inputs, without any network modifications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Fast semantic segmentation is particular important in real-time applications, where input is to be parsed quickly to facilitate responsive interactivity with the environment. Due to the increasing interest in autonomous systems and robotics, it is therefore evident that the research into realtime semantic segmentation has recently enjoyed significant gain in popularity <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b19">20]</ref>. We emphasize, faster than real-time performance is in fact often necessary, since semantic labeling is usually employed only as preprocessing step of other time-critical tasks. Furthermore, realtime semantic segmentation on embedded devices (without access to powerful GPUs) may enable many additional ap-plications, such as augmented reality for wearables.</p><p>We observe, in literature semantic segmentation is typically addressed by a deep convolutional neural network (DCNN) with an encoder-decoder framework <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b1">2]</ref>, while many runtime efficient implementations employ a two-or multi-branch architecture <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b16">17]</ref>. It is often the case that • a larger receptive field is important to learn complex correlations among object classes (i.e. global context),</p><p>• spatial detail in images is necessary to preserve object boundaries, and</p><p>• specific designs are needed to balance speed and accuracy (rather than re-targeting classification DCNNs).</p><p>Specifically in the two-branch networks, a deeper branch is employed at low resolution to capture global context, while a shallow branch is setup to learn spatial details at full input resolution. The final semantic segmentation result is then provided by merging the two. Importantly since the computational cost of deeper networks is overcome with a small input size, and execution on full resolution is only employed for few layers, real-time performance is possible on modern GPUs. In contrast to the encoder-decoder framework, the initial convolutions at different resolutions are not shared in the two-branch approach. Here it is worth noting, the guided upsampling network (GUN) <ref type="bibr" target="#b16">[17]</ref> and the image cascade network (ICNet) <ref type="bibr" target="#b35">[36]</ref> only share the weights among the first few layers, but not the computation.</p><p>In this work we propose fast segmentation convolutional neural network Fast-SCNN, an above real-time semantic segmentation algorithm merging the two-branch setup of prior art <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b35">36]</ref>, with the classical encoder-decoder framework <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b1">2]</ref>  <ref type="figure" target="#fig_0">(Figure 1</ref>). Building on the observation that initial DCNN layers extract low-level features <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b18">19]</ref>, we share the computations of the initial layers in the twobranch approach. We call this technique learning to downsample. The effect is similar to a skip connection in the  encoder-decoder model, but the skip is only employed once to retain runtime efficiency, and the module is kept shallow to ensure validity of feature sharing. Finally, our Fast-SCNN adopts efficient depthwise separable convolutions <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b9">10]</ref>, and inverse residual blocks <ref type="bibr" target="#b27">[28]</ref>. Applied on Cityscapes <ref type="bibr" target="#b5">[6]</ref>, Fast-SCNN yields a mean intersection over union (mIoU) of 68.0% at 123.5 frames per second (fps) on a modern GPU (Nvidia Titan Xp (Pascal)) using full (1024×2048px) resolution, which is twice as fast as prior art i.e. BiSeNet (71.4% mIoU) <ref type="bibr" target="#b33">[34]</ref>.</p><p>While we use 1.11 million parameters, most offline segmentation methods (e.g. DeepLab <ref type="bibr" target="#b3">[4]</ref> and PSPNet <ref type="bibr" target="#b36">[37]</ref>), and some real-time algorithms (e.g. GUN <ref type="bibr" target="#b16">[17]</ref> and ICNet <ref type="bibr" target="#b35">[36]</ref>) require much more than this. The model capacity of Fast-SCNN is kept specifically low. The reason is two-fold: (i) lower memory enables execution on embedded devices, and (ii) better generalisation is expected. In particular, pretraining on ImageNet <ref type="bibr" target="#b26">[27]</ref> is frequently advised to boost accuracy and generality <ref type="bibr" target="#b36">[37]</ref>. In our work, we study the effect of pre-training on the low capacity Fast-SCNN. Contradicting the trend of high-capacity networks, we find that results only insignificantly improve with pre-training or additional coarsely labeled training data (+0.5% mIoU on Cityscapes <ref type="bibr" target="#b5">[6]</ref>). In summary our contributions are:</p><p>1. We propose Fast-SCNN, a competitive (68.0%) and above real-time semantic segmentation algorithm (123.5 fps) for high resolution images (1024 × 2048px).</p><p>2. We adapt the skip connection, popular in offline DC-NNs, and propose a shallow learning to downsample module for fast and efficient multi-branch low-level feature extraction.</p><p>3. We specifically design Fast-SCNN to be of low capacity, and we empirically validate that running training for more epochs is equivalently successful to pretraining with ImageNet or training with additional coarse data in our small capacity network.</p><p>Moreover, we employ Fast-SCNN to subsampled input data, achieving state-of-the-art performance without the need for redesigning our network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We discuss and compare semantic image segmentation frameworks with a particular focus on real-time execution with low energy and memory requirements <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b17">18</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Foundation of Semantic Segmentation</head><p>State-of-the-art semantic segmentation DCNNs combine two separate modules: the encoder and the decoder. The encoder module uses a combination of convolution and pooling operations to extract DCNN features. The decoder module recovers the spatial details from the sub-resolution features, and predicts the object labels (i.e. the semantic segmentation) <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b1">2]</ref>. Most commonly, the encoder is adapted from a simple classification DCNN method, such as VGG <ref type="bibr" target="#b30">[31]</ref> or ResNet <ref type="bibr" target="#b8">[9]</ref>. In semantic segmentation, the fully connected layers are removed.</p><p>The seminal fully convolution network (FCN) <ref type="bibr" target="#b28">[29]</ref> laid the foundation for most modern segmentation architectures. Specifically, FCN employs VGG <ref type="bibr" target="#b30">[31]</ref> as encoder, and bilinear upsampling in combination with skip-connection from lower layers to recover spatial detail. U-Net <ref type="bibr" target="#b25">[26]</ref> further exploited the spatial details using dense skip connections.</p><p>Later, inspired by global image-level context prior to DCNNs <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16]</ref>, the pyramid pooling module of PSPNet <ref type="bibr" target="#b36">[37]</ref> and atrous spatial pyramid pooling (ASPP) of DeepLab <ref type="bibr" target="#b3">[4]</ref> are employed to encode and utilize global context.</p><p>Other competitive fundamental segmentation architectures use conditional random fields (CRF) <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b2">3]</ref> or recurrent neural networks <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b37">38]</ref>. However, none of them run in real-time.</p><p>Similar to the object detection <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b14">15]</ref>, speed became one important factor in image segmentation system design <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b19">20]</ref>. Building on FCN, SegNet <ref type="bibr" target="#b1">[2]</ref> introduced a joint encoder-decoder model and became one of the earliest efficient segmentation models. Following SegNet, ENet <ref type="bibr" target="#b19">[20]</ref> also design an encoder-decoder with few layers to reduce the computational cost.</p><p>More recently, two-branch and multi-branch systems were introduced. ICNet <ref type="bibr" target="#b35">[36]</ref>, ContextNet <ref type="bibr" target="#b20">[21]</ref>, BiSeNet <ref type="bibr" target="#b33">[34]</ref> and GUN <ref type="bibr" target="#b16">[17]</ref> learned global context with reducedresolution input in a deep branch, while boundaries are learned in a shallow branch at full resolution.</p><p>However, state-of-the-art real-time semantic segmentation remains challenging, and typically requires high-end GPUs. Inspired by two-branch methods, Fast-SCNN incorporates a shared shallow network path to encode detail, while context is efficiently learned at low resolution <ref type="figure" target="#fig_1">(Figure 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Efficiency in DCNNs</head><p>The common techniques of efficient DCNNs can be divided into four categories:</p><p>Depthwise Separable Convolutions: MobileNet [10] decomposes a standard convolution into a depthwise convolution and a 1 × 1 pointwise convolution, together known as depthwise separable convolution. Such a factorization reduces the floating point operations and convolutional parameters, hence the computational cost and memory requirement of the model is reduced.</p><p>Efficient Redesign of DCNNs: Chollet <ref type="bibr" target="#b4">[5]</ref> designed the Xception network using efficient depthwise separable convolution. MobleNet-V2 proposed inverted bottleneck residual blocks <ref type="bibr" target="#b27">[28]</ref> to build an efficient DCNN for the classification task. ContextNet <ref type="bibr" target="#b20">[21]</ref> used inverted bottleneck residual blocks to design a two-branch network for efficient realtime semantic segmentation. Similarly, <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b35">36]</ref> propose multi-branch segmentation networks to achieve realtime performance.</p><p>Network Quantization: Since floating point multiplications are costly compared to integer or binary operations, runtime can be further reduced using quantization techniques for DCNN filters and activation values <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>Network Compression: Pruning is applied to reduce the size of a pre-trained network, resulting in faster runtime, a smaller parameter set, and smaller memory footprint <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>Fast-SCNN relies heavily on depthwise separable convolutions and residual bottleneck blocks <ref type="bibr" target="#b27">[28]</ref>. Furthermore we introduce a two-branch model that incorporates our learning to downsample module, allowing for shared feature extraction at multiple resolution levels ( <ref type="figure" target="#fig_1">Figure 2</ref>). Note, even though the initial layers of the multiple branches extract similar features <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b18">19]</ref>, common two-branch approaches do not leverage this. Network quantization and network compression can be applied orthogonally, and is left to future work. Fast-SCNN encodes spatial detail and initial layers of global context in our learning to downsample module simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Pre-training on Auxiliary Tasks</head><p>It is a common belief that pre-training on auxiliary tasks boosts system accuracy. Earlier works on object detection <ref type="bibr" target="#b6">[7]</ref> and semantic segmentation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b36">37]</ref> have shown this with pre-training on ImageNet <ref type="bibr" target="#b26">[27]</ref>. Following this trend, other real-time efficient semantic segmentation methods are also pre-trained on ImageNet <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b16">17]</ref>. However, it is not known whether pre-training is necessary on low-capacity networks. Fast-SCNN is specifically designed with low capacity. In our experiments we show that small networks do not get significant benefit from pre-training. Instead, aggressive data augmentation and more number of epochs provide similar results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Fast-SCNN</head><p>Fast-SCNN is inspired by the two-branch architectures <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b16">17]</ref> and encoder-decoder networks with skip connections <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b25">26]</ref>. Noting that early layers commonly ex- tract low-level features. We reinterpret skip connections as a learning to downsample module, enabling us to merge the key ideas of both frameworks, and allowing us to build a fast semantic segmentation model. <ref type="figure" target="#fig_0">Figure 1</ref> and <ref type="table">Table 1</ref> present the layout of Fast-SCNN. In the following we discuss our motivation and describe our building blocks in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Motivation</head><p>Current state-of-the-art semantic segmentation methods that run in real-time are based on networks with two branches, each operating on a different resolution level <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b16">17]</ref>. They learn global information from lowresolution versions of the input image, and shallow networks at full input resolution are employed to refine the precision of the segmentation results. Since input resolution and network depth are main factors for runtime, these two-branch approaches allow for real-time computation.</p><p>It is well known that the first few layers of DCNNs extract the low-level features, such as edges and corners <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b18">19]</ref>. Therefore, rather than employing a two-branch approach with separate computation, we introduce learning to downsample, which shares feature computation between the low and high-level branch in a shallow network block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Network Architecture</head><p>Our Fast-SCNN uses a learning to downsample module, a coarse global feature extractor, a feature fusion module and a standard classifier. All modules are built using depthwise separable convolution, which has become a key building block for many efficient DCNN architectures <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b20">21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Operator <ref type="table">Table 2</ref>. The bottleneck residual block transfers the input from c to c channels with expansion factor t. Note, the last pointwise convolution does not use non-linearity f . The input is of height h and width w, and x/s represents kernel size and stride of the layer.</p><formula xml:id="formula_0">Output h × w × c Conv2D 1/1, f h × w × tc h × w × tc DWConv 3/s, f h s × w s × tc h s × w s × tc Conv2D 1/1, − h s × w s × c</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Learning to Downsample</head><p>In our learning to downsample module, we employ three layers. Only three layers are employed to ensure low-level feature sharing is valid, and efficiently implemented. The first layer is a standard convolutional layer (Conv2D) and the remaining two layers are depthwise separable convolutional layers (DSConv). Here we emphasize, although DSConv is computationally more efficient, we employ Conv2D since the input image only has three channels, making DSConv's computational benefit insignificant at this stage. All three layers in our learning to downsample module use stride 2, followed by batch normalization <ref type="bibr" target="#b11">[12]</ref> and ReLU. The spatial kernel size of the convolutional and depthwise layers is 3 × 3. Following <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b20">21]</ref>, we omit the nonlinearity between depthwise and pointwise convolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Global Feature Extractor</head><p>The global feature extractor module is aimed at capturing the global context for image segmentation. In contrast to common two-branch methods which operate on lowresolution versions of the input image, our module directly takes the output of the learning to downsample module (which is at 1 8 -resolution of the original input). The detailed structure of the module is shown in <ref type="table">Table 1</ref>. We use efficient bottleneck residual block introduced by MobileNet-V2 <ref type="bibr" target="#b27">[28]</ref> ( <ref type="table">Table 2</ref>). In particular, we employ residual connection for the bottleneck residual blocks when the input and output are of the same size. Our bottleneck block uses an efficient depthwise separable convolution, resulting in less number of parameters and floating point operations. Also, a pyramid pooling module (PPM) <ref type="bibr" target="#b36">[37]</ref> is added at the end to aggregate the different-region-based context information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Feature Fusion Module</head><p>Similar to ICNet <ref type="bibr" target="#b35">[36]</ref> and ContextNet <ref type="bibr" target="#b20">[21]</ref> we prefer simple addition of the features to ensure efficiency. Alternatively, more sophisticated feature fusion modules (e.g. <ref type="bibr" target="#b33">[34]</ref>) could be employed at the cost of runtime performance, to reach Higher resolution X times lower resolution -Upsample × X -DWConv (dilation X) 3/1, f Conv2D 1/1, − Conv2D 1/1, − add,f <ref type="table">Table 3</ref>. Features fusion module (FFM) of Fast-SCNN. Note, the pointwise convolutions are of desired output, and do not use nonlinearity f . Non-linearity f is employed after adding the features. better accuracy. The detail of the feature fusion module is shown in <ref type="table">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Classifier</head><p>In the classifier we employ two depthwise separable convolutions (DSConv) and one pointwise convolution (Conv2D). We found that adding few layers after the feature fusion module boosts the accuracy. The details of the classifier module is shown in the <ref type="table">Table 1</ref>.</p><p>Softmax is used during training, since gradient decent is employed. During inference we may substitute costly softmax computations with argmax, since both functions are monotonically increasing. We denote this option as Fast-SCNN cls (classification). On the other hand, if a standard DCNN based probabilistic model is desired, softmax is used, denoted as Fast-SCNN prob (probability).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Comparison with Prior Art</head><p>Our model is inspired by the two-branch framework, and incorporates ideas of encoder-decorder methods <ref type="figure" target="#fig_1">(Figure 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Relation with Two-branch Models</head><p>The state-of-the-art real-time models (ContextNet <ref type="bibr" target="#b20">[21]</ref>, BiSeNet <ref type="bibr" target="#b33">[34]</ref> and GUN <ref type="bibr" target="#b16">[17]</ref>) use two-branch networks. Our learning to downsample module is equivalent to their spatial path, as it is shallow, learns from full resolution, and is used in the feature fusion module <ref type="figure" target="#fig_0">(Figure 1)</ref>.</p><p>Our global feature extractor module is equivalent to the deeper low-resolution branch of such approaches. In contrast, our global feature extractor shares its computation of the first few layers with the learning to downsample module. By sharing the layers we not only reduce computational complexity of feature extraction, but we also reduce the required input size as Fast-SCNN uses 1 8 -resolution instead of 1 4 -resolution for global feature extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Relation with Encoder-Decoder Models</head><p>Proposed Fast-SCNN can be viewed as a special case of an encoder-decoder framework, such as FCN <ref type="bibr" target="#b28">[29]</ref> or U-Net <ref type="bibr" target="#b25">[26]</ref>. However, unlike the multiple skip connections in FCN and the dense skip connections in U-Net, Fast-SCNN only employs a single skip connection to reduce computations as well as memory. In correspondence with <ref type="bibr" target="#b34">[35]</ref>, who advocate that features are shared only at early layers in DCNNs, we position our skip connection early in our network. In contrast, prior art typically employ deeper modules at each resolution, before skip connections are applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluated our proposed fast segmentation convolutional neural network (Fast-SCNN) on the validation set of the Cityscapes dataset <ref type="bibr" target="#b5">[6]</ref>, and report its performance on the Cityscapes test set, i.e. the Cityscapes benchmark server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Implementation detail is as important as theory when it comes to efficient DCNNs. Hence, we carefully describe our setup here. We conduct experiments on the TensorFlow machine learning platform using Python. Our experiments are executed on a workstation with either Nvidia Titan X (Maxwell) or Nvidia Titan Xp (Pascal) GPU, with CUDA 9.0 and CuDNN v7. Runtime evaluation is performed in a single CPU thread and one GPU to measure the forward inference time. We use 100 frames for burn-in and report average of 100 frames for the frames per second (fps) measurement.</p><p>We use stochastic gradient decent (SGD) with momentum 0.9 and batch-size 12. Inspired by <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b9">10]</ref> we use 'poly' learning rate with the base one as 0.045 and power as 0.9. Similar to MobileNet-V2 we found that 2 regularization is not necessary on depthwise convolutions, for other layers 2 is 0.00004. Since training data for semantic segmentation is limited, we apply various data augmentation techniques: random resizing between 0.5 to 2, translation/crop, horizontal flip, color channels noise and brightness. Our model is trained with cross-entropy loss. We found that auxiliary losses at the end of learning to downsample and the global feature extraction modules with 0.4 weights are beneficial.</p><p>Batch normalization <ref type="bibr" target="#b11">[12]</ref> is used before every non-linear function. Dropout is used only on the last layer, just before the softmax layer. Contrary to MobileNet <ref type="bibr" target="#b9">[10]</ref> and Con-textNet <ref type="bibr" target="#b20">[21]</ref>, we found that Fast-SCNN trains faster with ReLU and achieves slightly better accuracy than ReLU6, even with the depthwise separable convolutions that we use throughout our model.</p><p>We found that the performance of DCNNs can be improved by training for higher number of iterations, hence we train our model for 1,000 epochs unless otherwise stated, using the Cityescapes dataset <ref type="bibr" target="#b5">[6]</ref>. It is worth noting here, Fast-SCNN's capacity is deliberately very low, as we employ 1.11 million parameters. Later we show that aggressive data augmentation techniques make overfitting unlikely. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation on Cityscapes</head><p>We evaluate our proposed Fast-SCNN on Cityscapes, the largest publicly available dataset on urban roads <ref type="bibr" target="#b5">[6]</ref>. This dataset contains a diverse set of high resolution images (1024×2048px) captured from 50 different cities in Europe. It has 5,000 images with high label quality: a training set of 2,975, validation set of 500 and test set of 1,525 images. The label for the training set and validation set are available and test results can be evaluated on the evaluation server. Additionally, 20,000 weakly annotated images (coarse labels) are available for training. We report results with both, fine only and fine with coarse labeled data. Cityscapes provides 30 class labels, while only 19 classes are used for evaluation. The mean of intersection over union (mIoU), and network inference time are reported in the following.</p><p>We evaluate overall performance on the withheld test set of Cityscapes <ref type="bibr" target="#b5">[6]</ref>. The comparison between the proposed Fast-SCNN and other state-of-the-art real-time semantic segmentation methods (ContextNet <ref type="bibr" target="#b20">[21]</ref>, BiSeNet <ref type="bibr" target="#b33">[34]</ref>, GUN <ref type="bibr" target="#b16">[17]</ref>, ENet <ref type="bibr" target="#b19">[20]</ref> and ICNet <ref type="bibr" target="#b35">[36]</ref>) and offline</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Class ContextNet <ref type="bibr" target="#b20">[21]</ref> 65  <ref type="table">Table 6</ref>. Class mIoU of different Fast-SCNN settings on the Cityscapes validation set. methods (PSPNet <ref type="bibr" target="#b36">[37]</ref> and DeepLab-V2 <ref type="bibr" target="#b3">[4]</ref>) is shown in Table 4. Fast-SCNN achieves 68.0% mIoU, which is slightly lower than BiSeNet (71.5%) and GUN (70.4%). Con-textNet only achieves 66.1% here. <ref type="table" target="#tab_4">Table 5</ref> compares runtime at different resolutions. Here, BiSeNet (57.3 fps) and GUN (33.3 fps) are significantly slower than Fast-SCNN (123.5 fps). Compared to Con-textNet (41.9 fps), Fast-SCNN is also significantly faster on Nvidia Titan X (Maxwell). Therefore we conclude, Fast-SCNN significantly improves upon state-of-the-art runtime with minor loss in accuracy. At this point we emphasize, our model is designed for low memory embedded devices. Fast-SCNN uses 1.11 million parameters, that is five times less than the competing BiSeNet at 5.8 million.</p><p>Finally, we zero-out the contribution of the skip connection and measure Fast-SCNN's performance. The mIoU reduced from 69.22% to 64.30% on the validation set. The qualitative results are compared in <ref type="figure" target="#fig_2">Figure 3</ref>. As expected, Fast-SCNN benefits from the skip connection, especially around boundaries and objects of small size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Pre-training and Weakly Labeled Data</head><p>High capacity DCNNs, such as R-CNN <ref type="bibr" target="#b6">[7]</ref> and PSPNet <ref type="bibr" target="#b36">[37]</ref>, have shown that performance can be boosted with pretraining through different auxiliary tasks. As we specifically design Fast-SCNN to have low capacity, we now want to test performance with and without pre-training, and in connection with and without additional weakly labeled data. To the best of our knowledge, the significance of pre-training and additional weakly labeled data on low capacity DCNNs has not been studied before. <ref type="table">Table 6</ref> shows the results.</p><p>We pre-train Fast-SCNN on ImageNet <ref type="bibr" target="#b26">[27]</ref> by replacing the feature fusion module with average pooling and the classification module now has a softmax layer only. Fast-SCNN achieves 60.71% top-1 and 83.0% top-5 accuracies on the ImageNet validation set. This result indicates that Fast-SCNN has insufficient capacity to reach comparable performance to most standard DCNNs on ImageNet (&gt;70% top-1) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b27">28]</ref>. The accuracy of Fast-SCNN with ImageNet pre-training yields 69.15% mIoU on the validation set of Cityscapes, only 0.53% improvement over Fast-SCNN without pre-training. Therefore we conclude, no significant boost can be achieved with ImageNet pre-training in Fast-SCNN.</p><p>Since the overlap between Cityscapes' urban roads and ImageNet's classification task is limited, it is reasonable to assume that Fast-SCNN may not benefit due to limited capacity for both domains. Therefore, we now incorporate the 20,000 coarsely labeled additional images provided by Cityscapes, as these are from a similar domain. Nevertheless, Fast-SCNN trained with coarse training data (with or without ImageNet) perform similar to each other, and only slightly improve upon the original Fast-SCNN without pretraining. Please note, small variations are insignificant and due to random initializations of the DCNNs.</p><p>It is worth noting here that working with auxiliary tasks is non-trivial as it requires architectural modifications in the network. Furthermore, licence restrictions and lack of resources further limit such setups. These costs can be saved, since we show that neither ImageNet pre-training nor weakly labeled data are significantly beneficial for our low capacity DCNN. <ref type="figure" target="#fig_4">Figure 4</ref> shows the training curves. Fast-SCNN with coarse data trains slow in terms of iterations because of the weak label quality. Both ImageNet pre-trained versions perform better for early epochs (upto 400 epochs for training set alone, and 100 epochs when trained with the additional coarse labeled data). This means, we only need to train our model for longer to reach similar accuracy when we train our model from scratch.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Lower Input Resolution</head><p>Since we are interested in embedded devices that may not have full resolution input, or access to powerful GPUs, we conclude our evaluation with the study of performance at half, and quarter input resolutions <ref type="table">(Table 7)</ref>.</p><p>At quarter resolution, Fast-SCNN achieves 51.9% accu-  <ref type="table">Table 7</ref>. Runtime and accuracy of Fast-SCNN at different input resolutions on Cityscapes' test set <ref type="bibr" target="#b5">[6]</ref>. racy at 485.4 fps, which significantly improves on (anonymous) MiniNet with 40.7% mIoU at 250 fps <ref type="bibr" target="#b5">[6]</ref>. At half resolution, a competitive 62.8% mIoU at 285.8 fps is reached. We emphasize, without modification, Fast-SCNN is directly applicable to lower input resolution, making it highly suitable for embedded devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We propose a fast segmentation network for above realtime scene understanding. Sharing the computational cost of the multi-branch network yields run-time efficiency. In experiments our skip connection is shown beneficial for recovering the spatial details. We also demonstrate that if trained for long enough, large-scale pre-training of the model on an additional auxiliary task is not necessary for the low capacity network.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Fast-SCNN shares the computations between two branches (encoder) to build a above real-time semantic segmentation network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Schematic comparison of Fast-SCNN with encoderdecoder and two-branch architectures. Encoder-decoder employs multiple skip connections at many resolutions, often resulting from deep convolution blocks. Two-branch methods employ global features from low resolution with shallow spatial detail.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Visualization of Fast-SCNN's segmentation results. First column: input RGB images; second column: outputs of Fast-SCNN; and last column: outputs of Fast-SCNN after zeroing-out the contribution of the skip connection. In all results, Fast-SCNN benefits from skip connections especially at boundaries and objects of small size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Training curves on Cityscapes. Accuracy over iterations (top), and accuracy over epochs are shown (bottom). Dash lines represent ImageNet pre-training of the Fast-SCNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>of the bottleneck block, number of output channels, number of times block is repeated and stride parameter which is applied to first sequence of the repeating block. The horizontal lines separate the modules: learning to down-sample, global feature extractor, feature fusion and classifier (top to bottom).</figDesc><table><row><cell>Input</cell><cell>Block</cell><cell>t c</cell><cell>n s</cell></row><row><cell cols="2">1024 × 2048 × 3 Conv2D</cell><cell>-32</cell><cell>1 2</cell></row><row><cell cols="2">512 × 1024 × 32 DSConv</cell><cell>-48</cell><cell>1 2</cell></row><row><cell>256 × 512 × 48</cell><cell>DSConv</cell><cell>-64</cell><cell>1 2</cell></row><row><cell>128 × 256 × 64</cell><cell cols="2">bottleneck 6 64</cell><cell>3 2</cell></row><row><cell>64 × 128 × 64</cell><cell cols="2">bottleneck 6 96</cell><cell>3 2</cell></row><row><cell>32 × 64 × 96</cell><cell cols="3">bottleneck 6 128 3 1</cell></row><row><cell>32 × 64 × 128</cell><cell>PPM</cell><cell cols="2">-128 --</cell></row><row><cell>32 × 64 × 128</cell><cell>FFM</cell><cell cols="2">-128 --</cell></row><row><cell cols="2">128 × 256 × 128 DSConv</cell><cell cols="2">-128 2 1</cell></row><row><cell cols="2">128 × 256 × 128 Conv2D</cell><cell>-19</cell><cell>1 1</cell></row><row><cell cols="4">Table 1. Fast-SCNN uses standard convolution (Conv2D), depth-</cell></row><row><cell cols="4">wise separable convolution (DSConv), inverted residual bottle-</cell></row><row><cell cols="4">neck blocks (bottleneck), a pyramid pooling module (PPM) and</cell></row><row><cell cols="4">a feature fusion module (FFM) block. Parameters t, c, n and s rep-</cell></row><row><cell>resent expansion factor</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative results of Fast-SCNN on Cityscapes<ref type="bibr" target="#b5">[6]</ref> validation set. First column: input RGB images; second column: ground truth labels; and last column: Fast-SCNN outputs. Fast-SCNN obtains 68.0% class level mIoU and 84.7% category level mIoU.</figDesc><table><row><cell>Input Size</cell><cell>Class FPS</cell></row><row><cell cols="2">1024 × 2048 68.0 123.5</cell></row><row><cell>512 × 1024</cell><cell>62.8 285.8</cell></row><row><cell>256 × 512</cell><cell>51.9 485.4</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deeplab</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<title level="m">Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02357</idno>
		<title level="m">Xception: Deep Learning with Depthwise Separable Convolutions</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<title level="m">Deep Residual Learning for Image Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Binarized Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>NIPS. 2016. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<title level="m">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Pruning Filters for Efficient ConvNets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Durdanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Samet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Graf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Are spatial and global constraints really necessary for segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">B</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Guided Upsampling Network for Real-Time Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mazzini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehta</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs]</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs]</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Caspi</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs]</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shapiro</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs]</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs]</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Espnet</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs]</orgName>
			</affiliation>
		</author>
		<idno type="arXiv">arXiv:1803.06815</idno>
		<title level="m">Efficient Spatial Pyramid of Dilated Convolutions for Semantic Segmentation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Feature visualization. Distill</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mordvintsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schubert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02147</idno>
		<title level="m">ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Contextnet: Exploring context and detail for semantic segmentation in real-time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Poudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Bonde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Yolo9000: Better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">ERFNet: Efficient Residual Factorized ConvNet for Real-Time Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Álvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional Networks for Biomedical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.04381</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Rigid-motion scattering for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Reseg: A recurrent neural network-based model for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Visin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ciccone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matteucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Training and Inference with Integers in Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bisenet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Visualizing and Understanding Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">ICNet for Real-Time Semantic Segmentation on High-Resolution Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pyramid Scene Parsing Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks. In ICCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
