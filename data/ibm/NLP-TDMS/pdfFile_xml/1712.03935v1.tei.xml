<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On the Benefit of Combining Neural, Statistical and External Features for Fake News Identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Bhatt</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology</orgName>
								<address>
									<settlement>Roorkee</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aman</forename><surname>Sharma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology</orgName>
								<address>
									<settlement>Roorkee</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivam</forename><surname>Sharma</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankush</forename><surname>Nagpal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology</orgName>
								<address>
									<settlement>Roorkee</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balasubramanian</forename><surname>Raman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology</orgName>
								<address>
									<settlement>Roorkee</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankush</forename><surname>Mittal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology</orgName>
								<address>
									<settlement>Roorkee</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Graphic Era University</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">On the Benefit of Combining Neural, Statistical and External Features for Fake News Identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>External features</term>
					<term>Statistical features</term>
					<term>Word embeddings</term>
					<term>Fake news</term>
					<term>Deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Identifying the veracity of a news article is an interesting problem while automating this process can be a challenging task. Detection of a news article as fake is still an open question as it is contingent on many factors which the current state-of-the-art models fail to incorporate. In this paper, we explore a subtask to fake news identification, and that is stance detection. Given a news article, the task is to determine the relevance of the body and its claim. We present a novel idea that combines the neural, statistical and external features to provide an efficient solution to this problem. We compute the neural embedding from the deep recurrent model, statistical features from the weighted n-gram bag-of-words model and hand crafted external features with the help of feature engineering heuristics. Finally, using deep neural layer all the features are combined, thereby classifying the headline-body news pair as agree, disagree, discuss, or unrelated. We compare our proposed technique with the current state-of-the-art models on the fake news challenge dataset. Through extensive experiments, we find that the proposed model outperforms all the state-of-the-art techniques including the submissions to the fake news challenge.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Fake news being a potential threat towards journalism and public discourse has created a buzz across the internet. With the recent advent of social media platforms such as Facebook and Twitter, it has become easier to propagate any information to the masses within minutes. While the propagation of information is proportional to growth of social media, there has been an aggravation in the authenticity of these news articles. These days it has become a lot easier to mislead the masses using a single Facebook or Twitter fake post. For an instance, in the US presidential election of 2016, the fake news has been cited as the foremost contributing factor that affected the outcome <ref type="bibr" target="#b23">[24]</ref>. <ref type="table">Table 1</ref>. Headline-body pairs along with their relative stance.</p><p>The root cause of this problem lies in the fact that none of the social networking sites use any automatic system that can identify the veracity of news flowing across these platforms. A possible reason for this failure is the open domain nature of the problem that adds to the intricacies. The recently organized Fake News Challenge (FNC-1) <ref type="bibr" target="#b12">[13]</ref> is an initiative in this direction. The aim of this challenge is to build an automatic system that has the capability to identify whether a news article is fake or not. More specifically, given a news article the task is to evaluate the relatedness of the news body towards its headline. The relatedness or stance is the relative perspective of a news article towards a relative claim (shown in <ref type="table">Table 1</ref>).</p><p>The idea behind building a countermeasure for fake news is to use machine learning and natural language processing (NLP) tools that can compute semantic and contextual similarity between the headline and the body, and classify the pairs into one of four categories. Deep learning models have been efficacious in solving many NLP problems that share similarities to fake news which includes but not limited to -computing semantic similarity between sentences <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18]</ref>, community based question answering <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>, etc. The basic building blocks of all deep models are recurrent networks such as recurrent neural networks (RNN) <ref type="bibr" target="#b22">[23]</ref>, long short-term memory networks (LSTM) <ref type="bibr" target="#b15">[16]</ref> and gated recurrent units (GRU) <ref type="bibr" target="#b10">[11]</ref>, and convolution networks such as convolution neural networks (CNN) <ref type="bibr" target="#b16">[17]</ref>. A deep architecture encodes the given sequence of words into fixed length vector representation which can be used to score the relevance of two textual entities, in our case, relevance of each headline-body pair.</p><p>Statistical information related to text can be encoded to vectors using the traditional bag-of-words (BOW) approach. The BOW approaches are often combined with term frequency (TF) and inverse document frequency (IDF), and ngrams that helps to encode more information related to the text <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b11">12]</ref>. These approaches, however simple, have been used to ameliorate the performance of deep models in complex NLP problems such as community question answering <ref type="bibr" target="#b31">[32]</ref> and answer sentence selection <ref type="bibr" target="#b32">[33]</ref>. Sometimes, it is beneficial to leverage feature engineering heuristics when combined with statistical approaches. The feature engineering heuristics or the external features are used to aid the learning model to successfully converge to a global solution <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34]</ref>. The external features includes common observations such as number of n-grams, number of words match between headline and the body, cosine similarity between the headline and the body vector, etc. The FNC-1 baseline also includes a combination of feature engineering heuristics that alone achieves a competitive performance, even outperforming several widely used deep learning architectures. In this pa-per, we combine external features introduced in the baseline with some more heuristics that have been shown to be successful in other NLP tasks.</p><p>These days it is common to use pre-trained word embeddings such as Word2vec <ref type="bibr" target="#b19">[20]</ref> and GloVe <ref type="bibr" target="#b24">[25]</ref> along with deep models for NLP tasks. Similar to word embedding, the recurrent models have been used to encode an entire sentence to a vector. Some of the widely used sentence-to-vector models include doc2vec <ref type="bibr" target="#b20">[21]</ref>, paragraph2vec <ref type="bibr" target="#b26">[27]</ref> and skip-thought vectors <ref type="bibr" target="#b17">[18]</ref>. These deep recurrent models helps to capture the semantic and contextual information of the textual pairs, in our case, body and its claim. In our work, we use the skip-thought vector to encode the headline and the body, and combine it with external features and statistical approaches.</p><p>Finally, the main contributions of the paper can be summarized as 1. We propose an approach that is based on the combination of statistical, neural and feature engineering heuristics which achieves state-of-the-art performance on the task of fake news identification. 2. We evaluate the proposed approach on FNC-1 challenge, and compare our results with the top-4 submissions to the challenge. We also analyze the applicability of several state-of-the-art deep models on FNC-1 dataset.</p><p>The rest of the paper is organized as follows. In section 2, we brief the previous idea over which our works builds, which is followed by applicability of stateof-the-art deep architectures on the problem of stance detection. In section 4 we describe the proposed approach in detail, followed by the experiment setup in section 5, that includes dataset description, training parameters, evaluation metrics used and results. Finally, our work is concluded in section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we discuss some previous work that is in relation to fake news identification such as rumor detection in news articles and hoax news identification. We also discuss the use of deep learning architecture used by some of the researchers with whom our work shares some similarity.</p><p>Fake news. From an NLP perspective, researchers have studied numerous aspects of credibility of online information. For example, <ref type="bibr" target="#b4">[5]</ref> applied the timesensitive supervised approach by relying on the tweet content to address the credibility of a tweet in different situations. <ref type="bibr" target="#b6">[7]</ref> used LSTM in a similar problem of early rumor detection. In an another work, <ref type="bibr" target="#b7">[8]</ref> aimed at detecting the stance of tweets and determining the veracity of the given rumor with convolution neural networks. A submission <ref type="bibr" target="#b2">[3]</ref> to the SemEval 2016 Twitter Stance Detection task focuses on creating a bag-of-words auto encoder, and training it over the tokenized tweets.</p><p>FNC-1 submissions. In their work, <ref type="bibr" target="#b25">[26]</ref> achieved a preliminary score of 0.8080, slightly above the competition baseline of 0.7950. They experimented on four basic models on which the final result was evaluated: Bag Of Words (BOW), basic LSTM, LSTM with attention and conditional encoding LSTM with attention (CEA LSTM). In our work, instead of using the models separately, we combine the best of these models.</p><p>Another team, <ref type="bibr" target="#b33">[34]</ref>, combined multiple models in an ensemble providing 50/50 weighted average between deep convolution neural network and a gradientboosted decision trees. Though this work seems to be similar to our work, the difference lies in the construction of ensemble of classifiers. In a similar attempt, a team <ref type="bibr" target="#b1">[2]</ref> concatenated various features vectors and passed it through an MLP model.</p><p>The work by <ref type="bibr" target="#b27">[28]</ref>, focuses on generating lexical and similarity features using (TF-IDF) representations of bag-of-words (BOW) which are then fed through a multi-layer perceptron (MLP) with one hidden layer. In their work, <ref type="bibr" target="#b5">[6]</ref> divided the problem into two groups: unrelated and related. They were able to achieve 90% accuracy on the related/unrelated task by finding maximum and average Jaccard similarity score across all sentences in the article and choosing appropriate threshold values. A similar work of splitting the problem into two subproblems (related and unrelated ) is also performed by <ref type="bibr" target="#b9">[10]</ref>. The work by <ref type="bibr" target="#b21">[22]</ref> focuses on the use of recurrent models for fake news stance detection.</p><p>3 Technique Used</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Deep Learning Architectures</head><p>To predict the stance for a given sample in FNC-1 dataset, a multi-channel deep neural network can be used to encode a given headline-body pair, which can be classified into one of the four stances. This is achieved by using a multi channel convolution neural network with sof tmax layer at the output (shown in <ref type="figure" target="#fig_0">Figure 1</ref>). Similarly, instead of using the convolution and pooling layers, LSTM and GRU can be used to encode the headline-body pairs. The LSTMs and GRUs encode the given sequence of words into fixed length vector representation which can be used to score the relevance of headline-body pair. However, for long sequences, such as the body of a news article (which typically contain hundreds of words), the RNN models fail to completely encode the entire information into a fixed length vector. A solution to this problem is given in the form of attentional mechanism <ref type="bibr" target="#b8">[9]</ref> which computes a weighted sum of all the encoder units that are passed on to the decoder. The decoder is learned in such a way that it gives importance to only some of the words. The attention mechanism also alleviates the bottleneck of encoding input sequences to fixed length vector and have been shown to outperform other RNN based encoder-decoder models on longer sequences <ref type="bibr" target="#b3">[4]</ref>. To alleviate the problem of limited memory we use attention mechanism as described in <ref type="bibr" target="#b3">[4]</ref>.</p><p>We experiment with some of the deep architectures that have been shown to be successful in NLP tasks (shown in <ref type="figure" target="#fig_0">Figure 1</ref>). Most of these architectures have been proven to be effective for non-factoid based question answering <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b13">14]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Proposed Idea</head><p>The unrelated headline-body pairs in the FNC-1 dataset are created by randomly assigning a news body to the given headline. This type of data augmentation has been successfully used in NLP problems such as non-factoid question answering where it results in reasonable performance by the deep learning models <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b18">19]</ref>. However, in the case of FNC-1 challenge, the agree, disagree, and discuss headline-body pairs are relatively smaller in quantity than the unrelated stance. This bias leads to a uneven distribution of dataset across the four classes, with the unrelated category being the least interesting. Interestingness of a headline-body pair is evaluated in terms of information that it contains; It is easier to evaluate a unrelated pair, while the other three are contingent on exploring contextual relationship between the headline and its body, and are considered more interesting.</p><p>The uneven distribution of FNC-1 dataset thwarts the performance of deep learning architectures introduced in Section 3. Moreover, news articles are heavily influenced by some words that are generally associated with news to describe its polarity. For example, words like crime, accident, and scandal are often used with negative connotation. If such words are present in both the news headline, or are present in one while absent from the other, then, it is easier to identify such a pair as agree or disagree. Deep learning models are dependent on a huge training corpus (few million headline-body pairs) in order to identify such nuances in patterns. The FNC-1 dataset, though the largest publicly available dataset on stance detection, does not satiate this criteria. For this reason, we introduce a much simpler strategy that consists of heavy use of feature engineering. We leveraged several widely used state-of-the-art features used in natural language processing, and use a feed-forward deep neural network which aggregates all the individual features and computes a score for each headline-body pair. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Neural Embeddings</head><p>We use skip-thought vectors which encodes sentences to vector embedding of length 4800 (shown in <ref type="figure" target="#fig_1">Figure 2</ref>). The skip-thought <ref type="bibr" target="#b17">[18]</ref> is a encoder-decoder based recurrent model that computes the relative occurrence of sentences. In our work, we use the pre-trained skip-thought embedding which is trained on BookCorpus <ref type="bibr" target="#b34">[35]</ref>. We make the use of a pre-trained model since the FNC-1 dataset is relatively smaller than the dataset required to efficiently train a recurrent encoder-decoder model like skip-thought.</p><p>We follow the work of <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b0">1]</ref> and compute two features from the skip-thought embeddings. These features have been shown to be effective in evaluating contextual similarity between sentences. The task of stance detection is analogous to the computation of contextual similarity between two sentences -headline and its body. We speculate that the features introduced by <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b0">1]</ref> should be effective for stance detection as well. Given the skip-thought encoding of news and headline as u news and v head , we compute two features</p><formula xml:id="formula_0">f eat 1 = u news .v head (1) f eat 2 = |u news − v head |<label>(2)</label></formula><p>where f eat 1 is the component-wise product and f eat 2 is the absolute difference between the skip-thought encoding of news and headlines. Both of these features results in a 4800 dimensional vector each.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Statistical Features</head><p>We capture the statistical information from the text to vectors with the help of BOW, TF-IDF and n-grams models. We follow the work of <ref type="bibr" target="#b27">[28]</ref> and <ref type="bibr" target="#b11">[12]</ref>, and produce the following vectors for each headline-body pair 1. 1-gram TF vector of the headline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">1-gram TF vector of the body.</head><p>This gives us a vector of 5000 dimension each. We concatenate both of the TF vectors and pass it to a MLP layer (as shown in <ref type="figure" target="#fig_1">Figure 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">External Features</head><p>The external features include feature engineering heuristics such as number of similar words in the headline and body, cosine similarity between vector encodings of headline-body pairs, number of n-grams matched between the pairs, etc. We leveraged ideas for computing the external features from the baseline and add some extra features, which includes 1. Number of characters n-grams match between the headline-body pair, where n = 2, · · · , 16. 2. Number of words n-grams match between the headline-body pair, where n = 2, · · · , 6. 3. Weighted TF-IDF score between headline and its body using the approach mentioned in <ref type="bibr" target="#b32">[33]</ref>. 4. Sentiment difference between the headline-body pair, also termed as polarity and is computed using lexicon based approach. 5. N-gram refuting feature which is constructed using BOW on a lexicon of n pre-defined words. It is similar to polarity based features with an addition of n-gram model.</p><p>All the external features adds up to a 50-dimensional feature vector and is passed to a MLP layer similar to neural and statistical features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimentations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dataset Description</head><p>We use the dataset provided in the FNC-1 challenge which is derived from the Emergent Dataset <ref type="bibr" target="#b14">[15]</ref>, provided by the fake news challenge administrators. The former consist of 49972 tuple with each tuple consisting of a headline-body pair followed by a corresponding class label stance of either agree, disagree, unrelated or discuss. Word counts roughly ranges between 8 to 40 for headlines and 600 to 7000 for article body. The distribution of FNC-1 dataset is shown in <ref type="table" target="#tab_0">Table 2</ref>. Cross-entropy <ref type="table">Table 3</ref>. Values of hyper-parameters. The first half of the table shows the parameters used in architectures for extracting individual features. The second half shows the parameter setting of the feature combination layer that is shown in <ref type="figure" target="#fig_1">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Training parameters</head><p>As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, the proposed model computes the feature vectors separately and then combine these with the help of a MLP layer. We use cross-entropy as the loss function to optimize our architecture with a softmax layer at the output which classify the given headline-body pair into agree, disagree, discuss, and unrelated. The hyper-parameter setting is shown in <ref type="table">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Baselines and compared methods</head><p>Organizers of FNC-1 have provided a baseline model that consists of a gradientboosting classifier over n-gram subsequences between the headline and the body along with several external features such as word overlap, occurrence of sentiment using a lexicon of highly-polarized words (like fraud and hoax ). With this simple yet elegant baseline it is possible to outperform some of the highly used deep learning architectures that we have used in our work. Following the work of <ref type="bibr" target="#b25">[26]</ref>, we also introduce three new baselines for the FNC-1 dataset: word2vec+external features baseline, skip-thought baseline, and TF-IDF baseline. All these baselines focuses on performance of neural, statistical, and external features, when used individually.</p><p>We compare our proposed approach with the submissions of top 4 teams at FNC-1 3 , which includes the work by <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b1">[2]</ref> and <ref type="bibr" target="#b27">[28]</ref>. Apart from the top submissions at FNC-1, we also compare the proposed architecture with four deep learning architectures introduced in Section 3, namely, CNN, biLSTM, BiLSTM+Attention and CNN+biLSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Evaluation metrics</head><p>From <ref type="table" target="#tab_0">Table 2</ref> it is evident that the FNC-1 dataset shows a heavy bias towards unrelated headline-body pairs. Recognizing this data bias and the simpler nature of the related/unrelated classification problems, the organizers of FNC-1 introduced the following weighted accuracy score as their final evaluation metric.</p><formula xml:id="formula_1">Score 1 = Accuracy Related,U nrelated<label>(3)</label></formula><p>Score 2 = Accuracy Agree,Disagree,Discuss</p><p>Score F N C = 0.25 * Score 1 + 0.75 * Score 2</p><p>We use the Score F N C as the main evaluation criteria while comparing the proposed model with other related techniques. We also use the class-wise accuracy for further evaluation of the performance of all the techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Results</head><p>The results on FNC-1 test dataset are shown in <ref type="table">Table 4</ref>. The first part of the table shows the performance of the baselines used in our work. The FNC-1 baseline achieves a score of 75.2 which is better than the performance of all deep architectures introduced in Section 3. The FNC-1 baseline is comprised of training gradient tree classifier on the hand crafted features (described in Section 4.3). Provided the simplicity of this baseline, it is indeed remarkable to achieve such a high score. The FNC-1 baselines achieves approx 7% higher class-wise accuracy on unrelated stance as compared to skip-thought baseline, whereas the latter receiving a higher Score F N C . Skip-thought baselines achieves a higher accuracy on agree and discuss than the unrelated stance. Since the interestingness of agree and discuss is higher than the unrealted stance, therefore, skip-thought achieves a higher Score F N C . This also explains the reason for the introduction of new scoring criterion by the FNC organizers (see <ref type="bibr">Section 5.4)</ref>. Finally, the Score F N C by skip-thought, external features, and TF-IDF baselines are higher than the FNC-1 baseline. Therefore, our speculation to combine these three baselines models, is guaranteed to achieve a higher score on Score F N C evaluation metric. Moreover, all the baselines achieves very low or zero score on the disagree stance. Therefore, apart from the Score F N C , the class-wise performance is worth considering as a performance criterion.</p><p>The performance of top-4 teams that participated in FNC-1 are shown in the middle part of <ref type="table">Table 4</ref>, with SOLAT in the SW EN <ref type="bibr" target="#b33">[34]</ref> winning the challenge achieving a score of 82.05. All the teams achieved higher score and class-wise accuracy on all stances except for the disagree stance. This should be a concern, since the importance of disagree is equivalent to the agree and discuss stance. We observed that the news pairs in the disagree category are not only very few, but also consists of divergent news articles. This is one of the reason for poor performance of most of the deep models, including the top teams, on identifying diagree stance.  <ref type="table">Table 4</ref> shows the performance of the proposed model along with other architectures used in our work. The proposed model achieves highest score and highest class-wise accuracy on discuss stance whereas achieving high accuracy on other stances that is comparable to top submissions at FNC-1. From <ref type="table">Table 5</ref>, it is evident that the overall accuracy achieved by the proposed model is slightly lower than <ref type="bibr" target="#b1">[2]</ref>, although the proposed model outperformed all the other techniques by a clear margin (in terms of Score F N C ). The possible reason for this deviation is that the <ref type="bibr" target="#b1">[2]</ref> gives more focus to the classification of unrelated stances rather than the rest, which is the reason for highest overall accuracy. Since unrelated stances are of least interest to us, this results in lower Score F N C . Finally, a confusion matrix is given in <ref type="table">Table 5</ref> that provides in-detail analysis of the performance of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The lowest section in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we explore the benefit of incorporating neural, statistical and external features to deep neural networks on the task of fake news stance de-</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Deep architectures used for stance detection on the FNC-1 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Combining the neural, statistical and external features using deep MLP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>FNC-1 dataset description.The final results are evaluated over a test dataset provided by fake news organization consisting of 25413 samples.</figDesc><table><row><cell cols="2">News articles unrelated discuss agree disagree</cell></row><row><cell>49972</cell><cell>73.13 % 17.83 % 7.36 % 1.68 %</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 4 .Table 5 .</head><label>45</label><figDesc>Performance of different models on FNC-1 Test Dataset. The first half of the table shows the baselines, followed by the top-4 submissions, and different architectures used in our work. Column 2-5 shows the class-wise accuracy in % while the last column shows the overall accuracy. Confusion matrix for the proposed model on FNC-1 testset.</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="6">ScoreF N C Agree Disagree Discuss Unrelated Overall</cell></row><row><cell>FNC-1 baseline</cell><cell></cell><cell>75.20</cell><cell>9.09</cell><cell>1.00</cell><cell>79.65</cell><cell>97.97</cell><cell>85.44</cell></row><row><cell cols="3">Word2vec + External Features 75.78</cell><cell cols="2">50.70 9.61</cell><cell>53.38</cell><cell>96.05</cell><cell>82.79</cell></row><row><cell>Skip-thought baseline</cell><cell></cell><cell>76.18</cell><cell>31.8</cell><cell>0.00</cell><cell>81.20</cell><cell>91.18</cell><cell>82.48</cell></row><row><cell>TF-IDF baseline</cell><cell></cell><cell>81.72</cell><cell cols="2">44.04 6.60</cell><cell>81.38</cell><cell>97.90</cell><cell>88.46</cell></row><row><cell cols="2">SOLAT in the SWEN [34]</cell><cell>82.05</cell><cell cols="2">58.50 1.86</cell><cell>76.18</cell><cell>98.70</cell><cell>89.08</cell></row><row><cell>Athene [2]</cell><cell></cell><cell>81.97</cell><cell cols="2">44.72 9.47</cell><cell>80.89</cell><cell>99.25</cell><cell>89.50</cell></row><row><cell cols="2">UCL Machine Reading [28]</cell><cell>81.72</cell><cell cols="2">44.04 6.60</cell><cell>81.38</cell><cell>97.90</cell><cell>88.46</cell></row><row><cell>Chips Ahoy! [29]</cell><cell></cell><cell>80.12</cell><cell cols="2">55.96 0.28</cell><cell>70.29</cell><cell>98.98</cell><cell>88.01</cell></row><row><cell>CNN</cell><cell></cell><cell>60.91</cell><cell cols="2">35.89 2.10</cell><cell>46.77</cell><cell>88.47</cell><cell>74.84</cell></row><row><cell>biLSTM</cell><cell></cell><cell>63.11</cell><cell cols="2">38.04 4.59</cell><cell>58.13</cell><cell>78.27</cell><cell>69.88</cell></row><row><cell>biLSTM + Attention</cell><cell></cell><cell>63.17</cell><cell cols="2">58.74 0.03</cell><cell>63.48</cell><cell>77.49</cell><cell>73.27</cell></row><row><cell>CNN + biLSTM</cell><cell></cell><cell cols="3">64.95 74.09 2.46</cell><cell>57.85</cell><cell>74.87</cell><cell>72.89</cell></row><row><cell>Proposed</cell><cell></cell><cell cols="3">83.08 43.82 6.31</cell><cell>85.68</cell><cell>98.04</cell><cell>89.29</cell></row><row><cell></cell><cell cols="5">Agree Disagree Discuss Unrelated Overall</cell><cell></cell></row><row><cell>Agree</cell><cell>834</cell><cell>15</cell><cell>945</cell><cell>109</cell><cell>43.82</cell><cell></cell></row><row><cell cols="2">Disagree 208</cell><cell>44</cell><cell>328</cell><cell>117</cell><cell>6.31</cell><cell></cell></row><row><cell cols="2">Discuss 401</cell><cell>23</cell><cell>3825</cell><cell>215</cell><cell>85.68</cell><cell></cell></row><row><cell cols="2">Unrelated 22</cell><cell>12</cell><cell>325</cell><cell>17990</cell><cell>98.04</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://www.fakenewschallenge.org/ https://competitions.codalab.org/competitions/16843#results</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>tection. We also presented in-depth analysis of several state-of-the-art recurrent and convolution architectures (shown in <ref type="figure">Figure 1</ref>). The presented idea leverages features extracted using skip-thought embeddings, n-gram TF-vectors and several introduced hand crafted features.</p><p>We found that the uneven distribution of FNC-1 dataset undermines the performance of most deep learning architectures. The fewer training samples adds further to this aggravation. Creating a dataset for a complex NLP problems such as fake news identification is indeed a cumbersome task, and we appreciate the work by the FNC organizers, yet, a more detailed and elaborate dataset should make this challenge more suitable to evaluate.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">sem 2013 shared task: Semantic textual similarity, including a pilot on typedsimilarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">* SEM 2013: The Second Joint Conference on Lexical and Computational Semantics. Association for Computational Linguistics. Citeseer</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Team athene on the fake news challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Hanselowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pvs</forename><surname>Avinesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caspelherr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Usfd at semeval-2016 task 6: Any-target stance detection on twitter with autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Augenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bontcheva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SemEval@ NAACL-HLT</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="389" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1409.0473" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Predicting information credibility in time-sensitive social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mendoza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poblete</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Internet Research</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="560" to="588" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Stance detection for the fake news challenge: Identifying textual relationships with deep neural nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Thun-Hohenstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Call attention to rumors: Deep attention based recurrent neural networks for early rumor detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05973</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Ikm at semeval-2017 task 8: Convolutional neural networks for stance detection and rumor verification. Proceedings of SemEval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Kao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation. CoRR, abs/1406.1078</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ç</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1406.1078" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Towards automatic identification of fake news: Headline-article stance detection with lstm attention models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Sholar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Fake news, real consequences: Recruiting neural networks for the fight against fake news</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Proctor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Fake news challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pomerleau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Applying deep learning to answer selection: A study and an open task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding (ASRU)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="813" to="820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Emergent: a novel data-set for stance classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vlachos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. ACL</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Framewise phoneme classification with bidirectional lstm and other neural network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="602" to="610" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-perspective sentence similarity modeling with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1576" to="1586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3294" to="3302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semanticz at semeval-2016 task 3: Ranking relevant answers in community question answering using semantic similarity based on fine-tuned word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SemEval@ NAACL-HLT</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="879" to="886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Fake news headline classification using neural networks with attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oswalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning text similarity with siamese recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Neculoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Versteegh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rotaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Amsterdam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">148</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">As fake news spreads lies, more readers shrug at the truth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nytimes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Stance detection for the fake news challenge with attention and conditional encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pfohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Triebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Legros</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Software Framework for Topic Modelling with Large Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Řehůřek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sojka</surname></persName>
		</author>
		<ptr target="http://is.muni.cz/publication/884893/en" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks</title>
		<meeting>the LREC 2010 Workshop on New Challenges for NLP Frameworks<address><addrLine>Valletta, Malta</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-05" />
			<biblScope unit="page" from="45" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A simple but tough-to-beat baseline for the fake news challenge stance detection task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Augenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">P</forename><surname>Spithourakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.03264</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Chips ahoy! at fake news challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Lstm-based deep learning models for non-factoid answer selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04108</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Beyond factoid qa: Effective methods for non-factoid answer sentence retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Spina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scholer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="115" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Wikiqa: A challenge dataset for opendomain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2013" to="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Deep learning for answer sentence selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1632</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Yuxi Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Sibley</surname></persName>
		</author>
		<ptr target="http://blog.talosintelligence.com/2017/06" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.06724</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
