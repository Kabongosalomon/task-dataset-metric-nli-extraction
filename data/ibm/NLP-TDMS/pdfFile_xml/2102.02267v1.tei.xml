<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DEFT: Detection Embeddings for Tracking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Chaabane</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>Aurora, Louisville</settlement>
									<region>CO</region>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Colorado State University</orgName>
								<address>
									<settlement>Fort Collins</settlement>
									<region>CO</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Uber ATG</orgName>
								<address>
									<settlement>Louisville</settlement>
									<region>CO</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ross Beveridge</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Colorado State University</orgName>
								<address>
									<settlement>Fort Collins</settlement>
									<region>CO</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>O&amp;apos;hara</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>Aurora, Louisville</settlement>
									<region>CO</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DEFT: Detection Embeddings for Tracking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most modern multiple object tracking (MOT) systems follow the tracking-by-detection paradigm, consisting of a detector followed by a method for associating detections into tracks. There is a long history in tracking of combining motion and appearance features to provide robustness to occlusions and other challenges, but typically this comes with the trade-off of a more complex and slower implementation. Recent successes on popular 2D tracking benchmarks indicate that top-scores can be achieved using a state-of-the-art detector and relatively simple associations relying on single-frame spatial offsets -notably outperforming contemporary methods that leverage learned appearance features to help re-identify lost tracks. In this paper, we propose an efficient joint detection and tracking model named DEFT, or "Detection Embeddings for Tracking." Our approach relies on an appearance-based object matching network jointly-learned with an underlying object detection network. An LSTM is also added to capture motion constraints. DEFT has comparable accuracy and speed to the top methods on 2D online tracking leaderboards while having significant advantages in robustness when applied to more challenging tracking data. DEFT raises the bar on the nuScenes monocular 3D tracking challenge, more than doubling the performance of the previous top method. Code is publicly available. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Visual Multi-Object Tracking (MOT) has made significant progress in recent years, motivated in part from highprofile mobile robotics and autonomous driving applica-1 https://github.com/MedChaabane/DEFT tions. Continued improvements in the accuracy and efficiency of Convolutional Neural Network (CNN) based object detectors has driven the dominance of the "tracking by detection" paradigm <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b39">38,</ref><ref type="bibr" target="#b45">43,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b1">2]</ref>. Recent work has shown that simple tracking mechanisms added to state of the art detectors <ref type="bibr" target="#b52">[50]</ref> can outperform more complex trackers reliant upon older detection architectures.</p><p>The tracking by detection approach is characterized by two main steps: 1) detection of objects in a single video frame, and 2) association to link objects detected in the current frame with those from previous frames. There are many ways to associate detections across frames, but those featuring learned associations are interesting because they have the promise of addressing edge-cases where modeling and heuristics-based approaches fail. Even with learned associations, the two-stage approach can lead to sub-optimal results in terms of accuracy and efficiency. A recent trend of jointly learning detection and tracking tasks in a single neural network has led to increases in performance on tracking benchmarks and related applications. However, existing end-to-end methods that combine appearance and motion cues can be complex and slow (see §2, Related Work).</p><p>We posit that a learned object matching module can be added to most contemporary CNN-based object detectors to yield a high performing multi-object tracker, and further, that by jointly training the detection and tracking (association) modules, both modules adapt to each other and together perform better. Using the same backbone for object detection and inter-frame association increases efficiency and accuracy when compared to methods that use detection as a black-box feeding input to the association logic.</p><p>In this paper we present an approach in which embeddings for each object are extracted from the multi-scale backbone of a detector network and used as appearance features in an object-to-track association sub-network. We name our approach "Detection Embeddings for Tracking" (DEFT). We show that DEFT can be applied effectively to several popular object detection backbones. Due to the gains of feature sharing in the network design, our approach using both appearance and motion cues for tracking has speed comparable to leading methods that use simpler association strategies. Because DEFT keeps a memory of appearance embeddings over time, it is more robust to occlusions and large inter-frame displacements than the top alternatives. This robustness allows DEFT to dramatically outperform competing methods on the challenging nuScenes 3D monocular vision tracking benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Tracking-by-detection. Most state-of-the-art trackers follow the approach of tracking-by-detection which heavily relies on the performance of the detector. Trackers in this approach often use detectors as black box modules and focus only on associating detections across frames. In the predeep learning era, trackers often used Kalman filters <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10]</ref>, Intersection-over-Union (IoU) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> or flow fields <ref type="bibr" target="#b35">[34,</ref><ref type="bibr" target="#b19">20]</ref> for association. These methods are simple and fast but they fail very easily in challenging scenarios. Recently, with the success of deep learning, many models have used appearance features to associate objects <ref type="bibr" target="#b12">[13]</ref>. DeepSORT <ref type="bibr" target="#b44">[42]</ref> for example, takes provided detections and associates them using an offline trained deep re-identification (ReID) model and Kalman filter motion model. SiameseCNN <ref type="bibr" target="#b25">[25]</ref> uses a Siamese network to directly learn the similarity between a pair of detections. The Deep Affinity Network of Sun et al. <ref type="bibr" target="#b38">[37]</ref> employs a Siamese network that takes 2 video frames as input, extracts multi-scale appearance embeddings and outputs similarity scores between all pairs of detections. Mahmoudi et al. <ref type="bibr" target="#b30">[29]</ref> use extracted visual features alongside dynamic and position features for object association. In <ref type="bibr" target="#b0">[1]</ref>, the authors combined the output of the CNN with shape and motion models. The main disadvantage of these models is that they use time-consuming feature extractors and they treat detection and association separately, leading to suboptimal accuracy and speed. In DEFT, association is jointly learned with detection in a unified network. Feature extraction for matching reuses the detection backbone, thus object associations are computed with only a small additional latency beyond detection.</p><p>Joint detection and tracking. The recent success of multi-task learning in deep neural networks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b40">39,</ref><ref type="bibr" target="#b51">49]</ref> has led to models that jointly learn detection and tracking tasks. Tracktor <ref type="bibr" target="#b1">[2]</ref> adapts the Faster RCNN detector <ref type="bibr" target="#b33">[32]</ref> to estimate the location of a bounding box in the new frame from the previous frame. Tracktor's drawback is that it works well only on high frame-rate videos where inter-frame motion is low. In <ref type="bibr" target="#b14">[15]</ref>, authors extend R-FCN <ref type="bibr" target="#b13">[14]</ref> detector to compute correlation maps between high level feature maps of consecutive frames to estimate inter-frame offsets between bounding boxes. Similarly, CenterTrack <ref type="bibr" target="#b52">[50]</ref> extends the CenterNet detector <ref type="bibr" target="#b53">[51]</ref> to estimate inter-frame offsets of bounding boxes. CenterTrack is a state-of-the-art method, but only associates objects in consecutive frames. Our method is more robust for longer occlusions and large inter-frame displacements, thus improving tracking under more challenging conditions, which we show in §4.4. Xu et al. <ref type="bibr" target="#b46">[44]</ref> presents an end-to-end MOT training framework, using a differentiable approximation of MOT metrics in the loss functions. They show improvements for existing deep MOT methods when extended with their training framework. Chaabane et al. <ref type="bibr" target="#b7">[8]</ref> propose an approach to jointly optimize detection and tracking, with a focus on static object tracking and geolocalization. Their model makes strong use of learned pose estimation features, and is less suitable as a general tracker for dynamic objects. JDE <ref type="bibr" target="#b41">[40]</ref> extends YOLOv3 <ref type="bibr" target="#b32">[31]</ref> with a reID branch to extract object embeddings for association. The feature extractor and detection branches share features and are jointly learned. Similarly, FairMOT <ref type="bibr" target="#b50">[48]</ref> improves on JDE and makes use of the Cen-terNet detector to improve tracking accuracy.</p><p>DEFT is similar in approach to JDE and FairMOT. These methods jointly learn detection and feature matching in a single network as the basis for visual multiple object tracking. DEFT provides additional evidence that jointly learning detection and matching features can provide a simple and effective (SOTA) tracking solution. DEFT overcomes some of the limitations of competing approaches such as CenterTrack when applied to challenging examples by providing a longer track memory over which similarity scores are aggregated and by applying a simple LSTM-based motion model to filter physically implausible matches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DEFT Network</head><p>Given the tracking-by-detection paradigm, we propose to exploit the representational power of the intermediate feature maps of the object detector ("detector backbone") to extract embeddings to be used in an object matching subnetwork that associates objects across frames. We jointly train the detector and the object matching network. During training, errors in object association propagate back through the detection backbone such that the appearance features are optimized for both detection and matching. DEFT also employs a low-dimensional LSTM module to provide geometric constraints to the object matching network. DEFT implemented with a CenterNet <ref type="bibr" target="#b53">[51]</ref> backbone achieves state-of-the-art performance on several tracking benchmarks, while being faster than most similarly-scoring alternatives. The speedup is in part due to the fact that in DEFT, object association is a small additional module within the detection network, thus adding only a few mil-Testing Training <ref type="figure" target="#fig_2">Figure 1</ref>: DEFT Training and Inference. DEFT adds an embedding extractor and a matching head to an object detection backbone to jointly train appearance features for both detection and association tasks. During inference, the matching head matches the current detections to the embeddings remembered for all active tracks and uses a motion forecasting module to eliminate implausible trajectories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>liseconds of latency beyond detection.</head><p>During inference (See <ref type="figure" target="#fig_2">Figure 1)</ref>, the embedding extractor head uses features maps and bounding boxes from the detector as input, and extracts appearance embeddings for each detected object. The matching head uses the embeddings to compute a similarity between the objects in the current frame and those remembered from previous frames (current tracks). A motion forecasting module (LSTM) prevents matches that lead to physically implausible trajectories. The Hungarian Algorithm is used to make the final online association of objects to tracks. Details for each module are provided below, followed by the training procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Object Embeddings</head><p>The object embeddings used in the matching network are extracted from the detection backbone, as described below. We label this the "embedding extractor" in <ref type="figure" target="#fig_2">Figure 1</ref> and subsequent text. The embedding extractor constructs representative embeddings from the intermediate feature maps of the detector backbone to help associate (or, "re-identify") objects during tracking. We use feature maps at different layers to extract appearance from multiple receptive fields (RFs), which provides additional robustness over single-RF embeddings. DEFT takes a video frame t as input and, via the detection head, outputs a set of bounding boxes</p><formula xml:id="formula_0">B t = {b t 1 , b t 2 , ..., b t Nt }.</formula><p>For convenience, we use N t = |B t | to represent the number of bounding boxes in frame t.</p><p>For each detected object, we extract feature embeddings from the estimated 2D object's center location. For 3D bounding boxes, we use projection of 3D center location into the image space as its estimated 2D center location. If the center of the i th object is at position (x, y) in the input frame of size W × H, then for a feature map of size</p><formula xml:id="formula_1">W m × H m × C m , we extract the C m -dimensional vector at position ( y H H m , x W W m )</formula><p>as the feature vector f m i for the i th object in feature map m. We concatenate features from M feature maps to construct the resulting e-dimensional feature embedding f i = f 1 i · f 2 i . . . f M i for the i th object. The dimension of the feature vector f m i affects its contribution to the resulting feature embedding. To change the contribution of some feature maps and to control the dimension of the embedding, we add a single convolution layer to some feature maps to increase/decrease the dimension from C m to C m before extracting the feature vectors. In practice, this serves to increase the dimension of features contributed by earlier maps while reducing those from later maps. More details on this can be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Matching Head</head><p>The matching head follows the Deep Affinity Network <ref type="bibr" target="#b38">[37]</ref>, using the object embeddings to estimate the similarity scores between all pairs of detections across the two frames. With N max maximum number of allowed objects in each frame, we construct the tensor E t,t−n ∈ R Nmax×Nmax×2e such that the feature embedding of each object in frame t is concatenated along the depth dimension with each feature embedding from objects in frame t − n and vice versa. To construct fixed size tensor E t,t−n we pad the rest of the tensor with zeros. E t,t−n is fed to the matching head which is composed of a few (4-6) layers of 1 × 1 convolutions. The output of the matching head is affinity matrix A t,t−n ∈ R Nmax×Nmax .</p><p>Since we learn similarity between embeddings, there is no guarantee that the resulting scores are symmetric when matching backwards or forwards across frames, i.e., when matching from (t → t − n) versus (t − n → t). As a result, we compute the affinities in both directions using a separate affinity matrix for each, denoted with superscripts "bwd" and "fwd" in the following.</p><p>To allow for objects that should not be associated between the frames (objects new to the scene, or departed), we add a column to A t,t−n filled with constant value c. We apply softmax to each row to obtain matrixÂ bwd , representing the final affinity including non-matched scores. The choice of c is not overly sensitive -the network will learn to assign affinities greater than c for true matches.</p><p>EachÂ bwd [i, j], represents the estimated probability of</p><formula xml:id="formula_2">associating b t i to b t−n j .Â bwd [i, N max + 1]</formula><p>represents the estimated probability that b t i is an object not present in frame t − n. Similarly, we construct the forward affinity matrix, using the transpose matrix A T t,t−n to which we add a column filled with constant value c and then we apply softmax to each row to obtain matrixÂ f wd . During the inference, the similarity score between b t i and b t−n j is given as the av-</p><formula xml:id="formula_3">erage ofÂ bwd [i, j] andÂ f wd [j, i].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Online Data Association</head><p>In DEFT, tracks remember the object embeddings from each observation comprising the track in the last δ frames. The association between a new detection and the existing tracks requires computing the similarity of the new object to the set of observations from each track in memory. To allow for occlusions and missed detections, track memory is maintained for a few seconds so that inactive tracks can be revived if a new observation is strongly associated to the previous observations. Tracks with no observations after N age frames are discarded from memory.</p><p>We define a track T as the set of associated detections from frame t − δ to t − 1, noting that tracks may not have a detection for each frame. The size of the track is given as |T|, representing the number of bounding boxes and associated embeddings it comprises. We define the distance between the i th new detection b t i and T j as:</p><formula xml:id="formula_4">d(b t i , T j ) = 1 |T j | b t−n k ∈TjÂ f wd t,t−n [k, i] +Â bwd t,t−n [i, k] 2<label>(1)</label></formula><p>The detections-to-tracks association problem is formulated as a bipartite matching problem so that exclusive correspondence is guaranteed. Let K = {T j } be the set of current tracks. We construct the detections-to-tracks similarity matrix D ∈ R |K|×(Nt+|K|) by appending the all-pairs detections-to-tracks distance (Eq. (1)) of size |K| × N t with a matrix X of size |K| × |K| used to represent when a track is associated with no detections in the current frame. The entries along the diagonal of X are computed as the average non-match score of the detections in the track, the off-diagonal entries are set to −∞. Specifically, D is constructed as follows:</p><formula xml:id="formula_5">D = [S|X]</formula><p>(2)</p><formula xml:id="formula_6">S[j, i] = d(b t i , Tj ) (3) X[j, k] =    1 |T j | b t−n k ∈T jÂ f wd t,t−n [k, Nmax + 1] j = k −∞, j = k<label>(4)</label></formula><p>Finally, we solve the bipartite matching problem defined by D with the Hungarian algorithm <ref type="bibr" target="#b24">[24]</ref>. We include only likely associations if the affinity is larger than a specified threshold γ 1 . Unmatched detections will start newborn tracks. Tracks that have not been associated for more than a predefined maximum age N age are considered to have left the scene and are deleted from the track set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Motion Forecasting</head><p>When learning to associate detections across frames using the appearance features from a detection backbone, there is some chance that two objects look similar enough in the embedding space to cause confusion. It is common practice to add additional geometric or temporal constraints to help resolve such ambiguities. This often takes the form of a Kalman Filter (e.g. <ref type="bibr" target="#b44">[42,</ref><ref type="bibr" target="#b10">11]</ref>) or an LSTM module (e.g. <ref type="bibr" target="#b36">[35]</ref>). DEFT uses an LSTM as our motion forecasting module. This module predicts future locations of each track in the next ∆T pred frames given its information in ∆T past past frames. Motion forecasting is used to constrain the associations between frames to those that are physically plausible. The motion forecasting module sets affinity scores in Eq. (1) to −∞ for detections that are too distant from the track's predicted location. Further details on the LSTM implementation are in the supplementary material. In §4.5, we provide an ablation study to show the impact of our LSTM motion forecasting module, and we also show that it modestly outperforms a Kalman Filter in our application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Training</head><p>During training, a pair of frames, n frames apart, is input to DEFT as shown in <ref type="figure" target="#fig_2">Figure 1</ref>. Following <ref type="bibr" target="#b6">[7]</ref>, the image pairs are separated by a random number of frames 1 ≤ n ≤ n gap to encourage the network to learn to be robust to temporary occlusions or missed detections.</p><p>For each training pair, we create two ground truth matching matrices M f wd and M bwd , representing the forward and backward associations, respectively. The ground truth matching matrices consist of entries [i, j] ∈ {0, 1}, and have dimensions N max × (N max + 1) to allow for unassociated objects. A value of 1 in the matrix encodes an association, and is also used for unassociated objects. Everywhere else, the value is 0.</p><p>To train DEFT for matching estimation, we use the loss function L match defined as the average of the two losses L f wd match and L bwd match , where L bwd match is the error of matching bounding boxes in frame t to those in frame t−n and L f wd match is the error of matching bounding boxes in frame t − n to those in frame t. The expression of the matching loss is given as follows, where "*" represents "fwd" or "bwd" as appropriate:</p><formula xml:id="formula_7">L * match = Nmax i=1 Nmax+1 j=1 M * [i, j]log(Â * [i, j]),<label>(5)</label></formula><formula xml:id="formula_8">L match = L f wd match + L bwd match 2(N t + N t−n ) ,<label>(6)</label></formula><p>Training optimizes the joint affinity and detection losses as defined in Eq. <ref type="bibr" target="#b6">(7)</ref>. For a better optimization of our proposed dual-task network, we use the strategy proposed in <ref type="bibr" target="#b22">[22]</ref> for automatic loss balancing the two tasks.</p><formula xml:id="formula_9">L joint = 1 e λ1 ( L t detect + L t−n detect 2 ) + 1 e λ2 L match + λ 1 + λ 2 (7)</formula><p>where L t detect is the detection loss for frame t and λ 1 and λ 2 are the balancing weights to the two tasks. Note that the balancing weights are modeled as learnable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Metrics</head><p>We evaluated the tracking performance of DEFT on a set of popular benchmarks: MOT Challenge (MOT16/MOT17) <ref type="bibr" target="#b31">[30]</ref>, KITTI tracking <ref type="bibr" target="#b15">[16]</ref>, and the nuScenes Vision Tracking benchmark <ref type="bibr" target="#b5">[6]</ref>. The MOT Challenge and KITTI benchmarks are used to assess 2D visual tracking, while nuScenes is used for monocular 3D visual tracking.</p><p>MOT16/MOT17. The MOT16 and MOT17 tracking challenges are part of the multi-object tracking benchmark MOT Challenge <ref type="bibr" target="#b31">[30]</ref>. They are composed of indoor and outdoor pedestrian tracking sequences. The videos have frame-rates between 14 and 30 FPS. Both challenges contain the same seven training sequences and seven test sequences. We test our tracker using public detections provided by the benchmark protocol, following <ref type="bibr" target="#b1">[2]</ref>, as well as with private detections output by DEFT. With public detections, we use the jointly-trained DEFT model, but employ the provided bounding boxes for embedding extraction -all else is the same. With private detections, we use the bounding boxes output from our model.</p><p>MOT Challenge benchmarks employ the following metrics: MOTA -multi-object tracking accuracy, MOTPmulti-object tracking precision, IDF1 -identity F1 Score, MT -mostly tracked, ML -mostly lost, FP -false positives, FN -false negatives, IDS -identity switches. We refer the reader to <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b34">33]</ref> for detailed definitions.</p><p>KITTI. The KITTI tracking benchmark is composed of 21 training sequences and 29 test sequences that were collected using cameras mounted on top of a moving vehicle. The videos are recorded at 10 FPS. We evaluate performance using the "Car" class because it is the only class with enough examples to allow effective training without external data sources. <ref type="bibr" target="#b1">2</ref> Public detections are not provided with KITTI. KITTI uses the same tracking metrics as the MOT Challenge benchmarks.</p><p>nuScenes. nuScenes is a large-scale data set for autonomous driving. nuScenes is composed of 1000 sequences, with 700, 150, 150 sequences for train, validation, and testing, respectively. Sequences were collected in Boston and Singapore, in both day and night, and with different weather conditions. Each sequence length is roughly 20 seconds with camera frequency of 12 FPS. Each sequence contains data from six cameras forming full 360 • field of view but box annotations are provided only for key frames (2 FPS). Given the ground truth format, only key frames are used for training and evaluation. The effectively low frame-rate of 2FPS makes this data set challenging for visual tracking as the inter-frame motion of objects can be large. We evaluate DEFT on the 7 annotated classes: Car, Truck, Trailer, Pedestrian, Bicycle, Motorcycle, Bus. nuScenes uses tracking metrics aggregated over the curve of operating thresholds <ref type="bibr" target="#b43">[41]</ref>. They are as follows: AMOTA -average MOTA, AMOTP -average MOTP, MO-TAR -recall-normalized MOTA score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>DEFT is implemented using PyTorch with an open source license. In all experiments, we used an Ubuntu server with a TITAN X GPU with 12 GB of memory. All hyper-parameters were chosen based on the best MOTA score for 3-fold cross validation for 2D tracking and best AMOTA score on the validation set for 3D tracking. Our implementation runs at approximately 12.5Hz on all data sets.</p><p>2D Tracking. DEFT was trained and evaluated with four object detectors including CenterNet <ref type="bibr" target="#b53">[51]</ref>, YOLO v3 <ref type="bibr" target="#b32">[31]</ref>, FPN <ref type="bibr" target="#b26">[26]</ref> and Faster R-CNN <ref type="bibr" target="#b33">[32]</ref>. All object detectors <ref type="bibr" target="#b32">[31]</ref> 85.6 6.5% 6.8% 1.1% DEFT + FPN <ref type="bibr" target="#b26">[26]</ref> 87.0 6.0% 6.3% 0.7% DEFT + Faster R-CNN <ref type="bibr" target="#b33">[32]</ref> 87.7 5.8% 5.8% 0.7% DEFT + CenterNet <ref type="bibr" target="#b53">[51]</ref> 88.1 5.7% 5.9% 0.3%  were pre-trained on the COCO dataset <ref type="bibr" target="#b27">[27]</ref>. We augmented the data with random cropping, flipping, scaling and photometric distortions. <ref type="table" target="#tab_0">Table 1</ref> shows the relative performance from 3-fold cross validation on KITTI. We also performed this evaluation using MOT17, which yielded the same ranking, not shown here for brevity. Since the DEFT+CenterNet variant was the strongest performing, we use it as the detection backbone for the remaining results in this paper. While DEFT achieves best performance with Center-Net, these results demonstrate that DEFT can achieve good tracking performance using different detector backbones. Interestingly, Faster R-CNN and CenterNet have similar detection performance on KITTI, however the association performance is better with CenterNet (fewer ID switches). This might be due the nature of CenterNet being an anchor-free detector and matches well to DEFT's design of using feature embeddings from the object center locations -allowing the features at center locations to benefit from the supervised signals of both tasks.</p><formula xml:id="formula_10">DEFT Variant MOTA↑ FP ↓ FN ↓ IDS ↓ DEFT + YOLO v3</formula><p>When training DEFT with CenterNet for the various experiments in this paper, we used the following procedure. We used CenterNet with the modified DLA-34 <ref type="bibr" target="#b53">[51]</ref> backbone. We train DEFT for 80 epochs with a starting learning rate of e −4 using the Adam <ref type="bibr" target="#b23">[23]</ref> optimizer, and batch size 8. The learning rate is reduced by 5 at epochs 30, 60, and 70. We extract feature embeddings from the 13 feature maps of the modified DLA-34 backbone.  <ref type="table">Table 3</ref>: KITTI car tracking. We compare to published online entries on the leaderboard.</p><p>3D Tracking. The nuScenes evaluation was performed on the full 360 • panorama and not with each camera separately. Following <ref type="bibr" target="#b52">[50]</ref>, we fused outputs from all cameras naively without any additional post-processing for handling duplicate detections between cameras or for associating objects across cameras. This ensures a fair comparison between all monocular 3D trackers. We performed additional analysis on the validation set allowing us to observe how performance varies when controlling for certain variables, including the amount of occlusion in the track and a measure of inter-frame displacement.</p><p>Parameter Settings See the supplemental for the parameter settings we used to support each benchmark.  We compare against other online tracking methods using the protocol appropriate for each benchmark. We follow the common practice of comparing against published/peerreviewed methods listed on the leaderboard. Tracking results for MOT, KITTI, and nuScenes benchmarks are computed by host test servers with hidden labels on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparative Evaluation</head><p>Both CenterTrack and DEFT use a CenterNet detection backbone. We see in <ref type="table" target="#tab_1">Table 2</ref> and <ref type="table">Table 3</ref> that both outperform the other published online trackers on the leaderboards, with a slight edge in performance to CenterTrack. This shows the power of the CenterNet detection backbone and provides support that jointly optimized detection and tracking methods can outperform those that have detection as a distinct step. DEFT achieves the best trajectory coverage on KITTI, providing evidence that DEFT maintains longer tracks better, possibly due to remembering embeddings for several observations in a track.</p><p>We observe a big performance advantage with DEFT on nuScenes <ref type="table" target="#tab_4">(Table 4)</ref>, which we attribute in large part due to differences in how well DEFT tolerates long occlusions and large inter-frame displacements of tracked objects. We explore this hypothesis in §4.4. Compared with Center-Track, DEFT achieves gain of 13.1 percentage points in AMOTA, 25.3 in MOTAR, and 11.3 in MOTA. As others have noted <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b47">45,</ref><ref type="bibr" target="#b54">52]</ref>, nuScenes is significantly more difficult and closer to real-world scenarios than KITTI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Performance Analysis</head><p>As previously stated, DEFT and CenterTrack perform similarly on 2D tracking benchmarks of MOT17 and KITTI, but DEFT outscores CenterTrack and all other methods on the nuScenes vision tracking leaderboard by a sizable margin. In this section we investigate what factors may explain the performance difference.</p><p>Our intuition is that the major performance gains on KITTI and MOT benchmarks are driven by improved detectors. For MOT and KITTI, the tracking/association logic can be weak and still produce top numbers. Others in the community seem to agree. The authors of Tracktor promote "tracking without bells and whistles" <ref type="bibr" target="#b1">[2]</ref>, while the creators of CenterTrack state that it "...trades the ability to reconnect long-range tracks for simplicity, speed, and high accuracy in the local regime...this trade-off is well worth it." <ref type="bibr" target="#b52">[50]</ref> The first row in <ref type="table" target="#tab_7">Table 5</ref> of our ablation study (see §4.5) provides additional support for this point of view. We observe that a simple baseline, using nothing other than a motion model and IOU associations, when applied to Cen-terNet detections, yields a MOTA score of 86.7 on KITTI (validation) and 63.5 on MOT17 (validation). While one cannot compare validation scores directly to test results, this is suggestive that many of the top leaderboard methods are only marginally better than a naive baseline coupled to a SOTA detector.  However, is tracking without bells and whistles sufficient for harder tasks? We divided the nuScenes validation data into partitions based on two factors, an occlusion score and a displacement score. The occlusion score for a track is the number of frames for which the object was temporarily occluded. We sum this over all tracks to score a video. The displacement score for a track is the mean of the 2D center displacements in consecutive frames. We use the mean of the top ten tracks as the video's score. Scores are linearly rescaled to the range [0, 1]. The distribution of scores led us to divide the occlusion factor into easy and hard categories (below/above the median respectively), and the displacement factor into approximately equal-sized easy/moderate/hard partitions. <ref type="bibr" target="#b2">3</ref> We also look at a combined difficulty factor, which is simply the maximum of the two normalized difficulty scores. <ref type="figure" target="#fig_0">Figure 2</ref> shows that the difference in performance between DEFT and CenterTrack is marginal on easier videos with respect to the displacement factor or the overall difficulty factor. CenterTrack outperforms DEFT on the easiest videos by less than one percentage point, which is consistent with the difference in performance we observe on KITTI and MOT benchmarks. However, when looking at moderate and hard videos, we observe that DEFT provides superior performance. For occlusions, CenterTrack drops 7.6 percentage points from easy to hard, whereas DEFT drops only 2.8, indicating that CenterTrack is more sensitive to occlusions than DEFT. <ref type="figure">Figure 3</ref> provides example frames from videos that feature occlusions and large displacements, where DEFT is more robust.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Studies</head><p>In <ref type="table" target="#tab_7">Table 5</ref>, we present the results of an ablation study on MOT17, KITTI and nuScenes benchmarks investigating the importance of various aspects of DEFT. The first row of the table shows baseline performance for tracking when using CenterNet detections followed by a simple non-learned motion+IOU association step. We observe that the baseline tracker performs reasonably well in MOT17 and KITTI validation data, but fails to perform well on the more challenging nuScenes test.</p><p>The next two rows compare the benefit of using feature embeddings extracted from different resolutions in the detector backbone versus using only the features from the final feature map. We controlled for dimensionality, so that the embeddings are the same size for both the single-scale and multi-scale variants. Using multi-scale embeddings leads to modest improvements across the board.</p><p>The  and KITTI, with a stronger effect on nuScenes. An alternative to the learned LSTM motion model would be to use a standard Kalman filter. We found the performance of the LSTM over the Kalman filter to be more pronounced in nuScenes. Finally, there are a few situations in which having an IOU-based association layer as a second-stage to ob-ject matching can provide a small performance gain. This is shown in the final row of the table.</p><p>On MOT and KITTI, the naive baseline performs wellenough that additional gains from DEFT result in a cumulative benefit of only a couple percentage points across most metrics, with the exception of MT and ML (mostly tracked, mostly lost) scores. There we see a big jump in performance from the baseline to multi-scale DEFT. This suggests that learned appearance-based detection-to-track associations help maintain longer tracks. On nuScenes, the value of DEFT is obvious. The gains in AMOTA from baseline to multi-scale DEFT is over 14 percentage points, with an additional 1.5 points gained by adding the motion forecasting LSTM module. This confirms that the learned matching-based association step is critical to overall performance, and that the motion model is a helpful addition, but relatively minor in terms of the overall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Most state of the art trackers on popular public benchmarks follow the tracking-by-detection paradigm, with substantial boosts in performance attributable in large part to improved object detectors. This has allowed top-scoring algorithms to use limited matching strategies while achieving high tracking performance and efficiency. The concept of tracking in the "local regime," that is, constraining the association logic to relatively short temporal and spatial extents, has been shown to be effective on at least two popular 2D tracking benchmarks (MOT, KITTI). However, not all tracking challenges are ones where the assumption holds true that the local regime dominates performance. In selfdriving car applications, objects tracked in side-mounted cameras experience large inter-frame displacements, and occlusions lasting a few seconds are not uncommon. Additionally, there are use-cases for tracking with lower framerate videos in bandwidth constrained domains.</p><p>We have shown that detection embeddings used with learned similarity scores provide an effective signal for tracking objects, and are more robust to occlusions and high inter-frame displacements. On KITTI and MOT tracking benchmarks, DEFT is comparable in both accuracy and speed to leading methods. On the more challenging nuScenes visual tracking benchmark, tracking performance more than doubles compared to the previous state of the art, CenterTrack (3.8x on AMOTA, 2.1x on MOTAR). Further, DEFT and CenterTrack perform near parity when occlusions and inter-frame displacements are low. However, when either factor becomes more challenging, DEFT performs better. Importantly, these are not corner cases -the moderate and hard difficulty samples represent the majority in nuScenes, not the minority. DEFT's significant improvement in these cases is of considerable practical significance.</p><p>Ongoing work includes applying DEFT to LiDAR based detectors and fused LiDAR+Vision detectors, and measuring performance on additional data sets and applications. The motion forecasting module predicts future locations of each track in the next ∆T pred frames given its information in ∆T past past frames. It uses features from past bounding boxes coordinates for each track. These features are presented differently for 2D and 3D tracking.</p><p>2D Tracking. For 2D tracking, features for each detection at time t are represented by 8-dimensional vector 3D Tracking. For 3D tracking, features for each detection at time t are represented by 11-dimensional vector (x t , y t , z t , w t , h t , l t , r t , v x t , v y t , v z t , v r t ) containing 3D the center location, height,width,length,orientation about the zaxis, velocity in the x, y and z directions and the rotational velocity v r t .</p><formula xml:id="formula_11">(x t , y t , w t , h t , v x t , v y t , ∆ wt ∆ t , ∆ ht ∆ t )</formula><p>2 Parameter Settings MOT16/MOT17. For MOT16 and MOT17, frames are resized to 960 × 544. The embedding extractor head outputs a feature embedding of e = 416 features. The hyper-parameters N max , n gap , δ, ∆T IoU , γ 1 , γ 2 , N age , c were set to 100, 60, 50, 5, 0.1, 0.4, 50, 10 respectively. For the motion forecasting module, ∆T past , ∆T pred were set to 15 and 10.</p><p>KITTI. For KITTI, the embedding extractor head outputs a feature embedding of e = 672 features. The hyper-parameters N max , n gap , δ, ∆T IoU , γ 1 , γ 2 , N age , c were set to 100, 30, 25, 3, 0.1, 0.6, 30, 10 respectively. For the motion forecasting module, ∆T past , ∆T pred were set to 10 and 5.</p><p>nuScenes For 3D monocular tracking in nuScenes, DEFT was trained and evaluated with CenterNet as 3D ob-ject detector backbone. The embedding extractor head outputs a feature embedding of e = 704 features. The frames are resized to 800×448. The hyper-parameters N max , n gap , δ, ∆T IoU , γ 1 , γ 2 , N age , c were set to 100, 6, 5, 1, 0.1, 0.2, 6, 10 respectively. For motion forecasting module, ∆T past , ∆T pred were set to 10 and 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Detectors Implementation Details</head><p>DEFT was trained and evaluated with four object detectors including CenterNet <ref type="bibr" target="#b53">[51]</ref>, YOLO v3 <ref type="bibr" target="#b32">[31]</ref>, FPN <ref type="bibr" target="#b26">[26]</ref> and Faster R-CNN <ref type="bibr" target="#b33">[32]</ref>. All object detectors were pre-trained on the COCO dataset <ref type="bibr" target="#b27">[27]</ref>. We followed same implementation details and hyper-parameters settings from their official public codes. With all detectors, the embedding extractor head outputs a feature embedding of e = 416 and e = 672 for MOT and KITTI respectively. For nuScenes, We only train and test with CenterNet and e is set to 704.</p><p>CenterNet. We used CenterNet with the modified DLA-34 <ref type="bibr" target="#b48">[46]</ref> backbone. We extract feature embeddings from all 13 feature maps of the modified DLA-34 backbone. More details of the backbone can be found in <ref type="bibr" target="#b53">[51]</ref>.  <ref type="table" target="#tab_1">Table 2</ref>: MOT16. We present the results of our approach with using public (provided) and private detections. JDE is not present on the public leaderboard, results are from their paper. <ref type="table" target="#tab_0">Table 1</ref> that when jointly training detection and tracking tasks, tracking performance is improved on both KITTI and nuScenes datasets without hurting the detection performance.</p><formula xml:id="formula_12">YOLO</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">MOT16 Results</head><p>The MOT16 tracking challenge is part of the multiobject tracking benchmark MOT Challenge <ref type="bibr" target="#b31">[30]</ref>. MOT16 contains the same seven training sequences and seven test sequences as MOT17, as described in the main paper. MOT16 results are generally similar to MOT17. Including them here allows a comparison to additional methods. <ref type="table" target="#tab_1">Table 2</ref> shows the accumulated results over all sequences of MOT16. One contemporary method that shares some conceptual similarity to DEFT is JDE, which has peerreviewed results on MOT16, but is not listed on the public leaderboard. We include the results from their paper for comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Difficulty Splits on nuScenes</head><p>We divided the nuScenes validation data into partitions based on three factors: an occlusion score, a displacement score and a combined score. <ref type="figure" target="#fig_2">Figure 1</ref> shows the scores distribution. We divided the occlusion factor into 76 easy and 74 hard videos (below/above the median respectively). We divided the displacement factor based on two thresholds of half standard deviation from the median score to obtain 44 easy, 55 moderate and 51 hard videos. Similarly, we divide the combined score into 43 easy,60 moderate and 47 hard videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Additional 3D Monocular Tracking results</head><p>Here we provide a per-class breakdown of performance when evaluating tracking performance on the nuScenes validation data, front-camera only imagery. The Trailer class is particularly difficult when using private (vision-based) detections. When using the provided lidar-based detections, tracking performance becomes more consistent with the other classes. This points out that some classes may not have enough training samples to train a robust visual detec-tor, lacking the lidar signal.</p><p>CenterTrack <ref type="bibr" target="#b52">[50]</ref>   <ref type="table">Table 3</ref>: nuScenes 3D monocular Tracking results on the validation set. We present the results of our approach with private detections (those from our network) and public detections (from the lidar-based MEGVII <ref type="bibr" target="#b54">[52]</ref> )</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>DEFT compared with CenterTrack on nuScenes validation front camera videos, according to difficulty factors of occlusion and inter-frame displacement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>containing the 2D center location, height,width, velocity in the x and y directions and change in width and height divided by time difference between consecutive detections .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Occlusion, Displacement, and Combined scores distribution. Red lines represent thresholds used for the split.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell cols="5">: 3-fold cross-validation results of implement-</cell></row><row><cell cols="5">ing DEFT with different object detector networks on</cell></row><row><cell>KITTI.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="3">MOTA↑ MOTP ↑ IDF1 ↑</cell><cell>IDS ↓</cell></row><row><cell>Tracktor17 [2]</cell><cell>53.5</cell><cell>78.0</cell><cell>52.3</cell><cell>2072</cell></row><row><cell>DeepMOT-Tracktor [44]</cell><cell>53.7</cell><cell>77.2</cell><cell>53.8</cell><cell>1947</cell></row><row><cell>Tracktor v2 [2]</cell><cell>56.3</cell><cell>78.8</cell><cell>55.1</cell><cell>1987</cell></row><row><cell>GSM Tracktor [28]</cell><cell>56.4</cell><cell>77.9</cell><cell>57.8</cell><cell>1485</cell></row><row><cell>CenterTrack [50]</cell><cell>61.5</cell><cell>-</cell><cell>59.6</cell><cell>2583</cell></row><row><cell>Ours (Public)</cell><cell>60.4</cell><cell>78.1</cell><cell>59.7</cell><cell>2581</cell></row><row><cell>CenterTrack (Private)</cell><cell>67.8</cell><cell>-</cell><cell>64.7</cell><cell>3039</cell></row><row><cell>Ours (Private)</cell><cell>66.6</cell><cell>78.83</cell><cell>65.42</cell><cell>2823</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>MOT17. Comparison uses public detec- tions (provided bounding boxes) and private detec- tions. Compared methods are from the MOT leader- board, except CenterTrack where the results are from their paper.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Method MOTA↑ MOTP ↑ MT ↑ ML ↓ IDS ↓</figDesc><table><row><cell>SMAT [17]</cell><cell>84.27</cell><cell>86.09</cell><cell>63.08</cell><cell>5.38</cell><cell>341</cell></row><row><cell>mono3DT [19]</cell><cell>84.52</cell><cell>85.64</cell><cell>73.38</cell><cell>2.77</cell><cell>377</cell></row><row><cell>mmMOT [47]</cell><cell>84.77</cell><cell>85.21</cell><cell>73.23</cell><cell>2.77</cell><cell>284</cell></row><row><cell>MASS [21]</cell><cell>85.04</cell><cell>85.53</cell><cell>74.31</cell><cell>2.77</cell><cell>301</cell></row><row><cell>TuSimple [12]</cell><cell>86.62</cell><cell>83.97</cell><cell>72.46</cell><cell>6.77</cell><cell>293</cell></row><row><cell>CenterTrack [50]</cell><cell>89.44</cell><cell>85.84</cell><cell>82.31</cell><cell>2.31</cell><cell>116</cell></row><row><cell>Ours</cell><cell>88.95</cell><cell>84.55</cell><cell>84.77</cell><cell>1.85</cell><cell>343</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>nuScenes Vision Tracking. We compare to published monocular 3D tracking entries on the leader- board.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>table also shows the effect of the motion forecasting module -having one improves results modestly on MOT17Figure 3: Qualitative results comparison between DEFT and CenterTrack [50] on nuScenes. Each pair of rows shows the results comparison for one sequence. The color of the boxes represents the identity of the tracks. Red arrows point at tracking errors (identity switches). Notice that DEFT is more robust to occlusions and large inter-frame displacements.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Sequence: 1</cell><cell cols="2">Difficulty score : 0.48</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>DEFT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Frame 19</cell><cell></cell><cell cols="2">Frame 20</cell><cell></cell><cell></cell><cell>Frame 22</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>CenterTrack</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Frame 19</cell><cell></cell><cell cols="2">Frame 20</cell><cell></cell><cell></cell><cell>Frame 22</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Sequence: 2</cell><cell cols="2">Difficulty score : 0.47</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>DEFT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Frame 12</cell><cell></cell><cell cols="2">Frame 14</cell><cell></cell><cell></cell><cell>Frame 16</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>CenterTrack</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Frame 12</cell><cell></cell><cell cols="2">Frame 14</cell><cell></cell><cell></cell><cell>Frame 16</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Sequence: 3</cell><cell cols="2">Difficulty score : 0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>DEFT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Frame 2</cell><cell></cell><cell></cell><cell cols="2">Frame 4</cell><cell></cell><cell></cell><cell>Frame 5</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>CenterTrack</cell><cell>Frame 2</cell><cell></cell><cell></cell><cell cols="2">Frame 4</cell><cell></cell><cell></cell><cell>Frame 5</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">MOT17</cell><cell></cell><cell></cell><cell>KITTI</cell><cell></cell><cell></cell><cell cols="2">NuScenes</cell></row><row><cell>Feature Embeddings</cell><cell cols="2">Motion Model</cell><cell>2D/3D IOU</cell><cell>MOTA↑</cell><cell>MT ↑</cell><cell>ML ↓</cell><cell>IDS ↓</cell><cell>MOTA↑</cell><cell>MT ↑</cell><cell>ML ↓</cell><cell>IDS ↓</cell><cell>AMOTA↑</cell><cell>MOTA↑</cell></row><row><cell>None</cell><cell cols="2">LSTM</cell><cell></cell><cell>63.5</cell><cell>19.6%</cell><cell>38.1%</cell><cell>2.8%</cell><cell>86.7</cell><cell>51.3%</cell><cell>27.9%</cell><cell>1.7%</cell><cell>4.2</cell><cell>5.0</cell></row><row><cell>Single-Scale</cell><cell></cell><cell></cell><cell></cell><cell>64.0</cell><cell>28.8%</cell><cell>28.5%</cell><cell>2.3%</cell><cell>87.4</cell><cell>81.46%</cell><cell>3.15%</cell><cell>1.0%</cell><cell>17.1</cell><cell>15.0</cell></row><row><cell>Multi-Scale</cell><cell></cell><cell></cell><cell></cell><cell>64.4</cell><cell>29.8%</cell><cell>27.5%</cell><cell>1.9%</cell><cell>87.7</cell><cell>82.55%</cell><cell>2.71%</cell><cell>0.7%</cell><cell>18.4</cell><cell>16.2</cell></row><row><cell>Multi-Scale</cell><cell cols="2">Kalman</cell><cell></cell><cell>65.2</cell><cell>30.0%</cell><cell>27.3%</cell><cell>1.1%</cell><cell>87.8</cell><cell>82.60%</cell><cell>2.66%</cell><cell>0.6%</cell><cell>18.9</cell><cell>16.4</cell></row><row><cell>Multi-Scale</cell><cell cols="2">LSTM</cell><cell></cell><cell>65.2</cell><cell>30.0%</cell><cell>27.3%</cell><cell>1.1%</cell><cell>88.0</cell><cell>82.81%</cell><cell>2.57%</cell><cell>0.4%</cell><cell>20.0</cell><cell>17.2</cell></row><row><cell>Multi-Scale</cell><cell cols="2">LSTM</cell><cell></cell><cell>65.4</cell><cell>30.3%</cell><cell>27.0%</cell><cell>0.9%</cell><cell>88.1</cell><cell>83.05%</cell><cell>2.36%</cell><cell>0.3%</cell><cell>20.9</cell><cell>17.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Ablation study of DEFT on MOT17, KITTI and nuScenes datasets. Results are obtained with 3-fold crossvalidation on the training sets for MOT17 and KITTI, for nuScenes the results are on the validation set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 1 :</head><label>1</label><figDesc>v3. We used YOLO v3 with Darknet-53 backbone. Darrknet-53 is composed of 53 convolutional layers. We extract feature embeddings from 12 features maps which are the output of layers 4, 10, 14, 19, 23, 27, 31, 35, 39, 44, 48 and 52 from Darknet-53. FPN and Faster R-CNN. For FPN and Faster R-CNN, we used ResNet101 [18] backbone. We extract feature embeddings from 11 features maps which are the output of layers 7, 10, 16, 22, 34, 46, 58, 70, 82, 91 and 100 from ResNet101.4 Joint vs Separate TrainingTo show the benefit of joint training, we compare DEFT with joint and separate training strategies. We can see from Tracking and detection results of implementing DEFT with two training strategies (jointly vs separately optimized) on KITTI and nuScenes. Results are obtained with 3-fold cross-validation for KITTI where detection is evaluated with 2D bounding box AP for three different difficulty levels: easy (AP E ), moderate (AP M ) and hard (AP H ). Results are obtained on the validation set for nuScenes where detection is evaluated with mean Average Precision (mAP) over all 7 classes.</figDesc><table><row><cell>KITTI</cell><cell>nuScenes</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">This shouldn't be interpreted as a lack of generality, as our results on MOT17 and nuScenes show.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">See supplemental for additional details.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Confidence-Based Data Association and Discriminative Deep Appearance Learning for Robust Online Multi-Object Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuk-Jin</forename><surname>Seung-Hwan Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Tracking without bells and whistles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Meinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Evaluating Multiple Object Tracking Performance: The CLEAR MOT Metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keni</forename><surname>Bernardin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Image and Video Processing</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Simple online and realtime tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongyuan</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lionel</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Upcroft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">High-Speed tracking-by-detection without using image information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Bochinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Volker Eiselein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sikora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">nuScenes: A multimodal dataset for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venice</forename><forename type="middle">Erin</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anush</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giancarlo</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">End-to-end Learning Improves Static Object Geo-localization in Monocular Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Chaabane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lionel</forename><surname>Gueguen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameni</forename><surname>Trabelsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Beveridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen O&amp;apos;</forename><surname>Hara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05232,2020.4</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">End-to-end Learning Improves Static Object Geo-localization in Monocular Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Chaabane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lionel</forename><surname>Gueguen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameni</forename><surname>Trabelsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Beveridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen O&amp;apos;</forename><surname>Hara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2063" to="2072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Looking ahead: Anticipating pedestrians crossing with future frames prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Chaabane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameni</forename><surname>Trabelsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathaniel</forename><surname>Blanchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Beveridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2297" to="2306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Real-Time Multiple People Tracking with Deeply Learned Candidate Selection and Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haizhou</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijie</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Shang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recurrent Metric Networks and Batch Multiple Hypothesis for Multi-Object Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longtao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingwu</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3093" to="3105" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Near-Online Multi-target Tracking with Aggregated Local Flow Descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep Learning in Video Multi-Object Tracking: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gioele</forename><surname>Ciaparrone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><forename type="middle">Luque</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siham</forename><surname>Tabik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><surname>Troiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Tagliaferri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">381</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="61" to="88" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">R-FCN: Object Detection via Region-based Fully Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Detect to Track and Track to Detect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Are we ready for Autonomous Driving? the KITTI Vision Benchmark Suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">SMAT: Smart Multiple Affinity Metrics for Multiple Object Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><forename type="middle">Franco</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andres</forename><surname>Ospina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Calvez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Analysis and Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="48" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Joint Monocular 3D Vehicle Detection and Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hou-Ning</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi-Zhi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">MP)2T: Multiple People Multiple Parts Tracker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Izadinia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imran</forename><surname>Saleemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Multiple Object Tracking With Attention to Appearance, Structure, Motion and Size</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hasith</forename><surname>Karunasekera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Handuo</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>IEEE Access</publisher>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="104423" to="104434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The Hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harold W Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Research Logistics Quarterly</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="83" to="97" />
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning by Tracking: Siamese CNN for Robust Target Association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Canton-Ferrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Feature Pyramid Networks for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common Objects in Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">GSM: Graph Similarity Model for Multi-Object Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiankun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>the International Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Mahmoudi</surname></persName>
		</author>
		<title level="m">Seyed Mohammad Ahadi, and Mohammad Rahmati. Multi-target tracking using CNN-based features: CNNMTT. Multimedia Tools and Applications</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.00831</idno>
		<title level="m">MOT16: A Benchmark for Multi-Object Tracking</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">YOLOv3: An Incremental Improvement</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Performance Measures and a Data Set for Multi-target, Multi-camera Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ergys</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Tracking in unstructured crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saad</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Tracking the Untrackable: Learning to Track Multiple Cues With Long-Term Dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<idno>2017. 4</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Disentangling Monocular 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Simonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>López-Antequera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep Affinity Network for Multiple Object Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveed</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huansheng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multiple People Tracking by Lifted Multicut and Person Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<idno>2017. 1</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A Pose Proposal and Refinement Network for Better 6D Object Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameni</forename><surname>Trabelsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Chaabane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathaniel</forename><surname>Blanchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Beveridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2382" to="2391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Towards Real-Time Multi-Object Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongdao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<idno>2020. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinshuo</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.03961</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">A Baseline for 3D Multi-Object Tracking. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Simple Online and Realtime Tracking with a Deep Association Metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolai</forename><surname>Wojke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietrich</forename><surname>Paulus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Spatial-Temporal Relation Networks for Multi-Object Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">How To Train Your Deep Multi-Object Tracker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljosa</forename><surname>Osep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Ban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Horaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier Alameda-Pineda</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">3DSSD: Point-Based 3D Single Stage Object Detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zetong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep layer aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2403" to="2412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Robust Multi-Modality Multi-Object Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fairmot</surname></persName>
		</author>
		<title level="m">On the Fairness of Detection and Re-Identification in Multiple Object Tracking. arXiv eprints</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">2004</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.08114</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">A Survey on Multi-Task Learning. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Tracking Objects as Points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Objects as Points. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Class-balanced Grouping and Sampling for Point Cloud 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengkai</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.09492</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
