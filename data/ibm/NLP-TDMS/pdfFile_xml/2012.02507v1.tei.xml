<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Coarse-to-Fine Entity Representations for Document-level Relation Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damai</forename><surname>Dai</surname></persName>
							<email>daidamai@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">MOE Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Ren</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">MOE Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Zeng</surname></persName>
							<email>zengs@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">MOE Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">MOE Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">MOE Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Coarse-to-Fine Entity Representations for Document-level Relation Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Document-level Relation Extraction (RE) requires extracting relations expressed within and across sentences. Recent works show that graph-based methods, usually constructing a document-level graph that captures document-aware interactions, can obtain useful entity representations thus helping tackle document-level RE. These methods either focus more on the entire graph, or pay more attention to a part of the graph, e.g., paths between the target entity pair. However, we find that document-level RE may benefit from focusing on both of them simultaneously. Therefore, to obtain more comprehensive entity representations, we propose the Coarse-to-Fine Entity Representation model (CFER) that adopts a coarse-to-fine strategy involving two phases. First, CFER uses graph neural networks to integrate global information in the entire graph at a coarse level. Next, CFER utilizes the global information as a guidance to selectively aggregate path information between the target entity pair at a fine level. In classification, we combine the entity representations from both two levels into more comprehensive representations for relation extraction. Experimental results on a large-scale document-level RE dataset show that CFER achieves better performance than previous baseline models. Further, we verify the effectiveness of our strategy through elaborate model analysis.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Relation Extraction (RE) aims to extract semantic relations between named entities from plain text. It is an efficient way to acquire structured knowledge automatically, thus benefiting various natural language processing (NLP) applications <ref type="bibr" target="#b35">(Yih et al. 2015;</ref><ref type="bibr" target="#b36">Yu et al. 2017;</ref><ref type="bibr" target="#b40">Zhang et al. 2017)</ref>, especially knowledge graph construction <ref type="bibr" target="#b16">(Luan et al. 2018</ref>). Most of the previous RE works focus on the sentence level, i.e., they extract the relations within only a single sentence <ref type="bibr" target="#b38">(Zeng et al. 2014a</ref><ref type="bibr" target="#b37">(Zeng et al. , 2015</ref><ref type="bibr" target="#b41">Zhou et al. 2016;</ref><ref type="bibr" target="#b13">Lin et al. 2016)</ref>. However, in real-world scenarios, sentence-level RE models may omit some inter-sentence relations while a considerable number of relations are expressed beyond a single sentence but across multiple sentences in a long document <ref type="bibr" target="#b33">(Yao et al. 2019)</ref>. Therefore, document-level RE has attracted much attention in recent years. * Equal Contribution. <ref type="bibr" target="#b43">1</ref> We will release our code once the paper is officially accepted.  <ref type="figure">Figure 1</ref>: An example for document-level RE. Word spans with background colors denote the mentions. Mentions that refer to the same entity have the same background color. The solid lines denote intra-sentence relations. The dotted line denotes an inter-sentence relation. Document-level RE requires extracting both intra-and inter-sentence relations between all the target entity pairs. <ref type="figure">Figure 1</ref> shows an example document and corresponding relational facts for document-level RE. In the example, to extract the relation between Benjamin Bossi and Columbia Records, two entities separated by several sentences, we need the following inference steps. First, we need to know that Benjamin Bossi is a member of Romeo Void. Next, we need to infer that Never Say Never is performed by Romeo Void, and released by Columbia Records. Based on these facts above, we can draw a conclusion that Benjamin Bossi is signed by Columbia Records. The example indicates that, to tackle document-level RE, a model needs the ability to capture the interactions between long-distance entities. In addition, since an entire document has an extremely long text, a model also needs the ability to integrate more global contextual information for words.</p><p>The band primarily consisted <ref type="bibr">(root)</ref> of saxophonist Benjamin Bossi , vocalist Debora Iyall , guitarist Peter Woods , and bassist Frank Zincavage .</p><p>Romeo Void was <ref type="bibr">(root)</ref> an American new wave band from San Francisco California , formed in 1979.</p><p>syntactic dependency adjacent word self-loop adjacent sentence coreferential mention <ref type="figure">Figure 2</ref>: An illustration of a document-level graph corresponding to a two-sentence document. Each node in the graph corresponds to a word in the document. We design five categories of edges to connect nodes in the graph. For the simplicity of the illustration, we omit some self-loop edges and adjacent word edges.</p><p>formation between the target entity pair in the graph <ref type="bibr">Gupta et al. 2019;</ref><ref type="bibr" target="#b2">Christopoulou, Miwa, and Ananiadou 2019)</ref>. They have the ability to alleviate the problem of modeling long-distance entity interactions, but they may fail to capture more global contextual information since they usually integrate only local contextual information for nodes in the graph. Therefore, to obtain more comprehensive entity representations, it is necessary to find a way to integrate global contextual information and model long-distance entity interactions simultaneously.</p><p>In this paper, we propose the Coarse-to-Fine Entity Representation model (CFER) to obtain comprehensive entity representations for document-level RE. More specifically, we first construct a document-level graph that captures rich document-aware interactions, reflected by syntactic dependencies, adjacent word connections, cross-sentence connections, and coreferential mention interactions. Based on the constructed graph, we design a coarse-to-fine strategy with two phases. First, we use Densely Connected Graph Convolutional Networks (DCGCN) <ref type="bibr" target="#b9">(Huang et al. 2017;</ref> to integrate global contextual information in the entire graph at a coarse level. Next, we adopt an attentionbased path encoding mechanism, which takes the global contextual information as a guidance, to selectively aggregate path information between the potentially long-distance target entity pair at a fine level. Given the entity representations from both two levels that feature global contextual information and long-distance interactions, respectively, we can obtain more comprehensive entity representations for relation extraction by combining them.</p><p>Our contributions are summarized as follows: • We propose a novel document-level RE model called CFER. It uses a coarse-to-fine strategy to integrate global contextual information and model long-distance interactions between the target entities simultaneously, thus obtaining comprehensive entity representations. • Experiments on a large-scale document-level RE dataset show that our model achieves better performance than previous baseline models. • Analysis of our model reveals the effectiveness of our coarse-to-fine strategy, and further highlights the robustness of our model to the uneven label distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Task Formulation</head><formula xml:id="formula_0">Let D denote a document consisting of N sentences D = {s i } N i=1 , where s i = {w j } M j=1</formula><p>denotes the i-th sentence containing M words denoted by w j . Let E denote an entity set</p><formula xml:id="formula_1">containing P entities E = {e i } P i=1 , where e i = {m j } Q j=1</formula><p>denotes the coreferential mention set of the i-th entity, containing Q word spans of corresponding mentions denoted by m j . Given D and E, document-level RE requires extracting all the relational facts in the form of triplets, i.e., extracting {(e i , r, e j )|e i , e j ∈ E, r ∈ R}, where R is a pre-defined relation category set. Since an entity pair may have multiple semantic relations, we formulate document-level RE as a multi-label classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Document-level Graph Construction</head><p>Given a document, we first construct a document-level graph that captures rich document-aware interactions, reflected by syntactic dependencies, adjacent word connections, crosssentence connections, and coreferential mention interactions. <ref type="figure">Figure 2</ref> shows an example document-level graph corresponding to a two-sentence document. The graph regards words in the document as nodes and captures documentaware interactions by five categories of edges. These undirected edges are described as follows. Syntactic dependency edge: Syntactic dependency information is proved effective for document-level or crosssentence RE in previous works <ref type="bibr" target="#b22">(Sahu et al. 2019;</ref><ref type="bibr">Gupta et al. 2019;</ref><ref type="bibr" target="#b19">Peng et al. 2017;</ref><ref type="bibr" target="#b25">Song et al. 2018</ref>). Therefore, we use the dependency parser in spaCy 2 to parse the syntactic dependency tree for each sentence. After that, we add edges between all node pairs that have dependency relations. Adjacent word edge: <ref type="bibr" target="#b21">Quirk and Poon (2017)</ref> point out that adding edges between adjacent words can mitigate the dependency parser errors. Therefore, we add edges between all node pairs that are adjacent in the document. Self-loop edge: For a node, in addition to its neighborhood information, the historical information of the node itself is also essential in information integration. Therefore, we add a self-loop edge for each node. Adjacent sentence edge: To ensure that information can be integrated across sentences, for each adjacent sentence pair, we add an edge between their dependency tree roots. Coreferential mention edge: Coreferential mentions may share information captured from their respective contexts with each other. This could be regarded as global crosssentence interactions. Therefore, we add edges between the first words of all mention pairs that refer to the same entity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Coarse-to-Fine Entity Representations</head><p>In this subsection, we describe our proposed Coarse-to-Fine Entity Representation model (CFER) in detail. As shown in <ref type="figure" target="#fig_1">Figure 3</ref>, our model is composed of a text encoding module, a coarse-level representation module, a fine-level representation module, and a classification module. Text Encoding Module: This module aims to encode each word in the document as a vector with text contextual information. The text encoding module consists of an embedding layer and a contextual encoder.</p><p>At the embedding layer, we use pretrained GloVe (Pennington, Socher, and Manning 2014) embeddings or the pretrained BERT <ref type="bibr" target="#b3">(Devlin et al. 2019)</ref> model to represent each word as a dense embedding vector. This layer outputs a set of word embeddings d i ∈ R de , where i denotes the word index in the document, d e denotes the embedding dimension.</p><p>After the embedding layer, we use a contextual encoder to further integrate text contextual information for each word. Specifically, we adopt a Bi-GRU <ref type="bibr" target="#b1">(Cho et al. 2014;</ref><ref type="bibr" target="#b23">Schuster and Paliwal 1997)</ref> model as the contextual encoder:</p><formula xml:id="formula_2">− → h i = −−−→ GRU −−→ h i−1 , d i , (1) ← − h i = ←−−− GRU ←−− h i+1 , d i ,<label>(2)</label></formula><p>where</p><formula xml:id="formula_3">− → h i , ← − h i ∈ R d h /2</formula><p>are the forward and backward GRU hidden states of the i-th word, respectively, with d h denoting the hidden dimension. We concatenate the hidden states of two directions to obtain the outputs of the contextual encoder:</p><formula xml:id="formula_4">h i = [ − → h i ; ← − h i ] ∈ R d h .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Coarse-level Representation Module:</head><p>This module aims to integrate local and global contextual information in the entire document-level graph. As indicated by , Densely Connected Graph Convolutional Networks (DCGCN) have the ability to capture rich local and global contextual information. Therefore, we adopt DCGCN layers as the coarse-level representation module. DCGCN layers are organized into n blocks and the k-th block has m k sublayers. At the l-th sub-layer in block k, the calculation for node i is defined as</p><formula xml:id="formula_5">h (k,l) i = ReLU   j∈N (i) W (k,l)ĥ (k,l) j + b (k,l)   , (3) h (k,l) j = [x (k) j ; h (k,1) j ; ...; h (k,l−1) j ],<label>(4)</label></formula><formula xml:id="formula_6">where x (k) j ∈ R d h is the block input of node j. h (k,l) i ∈ R d h /m k is the output of node i at the l-th sub-layer in block k.ĥ (k,l) j ∈ R d h +(l−1)d h /m k is the neighborhood input, ob- tained by concatenating x (k)</formula><p>j and the outputs of node j from all previous sub-layers in the same block. W (k,l) and b (k,l) are trainable parameters. N (i) denotes the neighbor set of node i in the document-level graph. Finally, block k adds up the block input and the concatenation of all sub-layer outputs, and then adopts a fully connected layer to compute the block output o</p><formula xml:id="formula_7">(k) i ∈ R d h : o (k) i = FC x (k) i + [h (k,1) i ; ...; h (k,m k ) i ] .<label>(5)</label></formula><p>Finally, we take the output of the final block o (n) i as the output of the coarse-level representation module. Fine-level Representation Module: The coarse-level representations can capture rich contextual information, but may fail to model long-distance entity interactions. Taking the global contextual information as a guidance, the fine-level representation module aims to utilize path information between the target entity pair to alleviate this problem. This module adopts an attention-based path encoding mechanism based on a path encoder and an attention aggregator.</p><p>For a target entity pair (e 1 , e 2 ), we denote the numbers of their corresponding mentions as |e 1 | and |e 2 |, respectively. We first extract |e 1 | × |e 2 | shortest paths between all mention pairs in a subgraph that contains only syntactic dependency and adjacent sentence edges. For the i-th path [w 1 , ..., w leni ], we then adopt a Bi-GRU model as the path encoder to compute the path-aware mention representations:</p><formula xml:id="formula_8">− − → p i,j = −−−→ GRU −−−→ p i,j−1 , o (n) wj ,<label>(6)</label></formula><formula xml:id="formula_9">← − − p i,j = ←−−− GRU ←−−− p i,j+1 , o (n) wj ,<label>(7)</label></formula><formula xml:id="formula_10">m (h) i = ← − − p i,1 , m (t) i = −−−→ p i,leni ,<label>(8)</label></formula><formula xml:id="formula_11">where − − → p i,j , ← − − p i,j ∈ R d h</formula><p>are the forward and backward GRU hidden states of the j-th node in the i-th path, respectively. m</p><formula xml:id="formula_12">(h) i , m (t) i ∈ R d h</formula><p>are the path-aware representations of the head and tail mentions in the i-th path, respectively.</p><p>Since not all the paths contain useful information, we design an attention aggregator, which takes the global contextual information as a guidance, to selectively aggregate the path-aware mention representations:</p><formula xml:id="formula_13">h = 1 |e 1 | j∈e1 o (n) j , t = 1 |e 2 | j∈e2 o (n) j ,<label>(9)</label></formula><formula xml:id="formula_14">α i = Softmax i W a [ h; t; m (h) i ; m (t) i ] + b a ,<label>(10)</label></formula><formula xml:id="formula_15">h = i m (h) i · α i , t = i m (t) i · α i ,<label>(11)</label></formula><p>where h, t ∈ R d h are the coarse-level head and tail entity representations, respectively, which are computed by averaging their corresponding coarse-level mention representations. W a and b a are trainable parameters. h, t ∈ R d h are the path-aware fine-level representations of the head and tail entities, respectively. Classification Module: In this module, we combine the entity representations from both two levels to obtain comprehensive representations that capture both global contextual information and long-distance entity interactions. Next, we predict the probability of each relation by a bilinear scorer:</p><formula xml:id="formula_16">P (r|e 1 , e 2 ) = Sigmoid [ h; h] T W c [ t; t] + b c r ,<label>(12)</label></formula><p>where W c ∈ R 2d h ×nr×2d h and b c ∈ R nr are trainable parameters with n r denoting the number of relation categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Optimization Objective</head><p>Considering that an entity pair may have multiple relations, we choose the binary cross entropy loss between the ground truth label y r and P (r|e 1 , e 2 ) as the optimization objective:</p><formula xml:id="formula_17">L = − r y r · log P (r|e 1 , e 2 ) + (1 − y r ) · log 1−P (r|e 1 , e 2 ) .<label>(13)</label></formula><p>3 Experiments and Analysis</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>We evaluate our model on a large-scale human-annotated dataset, DocRED <ref type="bibr" target="#b33">(Yao et al. 2019</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Baselines</head><p>We select 14 existing models on three tracks for comparison.</p><p>(1) Baseline track: On this track, we select four baseline models proposed along with DocRED <ref type="bibr" target="#b33">(Yao et al. 2019)</ref>, including a CNN model <ref type="bibr" target="#b39">(Zeng et al. 2014b</ref>), an LSTM model <ref type="bibr" target="#b8">(Hochreiter and Schmidhuber 1997)</ref>, a Bi-LSTM model <ref type="bibr" target="#b0">(Cai, Zhang, and Wang 2016)</ref>, and the Context-Aware model <ref type="bibr" target="#b26">(Sorokin and Gurevych 2017)</ref>. These four models are all text-based without constructing a graph.</p><p>(2) Non-BERT track: On this track, we select six models without using BERT <ref type="bibr" target="#b3">(Devlin et al. 2019)</ref>. GAT (Velickovic et al. 2018), GCNN <ref type="bibr" target="#b22">(Sahu et al. 2019)</ref>, AGGCN , and LSR-GloVe <ref type="bibr" target="#b18">(Nan et al. 2020)</ref> are graph-based models that leverage GNN to encode nodes. Different from the above four models, EoG  is an edge-oriented graphbased model. HIN-GloVe <ref type="bibr" target="#b27">(Tang et al. 2020</ref>) is a text-based model and it has a hierarchical architecture to make full use of information from several levels. Our model with GloVe (CFER-GloVe) is also on this track.</p><p>(3) BERT track: On this track, we select four models that adopt BERT. BERT-Two-Step <ref type="bibr" target="#b32">(Wang et al. 2019)</ref> is a textbased model that first predicts the existence of relations and then predicts the specific relations. HIN-BERT Base and LSR-BERT Base are the BERT versions of HIN-GloVe and LSR-GloVe, respectively. CorefBERT Base <ref type="bibr" target="#b34">(Ye et al. 2020)</ref> is another text-based model designed to capture the relations between coreferential noun phrases. Our model with BERT (CFER-BERT Base ) is also on this track. For a fair comparison, all of the models on the BERT track adopt BERT Base instead of other pretrained models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experimental Settings</head><p>We tune hyper-parameters on the development set. Generally, we use AdamW <ref type="bibr" target="#b15">(Loshchilov and Hutter 2017)</ref> as the optimizer, set the batch size to 16, set the dropout rate to 0.2, and use the DCGCN consisting of two blocks with 3 and 4 sub-layers, respectively. For CFER-GloVe, we set the initial learning rate to 10 −3 , the embedding dimension to 300, and the hidden dimension to 300. For CFER-BERT Base , we set the initial learning rate for BERT modules to 10 −5 , the initial learning rate for other modules to 10 −3 , the embedding dimension to 768, and the hidden dimension to 300. Following previous works, we choose micro F1 and micro Ign F1 as the evaluation metrics. Ign F1 denotes F1 excluding relational facts that appear in both the training set and the development or test set. We determine relation-specific thresholds δ r based on the micro F1 on the development set. With these thresholds, we classify a triplet (e 1 , r, e 2 ) as positive if P (r|e 1 , e 2 ) &gt; δ r or negative otherwise. <ref type="table">Table 1</ref> shows the main evaluation results on DocRED. Considering models on the baseline and non-BERT tracks, we have the following observations from <ref type="table">Table 1</ref>: <ref type="formula">(1)</ref> All the models on the baseline track are text-based, and they achieve the poorest performance. Another text-based model HIN-GloVe achieves relatively high performance, but still cannot exceed LSR-GloVe and CFER-GloVe, two graph-based models. On the whole, graph-based models have significant advantages over text-based models in the task of documentlevel RE. (2) EoG pays more attention to encoding paths. GAT, GCNN, and AGGCN focus on integrating information on the entire graph. These four models achieve similar performance. This suggests that these two kinds of models have their own characteristics, and no one has overwhelming advantages. (3) LSR-GloVe also focuses on integrating information on the entire graph, but it achieves better performance than other graph-based baselines. This is because it designs a latent structure that can be learned by iterative refinements, thus enabling the model to better integrate information. (4) CFER-GloVe, which integrates global contextual information and model long-distance interactions simultaneously, outperforms all baselines significantly. This verifies the effectiveness of our comprehensive entity representations produced by the coarse-to-fine strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Main Results</head><p>Considering models on the BERT track, we observe from <ref type="table">Table 1</ref> that: (1) Consistent with the conclusion on the baseline and non-BERT tracks, LSR-BERT Base and CFER-BERT Base , two graph-based models, have significant advantages over the other three text-based models.</p><p>(2) CFER-BERT Base achieves the best performance. In addition, considering that our model uses a static graph instead of a dynamic graph learned by iterative refinements, our graph structure is simpler and sparser than LSR-BERT Base . Nevertheless, with a simpler and sparser static graph structure, our model still outperforms LSR-BERT Base . This suggests that our coarse-to-fine strategy has the ability to maximize the use of the graph structure thus achieving good performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Model Analysis</head><p>In this subsection, to explain why and how our model with the coarse-to-fine strategy works, we comprehensively analyze our model by answering four questions.</p><p>What improvement does CFER make? To explore where our improvement is located, we divide all the 96 relations into 4 categories according to their ground-truth positive label numbers in the development set. We show micro F1 on each category achieved by three models 4 in <ref type="figure" target="#fig_2">Figure 4</ref>. As shown in the figure, compared to Bi-LSTM, BERT-Two-</p><p>Step makes more improvement on relations with more than 20 positive labels, denoted by major relations, but less improvement (10.15%) on long-tail relations with less than or equal to 20 positive labels. By contrast, keeping the improvement on major relations, our model makes a much more significant improvement <ref type="formula">(</ref>  <ref type="table" target="#tab_4">Table 2</ref>. To explore the effectiveness of fine-level representations, we modify our model in three ways: (1) Replace the attention aggregator by a simple mean aggregator (denoted by -Attention Aggregator).</p><p>(2) Replace all used shortest paths by a random shortest path (denoted by -Multiple Paths).</p><p>(3) Remove the whole fine-level representations from the final classification (denoted by -Fine-level Repr.). As shown in <ref type="table" target="#tab_4">Table 2</ref>, F1 scores of these three modified models decrease by 1.48, 2.72, and 4.12, respectively. This verifies the effectiveness of the fine-level representations. In addition, this also verifies that not all shortest paths contain useful information, and our attention aggregator for multiple paths has the ability to selectively aggregate useful information thus producing better fine-level representations. Does Coarse-level Representations Matter? To explore the effectiveness of coarse-level representations, we modify our model in two ways: (1) Remove the whole coarselevel representations from the final classification (denoted by -Coarse-level Repr.).</p><p>(2) Remove DCGCN blocks (denoted by -DCGCN Blocks). As shown in <ref type="table" target="#tab_4">Table 2</ref>, F1 scores of these two modified models decrease by 1.14 and 2.08, respectively. This verifies that coarse-level representations can benefit relation extraction. In addition, this also verifies the ability of DCGCN to capture rich local and global contextual information, since the quality of coarse-level representations drops significantly without DCGCN. What If Remove Both-level Modules? We have verified the effectiveness of coarse-and fine-level representations separately. To draw a more credible conclusion, we remove both coarse-and fine-level representation modules (denoted by -Both-level Modules) for comparison. This operation makes our model degenerate into a simple version similar to Bi-LSTM. As shown in <ref type="table" target="#tab_4">Table 2</ref>, this simple version achieves similar performance to Bi-LSTM as expected. This suggests that our text encoding module is not much different from the Bi-LSTM baseline, and the performance improvement is totally introduced by our coarse-to-fine strategy. <ref type="figure" target="#fig_3">Figure 5</ref> shows an extraction case selected from the development set. In this case, CFER-GloVe (-Both-level Modules), a simple version similar to the Bi-LSTM baseline, extracts only two simple relational facts. CFER-GloVe (-Fine-level Repr.) extracts three more relational facts since it adopts DCGCN blocks to integrate richer local and global contextual information. CFER-GloVe (-DCGCN Blocks) makes use of path information to model long-distance entity interactions, and also extracts three more relational facts compared to CFER-GloVe (-Both-level Modules). CFER-GloVe (Full) combines all the advantages of its modified versions. As a result, it extracts the most relational facts.   To reveal how our coarse-to-fine strategy works, we further analyze the relational fact (0, P140, 5), which is extracted by only CFER (Full). In <ref type="figure" target="#fig_3">Figure 5</ref>, for CFER (Full) and CFER (-DCGCN Blocks), we additionally show several high-weight paths along with their attention weights used for producing fine-level representations. From the shown case, we can find that CFER (Full) gives relatively smooth weights to its high-weight paths. This enables it to aggregate richer path information from several useful paths. In fact, CFER (Full) successfully aggregates both intra-sentence (within sentence 7) and inter-sentence (across sentences 5, 6, and 7) information. By contrast, without the guidance of global contextual information, CFER (-DCGCN Blocks) learns extremely unbalanced weights and pays almost all its attention to a sub-optimal path. As a result, it fails to extract the P140 relation. As for CFER-GloVe (-Fine-level Repr.) and CFER-GloVe (-Both-level Module), they do not consider any path information. Therefore, it is hard for them to achieve as good performance as CFER (Full).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Case Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Most of existing RE methods under document-level or crosssentence settings are graph-based. They usually construct a document-level graph, which represents words as nodes and uses edges between them to capture document-aware interactions. Some of them focus more on integrating informa-  tion for each node in the entire graph. <ref type="bibr" target="#b19">Peng et al. (2017)</ref> propose the Graph LSTM to encode nodes. <ref type="bibr" target="#b25">Song et al. (2018)</ref> design a multi-layer graph with information flowing through layers to avoid calculation loops. <ref type="bibr" target="#b22">Sahu et al. (2019)</ref> use a labeled edge GCN <ref type="bibr" target="#b17">(Marcheggiani and Titov 2017)</ref> to integrate information and adopt the Multi-Instance Learning (MIL) to address the problem introduced by multiple mentions.  and <ref type="bibr" target="#b18">Nan et al. (2020)</ref> apply graph convolutional networks to a complete graph with weighted edges. The edge weights are iteratively refined by the multihead self-attention <ref type="bibr" target="#b28">(Vaswani et al. 2017)</ref> or structured attention <ref type="bibr" target="#b11">(Kim et al. 2017;</ref><ref type="bibr" target="#b14">Liu and Lapata 2018)</ref> mechanisms. Although these methods consider the entire graph structure, they may fail to model the interactions between longdistance entities due to the inherent over-smoothing problem in GNN <ref type="bibr" target="#b12">(Li, Han, and Wu 2018)</ref>. Other methods attempt to encode path information between the target entity pair in the graph. <ref type="bibr" target="#b21">Quirk and Poon (2017)</ref> design feature templates to extract features from multiple paths for classification. <ref type="bibr">Gupta et al. (2019)</ref> use dependency subtree representations to augment word embeddings and adopt LSTM <ref type="bibr" target="#b8">(Hochreiter and Schmidhuber 1997)</ref> to compute the path representation. <ref type="bibr" target="#b2">Christopoulou, Miwa, and Ananiadou (2019)</ref> propose an edge-oriented model that represents paths between the target entity pair as an edge through a walk-based iterative inference mechanism. These methods are able to alleviate the problem of modeling long-distance entity interactions, but they usually integrate only local contextual information for nodes instead of more global contextual information. Compared to existing graph-based methods, our model can not only integrate global contextual information on the entire graph, but also model long-distance entity interactions by utilizing path information, simultaneously.</p><p>Besides graph-based methods, there are also text-based methods without constructing a graph. <ref type="bibr" target="#b30">Verga, Strubell, and McCallum (2018)</ref> use a transformer <ref type="bibr" target="#b28">(Vaswani et al. 2017)</ref> to encode the document and address the problem of multi-ple mentions by MIL. <ref type="bibr" target="#b24">Singh and Bhatia (2019)</ref> attempt to explicitly connect the target entity pair via a context token, and then utilize second-order relations to capture complex and long dependencies. <ref type="bibr" target="#b10">Jia, Wong, and Poon (2019)</ref> design a multiscale architecture that learns representations at different scales and then combines them for classification. <ref type="bibr" target="#b32">Wang et al. (2019)</ref> adopt BERT <ref type="bibr" target="#b3">(Devlin et al. 2019)</ref> to encode the document and predict the existence of relations before predicting the specific relations. <ref type="bibr" target="#b27">Tang et al. (2020)</ref> design a hierarchical architecture to make full use of information from several levels. <ref type="bibr" target="#b34">Ye et al. (2020)</ref> attempt to explicitly capture relations between coreferential noun phrases to coherently comprehend the whole document. All of these text-based methods ignore the graph structure information. However, this information is shown effective and even essential for document-level RE in other existing works. Compared to text-based methods, our model can utilize text information and graph structure information simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose CFER with a coarse-to-fine strategy to learn comprehensive representations for documentlevel RE. Our model integrates global contextual information and models long-distance interactions between the target entity pair simultaneously, thus addressing the disadvantages that existing graph-based models suffer from. Experimental results show that our model achieves better performance than previous baseline models. Further, elaborate analysis of our model verifies the effectiveness of our coarse-to-fine strategy and highlights the robustness of our model to the uneven label distribution. Note that our coarseto-fine strategy is not limited to only the task of documentlevel RE. It has the potential to be applied to a variety of other NLP tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>consisted of saxophonist Benjamin Bossi … (7) The success of their (0) Romeo Void was an American new wave band … (1) The band primarily second release, a 4-song EP, Never Say Never resulted in a distribution deal with Columbia Records …</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>An illustration of our proposed model, CFER. It is composed of four modules: a text encoding module, a coarse-level representation module, a fine-level representation module, and a classification module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Micro F1 on different categories of relations. Our model makes a much more significant improvement on longtail relations with fewer positive labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>An extraction case from the development set. We show the relational facts extracted by four versions of CFER-GloVe. For CFER-GloVe (Full) and CFER-GloVe (-DCGCN Blocks), we additionally show the paths along with their attention weights used for producing fine-level representations of entity 0 (Daniel Ajayi Adeniran) and entity 5 (Redeemed Christian Church).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>). It is constructed from Wikipedia and Wikidata<ref type="bibr" target="#b31">(Vrandecic and Krötzsch 2014)</ref>, and is currently the largest human-annotated dataset for general domain document-level RE. It contains 5, 053 documents, 132, 375 entities and 56, 354 relational facts divided into 96 relation categories. Among the annotated documents, 3, 053 are used for training, 1, 000 are used for development, and 1, 000 are used for testing. Among the relational facts, at least 40.7% require reading multiple sentences.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>86.18%) on long-tail relations. Usually, long-tail relations are harder to extract, while our model can better extract them since we can capture both global information and subtle clues that may include special features of long-tail relations. With high performance on long-tail relations, our model narrows the performance gap between major and long-tail relations by 17.22% (from 46.05 of Bi-LSTM to 38.12 of ours). This suggests that our model is more robust to the uneven label distribution. Does Fine-level Representations Matter? We show the ablation experiment results of CFER-GloVe on the development set of DocRED in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Ablation experiment results of our model on the development set of DocRED.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Entity &amp; Relation Dictionary Extracted Relational Facts Path Attention Weights for Relational Fact</head><label></label><figDesc>Pentecostal pastor from Nigeria .<ref type="bibr" target="#b43">(1)</ref> As of 2011 , he heads the expansion of the African -based Redeemed Christian Church of God in North America . (2) An educated man who worked as a civil servant in Nigeria , Ajayi Adeniran experienced problems with alcohol , and in 1989 visited the Redeemed Christian Church of God across the street from his home near Ibadan . (3) In 1990 , he converted to the doctrines of the Redeemed Christian Church and was ordained through that denomination 1994 . (4) He moved to the United States in 1995 because of the political conditions under the dictator Sani Abacha in Nigeria . (5) After arriving in the U.S. , Ajayi Adeniran became part of the first parish of the Redeemed Christian Church of God in North America located on Roosevelt Island . (6) Soon after , he became the pastor of a newly formed branch of the church , meeting in a Bronx storefront . (7) Ajayi Adeniran states his goal is to inform others of the mission of the Redeemed Christian so that in each household in the world there will be at least one member of Redeemed Christian Church of God .</figDesc><table><row><cell></cell><cell></cell><cell>Document Text</cell></row><row><cell cols="2">Entities: 0: Daniel Ajayi Adeniran 2: Nigeria 5: Redeemed Christian Church 6: North America 8: Ibadan (0) Daniel Ajayi Adeniran is a (0, P27, 2)</cell><cell>CFER-GloVe (Full)</cell><cell>(0, P140, 5) CFER-GloVe (-DCGCN Blocks)</cell></row><row><cell>9: 1990</cell><cell>(13, P27, 2)</cell><cell></cell></row><row><cell>11: the United States</cell><cell></cell><cell></cell></row><row><cell>13: Sani Abacha</cell><cell></cell><cell></cell></row><row><cell>14: Roosevelt Island</cell><cell>(8, P17, 2)</cell><cell></cell></row><row><cell>Relations:</cell><cell>(6, P527, 11)</cell><cell></cell></row><row><cell>P17: country</cell><cell>(14, P17, 11)</cell><cell>(11, P30, 6)</cell></row><row><cell>P27: country of citizenship</cell><cell></cell><cell></cell></row><row><cell>P30: continent</cell><cell></cell><cell></cell></row><row><cell>P140: religion</cell><cell>(0, P140, 5) (11, P361, 6)</cell><cell></cell></row><row><cell>P527: has part</cell><cell></cell><cell></cell></row><row><cell>P361: part of</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://spacy.io/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">More details of hyper-parameters are shown in the Appendix.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Here we select only those models with available and bug-free source codes for comparison.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices A Computing Infrastructure</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bidirectional Recurrent Convolutional Neural Network for Relation Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2016</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="756" to="765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ç</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Connecting the Dots: Document-level Neural Relation Extraction with Edge-oriented Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Christopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ananiadou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP 2019</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4924" to="4935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT 2019</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Attention Guided Graph Convolutional Networks for Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2019</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="241" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="297" to="312" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rajaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Runkler</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural Relation Extraction within and across Sentence Boundaries</title>
	</analytic>
	<monogr>
		<title level="m">AAAI 2019</title>
		<imprint>
			<biblScope unit="page" from="6513" to="6520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Densely Connected Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2017</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Document-Level N-ary Relation Extraction with Multiscale Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT 2019</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3693" to="3704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Structured Attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deeper Insights Into Graph Convolutional Networks for Semi-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI 2018</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3538" to="3545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural Relation Extraction with Selective Attention over Instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2016</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2124" to="2133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning Structured Text Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="63" to="75" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Fixing Weight Decay Regularization in Adam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno>abs/1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Multi-Task Identification of Entities, Relations, and Coreference for Scientific Knowledge Graph Construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3219" to="3232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2017</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1506" to="1515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Reasoning with Latent Structure Refinement for Document-Level Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sekulic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2020</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1546" to="1557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cross-Sentence N-ary Relation Extraction with Graph LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="101" to="115" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Glove: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2014</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Distant Supervision for Relation Extraction beyond the Sentence Boundary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1171" to="1182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Inter-sentence Relation Extraction with Documentlevel Graph Convolutional Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Sahu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Christopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ananiadou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2019</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4309" to="4316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Relation Extraction using Explicit Context Conditioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bhatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT 2019</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1442" to="1447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">N-ary Relation Extraction using Graph-State LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gildea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2018</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2226" to="2235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Context-Aware Representations for Knowledge Base Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sorokin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1784" to="1789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">HIN: Hierarchical Inference Network for Document-Level Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PAKDD 2020</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="197" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Attention is All you Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Graph Attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Simultaneously Self-Attending to All Mentions for Full-Abstract Biological Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT 2018</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="872" to="884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Wikidata: a free collaborative knowledgebase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vrandecic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krötzsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="78" to="85" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Fine-tune Bert for DocRED with Two-step Process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Focke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sylvester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>CoRR abs/1909.11898</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">DocRED: A Large-Scale Document-Level Relation Extraction Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2019</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="764" to="777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Coreferential Reasoning Learning for Language Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/2004.06870</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Semantic Parsing via Staged Query Graph Generation: Question Answering with Knowledge Base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1321" to="1331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Improved Neural Relation Detection for Knowledge Base Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">571</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1753" to="1762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Relation Classification via Convolutional Deep Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2014</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Relation Classification via Convolutional Deep Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2014</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Position-aware Attention and Supervised Data Improve Slot Filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="35" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2016</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="207" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">We run our experiments on a single NVIDIA GeForce GTX 1080Ti GPU with 11 GB memory. Our operating system is Ubuntu 16.04 LTS 5 . We mainly use Python3 6 as our coding language. Key libraries used in our code include</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note>PyTorch 1.4.0</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">B Details of Hyper-parameters</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">For other hyper-parameters, we state the values tried and the finally selected value for two models separately as follows. For CFER-GloVe: (1) We search the batch size in {8, 16, 32}, and finally select 16. (2) We search the dropout rate in {0.2, 0.5}, and finally select 0.2. (3) We search the initial learning rate for all modules in {10 −3 , 10 −4 , 10 −5 }, and finally choose 10 −3 . (4) We set the embedding dimension to 300, the same as the dimension of used GloVe embeddings. (5) We search the hidden size in {180, 300, 768}, and finally select 300. (6) For each hyper-parameter configuration, we train 300 epochs and select the best F1 achieved during these 300 epochs to evaluate the performance under this configuration. For CFER-BERT Base : (1) We search the batch size in {8, 16, 32}, and finally select 16. (2) We search the dropout rate in {0.2, 0.5}, and finally select 0.5. (3) We search the initial learning rate for BERT modules in {10 −3 , 10 −4 , 10 −5 }, and finally select 10 −5 . (4) We search the initial learning rate for the other modules in {10 −3 , 10 −4 , 10 −5 }, and finally select 10 −3 . (5) We set the embedding dimension to 768, the same as the output dimension of BERT Base</title>
	</analytic>
	<monogr>
		<title level="m">We adopt the grid search to find the best hyper-parameters that achieve the best F1 on the development set. Generally, for both CFER-GloVe and CFER-BERT Base , we use AdamW with a weight decay of 10 −4 as the optimizer, use ReLU as the activation function, and use the DCGCN consisting of two blocks with 3 and 4 sub-layers, respectively</title>
		<imprint/>
	</monogr>
	<note>6) We search the hidden size in {180, 300, 768}, and finally select 300. (7) For each hyper-parameter configuration, we train 200 epochs and select the best F1 achieved during these 200 epochs to evaluate the performance under this configuration</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
