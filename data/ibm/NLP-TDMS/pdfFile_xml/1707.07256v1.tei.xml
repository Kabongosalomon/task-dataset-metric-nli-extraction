<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deeply-Learned Part-Aligned Representations for Person Re-Identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liming</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University ‡ Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University ‡ Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University ‡ Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University ‡ Microsoft Research</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
							<email>yzhuang@zju.edu.cnjingdw@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University ‡ Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deeply-Learned Part-Aligned Representations for Person Re-Identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we address the problem of person reidentification, which refers to associating the persons captured from different cameras. We propose a simple yet effective human part-aligned representation for handling the body part misalignment problem. Our approach decomposes the human body into regions (parts) which are discriminative for person matching, accordingly computes the representations over the regions, and aggregates the similarities computed between the corresponding regions of a pair of probe and gallery images as the overall matching score. Our formulation, inspired by attention models, is a deep neural network modeling the three steps together, which is learnt through minimizing the triplet loss function without requiring body part labeling information. Unlike most existing deep learning algorithms that learn a global or spatial partition-based local representation, our approach performs human body partition, and thus is more robust to pose changes and various human spatial distributions in the person bounding box. Our approach shows state-of-the-art results over standard datasets, Market-1501, CUHK03, CUHK01 and VIPeR. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Person re-identification is a problem of associating the persons captured from different cameras located at different physical sites. If the camera views are overlapped, the solution is trivial: the temporal information is reliable to solve the problem. In some real cases, the camera views are significantly disjoint and the temporal transition time between cameras varies greatly, making the temporal information not enough to solve the problem, and thus this problem becomes more challenging. Therefore, a lot of solutions exploiting various cues, such as appearance <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26]</ref>, which is also the interest in this paper, have been developed.</p><p>Recently, deep neural networks have been becoming a  <ref type="figure">Figure 1</ref>. Illustrating the necessity of body part partition (best viewed in color). Using spatial partition without further processing, the regions <ref type="formula" target="#formula_1">(1)</ref> and <ref type="formula" target="#formula_2">(2)</ref>, as well as <ref type="formula" target="#formula_4">(4)</ref> and <ref type="formula" target="#formula_6">(5)</ref>, are not matched though they are from the same person; but the regions <ref type="formula" target="#formula_1">(1)</ref> and <ref type="formula" target="#formula_3">(3)</ref>, as well as <ref type="bibr" target="#b4">(5)</ref> and <ref type="formula" target="#formula_7">(6)</ref>, which are from different persons, are matched. With body part decomposition, there is no such mismatch. More examples are shown in d, e, and f. dominate solution for the appearance representation. The straightforward way is to extract a global representation <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b5">6]</ref>, using the deep network pretrained over ImageNet and optionally fine-tuned over the person re-identification dataset. Local representations are computed typically by partitioning the person bounding box into cells, e.g., dividing the images into horizontal stripes <ref type="bibr" target="#b56">[56,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b44">44]</ref> or grids <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b0">1]</ref>, and extracting deep features over the cells. These solutions are based on the assumption that the human poses and the spatial distributions of the human body in the bounding box are similar. In real cases, for example, the bounding box is detected rather than manually labeled and thus the human may be at different positions, or the human poses are different, such an assumption does not hold. In other words, spatial partition is not well aligned with human body parts. Thus, person re-identification, even with subsequent complex matching techniques (e.g., <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">23]</ref>) to eliminate the misalignment, is often not quite reliable. <ref type="figure">Figure 1</ref> provides illustrative examples.</p><p>In this paper, we propose a part-aligned human representation, which addresses the above problem instead in the representation learning stage. The key idea is straightforward: detect the human body regions that are discrimina-tive for person matching, compute the representations over the parts, and then aggregate the similarities that are computed between the corresponding parts. Inspired by attention models <ref type="bibr" target="#b53">[53]</ref>, we present a deep neural network method, which jointly models body part extraction and representation computation, and learns model parameters through maximizing the re-identification quality in an end-to-end manner, without requiring the labeling information about human body parts. In contrast to spatial partition, our approach performs human body part partition, thus is more robust to human pose changes and various human spatial distributions in the bounding box. Empirical results demonstrate that our approach achieves competitive/superior performance over standard datasets: Market-1501, CUHK03, CUHK01 and VIPeR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>There are two main issues in person re-identification: representation and matching. Various solutions, separately or jointly addressing the two issues, have been developed.</p><p>Separate solutions. Various hand-crafted representations have been developed, such as the ensemble of local features (ELF) <ref type="bibr" target="#b14">[15]</ref>, fisher vectors (LDFV) <ref type="bibr" target="#b29">[29]</ref>, local maximal occurrence representation (LOMO) <ref type="bibr" target="#b25">[26]</ref>, hierarchal Gaussian descriptor (GOG) <ref type="bibr" target="#b31">[31]</ref>, and so on. Most of the representations are designed with the goal of handling light variance, pose/view changes, and so on. Person attributes or salient patterns, such as female/male, wearing hat or not, have also been exploited to distinguish persons <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b61">61]</ref>.</p><p>A lot of similarity/metric learning techniques <ref type="bibr" target="#b57">[57,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b18">19]</ref> have been applied or designed to learn metrics, robust to light/view/pose changes, for person matching. The recent developments include soft and probabilistic patch matching for handling pose misalignment <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b36">36]</ref>, similarity learning for dealing with probe and gallery images with different resolutions <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b16">17]</ref>, connection with transfer learning <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b38">38]</ref>, reranking inspired by the connection with image search <ref type="bibr" target="#b65">[65,</ref><ref type="bibr" target="#b12">13]</ref>, partial person matching <ref type="bibr" target="#b66">[66]</ref>, human-in-the-loop learning <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b46">46]</ref>, and so on.</p><p>Deep learning-based solutions. The success of deep learning in image classification has been inspiring a lot of studies in person re-identification. The off-the-shelf CNN features, extracted from the model trained over ImageNet, without fine tuning, does not show the performance gain <ref type="bibr" target="#b33">[33]</ref>. The promising direction is to learn the representation and the similarity jointly, except some works <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b62">62]</ref> that do not learn the similarity but adopt the classification loss by regarding the images about one person as a category.</p><p>The network typically consists of two subnetworks: one for feature extraction and the other for matching. The feature extraction subnetwork could be simply (i) a shallow network <ref type="bibr" target="#b22">[23]</ref> with one or two convolutional and max-pooling layers for feature extraction, or (ii) a deep network, e.g., VGGNet and its variants <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b49">49]</ref> and GoogLeNet <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b59">59]</ref>, which are pretrained over ImageNet and fine-tuned for person re-identification. The feature representation can be (i) a global feature, e.g., the output of the fully-connected layer <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b52">52]</ref>, which does not explicitly model the spatial information, or (ii) a combination (e.g., concatenation <ref type="bibr" target="#b56">[56,</ref><ref type="bibr" target="#b8">9]</ref> or contextual fusion <ref type="bibr" target="#b44">[44]</ref>) of the features over regions, e.g., horizontal stripes <ref type="bibr" target="#b56">[56,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b44">44]</ref>, or grid cells <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b0">1]</ref>, which are favorable for the later matching process to handle body part misalignment. Besides, the cross-dataset information <ref type="bibr" target="#b51">[51]</ref> is also exploited to learn an effective representation.</p><p>The matching subnetwork can simply be a loss layer that penalizes the misalignment between learnt similarities and ground-truth similarities, e.g., pairwise loss <ref type="bibr" target="#b56">[56,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b37">37]</ref>, triplet loss and its variants <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b45">45]</ref>. Besides using the off-the-shelf similarity function <ref type="bibr" target="#b56">[56,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b8">9]</ref>, e.g., cosine similarity or Euclidean distance, for comparing the feature representation, specific matching schemes are designed to eliminate the influence from body part misalignment. For instance, a matching subnetwork conducts convolution and max pooling operations, over the differences <ref type="bibr" target="#b0">[1]</ref> or the concatenation <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b59">59]</ref> of the representations over grid cells of a pair of person images, to handle the misalignment problem. The approach with so called single-image and cross-image representations <ref type="bibr" target="#b45">[45]</ref> essentially combines the off-the-shelf distance and the matching network handling the misalignment. Instead of only matching the images over the final representation, the matching map in the intermediate features is used to guide the feature extraction in the later layers through a gated CNN <ref type="bibr" target="#b43">[43]</ref>.</p><p>Our approach. In this paper, we focus on the feature extraction part and introduce a human body part-aligned representation. Our approach is related to but different from the previous part-aligned approaches (e.g., part/pose detection <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b63">63]</ref>), which need to train a part/pose segmentation or detection model from the labeled part mask/box or pose ground-truth and subsequently extract representations, where the processes are conducted separately. In contrast, our approach does not require those labeling information, but only uses the similarity information (a pair of person images are about the same person or different persons), to learn the part model for person matching. The learnt parts are different from the conventional human body parts, e.g., Pascal-Person-Parts <ref type="bibr" target="#b6">[7]</ref>, and are specifically for person matching, implying that our approach potentially performs better, which is verified by empirical comparisons with the algorithms based on the state-of-the-art part segmentation approach (deeplab <ref type="bibr" target="#b4">[5]</ref>) and pose estimator (convolutional pose machine <ref type="bibr" target="#b47">[47]</ref>).</p><p>Our human body part estimation scheme is inspired by the attention model that is successfully applied to many applications such as image captioning <ref type="bibr" target="#b53">[53]</ref>. Compared to the work <ref type="bibr" target="#b27">[28]</ref> that is based on attention models and LSTM, our approach is simple and easily implemented, and empirical results show that our approach performs better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head><p>Person re-identification aims to find the images that are about the same identity with the probe image from a set of gallery images. It is often regarded as a ranking problem: given a probe image, the gallery images about the same identity are thought closer to the probe image than the gallery images about different identities.</p><p>The training data is typically given as follows. Given a set of images I = {I 1 , I 2 , . . . , I N }, we form the training set as a set of triplets,</p><formula xml:id="formula_0">T = {(I i , I j , I k )}, where (I i , I j )</formula><p>is a positive pair of images that are about the same person and (I i , I k ) is a negative pair of images that are about different persons.</p><p>Our approach formulates the ranking problem using the triplet loss function,</p><formula xml:id="formula_1">triplet (I i , I j , I k ) = [d(h(I i ), h(I j )) − d(h(I i ), h(I k )) + m] + .<label>(1)</label></formula><p>Here (I i , I j , I k ) ∈ T . m is the margin by which the distance between a negative pair of images is greater than that between a positive pair of images. In our implementation, m is set to 0.2 similar to <ref type="bibr" target="#b35">[35]</ref>. d(x, y) = x − y 2 2 is a Euclidean distance.</p><p>[z] + = max(z, 0) is the hinge loss. h(I) is a feature extraction network that extracts the representation of the image I and will be discussed in detail later. The whole loss function is as follows,</p><formula xml:id="formula_2">L(h) = 1 |T | (Ii,Ij ,I k )∈T triplet (I i , I j , I k ),<label>(2)</label></formula><p>where |T | is the number of triplets in T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Part-Aligned Representation</head><p>The part-aligned representation extractor, is a deep neural network, consisting of a fully convolutional neural network (FCN) whose output is an image feature map, followed by a part net which detects part maps and outputs the part features extracted over the parts. Rather than partitioning the image box spatially to grid cells or horizontal stripes, our approach aims to partition the human body to aligned parts.</p><p>The part net, as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>, contains several branches. Each branch receives the image feature map from the FCN as the input, detects a discriminative region (part 2 ), and extracts the feature over the detected region as the output. As we will see, the detected region usually lies in the  human body region, which is as expected because these regions are informative for person matching. Thus, we call the net as a part net. Let a 3-dimensional tensor T represent the image feature maps computed from the FCN and thus t(x, y, c) represent the cth response over the location (x, y). The part map detector estimates a 2-dimensional map M k , where m k (x, y) indicates the degree that the location (x, y) lies in the kth region, from the image feature map T:</p><formula xml:id="formula_3">M k = N MapDetector k (T),<label>(3)</label></formula><p>whereN MapDetector k (·) is a region map detector implemented as a convolutional network. The part feature map T k for the kth region is computed through a weighting scheme,</p><formula xml:id="formula_4">t k (x, y, c) = t(x, y, c) × m k (x, y),<label>(4)</label></formula><p>followed by an average pooling operator,</p><formula xml:id="formula_5">f k = AvePooling(T k ), wheref k (c) = Average x,y [t k (x, y, c)].</formula><p>Then a linear dimension-reduction layer, implemented as a fully-connected layer, is performed to reducef k to a ddimensional feature vector f k = W F C kf k . Finally, we concatenate all the part features,</p><formula xml:id="formula_6">f = [f 1 f 2 . . . f K ] ,<label>(5)</label></formula><p>and perform an L 2 normalization, yielding the human representation h(I).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Optimization</head><p>We learn the network parameters, denoted by θ, by minimizing the summation of triplet loss functions over triplets formulated in Equation 2. The gradient is computed as</p><formula xml:id="formula_7">∂L ∂θ = 1 |T | (Ii,Ij ,I k )∈T ∂ triplet (I i , I j , I k ) ∂θ .<label>(6)</label></formula><p>We have 3</p><formula xml:id="formula_8">∂ triplet (I i , I j , I k ) ∂θ = δ triplet (Ii,Ij ,I k )&gt;0 × 2[ ∂h(I i ) ∂θ (h(I k ) − h(I j ))+ ∂h(I j ) ∂θ (h(I j ) − h(I i )) + ∂h(I k ) ∂θ (h(I i ) − h(I k ))].</formula><p>Thus, we transform the gradient to the following form,</p><formula xml:id="formula_9">∂L ∂θ = 1 |T | N n=1 ∂h(I n ) ∂θ α n ,<label>(7)</label></formula><p>where α n is a weight vector depending on the current network parameters, and computed as follows,</p><formula xml:id="formula_10">α n = 2[ (In,Ij ,I k )∈T δ triplet (In,Ij ,I k )&gt;0 (h(I k ) − h(I j ))+ (Ii,In,I k )∈T δ triplet (Ii,In,I k )&gt;0 (h(I n ) − h(I i ))+ (Ii,Ij ,In)∈T δ triplet (Ii,Ij ,In)&gt;0 (h(I i ) − h(I n ))].<label>(8)</label></formula><p>Equation <ref type="formula" target="#formula_9">7</ref> suggests that the gradient for the triplet loss is computed like that for the unary classification loss. Thus, in each iteration of SGD (stochastic gradient descent) we can draw a mini-batch of (M ) samples rather than sample a subset of triplets: one pass of forward propagation to compute the representation h(I n ) of each sample, compute the weight α n over the mini-batch, compute the gradient ∂h(In) θ , and finally aggregate the gradients over the minibatch of samples. Directly drawing a set of triplets usually leads to that a larger number of (more than M ) samples are contained and thus the computation is more expensive than our mini-batch sampling scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Implementation details</head><p>Network architecture. We use a sub-network of the first version of GoogLeNet <ref type="bibr" target="#b42">[42]</ref>, from the image input to the output of inception 4e, followed by a 1 × 1 convolutional layer with the output of 512 channels, as the image feature map extraction network. Specifically, the person image box is resized to 160 × 80 as the input, and thus the size of the feature map of the feature map extraction network is 10 × 5 with 512 channels. For data preprocessing, we use the standard horizontal flips of the resized image. In the part net, the part estimator (N MapDetector k in Equation 3) is simply a 1 × 1 convolutional layer followed by a nonlinear sigmoid layer. There are K part detectors, where K is determined by cross-validation and empirically studied in Section 4.3.</p><p>Network Training. We use the stochastic gradient descent algorithm to train the whole network based on Caffe <ref type="bibr" target="#b15">[16]</ref>. The image feature map extraction part is initialized using the GoogLeNet model, pretrained over ImageNet. In each iteration, we sample a mini-batch of 400 images, e.g., there are on average 40 identities with each containing 10 images on Market-1501 and CUHK03. In total, there are about 1.4 million triplets in each iteration. From Equation <ref type="bibr" target="#b7">8</ref>, we see that only a subset of triplets, whose predicted similarity order is not consistent to the ground-truth order, i.e., <ref type="bibr">Map</ref>   <ref type="figure">Figure 3</ref>. Examples of the part maps learnt by the part map estimator for test images (best viewed in color).</p><p>triplet (I n , I j , I k ) &gt; 0, are counted for the weight (θ) update, and accordingly we use the number of counted triplets to replace |T | in Equation 7.</p><p>We adopt the initial learning rate, 0.01, and divide it by 5 every 20K iterations. The weight decay is 0.0002 and the momentum for gradient update is 0.9. Each model is trained for 50K iterations within around 12 hours on a K40 GPU. For testing, it takes on average 0.005 second on one GPU to extract the part-aligned representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Discussions</head><p>Body part partition and spatial partition. Spatial partition, e.g., grid or stride-based, may not be well aligned with human body parts, due to pose changes or various human spatial distributions in the human image box. Thus, matching techniques, e.g., through complex networks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b59">59]</ref>, have been developed to eliminate the misalignment problem. In contrast, our approach addresses this problem in the representation stage, with a simple Euclidean distance for person matching, which potentially makes existing fast similarity search algorithms easily applied, and thus the online search stage more efficient. <ref type="figure">Figure 3</ref> shows the examples about the parts our approach learns for the test images. It can be seen that the parts are generally well aligned for the pair of images about the same person: the parts almost describe the same human body regions, except that one or two parts in the pair of images describe different regions, e.g., the first part in <ref type="figure">Figure 3 (b)</ref>. In particular, the alignment is also good for the examples of <ref type="figure">Figure 3 (c, d)</ref>, where the person in the second image is spatially distributed very differently from the person in the first image: one is on the right in <ref type="figure">Figure 3</ref> (c), and one is small and on the bottom in <ref type="figure">Figure 3 (d)</ref>.</p><p>In addition, we empirically compare our approach with two spatial partition based methods: dividing the image box into 5 horizontal stripes or 5 × 5 girds to form region maps. We use the region maps to replace the part mask in our approach and then learn the spatial partition-based representation. The results shown in <ref type="table" target="#tab_2">Table 1</ref> demonstrate that the human body part partition method is more effective. Learnt body parts. We have several observations about the learnt parts. The head region is not included. This is because the face is not frontal and with low resolution and accordingly not reliable for differentiating different persons. The skin regions are often also not included except the arms located nearby the top body in <ref type="figure">Figure 3</ref> (c) as the skin does not provide discriminant information, e.g., the leg skins in <ref type="figure">Figure 3</ref> (c) are not included while the legs with trousers in <ref type="figure">Figure 3</ref> (b) are included in Map4-6 . From <ref type="figure">Figure 3</ref>, we can see that the first three maps, Map1 -Map3, are about the top clothing. There might be some redundancy.</p><p>In the examples of <ref type="figure">Figure 3</ref>  Separate part segmentation. We conduct an experiment with separate part segmentation. We use the state-of-theart part segmentation model <ref type="bibr" target="#b4">[5]</ref> learnt from the PASCAL-Person-Part dataset <ref type="bibr" target="#b6">[7]</ref> (6 part classes), to compute the mask for both training and test images. We modify our network by replacing the masks from the part net with the masks from the part segmentation model. In the training stage, we learn the modified network (the mask fixed) using the same setting with our approach. The results are shown in <ref type="table" target="#tab_3">Table 2</ref> and the performance is poor compared with our method. This is reasonable because the parts in our approach are learnt directly for person re-identification while the parts learnt from the PASCAL-Person-Part dataset might not be very good because it does not take consideration into the person re-identification problem. We also think that if the human part segmentation of the person re-identification training images is available, exploiting the segmentation as an extra supervision, e.g., the learnt part corresponds to a human part, or a sub-region of the human part, is helpful for learning the part net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>Market-1501. This dataset <ref type="bibr" target="#b64">[64]</ref> is one of the largest benchmark datasets for person re-identification. There are six cameras: 5 high-resolution cameras, and one low-resolution camera. There are 32, 668 DPM-detected pedestrian image boxes of 1, 501 identities: 750 identifies are used for training and the remaining 751 for testing. There are 3, 368 query images and the large gallery (database) include 19, 732 images with 2, 793 distractors.</p><p>CUHK03. This dataset <ref type="bibr" target="#b22">[23]</ref> consists of 13, 164 images of 1, 360 persons, captured by six cameras. Each identity only appears in two disjoint camera views, and there are on average 4.8 images in each view. We use the provided training/test splits <ref type="bibr" target="#b22">[23]</ref> on the labeled data set. For each test identity, two images are randomly sampled as the probe and gallery images, respectively, and the average performance over 20 trials is reported as the final result.</p><p>CUHK01. This dataset <ref type="bibr" target="#b21">[22]</ref> contains 971 identities captured from two camera views in the same campus with CUHK03. Each person has two images, each from one camera view. Following the setup <ref type="bibr" target="#b0">[1]</ref>, we report the results of two different settings: 100 identifies for testing, and 486 identities for testing.</p><p>VIPeR. This dataset <ref type="bibr" target="#b13">[14]</ref> contains two views of 632 persons. Each pair of images about one person are captured by different cameras with large viewpoint changes and various illumination conditions. The 632 person images are divided into two halves, 316 for training and 316 for testing. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Metrics</head><p>We adopt the widely-used evaluation protocol <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b0">1]</ref>. In the matching process, we calculate the similarities between each query and all the gallery images, and then return the ranked list according to the similarities. All the experiments are under the single query setting. The performances are evaluated by the cumulated matching characteristics (CMC) curves, which is an estimate of the expectation of finding the correct match in the top n matches. We also report the mean average precision (mAP) score <ref type="bibr" target="#b64">[64]</ref> over Market-1501.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Empirical Analysis</head><p>The number of parts. We empirically study how the number of parts affects the performance. We conduct an experiment over CUHK03: randomly partition the training dataset into two parts, one for model learning and the remaining for validation. The performances for various numbers of parts, K = 1, 2, 4, 8, 12, are given in <ref type="table">Table 3</ref>. It can be seen that (i) more parts for the rank-1 score lead to better scores till 8 parts and then the scores become stable, and (ii) the scores of different number of parts at positions 5, 10, and 20 are close except the score of 1 part at position 5. Thus, in our experiments, we choose K = 8 in the part net for all the four datasets. It is possible that in other datasets the optimal K obtained through validation is different.</p><p>Human segmentation and body part segmentation. The benefit from the body part segmentation lies in two points: (i) remove the background and (ii) part alignment. We compare our approach and the approach with human segmentation that is implemented as our approach with 1 part and is able to remove the background. The comparison shown from <ref type="table">Table 4</ref> over Market-1501 and CUHK03 shows that body part segmentation performs superiorly in general. The results imply that body part segmentation is beneficial.</p><p>Comparison with non-human/part-segmentation. We compare the performances of two baseline networks without segmentation, which are modified from our network: (i) replace the part net with a fully-connected layer outputting the feature vector with the same dimension (512-d) and (ii) replace the part net with an global average-pooling layer which also produces a 512-d feature vector.</p><p>The fully-connected layer followed by the last convolutional layer in (i) has some capability to differentiate different spatial regions to some degree through the linear weights, which are however the same for all images, yielding limited ability of differentiation. The average-pooling method in (ii) ignores the spatial information, though it is robust to the translations. In contrast, our approach is also able to differentiate body regions and the differentiation is adaptive to each input image for translation/pose invariance.</p><p>The comparison over two datasets, Market-1501 and CUHK03, is given in <ref type="table" target="#tab_5">Table 5</ref>. It can be seen that our approach outperforms these two baseline methods, which indicates that the part segmentation is capable of avoiding the mismatch due to part misalignment in spatial partition and improving the performance.</p><p>Image feature map extraction networks. We show that the part net can boost the performance for various feature map extraction FCNs. We report two extra results with using AlexNet <ref type="bibr" target="#b20">[21]</ref> and VGGNet <ref type="bibr" target="#b39">[39]</ref> as well as the result using GoogLeNet <ref type="bibr" target="#b42">[42]</ref>. For AlexNet and VGGNet, we remove the fully connected layers and use all the remaining convolutional layers as the feature map extraction network, and the training settings are the same as provided in Section 3.3. The results are depicted in <ref type="figure">Figure 4</ref>. It can be seen that our approach consistently gets the performance gain for AlexNet, VGGNet and GoogLeNet. In particular, the gains with AlexNet and VGGNet are more significant: compared with the baseline method with FC, the gains are 6.8, 6.4, and 5.1 for AlexNet, VGGNet and GoogLeNet, respectively, and compared with the baseline method with pooling, the gains are 5.9, 4.4, and 3.0, respectively.</p><p>Comparison with other attention models. The part map detector is inspired by the spatial attention model. It is slightly different from the standard attention model: using sigmoid to replace softmax, which brings more than 2% gain for rank-1 scores. The comparative attention network (CAN) approach <ref type="bibr" target="#b27">[28]</ref> is also based on the attention model and adopts LSTM to help learn part maps. It is not easy for us to have a good implementation for CAN. Thus, we  <ref type="figure">Figure 4</ref>. The performance of our approach and the two baseline networks (FC and Pooling) with different feature map extraction networks over CUHK03. Our approach consistently boosts the performance for all the three networks (best viewed in color). <ref type="table">Table 6</ref>. Compared with softmax over spatial responses and CAN <ref type="bibr" target="#b27">[28]</ref>. All are based on AlexNet. Larger is better. report the results with AlexNet, which CAN is based on, as our base network. The comparison is given in <ref type="table">Table 6</ref>.</p><p>We can see that the overall performance of our approach is better except on the CUHK01 dataset for 100 test IDs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with State-of-the-Arts</head><p>Market-1501. We compare our method with recent state-of-the-arts, which are separated into four categories: feature extraction (F), metric learning (M), deeply learnt feature representation (DF), deep learning with matching subnetwork (DMN). The results in <ref type="table" target="#tab_8">Table 7</ref> are obtained under the single query setting. The competitive algorithm, pose-invariant embedding (PIE) <ref type="bibr" target="#b63">[63]</ref> extracts part-aligned representation, based on state-of-the-art pose estimator CPM <ref type="bibr" target="#b47">[47]</ref> for part detection that is different from ours. PIE uses ResNet-50 which is more powerful than GoogLeNet our approach uses. We observe that our approach performs the best and outperforms PIE: 2.35 gain for rank-1 and 9.5 gain for mAP compared to PIE w/o using KISSME, and 1.67 for rank-1 and 7.4 gain for mAP compared to PIE w/ using KISSME.</p><p>CUHK03. There are two versions of person boxes: one is manually labeled and the other one is detected with a pedestrian detector. We report the results for both versions and all the previous results on CUHK03 are reported on the labeled version. The results are given in <ref type="table" target="#tab_9">Table 8</ref> for manuallylabeled boxes and in <ref type="table" target="#tab_10">Table 9</ref> for detected boxes.  Our approach performs the best on both versions. On the one hand, the improvement over the detected boxes is more significant than that over the manually-labeled boxes. This is because the person body parts in the manually-labeled boxes are spatially distributed more similarly. On the other hand, the performance of our approach over the manuallylabeled boxes are better than that over the detected-labeled boxes. This means that the person position in the box (manually-labeled boxes are often better) influences the part extraction quality, which suggests that it is a necessity to learn a more robust part extractor with more supervision information or over a larger dataset.</p><p>Compared with the competitive method DCSL <ref type="bibr" target="#b59">[59]</ref> which is also based on the GoogLeNet, the overall performance of our approach, as shown in <ref type="table" target="#tab_9">Table 8</ref>, is better on CUHK03 except that the rank-5 score of DCSL is slightly better by 0.1%. This is an evidence demonstrating the powerfulness of the part-aligned representation though DCSL adopts the strong matching subnetwork to improve the matching quality. Compared with the second best method, PIE, on the detected case as shown in <ref type="table" target="#tab_10">Table 9</ref>, our approach achieves 4.5 gain at rank-1.</p><p>CUHK01. There are two evaluation settings <ref type="bibr" target="#b0">[1]</ref>: 100 test IDs, and 486 test IDs. Since there are a small number (485) of training identities for the case of 486 test IDs, as done   <ref type="table" target="#tab_2">Table 10</ref> and <ref type="table" target="#tab_2">Table 11</ref>, respectively. Our approach performs the best among the algorithms w/o using matching subnetwork. Compared to the competitive algorithm DCSL <ref type="bibr" target="#b59">[59]</ref> that uses matching subnetwork, we can see that for 100 test IDs, our approach performs better in general except a slightly low rank-1 score and that for 486 test IDs our initial approach performs worse and with a simple trick, removing one pooling layer to double the feature map size, the performance is much closer. One notable point is that our approach is advantageous in scaling up to large datasets.</p><p>VIPeR. The dataset is relatively small and the training images are not enough for training. We fine-tune the model learnt from CUHK03 following <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b0">1]</ref>. The results are presented in <ref type="table" target="#tab_2">Table 12</ref>. Our approach outperforms other deep learning-based approaches except PIE <ref type="bibr" target="#b63">[63]</ref> with complicated schemes while performs poorer than the bestperformed feature extraction approach GOG <ref type="bibr" target="#b31">[31]</ref> and met- ric learning method SCSP <ref type="bibr" target="#b2">[3]</ref>. In comparison with PIE <ref type="bibr" target="#b63">[63]</ref>, our approach performs better than PIE with data augmentation Mirror <ref type="bibr" target="#b7">[8]</ref> and metric learning MFA <ref type="bibr" target="#b55">[55]</ref> and lower than PIE with a more complicated fusion scheme, which our approach might benefit from. In general, the results suggest that like in other tasks, e.g., classification, training deep neural networks from a small data is still an open and challenging problem.</p><p>Summary. The overall performance of our approach is the best in the category of deeply-learnt feature representation (DF) and better than non-deep learning algorithms except in the small dataset VIPeR. In comparison to the category of deep learning with matching subnetwork (DMN), our approach in general is good, and performs worse than DCSL in CUHK01 with 486 test IDs. It is reasonable as matching network is more complicated than the simple Euclidean distance in our approach. One notable advantage is that our approach is efficient in online matching and cheap in storage, while DCSL stores large feature maps of gallery images for online similarity computation, resulting in larger storage cost and higher online computation cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we present a novel part-aligned representation approach to handle the body misalignment problem. Our formulation follows the idea of attention models and is in a deep neural network form, which is learnt only from person similarities without the supervision information about the human parts. Our approach aims to partition the human body instead of the human image box into grids or strips, and thus is more robust to pose changes and different human spatial distributions in the human image box and thus the matching is more reliable. Our approach learns more useful body parts for person re-identification than separate body part detection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Illustrating the part net. It consists of K branches. Each branch takes the image feature map as the input and estimates a part map, which is used for weighting the image feature map followed by an average pooling operator. The part features from the K branches are concatenated as the final human representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(c,d), the first two masks are very close. In contrast, in the examples of Figure 3 (b), the masks are different, and are different regions of the top, though all are about the top clothing. In this sense, the first three masks act like a mixture model to describe the top clothing as the top parts are various due to pose and view variation. Similarly, Map4 and Map6 are both about the bottom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>The performance (%) of our approach and spatial partition based methods (stripe and grid) over Market-1501 and CUHK03.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Method rank-1</cell><cell cols="3">rank-5 rank-10 rank-20</cell></row><row><cell>Market-1501</cell><cell>ours stripe grid</cell><cell>81.0 74.1 73.4</cell><cell>92.0 89.0 88.2</cell><cell>94.7 92.3 91.8</cell><cell>96.4 95.1 94.4</cell></row><row><cell>CUHK03</cell><cell>ours stripe grid</cell><cell>85.4 81.4 78.2</cell><cell>97.6 97.1 96.7</cell><cell>99.4 99.3 99.2</cell><cell>99.9 99.7 99.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>The performance of our approach, and separate part segmentation over Market-1501 and CUHK-03.</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell cols="4">rank-1 rank-5 rank-10 rank-20</cell></row><row><cell>Market-1501</cell><cell>ours (6 parts) part seg. (6 parts)</cell><cell>80.4 61.2</cell><cell>91.5 80.3</cell><cell>94.3 86.9</cell><cell>96.4 91.0</cell></row><row><cell>CUHK03</cell><cell>ours (6 parts) part seg. (6 parts)</cell><cell>85.1 70.7</cell><cell>97.6 90.4</cell><cell>98.2 94.8</cell><cell>99.4 97.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>The validation performance with different numbers (K) of parts over CUHK03. The model is trained over a random half of the training data, and the performance is reported over the remaining half (as the validation set). The best results are in bold.#parts rank-1 rank-5 rank-10 rank-20 The performances of our approach and human segmentation over Market-1501 and CUHK03.</figDesc><table><row><cell>1</cell><cell>77.7</cell><cell>95.6</cell><cell>98.4</cell><cell>99.7</cell><cell></cell></row><row><cell>2</cell><cell>80.4</cell><cell>96.7</cell><cell>98.4</cell><cell>99.4</cell><cell></cell></row><row><cell>4</cell><cell>82.0</cell><cell>96.7</cell><cell>98.8</cell><cell>99.7</cell><cell></cell></row><row><cell>8</cell><cell>83.8</cell><cell>96.9</cell><cell>98.3</cell><cell>99.7</cell><cell></cell></row><row><cell>12</cell><cell>83.6</cell><cell>97.3</cell><cell>98.8</cell><cell>99.6</cell><cell></cell></row><row><cell>Dataset</cell><cell>Method</cell><cell cols="2">rank-1 rank-5</cell><cell>rank-10</cell><cell>mAP</cell></row><row><cell>Market-1501</cell><cell>ours human seg.</cell><cell>81.0 74.2</cell><cell>92.0 90.0</cell><cell>94.7 93.8</cell><cell>63.4 58.9</cell></row><row><cell>CUHK03</cell><cell>ours human seg.</cell><cell>85.4 82.7</cell><cell>97.6 95.9</cell><cell>99.4 97.9</cell><cell>90.9 88.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>The performances of our approach, two baseline networks without segmentation, modified by replacing the part net in our network with a fully-connected (FC) layer and an average pooling (pooling) layer over Market-1501 and CUHK03.</figDesc><table><row><cell>Dataset</cell><cell cols="4">Method rank-1 rank-5 rank-10</cell><cell>mAP</cell></row><row><cell>Market-1501</cell><cell>ours FC pooling</cell><cell>81.0 75.9 75.9</cell><cell>92.0 89.3 89.0</cell><cell>94.7 92.9 92.2</cell><cell>63.4 54.3 55.6</cell></row><row><cell>CUHK03</cell><cell>ours FC pooling</cell><cell>85.4 80.3 82.4</cell><cell>97.6 95.5 96.8</cell><cell>99.4 98.6 99.0</cell><cell>90.9 87.3 88.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 .</head><label>7</label><figDesc>Performance comparison of state-of-the-art methods on the recently released challenging dataset, Market-1501. The methods are separated into four categories: feature extraction (F), metric learning (M), deeply learnt feature representation (DF), deep learning with matching subnetwork (DMN).</figDesc><table><row><cell></cell><cell>Method</cell><cell cols="4">rank-1 rank-5 rank-10 mAP</cell></row><row><cell>F</cell><cell>LOMO [26] (CVPR15) BoW [64] (ICCV15)</cell><cell>26.1 35.8</cell><cell>-52.4</cell><cell>-60.3</cell><cell>7.8 14.8</cell></row><row><cell>M</cell><cell>KISSME [20] (CVPR12) WARCA [18] (ECCV16) TMA [30] (ECCV16) SCSP [3] (CVPR16) DNS [57] (CVPR16)</cell><cell>44.4 45.2 47.9 51.9 55.4</cell><cell>63.9 68.2 -72.0 -</cell><cell>72.2 76.0 -79.0 -</cell><cell>20.8 -22.3 26.4 29.9</cell></row><row><cell>DMN</cell><cell>PersonNet [49] (ArXiv16) Gated S-CNN [43] (ECCV16)</cell><cell>37.2 65.9</cell><cell>--</cell><cell>--</cell><cell>18.6 39.6</cell></row><row><cell>DF</cell><cell cols="2">78.65 PIE [63] + KISSME [20] (Arxiv 2016) 79.33 PIE [63] SSDAL [41] (ECCV16) 39.4 Our Method 81.0</cell><cell>90.26 90.76 -92.0</cell><cell>93.59 94.41 -94.7</cell><cell>53.87 55.95 19.6 63.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 .</head><label>8</label><figDesc></figDesc><table><row><cell></cell><cell cols="5">Performance comparison on CUHK03 for manually la-</cell></row><row><cell cols="2">beled human boxes. Method</cell><cell cols="4">rank-1 rank-5 rank-10 rank-20</cell></row><row><cell>F</cell><cell>BoW [64] (ICCV15) LOMO [26] (CVPR15) GOG [31] (CVPR16)</cell><cell>18.9 52.2 67.3</cell><cell>36.2 82.2 91.0</cell><cell>46.8 92.1 96.0</cell><cell>-96.3 -</cell></row><row><cell>M</cell><cell>KISSME [20] (CVPR12) SSSVM [58] (CVPR16) DNS [57] (CVPR16) Ensembles [33] (CVPR15) WARCA [18] (ECCV16)</cell><cell>47.9 57.0 58.9 62.1 78.4</cell><cell>69.3 84.8 85.6 89.1 94.6</cell><cell>78.9 92.5 92.5 94.3 97.5</cell><cell>87.0 96.4 96.3 97.8 99.1</cell></row><row><cell>DMN</cell><cell>DeepReID [23] (CVPR14) IDLA [1] (CVPR15) PersonNet [49] (ArXiv16) DCSL [59] (IJCAI16)</cell><cell>20.7 54.7 64.8 80.2</cell><cell>51.3 86.4 89.4 97.7</cell><cell>68.7 93.9 94.9 99.2</cell><cell>83.1 98.1 98.2 99.8</cell></row><row><cell>DF</cell><cell>Deep Metric [37] (ECCV16) Our Method</cell><cell>61.3 85.4</cell><cell>88.5 97.6</cell><cell>96.0 99.4</cell><cell>99.0 99.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 .</head><label>9</label><figDesc>Performance comparison on CUHK03 for detected boxes.Table 10. Performance comparison on CUHK01 for 100 test IDs.</figDesc><table><row><cell></cell><cell>Method</cell><cell cols="4">rank-1 rank-5 rank-10 rank-20</cell></row><row><cell>F</cell><cell>LOMO [26] (CVPR15) GOG [31] (CVPR16)</cell><cell>46.3 65.5</cell><cell>78.9 88.4</cell><cell>88.6 93.7</cell><cell>94.3 -</cell></row><row><cell>M</cell><cell>LMNN [48] (NIPS05) KISSME [20] (CVPR12) SSSVM [58] (CVPR16) DNS [57] (CVPR16)</cell><cell>6.3 11.7 51.2 53.7</cell><cell>17.5 33.9 81.5 83.1</cell><cell>28.2 48.2 89.9 93.0</cell><cell>45.0 65.0 95.0 94.8</cell></row><row><cell>DMN</cell><cell>DeepReID [23] (CVPR14) IDLA [1] (CVPR15) SIR-CIR [45] (CVPR16)</cell><cell>19.9 45.0 52.2</cell><cell>50.0 76.0 85.0</cell><cell>64.0 83.5 92.0</cell><cell>78.5 93.2 97.0</cell></row><row><cell>DF</cell><cell cols="2">PIE [63] + KISSME [20] (Arxiv 2016) 67.10 52.1 Deep Metric [37] (ECCV16) Our Method 81.6</cell><cell>92.20 84.0 97.3</cell><cell>96.60 92.0 98.4</cell><cell>98.10 96.8 99.5</cell></row><row><cell></cell><cell>Method</cell><cell cols="4">rank-1 rank-5 rank-10 rank-20</cell></row><row><cell>DMN</cell><cell>DeepReID [23] (CVPR14) IDLA [1] (CVPR15) Deep Ranking ( [6] (TIP16)) PersonNet [49] (ArXiv16) SIR-CIR [45] (CVPR16) DCSL [59] (IJCAI16)</cell><cell>27.9 65.0 50.4 71.1 71.8 89.6</cell><cell>58.2 88.7 70.0 90.1 91.6 97.8</cell><cell>73.5 93.1 84.8 95.0 96.0 98.9</cell><cell>86.3 97.2 92.0 98.1 98.0 99.7</cell></row><row><cell>DF</cell><cell>Deep Metric [37] (ECCV16) Our Method</cell><cell>69.4 88.5</cell><cell>90.8 98.4</cell><cell>96.0 99.6</cell><cell>-99.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 .</head><label>11</label><figDesc>Performance comparison on CUHK01 for 486 test IDs. , 59], we fine-tune the model, which is learnt from the CUHK03 training set, over the 485 training identities: the rank-1 score from the model learnt from CUHK03 is 44.59% and it becomes 72.3% with the fine-tuned model.</figDesc><table><row><cell></cell><cell>Method</cell><cell cols="4">rank-1 rank-5 rank-10 rank-20</cell></row><row><cell>F</cell><cell>Semantic [38] (CVPR15) MirrorRep [8] (IJCAI15) LOMO [26] (CVPR15) GOG [31] (CVPR16)</cell><cell>31.5 40.4 49.2 57.8</cell><cell>52.5 64.6 75.7 79.1</cell><cell>65.8 75.3 84.2 86.2</cell><cell>77.6 84.1 90.8 92.1</cell></row><row><cell>M</cell><cell>LMNN [48] (NIPS05) SalMatch [60] (ICCV13) DNS [57] (CVPR16) WARCA [18] (ECCV16) SSSVM [58] (CVPR16)</cell><cell>13.5 28.5 65.0 65.6 66.0</cell><cell>31.3 45.9 85.0 85.3 89.1</cell><cell>42.3 55.7 89.9 90.5 92.8</cell><cell>54.1 68.0 94.4 95.0 96.5</cell></row><row><cell>DMN</cell><cell>IDLA [1] (CVPR15) Deep Ranking [6] (TIP16) DCSL [59] (IJCAI16)</cell><cell>47.5 50.4 76.5</cell><cell>71.6 70.0 94.2</cell><cell>80.3 84.8 97.5</cell><cell>87.5 92.0 -</cell></row><row><cell>DF</cell><cell>TCP-CNN [9] (CVPR16) Our Method Our Method + remove pool3</cell><cell>53.7 72.3 75.0</cell><cell>84.3 91.0 93.5</cell><cell>91.0 94.9 95.7</cell><cell>96.3 97.2 97.7</cell></row><row><cell>in [1, 6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>The results are reported in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 12 .</head><label>12</label><figDesc>Results on a relatively small dataset, VIPeR.</figDesc><table><row><cell></cell><cell>Method</cell><cell cols="4">rank-1 rank-5 rank-10 rank-20</cell></row><row><cell>F</cell><cell>ELF [15] (ECCV 2008) BoW [64] (ICCV15) LOMO [26] (CVPR15) Semantic [38] (CVPR15) MirrorRep [8] (IJCAI15) GOG [31] (CVPR16)</cell><cell>12.0 21.7 40.0 41.6 43.0 49.7</cell><cell>44.0 42.0 68.1 71.9 75.8 79.7</cell><cell>47.0 50.0 80.5 86.2 87.3 88.7</cell><cell>61.0 60.9 91.1 95.1 94.8 94.5</cell></row><row><cell>M</cell><cell>LMNN [48] (NIPS05) KISSME [20] (CVPR12) LADF [25] (CVPR13) WARCA [18] (ECCV16) DNS [57] (CVPR16) SSSVM [58] (CVPR16) TMA [30] (ECCV16) SCSP [3] (CVPR16)</cell><cell>11.2 19.6 30.0 37.5 42.3 42.7 43.8 53.5</cell><cell>32.3 47.5 64.7 70.8 71.5 --82.6</cell><cell>44.8 62.2 79.0 82.0 82.9 84.3 83.9 91.5</cell><cell>59.3 77.0 91.3 92.0 92.1 91.9 91.5 96.7</cell></row><row><cell>DMN</cell><cell>IDLA [1] (CVPR15) Gated S-CNN [43] (ECCV16) SSDAL [41] (ECCV16) SIR-CIR [45] (CVPR16) Deep Ranking [6] (TIP16) DCSL [59] (IJCAI16)</cell><cell>34.8 37.8 37.9 35.8 38.4 44.6</cell><cell>63.6 66.9 65.5 67.4 69.2 73.4</cell><cell>75.6 77.4 75.6 83.5 81.3 82.6</cell><cell>84.5 -88.4 -90.4 91.9</cell></row><row><cell>DF</cell><cell>PIE [63] + Mirror [8] + MFA [55] (Arxiv 2016) Fusion [63] + MFA [55] (Arxiv 2016) Deep Metric [37] (ECCV16) TCP-CNN [9] (CVPR16) Our Method</cell><cell>43.3 54.5 40.9 47.8 48.7</cell><cell>69.4 84.4 67.5 74.7 74.7</cell><cell>80.4 92.2 79.8 84.8 85.1</cell><cell>90.0 96.9 -91.1 93.0</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">In this paper, we use the two terms, part and region, interchangeably for the same meaning.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The gradient at the non-differentiable point is omitted like the common way to handle this case in deep learning.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An improved deep learning architecture for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Person re-identification using spatial covariance regions of human body parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Corvée</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Brémond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thonnat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AVSS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="435" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Similarity learning with spatial constraints for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Similarity learning on an explicit polynomial kernel feature map for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1565" to="1573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep ranking for person re-identification via joint representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2353" to="2367" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Detect what you can: Detecting and representing objects using holistic models and body parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1971" to="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mirror representation for modeling view-specific transform in person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3402" to="3408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Person re-identification by multi-channel parts-based cnn with improved triplet loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Custom pictorial structures for re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stoppa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep feature learning with relative distance comparison for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2993" to="3003" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Person re-identification by symmetry-driven accumulation of local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Farenzena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010-06" />
			<biblScope unit="page" from="2360" to="2367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Person re-identification ranking optimisation by discriminant context information analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Martinel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Micheloni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gardel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Evaluating appearance models for recognition, reacquisition, and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Workshop on PETS</title>
		<meeting>IEEE International Workshop on PETS</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Viewpoint invariant pedestrian recognition with an ensemble of localized features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno>abs/1408.5093</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Super-resolution person re-identification with semi-coupled low-rank discriminant dictionary learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Scalable metric learning via weighted approximate rank component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Person reidentification by unsupervised 1 graph learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="178" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Large scale metric learning from equivalence constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Köstinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2288" to="2295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Human reidentification with transferred metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multiscale learning for low-resolution person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning locally-adaptive decision functions for person verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3610" to="3617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient psd constrained asymmetric metric learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">End-to-end comparative attention networks for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1606.04404</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Local descriptors encoded by fisher vectors for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="413" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Temporal model adaptation for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Martinel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Micheloni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="858" to="877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hierarchical gaussian descriptor for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pcca: A new approach for distance learning from sparse pairwise constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mignon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2666" to="2672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning to rank in person re-identification with metric ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paisitkriangkrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised cross-dataset transfer learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Person re-identification with correspondence structure learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Embedding deep metric for person re-identification: A study against large variations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="732" to="748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Transferring a semantic representation for person re-identification and search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multi-task learning with low rank attribute embedding for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep attributes driven multi-camera person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="475" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Gated siamese convolutional neural network architecture for human reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Varior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haloi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="791" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A siamese long short-term memory architecture for human reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Varior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="135" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Joint learning of single-image and cross-image representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Human-in-the-loop person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="405" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4724" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1473" to="1480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Personnet: Person re-identification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
		<idno>abs/1601.07255</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">An enhanced deep feature representation for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning deep feature representations with domain guided dropout for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">End-toend deep learning for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1604.01850</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Human re-identification by matching compositional template with cluster sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3152" to="3159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Graph embedding and extensions: A general framework for dimensionality reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="40" to="51" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deep metric learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning a discriminative null space for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Sample-specific svm learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Semantics-aware deep correspondence structure learning for robust person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3545" to="3551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Person re-identification by salience matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning mid-level filters for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">MARS: A video benchmark for large-scale person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="868" to="884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Pose invariant embedding for deep person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno>abs/1701.07732</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Query-adaptive late fusion for image search and person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Partial person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
