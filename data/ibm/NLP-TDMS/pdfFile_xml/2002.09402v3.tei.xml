<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ADDRESSING SOME LIMITATIONS OF TRANSFORMERS WITH FEEDBACK MEMORY</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
							<email>angelafan@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<country>† LORIA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
							<email>thibautlav@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<country>† LORIA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
							<email>egrave@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<country>† LORIA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
							<email>ajoulin@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<country>† LORIA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<country>† LORIA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ADDRESSING SOME LIMITATIONS OF TRANSFORMERS WITH FEEDBACK MEMORY</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformers have been successfully applied to sequential, auto-regressive tasks despite being feedforward networks. Unlike recurrent neural networks, Transformers use attention to capture temporal relations while processing input tokens in parallel. While this parallelization makes them computationally efficient, it restricts the model from fully exploiting the sequential nature of the input. The representation at a given layer can only access representations from lower layers, rather than the higher level representations already available. In this work, we propose the Feedback Transformer architecture that exposes all previous representations to all future representations, meaning the lowest representation of the current timestep is formed from the highest-level abstract representation of the past. We demonstrate on a variety of benchmarks in language modeling, machine translation, and reinforcement learning that the increased representation capacity can create small, shallow models with much stronger performance than comparable Transformers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In recent years, the Transformer architecture <ref type="bibr" target="#b40">(Vaswani et al., 2017)</ref> has brought large improvements to a wide range of Natural Language Processing tasks such as machine translation, sentence representation <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref>, and summarization <ref type="bibr" target="#b7">(Edunov et al., 2019)</ref>. Transformers are also successfully used as an autoregressive model on sequential tasks such as language modeling <ref type="bibr" target="#b4">(Dai et al., 2019;</ref><ref type="bibr" target="#b31">Rae et al., 2020)</ref> and reinforcement learning <ref type="bibr" target="#b29">(Parisotto et al., 2019)</ref>. Unlike more traditional recurrent architectures such as RNNs and LSTMs, the Transformer architecture processes a sequence in parallel in an order-invariant way. Techniques such as position embeddings <ref type="bibr" target="#b37">(Sukhbaatar et al., 2015;</ref><ref type="bibr" target="#b35">Shaw et al., 2018)</ref> and attention masking are required to capture input order information. In this work, we focus on several limitations of the Transformer architecture as an autoregressive model and present a straightforward solution -Feedback memory. These limitations and our proposed solution target sequential token prediction tasks, such as language modeling or other auto-regressive generative tasks.</p><p>The feedforward nature of Transformers makes them efficient on modern hardware, but restricts the Transformer from taking full advantage of the input's sequential property. In particular, the current hidden representation of a Transformer only accesses the past representations of lower layers, even though higher level representations of the past have already been computed as an autoregressive model. At generation, the Transformer generates only one token at a time, so it could access these representations for better performance, but does not exploit these at training time due to parallelization. However, if these past higher level representations could be used at training time, they would enrich future lower level representations, enabling shallower models to have the same representation power.</p><p>Another inherent limitation of Transformers on sequential tasks is the lack of recursive computation <ref type="bibr" target="#b5">(Dehghani et al., 2018)</ref>, and the number of transformations possible on the input is bounded by the model depth. Such disadvantages have impact on tasks that require careful tracking of a world state or modeling hierarchical structures <ref type="bibr" target="#b39">(Tran et al., 2018;</ref><ref type="bibr" target="#b12">Hahn, 2020)</ref>. On the other hand, while RNNs can maintain an internal state for an unbounded time while accumulating more computations upon it, the size of this internal state is limited by the dimension of the hidden state.</p><p>In this work, we propose a novel autoregressive model, the Feedback Transformer, that makes all previous hidden representations accessible to the computation of a representation at any depth -the model feeds back previous computations to itself. The feedback allows the model to perform recursive computation, building stronger representations iteratively upon previous states. To achieve this, we modify self-attention to attend to higher level representations rather than lower ones.</p><p>As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the Feedback Transformer merges the hidden states from all layers into a single vector for every time step and stores them in a memory. Instead of self-attention, all subsequent layers attend to this memory, which means every previously computed representation is accessible by all future layers, mediated by the memory. This allows Feedback Transformers to recursively compute and transform an input as many times as the input length, which is something Transformers cannot achieve. While RNNs can perform recursive computation, the amount of information that Feedback Transformers can maintain is not limited by the number of layers.</p><p>There are computational benefits to this straightforward modification. First, it uses less memory because all the layers share a single Feedback memory, thus reducing the memory size by L times, where L is the number of layers. There is also less computation because we share the key and value projections during attention computation, which increases the speed of the attention over the Feedback Memory. Further, the GPU memory usage is reduced due to the memory sharing -the overall model is 2x smaller -allowing the batch size to be increased for computational efficiency. During inference, the increased batch size contributes to substantially faster decoding speeds.</p><p>In summary, our main contributions are: (1) The Feedback Transformer architecture, which completely changes the way a Transformer works to access available higher level representations immediately.</p><p>(2) We show the Feedback Transformer can achieve state of the art results with smaller, shallower models that have faster decoding speed and smaller memory footprint. (3) The Feedback Transformer uses substantially less memory during training and inference time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Several previous works have analyzed the limitations of Transformer architectures, such as the inability to process input sequentially <ref type="bibr" target="#b5">(Dehghani et al., 2018)</ref> or represent hierarchical structure <ref type="bibr" target="#b39">(Tran et al., 2018)</ref>. <ref type="bibr" target="#b12">Hahn (2020)</ref> demonstrate that Transformers cannot model structures involving bounded recursion, such as closing parentheses. <ref type="bibr" target="#b30">Pérez et al. (2019)</ref> study Transformers in the context of Turing machines, where they must produce unbounded numbers of decoding steps. Various work in probing Transformers identified several limitations where Transformers may not have the computational capacity of recurrent architecture like an LSTM <ref type="bibr" target="#b12">(Hahn, 2020)</ref>.</p><p>From the architectural perspective, our work shares similarities with recurrent networks augmented with external shared memories <ref type="bibr" target="#b11">(Graves et al., 2014;</ref><ref type="bibr" target="#b17">Joulin &amp; Mikolov, 2015;</ref><ref type="bibr" target="#b37">Sukhbaatar et al., 2015)</ref>. For example, the stack augmented RNN of <ref type="bibr" target="#b17">Joulin &amp; Mikolov (2015)</ref> adds an external memory to a recurrent network to keep long term dependencies. Closer to our work, the Neural Turing Machine of <ref type="bibr" target="#b11">Graves et al. (2014)</ref> models an unconstrained memory that resembles the self-attention layer of a Transformer. Further improvements to recurrent networks, such as the Gated Feedback RNN <ref type="bibr" target="#b3">(Chung et al., 2015)</ref>, are based on better controlling signal from different layers and extended to feedback through multiple pathways <ref type="bibr" target="#b16">(Jin et al., 2017)</ref>. These works are built on recurrent networks with additional components to store long term dependencies.</p><p>Other works have studied modifications to the Transformer architecture by enriching its structure with components inspired by recurrent networks. For example,  propose adding a local recurrent sublayer to the Transformer layer to remove the need for position embeddings in the multi-head self-attention layers. Universal Transformer <ref type="bibr" target="#b5">(Dehghani et al., 2018)</ref> share the parameters between the layers of a Transformer, leading a recurrent network in depth. <ref type="bibr" target="#b13">Hao et al. (2019)</ref> and <ref type="bibr" target="#b1">Chen et al. (2018)</ref> augment Transformers with a second, recurrent encoder. As opposed to our work, these prior investigations do not change the computational path in a Transformer to reduce the discrepancy between the training and inference time. Closer to our work, <ref type="bibr" target="#b25">Merity (2019)</ref> proposes adding a self-attention layer on top of the past outputs from an LSTM cell. However, this approach keeps the recurrent and the self-attention mechanisms decoupled, as opposed to ours which makes the attention mechanism recurrent. In particular, the LSTM layer of Merity (2019) still intrinsically has a bottleneck corresponding to the dimension of the hidden layer.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>In this section, we propose the Feedback Transformer, which provides capacity to build richer representations of each timestep t of a sequential modeling task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">TRANSFORMER ARCHITECTURES</head><p>We briefly describe the Transformer <ref type="bibr" target="#b40">(Vaswani et al., 2017)</ref>. Each layer is composed of a multihead self-attention sublayer (Attn) followed by a feedforward sublayer (FF), and each sublayer is followed by an add-norm operation that combines a skip-connection <ref type="bibr" target="#b14">(He et al., 2016)</ref> and layer normalization <ref type="bibr" target="#b22">(Lei Ba et al., 2016)</ref>. The l-th layer of a Transformer processes an input sequence of vectors X l = (x l 1 , . . . , x l t ) into a sequence of vectors of the same length. First, the self-attention sublayer computes a representation for each time step t by taking its related input vector x t along with its past context, {x l t−τ , ..., x l t−1 }: z l t = Attn(x l t , {x l t−τ , . . . , x l t−1 }). Within the self-attention sublayer, x l t is used to form query vectors while its context is used to compute key and value vectors, forming a memory of the past information. Then the feedforward sublayer processes each vector z l t independently, i.e., x l+1 t = FF(z l t ). The Transformer layer transforms its input sequence into an output sequence X l+1 = FF(Attn(X l )).</p><p>In practice, a block of steps {x l t−M +1 , . . . , x l t } is computed in parallel during training, where M can be seen as the backpropagation through time (BPTT) length. This makes training Transformers efficient on hardware such as GPUs. However, to operate on sequences of unbounded length, Transformers require modifications such as caching and relative position embeddings <ref type="bibr" target="#b4">(Dai et al., 2019;</ref><ref type="bibr" target="#b38">Sukhbaatar et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">LIMITATIONS OF TRANSFORMERS</head><p>Previous work has analyzed the impact of several limitations of the Transformer architecture, such as the inability to track long sequences and process hierarchical inputs <ref type="bibr" target="#b12">(Hahn, 2020)</ref>. In this work, we focus on two major limitations of Transformer architectures.</p><p>Limited Access to Higher Level Representations. Layer by layer, Transformers build more abstract, high level representations of the input sequence. At each layer, the representations for the input sequence are treated in parallel. As a consequence, a Transformer does not leverage the highest level representations from the past to compute the current representation, even though these highest level representations have already been computed for autoregressive models.</p><p>Maintaining a Belief State. Many sequential tasks require models to maintain an internal state for two main purposes. First, internal states act as memory for recalling past inputs, where Transformers excel because their internal state x l t is directly accessible to future steps through self-attention.</p><p>The second role of an internal state is to act as a belief state that tracks the world state that is not directly observable in inputs. For example, when inputs are actions taken on a Markov Decision Process, an internal state can apply those changes to the current belief state and correctly predict the outcome. As a feedforward model, Transformer have inherent limitations in this area -only a fixed number of transformations can be applied to its internal states. Since both Attn and FF sublayers contain a fixed number of transformations and there are L layers of them, the total number of transformations between the input and output is limited by the depth. This means Transformers cannot maintain an internal state for long time if it has to be frequently updated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">FEEDBACK TRANSFORMER</head><p>We propose to change the Transformer architecture by using the most abstract representations from the past directly as inputs for the current timestep. This means that the model does not form its representation in parallel, but sequentially token by token. More precisely, we replace the context inputs to attention modules with memory vectors that are computed over the past, i.e.,</p><formula xml:id="formula_0">z l t = Attn(x l t , {m t−τ , . . . , m t−1 }),</formula><p>where memory vectors m t are computed by summing the representations of all layers at time step t:</p><formula xml:id="formula_1">m t = L l=0 Softmax(w l )x l t ,<label>(1)</label></formula><p>where w l are learnable scalar parameters. Note these scalars are the only new parameters introduced by our change, with all else the same as the standard Transformer. Here l = 0 corresponds to token embeddings. The weighting of different layers by a softmax output gives the model more flexibility as it can average them or select one of them.</p><p>This modification of the self-attention input adapts the computation of the Transformer from parallel to sequential, summarized in <ref type="figure" target="#fig_1">Figure 2</ref>. Indeed, it provides the ability to formulate the representation x l t+1 based on past representations from any layer l , while in a standard Transformer this is only true for l &lt; l. This change can be viewed as exposing all previous computations to all future computations, providing better representations of the input. Such capacity would allow much shallower models to capture the same level of abstraction as a deeper architecture. This has several practical advantages, as more shallow models have reduced memory footprint and increased decoding speed.</p><p>An alternative view of such an architecture modification is providing the capacity for recursive computation -outputs from a sublayer can feed back to the same sublayer through the memory. The model can then maintain an internal state for unbounded time. This is a clear advantage over Transformers, in which a submodule never looks at its own output. While an RNN can also repeat its computation on its internal state, its internal state has a limited capacity determined by the number of layers and their hidden dimension. In contrast, the internal state of a Feedback Transformer is its whole memory, which can grow with the input length. This allows the model to keep track of a large number of things within its internal state.</p><p>While our modification requires sequential computation, we significantly improve training speed by sharing the key and value projections W l k and W l v across all layers. This sharing reduces computation because we need to compute key and value vectors only once instead of computing them per layer</p><formula xml:id="formula_2">k l t = k t = W k m t v l t = v t = W v m t .</formula><p>For the same reason, the memory footprint is smaller than a standard Transformer because only one set of k t , v t needs to be stored. To be more precise, the memory requirement for processing a single token is reduced from O(L × T ) to O(T ), where L is the number of layers and T is the context size. Further, the reduced memory usage allows the batch size to be increased to recover some of the lost parallelism, which improves training speed. Thus, the Feedback Transformer is not much slower compared to the standard Transformer. Note that the same sharing of projections will not make the standard Transformer efficient because those projections are applied to different representations at each layer (the key and value vectors will not the same for all layers).</p><p>Lastly, we note that the sequential nature of the Feedback Transformer does not affect the performance during generation where one needs to compute one step at a time anyway. The same is true for online reinforcement learning where the input must be processed sequentially even during training.  <ref type="figure">Figure 3</ref>: Results on the Corridor task. The Transformer degrades as the memory size decreases, but the Feedback Transformer maintains performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We explore different sequential input tasks in natural language processing and reinforcement learning. First, we demonstrate the downsides of the standard Transformer architecture on tasks where the Transformer performs poorly. We show that the Feedback Transformer is able to overcome challenges and retain long memory. Next, we highlight the strength of the Feedback architecture in building complex, high level representations even with shallow models. We demonstrate that the Feedback model can achieve significantly stronger results than Transformer models, an effect that is exaggerated as models get smaller. Finally, we compare the Feedback architecture to the Transformer architecture with other work on standard long-context language modeling tasks. In experiments on large datasets, we use the shared key-value projections to improve training time. Additional experimental details and results can be found in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">LIMITATIONS OF TRANSFORMER: ILLUSTRATIVE TASKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">LIMITED ACCESS TO LONG MEMORY</head><p>First, we examine the Transformer's limited access to long memory on several simple, straightforward tasks that illustrate this. Unlike the standard Transformer, the Feedback architecture is able to remember information over many timesteps.</p><p>Walking down a Corridor. In this reinforcement learning task, each agent is placed at the start of a long corridor with either a blue or green object. The agent must look at the object's color, walk down the corridor, and go through the corresponding colored door at the end. The only task is to remember the color and not become distracted by walking down the very long hallway. Results are shown in <ref type="figure">Figure 3</ref> and show that the performance of the Transformer degrades quickly as the memory size shrinks, but the Feedback Transformer maintains strong performance at all memory sizes.</p><p>Copy and Reverse. We experiment next on two algorithmic tasks, copy and reverse <ref type="bibr" target="#b18">(Kaiser &amp; Sutskever, 2015)</ref>. We train on sequences of length 40 consisting of integers 0 through 9, and test on sequences of length 400. Models read the input and then either copy or reverse, which requires memory over the sequence and the ability to track position, as well as generalization capability as the train and test settings are different lengths. We consider two variations of copying and reversing: either at the character level or at the sequence level. Results are shown in <ref type="table" target="#tab_0">Table 1</ref>. The Feedback architecture has large improvements in accuracy, indicating improved memory and positional tracking.</p><p>Counting. Finally, we experiment on a counting task, where models have a sequence of A's in a row, and must output the corresponding quantity of the letter B. The model must count the number of the A's to output the correct number of B's. We consider two settings: training on short sequences of lengths up to 50 and training on long sequences of lengths up to 1000. We show results in <ref type="table" target="#tab_0">Table 1</ref>, where we demonstrate the Feedback model is much better at counting over long sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">LIMITED STATE UPDATES</head><p>The complexity of the representations the Transformer is able to formulate is strictly dependent on the depth, as each layer of the Transformer allows for additional nonlinearity. The Transformer, then, can only update its state the same number of times as it has layers. We demonstrate that the Feedback Transformer does not have this limitation -in tasks where the model must carefully track and update its state, the Feedback architecture is able to update its state at each timestep.</p><p>Random Walk. We consider a random walk in a small grid where actions are: go forward 1 step, left turn, and right turn. Given a history of actions and the agent's initial position, it is strictly possible to calculate the current position. The task is trivial because a human could write down the current location and direction and keep updating with each action. However, Transformers cannot do this because they lack a storage that can be updated with each input. Its hidden state can store this information, but with each update, that information has to go up one layer.</p><p>An alternative approach to this task is to solve it all at once given a sequence of actions, which is feasible for Transformers since they can access all inputs with their attention. However, this approach is challenging because the effect of each action depends on the direction at that point and whether the agent is on the edges, which itself is not known yet. This can be seen in <ref type="table" target="#tab_0">Table 1</ref>, where the Transformer struggles and only reaches 68% accuracy. In contrast, the Feedback Transformer achieves 100% accuracy, which indicates the ability to track state for a long period of time. Both models are trained on 10K sequences, each containing 100 random actions and positions.</p><p>Algorithmic task. A more complex setting where tracking and updating of a state is crucial is code executions. A model needs keep track of all variable values and update them if necessary. To demonstrate this, we create a simple algorithmic task that consists of the following simple statements: assignments (e.g. x=5), increments and decrements (e.g. y--), conditionals (e.g. if x==4: y++), and print commands (e.g. print(x)). Each task consists of 100 randomly selected statements. We consider two settings with 3 and 5 different variables.</p><p>Processing of each statement in parallel will not work because conditional statements cannot be executed without knowing the current variable value, which itself can depend on another conditional. As shown <ref type="table" target="#tab_0">Table 1</ref>, Transformers cannot solve this task because every time a variable increment or decrement, its value can only be found one layer up in the model, and eventually will be lost. Doubling their layers from 4 to 8 does help little, bringing the accuracy to 47.4% on the 3 variable version and 29.1% on the 5 variable version, but their performance is far from perfect. A recurrent model like LSTM is capable of storing a variable value while updating it, thus perform well on the 3 variables version with an accuracy of 82.8%. However, its performance drop to 32.1% when there are more variables because it has to store all their values in a single vector. The Feedback Transformer does not have this bottleneck, and can access updated variable values from the lowest layer, so it gives strong performance on this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">ADVANTAGES OF FEEDBACK ARCHITECTURE</head><p>We examined two limitations of standard Transformers that we improve upon: limited memory span and limited ability to update state. In the Feedback model, we improve on these limitations and now analyze performance on practical tasks including translation and reinforcement learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">STRONG PERFORMANCE WITH SMALL, SHALLOW MODELS</head><p>The Feedback Transformer is able to create higher level, more abstract representations with fewer layers and less capacity, as a layer can use all of the most recently created representations of previous timesteps. We demonstrate on neural machine translation that the Feedback model performs much better than Transformers at small, shallow sizes. Note that for sequence to sequence, we use Feedback Transformers only in the decoder because the encoder inputs are available simultaneously. We evaluate the performance of the Feedback Transformer on the WMT14 En-De machine translation benchmark of 4.5 million pairs. We follow <ref type="bibr" target="#b40">Vaswani et al. (2017)</ref> and train on WMT16 using newstest2013 as dev and newstest2014 as test. We learn 32K joint byte pair encodings <ref type="bibr" target="#b34">(Sennrich et al., 2016)</ref>, generate with beam size 5, tuning a length penalty on the dev set. We average the last 10 checkpoints and apply compound splitting and compute tokenized BLEU.</p><p>In <ref type="figure" target="#fig_2">Figure 4</ref> (left), we display results when making the model shallower only -layers are removed from a Feedback Transformer decoder compared to Transformers. As the decoder becomes shallow, the gap in performance between the two architectures widens. While the 1-layer Transformer model can only reach 27.3, the Feedback Transformer has 28.3 BLEU. Shallow decoders are critical to fast inference -reducing to 1-layer improves decoding speed by 4.2x, while only losing 1 BLEU with the Feedback architecture. Such results are useful for practical applications, where the speed of producing a translation is very important. We report decoding speed in tokens per second on 1 GPU.</p><p>We further experiment with large encoder but shallow decoders. The Feedback Transformer achieves 29.0 BLEU with 12 layer encoder and 2 layer decoder. As the encoder is parallelized even during inference, the increased size of the encoder has negligible impact on decoding speed. To stabilize the training of deeper models, we use LayerDrop <ref type="bibr" target="#b9">(Fan et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">LONG MEMORY TRACKS STATE</head><p>We apply Feedback to a reinforcement learning maze task that requires long memory to optimally solve because agents have limited vision. Note that in such reinforcement learning tasks, the models are trained online using A2C, so the input must be processed sequentially even during training time. Thus, the non-parallelized nature of the Feedback Transformer is not a drawback, and training Feedback Transformers is as fast as Transformers.</p><p>The goal is to navigate a procedurally generated random maze where colored objects are placed. One of the colors will be randomly selected as a target, and the agent has to reach it for a reward and a new target. For optimal performance, the agent must remember the maze and object locations. In addition, the agent has turn actions like the Random Walk task, which makes it necessary to keep track of its location and orientation. As shown in <ref type="figure" target="#fig_2">Figure 4 (right)</ref>, the Feedback Transformer converges to reach higher average reward, compared to Transformers. Results are shown averaged over 10 trials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">COMPARISON TO OTHER ARCHITECTURES</head><p>In this section, we first, we compare Feedback to recurrent architectures such as LSTM, as well as hybrid RNN-Transformer architectures, and show that the Feedback is more powerful than recurrence alone. Next, we compare our construction of the Feedback Memory with other possible compositions. Lastly, we compare to other Transformer architectures on competitive benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Test</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recurrent Architectures</head><p>DenseNMT <ref type="bibr" target="#b36">Shen et al. (2018)</ref> 25.5 RNMT+ <ref type="bibr" target="#b1">(Chen et al., 2018)</ref> 28.5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hybrid Architectures</head><p>BiARN <ref type="bibr" target="#b13">(Hao et al., 2019)</ref> 28.9 SRU <ref type="bibr" target="#b21">(Lei et al., 2017)</ref> 28.4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformer Architectures</head><p>Transformer <ref type="bibr" target="#b40">(Vaswani et al., 2017)</ref>28.4 Transformer <ref type="bibr" target="#b28">(Ott et al., 2018)</ref> 29.3 Feedback Transformer 29.5  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">COMPARISON TO RECURRENT ARCHITECTURES</head><p>We compare the Feedback Transformer architecture to recurrent architectures like LSTMs as well as hybrid RNN-Transformer architectures. In <ref type="table" target="#tab_1">Table 2</ref>, we display that the Feedback Transformer has stronger performance than the Transformer, RNN, and RNN-Transformer hybrid model. We note that recurrent models address some limitations of Transformer architectures, but the Feedback mechanism goes beyond that. By allowing all past representations to be immediately available for the computation of future representations, Feedback is stronger than Recurrence alone -Recurrent models can only see representations from the previous layer (as depicted in <ref type="table" target="#tab_1">Table 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">MEMORY COMPOSITION</head><p>We next investigate the importance of the specific memory mechanism of the Feedback architecture on char-PTB. The Feedback architecture uses all layers when creating the memory, motivated by providing access to the entire past of all computations, but other ways of creating the memory as possible. For example, Recurrent architectures have a different memory structure. In multi-layer RNNs, each layer has recurrent connections to the same layer, but not to higher layers. This is an advantage of Feedback architectures -even the highest level abstractions are immediately available.</p><p>In <ref type="figure" target="#fig_3">Figure 5</ref>, we examine the construction of the Feedback memory, comparing our choice of making all computation accessible with recurrent memory that can access all previous layers plus the same layer, and top-only memory that can attend only to the topmost layer. The Feedback Transformer has the best performance, closely matched by top-only memory. This indicates the importance of high level representations (see Appendix 6.4 for further analysis on this). Note that recurrence alone is not enough for good performance, and thus the Feedback memory provides richer representations beyond the capacity of recurrent networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">COMPARISON TO OTHER TRANSFORMER ARCHITECTURES</head><p>We examine the performance of Feedback Transformer on long context language modeling benchmarks. We use caching <ref type="bibr" target="#b4">(Dai et al., 2019)</ref> and relative position embeddings. Mechanisms applied at inference time <ref type="bibr" target="#b19">(Khandelwal et al., 2019;</ref><ref type="bibr" target="#b20">Krause et al., 2019)</ref> can further improve all models, so we do not focus on these.</p><p>Wikitext-103. We evaluate on word-level language modeling on Wikitext-103 <ref type="bibr" target="#b26">(Merity et al., 2017)</ref>. Our Feedback architecture takes 3.5 days to train, compared to the Transformer which takes 1.2 days. We train a small Feedback model, about half the size of Transformer-XL, and find that it can match the performance of Transformer-XL, as shown in <ref type="table">Table 3</ref>. This indicates the additional representational capacity of Feedback memory. If we train a standard Transformer that is approximately the same size as our Feedback Transformer, we find it has worse performance</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Params Test</head><p>Best Existing <ref type="bibr" target="#b32">(Roy et al., 2020)</ref> -15.8 Trans-XL <ref type="bibr" target="#b4">(Dai et al., 2019)</ref> 257M 18.3</p><p>Our Transformer 140M 19.9 Feedback Transformer 126M 18.3 <ref type="table">Table 3</ref>: Results on WikiText-103. We report perplexity on test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Params Test</head><p>Best Existing <ref type="bibr" target="#b31">(Rae et al., 2020)</ref> 277M 0.97 Trans-XL <ref type="bibr" target="#b4">(Dai et al., 2019)</ref> 277M 0.99</p><p>Feedback Transformer 77M 0.96  Enwiki8. Finally, we test our model on character-level language modeling in Enwiki8 <ref type="bibr" target="#b24">(Mahoney, 2011)</ref>, containing 100M unprocessed bytes from Wikipedia. We train a relatively small 12-layer model, that is one third of the size of the Transformer-XL baseline. Since the task requires very long context, we use adaptive attention span <ref type="bibr" target="#b38">(Sukhbaatar et al., 2019)</ref>. As shown in <ref type="table" target="#tab_2">Table 4</ref>, the Feedback Transformer model achieves a new SOTA performance of 0.96 bit-per-byte despite its small size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">TRAINING AND INFERENCE SPEED</head><p>Finally, we compare the training and inference speed of the Feedback Transformer with standard Transformer architectures. Results are shown in <ref type="table" target="#tab_3">Table 5</ref>. The Feedback Transformer has faster inference, because the key-value projection sharing substantially reduces the memory footprint of the model and reduces computation. Further, shallow Feedback models perform well, so the batch size can be increased. In language modeling, for example, sharing key-value provides almost 3X inference speed improvement. The shallow model size provides the remaining 10% of speed improvement at inference time. Finally, note that for certain problems (such as in RL), the data must be processed strictly sequentially anyway and Feedback Transformer is not any slower.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We propose a novel reformulation of the Transformer that fully exploits sequential input -the increased representation power and recursive computation of the Feedback Transformer allows shallow and small models to have much stronger performance compared to a Transformer of the same size. This architecture addresses two fundamental limitations of Transformers as an autoregressive model -limited access to long memory and limited ability to update state. We demonstrate on a variety of tasks the advantages of the Feedback architecture to illustrate the strong performance of this straightforward modification. 6 ADDITIONAL RESULTS</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">REINFORCEMENT LEARNING</head><p>Maze Navigation Easy. We experiment with a slightly different version of the Maze Navigation task. Instead of an agent with forward, turn-left and turn-right actions, the agent has no orientation and there are only 4 movement actions corresponding to 4 cardinal directions. This makes navigation easier because the agent do not need to keep track of its orientation. Further, it is much easier to compute relative locations given a history of actions. This might explain why standard Transformers are not far behind Feedback Transformers in performance as shown in <ref type="figure" target="#fig_4">Figure 6</ref> (left). We also compare to LSTMs, which performs much worse. See Section 7.2 for more implementation details.</p><p>Water Maze. We modify the Morris Water Maze task (Morris, 1981) to make it more challenging. The maze is defined by a goal position and a mapping of cell to ID -these remain fixed within an episode but change between episodes. The agent receives as an observation the cell IDs of its current location and the target cell. When the agent finds the target, it receives +1 reward and is randomly teleported. During the same episode, if the agent reaches a previously seen cell, it needs to remember how it reached the target from there to go back. Results are shown averaged over 10 trials (the reward is reported averaged over the last 500 episodes for each trial). As shown in <ref type="figure" target="#fig_4">Figure 6</ref> (right), the Feedback Transformer converges to higher average reward. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">IWSLT DE-EN</head><p>We additionally evaluate the Feedback Transformer on IWSLT De-En, a small machine translation dataset. We train a small Transformer model with 6 layers. For generation, we use beam size 5 without checkpoint averaging. Model quality is evaluated using tokenized BLEU. Results are shown in <ref type="figure">Figure 7</ref> (left) and show that for shallower models, the Feedback Transformer has better performance than the standard Transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">SUMMARIZATION ON CNN-DAILYMAIL</head><p>We evaluate on the CNN-Dailymail multi-sentence summarization benchmark of 280K news articles <ref type="bibr" target="#b15">Hermann et al. (2015)</ref>, modeling the first 400 words of the article <ref type="bibr" target="#b33">See et al. (2017)</ref>. We evaluate using <ref type="bibr">ROUGE Lin (2004)</ref>. and use 3-gram blocking and tune length <ref type="bibr" target="#b8">Fan et al. (2017)</ref>. <ref type="figure">Figure 7  (</ref>  <ref type="figure">Figure 8</ref>: Ablation results on char-PTB: instead of a weighted sum of all layers as Feedback memory, only a single layer is used as memory for all layers. We also include a setting where the average of all layers is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">ABLATION STUDIES ON LANGUAGE MODELS</head><p>We investigate which layer of a model has the best representation to be used as a Feedback memory. In Feedback Transformers, a weighted sum of all layers is used as the memory, and feeds to all layers. An alternative approach is to manually select one of the layers as the memory and let all layers attend to it. In <ref type="figure">Figure 8</ref>, we explore this approach, using the same 6-layer char-PTB models as Section 4.3.2 (top-only memory there corresponds to using the last 6th layer as memory). We can see that representations from higher layers work better as memory, confirming our assumption of the importance of higher level representations. Simply averaging all layers together works reasonably well as well. Interestingly, when all layer attend to the first layer output, it works as good as the standard Transformer. The weighted sum approach matches the best performance because it can adopt to select any of the layers.</p><p>Here we study how different techniques affect the model performance on WikiText-103. The results shown in <ref type="table">Table 6</ref> indicate:</p><p>• Pre-normalization combined with higher learning rates helps the performance, particularly for the standard Transformer. • Increasing the context size with adaptive span further improves the performance for both models.  isolate the effect of depth. This is achieved by proportionally increasing the head dimension and the ReLU layer size when we decrease the number of layers. The results in <ref type="figure" target="#fig_5">Figure 9</ref> demonstrate that for the standard Transformer improves as the depth increase. In contrast, the Feedback architecture is much robust reduced depth, even achieving the best performance on char-PTB with only two layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">ADDITIONAL IMPLEMENTATION DETAILS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">RANDOM WALK TASK DETAILS</head><p>We provide additional details for the random walk toy task we explore. The agent starts at a fixed position of a 8 × 8 grid. Available actions are 1) move one step forward, 2) turn left and 3) turn right. At every time step, the agent randomly picks on of the three actions and executes it. An action would be ignored if it can't be executed like going out of the grid. After 100 actions, the agent is reset back to the initial position.</p><p>The input to the model is a sequence of actions taken by the agent, and a special symbol if there was a reset. The output is a sequence of location symbols corresponding to the agent's location after each action. We generate 10k training episodes, totalling 1M tokens.</p><p>We use the same setup as our language modeling experiments, except now the model predicts separate output tokens rather than a next token. We concatenate all the episodes and feed them to the model as a single sequence. The training is done with the negative-log-likelihood loss. See <ref type="table" target="#tab_8">Table 9</ref> for the hyperparameters used in the experiment. The attention span is set to 100, so that the models can attend to all the information they needs to solve the task.  <ref type="table">Table 7</ref>: An example program from the algorithmic task with 3 variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">MAZE NAVIGATION DETAILS</head><p>We generate random 9 × 9 mazes using Kruskal's algorithm. Dead ends are eliminated by randomly removing one of the blocks surrounding them. We randomly place 8 target objects with different colors as shown in <ref type="figure" target="#fig_0">Figure 10 (left)</ref>. The agent is given a randomly selected color as a target. If the agent manages to reach the correct target, it gets a reward of +1 and a new target color is sampled. An episode ends after 200 steps. The observation includes the 3 × 3 area around the agent and target color.</p><p>We train 2-layer Transformers with a hidden size 256 and 4 heads. We set the BPTT to 100 and the batch size to 1024. The reward discount rate is 0.99. The attention span is 200 so the agent can keep an entire episode in memory. All agents were trained using A2C with Adam with a learning rate of 0.0003 and a entropy cost of 0.0005. For the easy version of the task, we use RMSprop with a batch size of 128 and a learning rate of 0.0003. The RMSProp epsilon regularization parameter is set to 0.01 The LSTM model is a 3-layer LSTM with a hidden size of 256.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">WATER MAZE DETAILS</head><p>The water maze task we designed is depicted visually in <ref type="figure" target="#fig_0">Figure 10 (</ref>   A program is generated by randomly choosing a statement one after another, but with the following conditions: a variable must be initialized before being used, and a variable value have to between 1 and 10. The training data contains 10k such programs concatenated with a special separator keyword. We generate two version the data with 3 and 5 different variables in them. An example program is shown in <ref type="table">Table 7</ref>. We used the same hyperparameters as the random walk task as show in <ref type="table" target="#tab_8">Table 9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">MACHINE TRANSLATION AND SUMMARIZATION</head><p>We detail the hyperparameters in <ref type="table" target="#tab_7">Table 8</ref>. Summarization experiments are done with the Transformer base architecture size and WMT En-De experiments are done with the Transformer big architecture size. As IWSLT De-En is a smaller dataset, we use a smaller model. For all sequence to sequence experiments, only the decoder is modified to have the Feedback Transformer architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.6">LANGUAGE MODELING</head><p>In the language modeling experiments, we added several improvements on top of the original Transformer <ref type="bibr" target="#b40">Vaswani et al. (2017)</ref> to better adapt to unbounded sequences:</p><p>• Hidden representation caching <ref type="bibr" target="#b4">Dai et al. (2019)</ref>: Since the input to the model is an unbounded sequence and the model needs to process it in small blocks, hidden representations from previous blocks are kept in cache so that any token in the current block will the same context length regardless of its position in the block. • Relative position embedding <ref type="bibr" target="#b35">Shaw et al. (2018)</ref>: Relative position embeddings allow each token in a block to be processed in the same way regardless of its absolute position in the block. We found that adding shared embeddings to key vectors at every layer to be effective. • Adaptive attention span <ref type="bibr" target="#b38">Sukhbaatar et al. (2019)</ref> Language modeling requires a model to have a very long attention span, which is computationally expensive. The adaptive span mechanism allows each attention head to learn different attention spans for efficiency. • Pre-normalization <ref type="bibr" target="#b2">Child et al. (2019)</ref>: We observed that pre-normalization makes training more stable for Transformers, which allowed us to use larger batch sizes for better parallelization.</p><p>Dropouts are applied to attention and ReLU activations. In WikiText-103 models, additional dropouts are added to the embedding layer output and the last sublayer output.</p><p>In <ref type="table" target="#tab_8">Table 9</ref>, we present the hyperparameter values used for our experiments. We use the same hyperparameters for both Transformers and Feedback Transformers, and optimize them with Adam. The final performances are obtained by finetuning the models with a 10x smaller learning rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Details on the char-PTB experiments</head><p>We trained the models for 15k updates (or earlier if the validation loss stops decreasing), and funetined them for 1k steps. We varied the depth of the models while keeping the number of parameters constant. This is achieved by changing the FF size and the head dimension inverse proportionally to the depth.</p><p>Details on the enwik8 experiments We used an adaptive span limited to 8192 tokens with a loss of 0.0000005. The training is done for 100k updates and another 10k steps is used for finetuning. The warming up BPTT length is used for speeding up the training, where the BPTT length is decreased to 64 for the first half of the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Details for Training on WikiText-103</head><p>We employed the adaptive input  and the adaptive softmax <ref type="bibr" target="#b10">Grave et al. (2017)</ref> techniques for reducing the number of parameters within word embeddings. The models are trained for 200k steps and the finetuned for additional 10k steps.</p><p>While most of the models have a fixed attention span of 512, the best performance is achieved by extending the attention span to 2048 with adaptive span loss 0.00001.</p><p>After training our models, we noticed that our tokenization method differed from others by omitting end-of-line (EOL) symbols. Since our dictionary already contained the EOL token, we were able finetune our trained models on the data with EOL tokens, rather than training them from scratch. This change alone brought about 1ppl improvement.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The Feedback Transformer merges past hidden representations from all layers into a single vector and stores it in memory.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Difference between Feedback and Transformer. t indicates the timestep and l indicates the layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>(left) Machine Translation on WMT14 En-De, test set BLEU and decoding speed in words-per-second for varying decoder depths. (right) Maze Navigation in Gridworld. We display average reward comparing Feedback Transformer to standard Transformers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Comparison of different memory composition strategies on char-PTB. The recurrent connection alone is not as effective as feedback connections from a higher layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Averaged cumulative reward during training on (left) Maze Navigation Easy and (right) Water Maze tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 :</head><label>9</label><figDesc>The performance on (left) char-PTB and (right) Wikitext-103 as a function of the model depth. The number of parameters is kept constant by increasing the width.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 :</head><label>10</label><figDesc>(left) Maze Navigation task and (right) Water Maze task.x = 1 ; print x ; x ++ ; print x ; z = 8 ; print z ; print z ; x --; if x &gt; z : z --; z ++ ; print z ; print x ; print x ; if z &lt; x : z ++ ; x ++ ; z --; x --; if z &gt; x : z --; z ++ ; if x &gt; z : z ++ ; if z &lt; 5 : y = 7 ; print x ; if x &gt; z : z ++ ; x ++ ; y = 7 ; if x &gt; 10 : x --; y --; x ++ ; z ++ ; print z ; y --; print x ; print x ; z ++ ; y ++ ; y ++ ; if z &lt; 3 : y ++ ; if x &gt; 4 : x ++ ; z --; x --; x --; print x ; y ++ ; z ++ ; y --; if x &gt; z : z --; x ++ ; z --; print x ; z ++ ; print y ; y ++ ; y --; x --; print x ; y ++ ; print y ; y --; if z &lt; x : x ++ ; if z &gt; 4 : y --; z --; x ++ ; if y &lt; x : y ++ ; print y ; print z ; z --; y --; x ++ ; y --; y ++ ; if y &gt; 3 : z --; y ++ ; if z &lt; 10 : z ++ ; z ++ ; y --; z ++ ; print z ; x --; y --; x --; x ++ ; if x &lt; 4 : y --; print y ; print z ; if z &gt; x : y --; print z ; if y &lt; x : x --; print x ; print z ; if x &lt; 4 : z --; if z &lt; y : z ++ ; z --; x --; print x ; if z &lt; x : y ++ ; print x ; print z ; y --; if z &lt; 6 : x ++ ; z --; END</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Accuracy on toy tasks. Char is character accuracy, Seq is sequence accuracy.</figDesc><table><row><cell>Task</cell><cell></cell><cell cols="2">Trans-Feedback</cell><cell></cell><cell>0.9</cell><cell></cell></row><row><cell></cell><cell></cell><cell>former</cell><cell>Trans.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Copy Reverse</cell><cell>Char Seq Char Seq</cell><cell>59.1 6.2 50.2 5.9</cell><cell>76.2 23.6 74.8 29.2</cell><cell>Success rate</cell><cell>0.8 0.6 0.7</cell><cell></cell></row><row><cell>Counting</cell><cell>Len 50</cell><cell>99.6</cell><cell>99.7</cell><cell></cell><cell></cell><cell cols="2">Transformer</cell></row><row><cell></cell><cell>Len 1K</cell><cell>82.4</cell><cell>95.3</cell><cell></cell><cell>0.5</cell><cell cols="2">Feedback Transformer</cell></row><row><cell>Random Walk</cell><cell></cell><cell>68</cell><cell>100</cell><cell></cell><cell>20</cell><cell>40 Memory size 60</cell><cell>80</cell><cell>100</cell></row><row><cell cols="2">Algorithmic 3 vars</cell><cell>33.7</cell><cell>99.1</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>5 vars</cell><cell>37.5</cell><cell>92.6</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results</figDesc><table><row><cell>on WMT En-De compar-</cell></row><row><cell>ing the Feedback Transformer to Recurrent</cell></row><row><cell>architectures, hybrid Recurrent-Transformer</cell></row><row><cell>models, and standard Transformers.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Results on Enwiki8. We report bitper-byte on test.</figDesc><table><row><cell>Task</cell><cell>Model</cell><cell cols="2">Training Speed Inference Speed</cell></row><row><cell>Language Modeling</cell><cell>Transformer Feedback Transformer</cell><cell>296K 84.4K</cell><cell>592 2176</cell></row><row><cell>Translation</cell><cell>Transformer Feedback Transformer</cell><cell>280K 126K</cell><cell>3190 5410</cell></row><row><cell>Reinforcement Learning</cell><cell>Transformer Feedback Transformer</cell><cell>22.3K 22.3K</cell><cell>--</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Results comparing Training and Inference Speed for three different tasks. For language modeling, we measure words-per-second on Wikitext-103 fixing model size and attention span. For translation, we measure words-per-second on WMT En-De, both models with a 6 layer encoder and 2 layer decoder. For RL, we measure the training frame-per-second on maze navigation (with 20 CPU cores and 1 GPU). All inference speed is reported on 1 GPU.(19.9 PPL rather than 18.3). Further, mechanisms like the Routing Transformer can be added to the Feedback Transformer as well. We focus on starting with Transformer-XL as a baseline and showing we can match the performance with a much smaller model.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>•</head><label></label><figDesc>The technique of increasing the BPTT length during training for efficiency does not affect the final performance. • The gap between two model is consistent along those variations.Next, we examine the effect of the model depth on performance on char-PTB and WikiText-103 This time, we keep the total number of parameters constant and only vary the number of layers to</figDesc><table><row><cell></cell><cell></cell><cell>Model</cell><cell cols="5">Pre-norm + Adapt. Increase dev</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">higher LR</cell><cell>span</cell><cell>BPTT</cell><cell>ppl</cell></row><row><cell></cell><cell></cell><cell>Transformer</cell><cell></cell><cell>no</cell><cell>no</cell><cell>no</cell><cell>22.9</cell></row><row><cell></cell><cell></cell><cell>Transformer</cell><cell></cell><cell>no</cell><cell>no</cell><cell>yes</cell><cell>22.9</cell></row><row><cell></cell><cell></cell><cell>Transformer</cell><cell></cell><cell>yes</cell><cell>no</cell><cell>yes</cell><cell>21.0</cell></row><row><cell></cell><cell></cell><cell>Transformer</cell><cell></cell><cell>yes</cell><cell>yes</cell><cell>no</cell><cell>20.6</cell></row><row><cell></cell><cell></cell><cell>Feedback</cell><cell></cell><cell>no</cell><cell>no</cell><cell>no</cell><cell>19.7</cell></row><row><cell></cell><cell></cell><cell>Feedback</cell><cell></cell><cell>no</cell><cell>no</cell><cell>yes</cell><cell>19.9</cell></row><row><cell></cell><cell></cell><cell>Feedback</cell><cell></cell><cell>yes</cell><cell>no</cell><cell>yes</cell><cell>19.6</cell></row><row><cell></cell><cell></cell><cell>Feedback</cell><cell></cell><cell>yes</cell><cell>yes</cell><cell>yes</cell><cell>19.0</cell></row><row><cell cols="9">Table 6: Ablation on WikiText-103 of various modeing choices. Results are shown without</cell></row><row><cell cols="2">finetuning.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2.0</cell><cell cols="2">Transformer</cell><cell></cell><cell></cell><cell>35</cell><cell cols="2">Transformer</cell></row><row><cell></cell><cell>1.8</cell><cell cols="3">Feedback Transformer</cell><cell></cell><cell></cell><cell cols="2">Feedback Transformer</cell></row><row><cell>Dev. (bpc)</cell><cell>1.4 1.6</cell><cell></cell><cell></cell><cell></cell><cell>Dev. (ppl)</cell><cell>25 30</cell><cell></cell></row><row><cell></cell><cell>1.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>20</cell><cell></cell></row><row><cell></cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell></cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell></row><row><cell></cell><cell></cell><cell>Model depth</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Model Depth</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>right). The grid size is 15 × 15. To help exploration, the agent can see if the goal is within a 3 × 3 area around it. An episode ends after 200 steps. We train for 500M steps (2.5M episodes). We use 2-layer Transformers with hidden size of 64 and 1 head. The attention span is 200 so the agent can put an entire episode in memory.All agents where trained using A2C with RMSprop with entropy cost of 0.0001, RMSProp epsilon regularisation parameter of 0.01, batch size of 64, and BPTT 200. Feedback Transformer and Transformer baseline were trained with a learning rate of 0.0003. LSTM model is a 2-layer LSTM with hidden size of 64. For LSTM model we used a learning rate of 0.0004. Initialization. Assign an initial value to a variable like x=3. A variable can only be initialized once in each program.</figDesc><table><row><cell>7.4 ALGORITHMIC TASK DETAILS In this task, each program consists of 100 simple statements that should be sequentially executed. The available statement types are: Encoder Layers 6 6 6 Decoder Layers 6 6 6 FFN Size 2048 4096 1024 Attention Heads 8 16 4 Dropout 0.3 0.3 0.3 Hidden Size 512 1024 512 1. Hyperparameter Summarization WMT En-De IWSLT De-En Learning Rate 0.0005 0.001 0.0005</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Hyperparamers for sequence to sequence experiments.</figDesc><table><row><cell>Hyperparameter</cell><cell cols="5">Random Walk char-PTB Enwik8 WikiText-103 WikiText-103</cell></row><row><cell></cell><cell>Algorithmic</cell><cell></cell><cell></cell><cell>small</cell><cell>large</cell></row><row><cell>Layers</cell><cell>4</cell><cell>6</cell><cell>12</cell><cell>4</cell><cell>8</cell></row><row><cell>Hidden size (d)</cell><cell>256</cell><cell>384</cell><cell>512</cell><cell>512</cell><cell>1024</cell></row><row><cell>FF size</cell><cell>4d</cell><cell>4d</cell><cell>8d</cell><cell>8d</cell><cell>4d</cell></row><row><cell>Head count (h)</cell><cell>4</cell><cell>4</cell><cell>8</cell><cell>8</cell><cell>8</cell></row><row><cell>Head dim</cell><cell>d/h</cell><cell>d/h</cell><cell>2d/h</cell><cell>2d/h</cell><cell>d/h</cell></row><row><cell>Attention span</cell><cell>100</cell><cell>512</cell><cell>8192*</cell><cell>512</cell><cell>512, 2048*</cell></row><row><cell>Dropout rate</cell><cell>0.2</cell><cell>0.5</cell><cell>0.5</cell><cell>0.1</cell><cell>0.3</cell></row><row><cell>Embed. dropout</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.1</cell><cell>0.2</cell></row><row><cell>BPTT len (M )</cell><cell>64</cell><cell>128</cell><cell>128</cell><cell>256</cell><cell>256</cell></row><row><cell>Batch size (B)</cell><cell>512</cell><cell>2048</cell><cell>1024</cell><cell>512</cell><cell>512</cell></row><row><cell>Learning rate</cell><cell>0.0001</cell><cell>0.0015</cell><cell>0.0015</cell><cell>0.0007</cell><cell>0.0007</cell></row><row><cell>Gradient clip</cell><cell>0.1</cell><cell>1.0</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell>LR warm-up steps</cell><cell>1k</cell><cell>1k</cell><cell>8k</cell><cell>8k</cell><cell>8k</cell></row><row><cell>Parameters</cell><cell>3.2M</cell><cell>10.7M</cell><cell>77M</cell><cell>44M</cell><cell>139M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Hyperparamers for language modeling experiments. Here * indicates the adaptive span.2. Increment and decrement. Increment or decrement a variable value by 1, like x++ or y--.3. Print. Output the value of a certain variable like print(y). Only this statement requires model to make a prediction.4.Conditional. Execute the nested statement only if a variable has a certain value, e.g., if x==4: y--. Note that conditional and print statements cannot be nested.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Adaptive input representations for neural language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mia</forename><surname>Xu Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.09849</idno>
		<title level="m">The best of both worlds: Combining recent advances in neural machine translation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Gated feedback recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2067" to="2075" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02860</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03819</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Universal transformers. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT (1)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.09722</idno>
		<title level="m">Pre-trained language model representations for language generation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05217</idno>
		<title level="m">Controllable abstractive summarization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11556</idno>
		<title level="m">Reducing transformer depth on demand with structured dropout</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient softmax approximation for gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cissé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural turing machines</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Theoretical limitations of self-attention in neural sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Hahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="156" to="171" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Modeling recurrence for transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baosong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03092</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočiský</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-path feedback recurrent neural networks for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Inferring algorithmic patterns with stack-augmented recurrent nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="190" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.08228</idno>
		<title level="m">Neural gpus learn algorithms</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Generalization through memorization: Nearest neighbor language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.00172</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Dynamic evaluation of transformer language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Kahembwe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Renals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08378</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Simple recurrent units for highly parallelizable recurrence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Artzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.02755</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text summarization branches out</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Large text compression benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Mahoney</surname></persName>
		</author>
		<ptr target="http://www.mattmahoney.net/text/text.html" />
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Single headed attention rnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.11423</idno>
	</analytic>
	<monogr>
		<title level="m">Stop thinking with your head</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Regularizing and optimizing lstm language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02182</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Spatial localization does not require the presence of local cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Learning and motivation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="239" to="260" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Scaling neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00187</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilio</forename><surname>Parisotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Çaglar</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lopez Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Clark</surname></persName>
		</author>
		<idno>abs/1910.06764</idno>
	</analytic>
	<monogr>
		<title level="m">Seb Noury, M. Botvinick, N. Heess, and Raia Hadsell. Stabilizing transformers for reinforcement learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">On the turing completeness of modern neural network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Marinković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Barceló</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.03429</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Compressive transformers for long-range sequence modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Potapenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SylKikSYDH" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Efficient content-based sparse attention with routing transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Saffar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.05997</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Get to the point: Summarization with pointer-generator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04368</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Self-attention with relative position representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT (2)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dense information flow for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1294" to="1303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Adaptive attention span in transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">The importance of being recurrent for modeling hierarchical structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03585</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zitao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiliang</forename><surname>Tang. R-Transformer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.05572</idno>
		<title level="m">Recurrent neural network enhanced transformer</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
