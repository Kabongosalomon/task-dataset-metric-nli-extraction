<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Graph Neural Network Representations of Logical Formulae with Subgraph Pooling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Crouse</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Qualitative Reasoning Group</orgName>
								<orgName type="institution" key="instit1">Northwestern University ‡ IBM T.J. Watson Research Center</orgName>
								<orgName type="institution" key="instit2">IBM Research § MIT-IBM Watson AI Lab</orgName>
								<orgName type="institution" key="instit3">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibrahim</forename><surname>Abdelaziz</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Qualitative Reasoning Group</orgName>
								<orgName type="institution" key="instit1">Northwestern University ‡ IBM T.J. Watson Research Center</orgName>
								<orgName type="institution" key="instit2">IBM Research § MIT-IBM Watson AI Lab</orgName>
								<orgName type="institution" key="instit3">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristina</forename><surname>Cornelio</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Qualitative Reasoning Group</orgName>
								<orgName type="institution" key="instit1">Northwestern University ‡ IBM T.J. Watson Research Center</orgName>
								<orgName type="institution" key="instit2">IBM Research § MIT-IBM Watson AI Lab</orgName>
								<orgName type="institution" key="instit3">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veronika</forename><surname>Thost</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Qualitative Reasoning Group</orgName>
								<orgName type="institution" key="instit1">Northwestern University ‡ IBM T.J. Watson Research Center</orgName>
								<orgName type="institution" key="instit2">IBM Research § MIT-IBM Watson AI Lab</orgName>
								<orgName type="institution" key="instit3">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Qualitative Reasoning Group</orgName>
								<orgName type="institution" key="instit1">Northwestern University ‡ IBM T.J. Watson Research Center</orgName>
								<orgName type="institution" key="instit2">IBM Research § MIT-IBM Watson AI Lab</orgName>
								<orgName type="institution" key="instit3">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Forbus</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Qualitative Reasoning Group</orgName>
								<orgName type="institution" key="instit1">Northwestern University ‡ IBM T.J. Watson Research Center</orgName>
								<orgName type="institution" key="instit2">IBM Research § MIT-IBM Watson AI Lab</orgName>
								<orgName type="institution" key="instit3">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Achille</forename><surname>Fokoue</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Qualitative Reasoning Group</orgName>
								<orgName type="institution" key="instit1">Northwestern University ‡ IBM T.J. Watson Research Center</orgName>
								<orgName type="institution" key="instit2">IBM Research § MIT-IBM Watson AI Lab</orgName>
								<orgName type="institution" key="instit3">IBM Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Graph Neural Network Representations of Logical Formulae with Subgraph Pooling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advances in the integration of deep learning with automated theorem proving have centered around the representation of logical formulae as inputs to deep learning systems. In particular, there has been a growing interest in adapting structure-aware neural methods to work with the underlying graph representations of logical expressions. While more effective than character and token-level approaches, graph-based methods have often made representational trade-offs that limited their ability to capture key structural properties of their inputs. In this work we propose a novel approach for embedding logical formulae that is designed to overcome the representational limitations of prior approaches. Our architecture works for logics of different expressivity; e.g., first-order and higher-order logic. We evaluate our approach on two standard datasets and show that the proposed architecture achieves state-of-the-art performance on both premise selection and proof step classification.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automated theorem proving studies the design of automated systems that reason over logical theories (collections of axioms that are formulae known to be true) to generate formal proofs of given conjectures. It has been a longstanding, active area of artificial intelligence research that has demonstrated utility in the design of operating systems <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, distributed systems <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, compilers <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>, microprocessor design <ref type="bibr" target="#b6">[7]</ref>, and in general mathematics <ref type="bibr" target="#b7">[8]</ref>.</p><p>Classical automated theorem provers (ATPs) have historically been most useful for solving problems that require complex chains of reasoning steps to be executed over smaller sets of axioms (see TPTP <ref type="bibr" target="#b8">[9]</ref> for examples). When faced with problems for which thousands to millions of axioms are provided (only a handful of which may be needed at a time), even state-of-the-art theorem provers have difficulty <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>. This deficiency has become more evident in recent years, as large logical theories <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref> have become more widely available. A natural way to scale ATPs to broader domains has been to design sophisticated mechanisms that allow them to determine which axioms or intermediate proof outputs merit exploration in the proof search process. These mechanisms thus prune an otherwise unmanageably large proof search space down to a size that can be handled efficiently by classical theorem provers. The task of classifying axioms as being useful to prove a given conjecture is referred to as premise selection, while the task of classifying intermediate proof steps as being a part of a successful proof for a conjecture is referred to as proof step classification.</p><p>Initial approaches for solving these two tasks proposed heuristics based on simple symbol cooccurrences between formulae <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">16]</ref>. While effective, these methods were soon surpassed by machine-learning techniques which could automatically adjust themselves to the needs of particular domains <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b18">18]</ref>. At present, there has been a rising interest in developing neural approaches for both premise selection and proof step classification <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b21">21]</ref>; however, the complex and structured nature of logical formulae has made this development challenging. Neural approaches that take into account a formula's structure (e.g., its parse tree), have been shown to outperform their more basic counterparts which operate on only a formula's symbols <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b23">23]</ref>. The two most commonly used structure-aware neural methods have been Tree LSTMs <ref type="bibr" target="#b24">[24]</ref> and GNNs <ref type="bibr" target="#b25">[25]</ref>. However, as they have been applied in this domain, these methods appear to be leaving out useful structural information.</p><p>When used to embed the parse tree of a logical formula, Tree LSTMs generate embeddings that represent the parse tree globally, but they miss logically important information like shared subexpressions and variable quantifications. Conversely, traditional GNN approaches appear to better capture shared subexpressions and variable quantifications; however, the global graph embedding they produce for the whole formula consists of a simple pooling operation over individual node embeddings; each representing only themselves and their local neighborhoods. Additionally, most prior approaches have embedded the premise and conjecture formulae independently of each other <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b18">18]</ref>. They first embed the graph of the premise and then separately embed the conjecture graph, resulting in the contents of one formula having no influence on the embedding of the other.</p><p>To address these issues, we present a novel, two-phase embedding approach that operates over the DAG representations of logical formulae and is designed with careful consideration to their particular properties. Our method first produces an initial set of high-quality embeddings for nodes that incorporates more than just their local neighborhoods. Then, it pools the embeddings together in a structure-dependent way to generate a single graph-level embedding. This decoupling provides a clear point at which information between formulae can be exchanged, which allows us to define a localized attention-based exchange mechanism that can regulate information flow between the concurrent formula embedding processes.</p><p>We demonstrate the effectiveness and generality of our approach by evaluating classification performance on two standard datasets that involve different logical formalisms; the Mizar dataset <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b18">18]</ref> for first-order logic and the Holstep dataset <ref type="bibr" target="#b20">[20]</ref> for higher-order logic. Our experiments show that the architecture introduced in this paper outperforms all previous approaches on the binary classification tasks of premise selection and proof step classification for both datasets. We also demonstrate how to easily integrate our approach with E, a well-established theorem prover <ref type="bibr" target="#b26">[26]</ref>, as its premise selection mechanism, allowing it to find more proofs (61.6% improvement) in a large-theory setting.</p><p>To summarize, our main contributions are: 1) We show how to leverage the DAG structure implicit in logical formulae to produce more effective embeddings than traditional approaches operating over the local neighborhoods of individual nodes; 2) We introduce a novel neural architecture that employs a localized attention mechanism to allow formulae to exchange information during the embedding process; 3) We provide an extensive series of experiments and compare a range of neural architectures, showing significant improvement over existing state-of-the-art methods on two standard ATP classification datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>We note that premise selection and proof step classification are not intrinsically machine learning tasks. The earliest approaches to premise selection <ref type="bibr" target="#b10">[11]</ref> were simple heuristics capturing the (transitive) co-occurrence of symbols in a given axiom and conjecture. Soon after, it was recognized that machine-learning techniques would be effective tools for solving this problem. The work of <ref type="bibr" target="#b17">[17]</ref> introduced a kernel method for premise selection where the similarity between two formulae was computed by the number of common subterms and symbols. Deepmath <ref type="bibr" target="#b18">[18]</ref> was the first deep learning approach to this problem, comparing the performance of sequence models over character and symbol-level representations of logical formulae. In <ref type="bibr" target="#b27">[27]</ref>, the authors proposed a symbol-level method that learned low-dimensional distributed representations of function symbols and used those to construct embedded representations of given formulae for premise selection. The work of <ref type="bibr" target="#b28">[28]</ref> introduced a GNN for representing specifically first-order logic formulae in conjuctive normal form that captured certain logical invariances like reorderings of clauses and literals.</p><p>Recently, Holstep <ref type="bibr" target="#b20">[20]</ref>, a new formal dataset designed to be large enough to evaluate neural methods for premise selection and proof step classification (among other tasks), was introduced. Along with the dataset came a set of benchmark deep learning models that operated over character and symbol-level representations of higher-order logic formulae. FormulaNet <ref type="bibr" target="#b22">[22]</ref> was the first approach to transform a formula into a rooted DAG (a modified version of the parse tree) and then process the resulting graph with a GNN. Their GNN produced embeddings for each node within a formula's graph representation and then max pooled across node embeddings to get a formula-level embedding.</p><p>There are several other related works in this area that focus on different tasks (e.g., proof guidance, the combined, online version of the aforementioned two tasks). Deep learning approaches to proof guidance include <ref type="bibr" target="#b21">[21]</ref>, where the authors explored a number of neural architectures in their implementation (including a Tree LSTM that encoded parse trees of logical formulae). <ref type="bibr" target="#b23">[23]</ref> represented formulae as DAGs with shared subexpressions and used message-passing GNNs (MPNNs) to generate embeddings that could be used to guide theorem proving on the higher-order logic benchmark of <ref type="bibr" target="#b19">[19]</ref>. However, like <ref type="bibr" target="#b22">[22]</ref>, the graph-level embeddings produced by their approach were simple, consisting only of a max pooling over individual node embeddings. The work of <ref type="bibr" target="#b29">[29]</ref> introduced a dataset for evaluating neural models on entailment for propositional logic and explored the use of several popular neural architectures on the proposed task. <ref type="bibr" target="#b30">[30]</ref> where the authors introduced the GamePad dataset for evaluating neural models on the tasks of position evaluation and tactic prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Formula Representation</head><p>Background: First-order logic formulae are formal expressions based on an alphabet of predicate, function, and variable symbols which are combined by logical connectives. A term is either a variable, a constant (function with no arguments), or, inductively, a function applied to a tuple of terms. A formula is either a predicate applied to a tuple of terms or, inductively, a connective (e.g., ∧ read as "and") applied to some number of formulae. In addition, variables in formulae can be quantified by quantifiers (e.g., by ∀ read as "for all"), where a quantifier introduces an additional semantic restriction for the interpretation of the variables it quantifies. Higher-order logic formulae also allow for quantification over predicate and function symbols or the application of predicates over other predicates. For more details on both first and higher-order logic, we refer the reader to <ref type="bibr" target="#b31">[31]</ref>.</p><p>Logical Formulae as Graphs: While the earliest work on integrating deep learning with reasoning techniques used symbol-or word-level representations of input formulae <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b20">20]</ref> (considering formula strings as words), subsequent work explored using formula parse trees <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b30">30]</ref> or rooted DAG forms <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b23">23]</ref>. On the Holstep <ref type="bibr" target="#b20">[20]</ref> and Holist <ref type="bibr" target="#b19">[19]</ref> datasets, the DAG forms of logical formulae were found to be the more useful than bag-of-symbols and tree-structured encodings <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b23">23]</ref>. We focus on rooted-DAG representations of formulae; <ref type="figure" target="#fig_0">Figure 1</ref> shows an unmodified example of such a representation. The DAG associated to a formula corresponds to its parse tree, where directed edges are added from parents to their arguments and shared subexpressions are mapped to the same subgraphs. As in <ref type="bibr" target="#b22">[22]</ref>, all instances of the same variable are collapsed into a single node (which maintains all prior connections) and the name of each variable is replaced by a generic variable token.</p><p>Edge Labeling: Capturing the ordering of arguments of logical expressions is still an open topic of research. <ref type="bibr" target="#b22">[22]</ref> used a so-called treelet encoding scheme that represents the position of a node relative to other arguments of the same parent as triples. <ref type="bibr" target="#b23">[23]</ref> used positional edge labels, assigning to each edge a label reflecting the position of its target node in the argument list of the node's parent. We follow the latter strategy, albeit, with modifications. In our formulation, edge labels are determined by a partial ordering. For unordered logical connectives (e.g., ∧, ⇔) and predicates (e.g., =) all arguments are of the same rank. For other predicates, functions, and logical connectives the arguments are instead linearly ordered. However, we also support hybrid cases like simultaneous quantification over multiple variables. The label given to each argument edge in the graph is the rank of the corresponding argument with respect to the parent concatenated with the type of the parent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Our Approach</head><p>In this work, we broadly distinguish between node embedding methods by reachability. More formally, consider a binary adjacency relation R defined for a set of graphs G. The k-reachability relation R k is given as the k-th power of R, which is defined recursively with R k = R k−1 • R and R 1 = R. We can define the transitive closure of R as simply R + = R ∞ . Letting the set of all nodes beV = G=(V,E)∈G V , we say that a graph embedding function f is a R k embedding method if there exists a k ∈ N such that R k = R + where for every u inV we have that f computes the value of u as a function of only the embeddings for {v ∈V | R k (u, v)}. Naturally, we define a R + embedding method as one for which the opposite holds, i.e. for each u we have that f computes the value of u as a function of the embeddings for all v where R + (u, v) holds. This distinction is particularly useful to make for graphs in the logic domain, as the transitive closure of adjacency is necessary for many key logical operations. As a trivial but important example, consider checking for the resolvability or unifiability of two ground formulas. Potentially all nodes of the two formulas would need to be examined, meaning that if both formulas had depth &gt; k then a procedure defined with R k that checks only some subset of nodes within a fixed range of the root would be insufficient.</p><p>We view R + embedding methods as those that perform a sophisticated type of subgraph pooling. That is, a R + node embedding method computes the embedding for a node u as a function of the embeddings of all nodes reachable from u, i.e. a pooling of all such node embeddings. By definition, they incorporate as much graph context as is possible (i.e., the transitive closure of R). While R + node embedding methods naturally lend themselves to graph-level readout functions (and we will also use them in this way), we note that these concepts are defined for node-level embeddings (an important distinction to make, as for certain applications the input graphs could be disconnected).</p><p>Our approach operates in two stages (see <ref type="figure" target="#fig_1">Figure 2</ref>). First, a neural network generates embeddings for each node of an input formula's graph representation. Then, the node embeddings are passed into a follow-up R + embedding method, referred to as the pooling method, that has R as the parent relation. The embedding for the root node of the input formula is returned, which is a function of all nodes in its graph. Our approach is very modular, with any node-level embedding method capable of serving as the initial node embedder (though we are mainly interested in R + embedding methods) and any R + embedding method being usable as the pooling method. Thus, in the next sections we describe the node embedding methods independently, and then we describe the classification process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">R k Embedding Methods</head><p>Message-Passing Graph Neural Networks: The MPNN framework can be thought of as an iterative update procedure that represents a node as an aggregation of information from its local neighborhood. To begin, our MPNN assigns each node v and edge e of the input graph G = (V, E) an initial embedding, x v and x e . Then, following <ref type="bibr" target="#b22">[22]</ref>, initial node states are computed by passing each such embedding through batch normalization <ref type="bibr" target="#b32">[32]</ref> and a ReLU, producing node states h</p><formula xml:id="formula_0">(0) v = F V (x v )</formula><p>and edge states h e = F E (x e ). Lastly, a message-passing phase runs for t = 1, . . . , k rounds</p><formula xml:id="formula_1">m (t) vp = w∈P A (v) F (t) M A [h (t−1) v ||h (t−1) w ||h evw ] , m (t) vc = w∈P C (v) F (t) M C [h (t−1) v ||h (t−1) w ||h evw ] h (t) v = h (t−1) v + F (t) A [h (t−1) v ||m (t) vp ||m (t) vc ]</formula><p>where P A and P C are functions that take a node v and return the immediate ancestors / children of v in G, and F (t)</p><formula xml:id="formula_2">M A , F (t) M C , and F (t)</formula><p>A are feed-forward networks unique to the t-th round of updates, and || denotes vector concatenation. The reachability relation R in this context is defined as</p><formula xml:id="formula_3">R(u, v) = A(u, v) ∨ C(u, v)</formula><p>where A and C are relations that hold true for immediate ancestor and child relationships, respectively. Similar to <ref type="bibr" target="#b33">[33]</ref>, m Graph Convolutional Neural Networks: Like with our MPNNs, for our Graph Convolutional Networks (GCNs) <ref type="bibr" target="#b25">[25]</ref>, the reachability relation R is given as the undirected adjacency relation, i.e., for nodes u and v we have R(u, v) = A(u, v) ∨ C(u, v). First, each node v ∈ V is associated with an embedding h v . Then, for t = 1, . . . , k rounds, updated embeddings are computed as</p><formula xml:id="formula_4">h (t) v = φ W (t) h (t−1) v |N (v)| + w∈N (v) h (t−1) w |N (v)||N (w)|</formula><p>where φ is a non-linearity (in this work, we use a ReLU), N (u) = P A (u) ∪ P C (u), and W (t) is a learned matrix specific to the t-th round of updates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">R + Embedding Methods</head><p>DAG LSTMs: DAG LSTMs can be viewed as the generalization of Tree LSTMs <ref type="bibr" target="#b24">[24]</ref> to DAGstructured data. With initial node embeddings s v , the DAG LSTM uses the same N-ary equations as the Tree LSTM to compute node states h v</p><formula xml:id="formula_5">i v = σ W i s v + w∈P R (v) U (evw) i h w + b i f vw = σ W f s v + U (evw) f h w + b f o v = σ W o s v + w∈P R (v) U (evw) o h w + b o c v = i v ĉ v + w∈P R (v) f vw c ŵ c v = tanh W c s v + w∈P R (v) U (evw) c h w + b c h v = o v tanh c v</formula><p>where denotes element-wise multiplication, σ is the sigmoid function and U , and f is a forget gate that modulates the flow of information from individual arguments into a node's computed state. P R is a predecessor function that returns the set of nodes for which R holds true, i.e. P R (u) = {v ∈ V | R(u, v)}. In this work, it returns either the parents or the children, depending on whether the direction of accumulation is desired to go upwards or downwards. For readability, we omitted the layer normalization <ref type="bibr" target="#b34">[34]</ref> applied to each matrix multiplication (e.g., W i s v , U i h w , etc.) from the above equations. Each instance of layer normalization maintained its own separate parameters.</p><p>The DAG LSTM we propose here is nearly the same as the Tree LSTM of <ref type="bibr" target="#b24">[24]</ref>, however there are key implementational differences between the two approaches. In Tree LSTMs, P R typically returns child nodes (since a node can have only one parent), while in our work it can return either children or parents. In addition, batching together node updates in a Tree LSTM can be done at the level of depth (i.e., all nodes at the same depth in the tree can have their updates computed simultaneously); however, with DAGs this batching strategy could cause a node to be updated and overwritten multiple times. To solve this, we propose the use of topological batching. In our approach, node updates are computed in the order given by a topological sort of the graph, starting from the leaves (or root depending on P), with updates batched together at the level of topological equivalence, i.e., every node with the same rank can have the updates computed simultaneously.</p><p>Attention-Enhanced DAG LSTMs: In order to allow the contents of the premise and conjecture to influence one another during the embedding process, we introduce a localized attention mechanism that exchanges information between the two graph embeddings. Let S P and S C be the sets of node embeddings computed by some initial node embedder for the premise and conjecture graphs. Let I be a function that takes a node and either S P or S C and returns all node embeddings from the set where the associated node has an identical label to the given node, i.e. I(u, S C ) = {s v ∈ S C |u ≡ v}. Our approach computes multi-headed attention scores <ref type="bibr" target="#b35">[35]</ref> between identically labeled nodes and uses those attention scores to build new embeddings that provide cross graph information to the pooling procedure. For an input u, for each k j ∈ I(u, S C ) we computê</p><formula xml:id="formula_6">q i = W (q) i s u , k ij = W (k) i k j , v ij = W (v) i k j where W (q) i , W (k) i , and W (v) i</formula><p>are learned matrices for each of the i = 1, . . . , N attention heads.</p><formula xml:id="formula_7">w ij =q i k ij bq , α ij = exp (w ij ) j exp(w ij ) ,</formula><p>where bq is the dimensionality ofq i and α ij is computed by the standard softmax</p><formula xml:id="formula_8">q i = j α ij v ij , s u = σ(W (g) r u ) (W (o) N i=1 q i )</formula><p>The final vector s u for input s u is a combination of its N transformations, with W (g) and W (o) being learned matrices, r u a learned vector for the type (e.g., quantifier, predicate, etc.) of node u, and denoting vector concatenation over the N attention vectors. The gating mechanism σ W (g) r u ) we propose here simply allows for the architecture to cut off information flow between the two graphs if doing so improves loss, thus turning the architecture into the simpler DAG LSTM introduced previously. Following the attention computation, each s u is replaced bys u = s u ||s u and a DAG LSTM then processes each node embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bidirectional DAG LSTMs:</head><p>We also explore a simple extension of the DAG LSTM from above to a Bidirectional DAG LSTM. This extends the R relation from being either ancestors or children to being both, i.e. R(u, v) = A(u, v) ∨ C(u, v). For a node u and graph G, the embedding s u is</p><formula xml:id="formula_9">s u = F BD [DAG-LSTM ↑ (u, G) || DAG-LSTM ↓ (u, G)]</formula><p>where F BD is a feed-forward network, and DAG-LSTM ↑ / DAG-LSTM ↓ are both DAG LSTMs (following the design presented before) which set R as A and C, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Classification Process</head><p>In our approach, the final graph embeddings for the premise and conjecture are taken to be the embeddings for the root nodes of the premise and conjecture, s P = h P root and s C = h C root . For ablation experiments using only local neighborhood-based node embedders (MPNN / GCN from Section 4.1), the inputs to the classifier network would be a max pooling of the individual node embeddings for each graph. In either case, the two graph embeddings are concatenated and passed to a classifier feed-forward network F CL for the final prediction F CL ([s P ; s C ]).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and Results</head><p>In this section, we evaluate the performance of our approach to show 1) how accurately can it predict the label of an axiom or proof step, 2) an ablation study that shows the effect of different node embedding and pooling mechanisms, and 3) its impact when integrated with an existing theorem prover in terms of number found proofs. We compare our approach to prior works using two standard Kucik &amp; Korovin (2018) <ref type="bibr" target="#b27">[27]</ref> 76.5% -DeepWalk (2014) <ref type="bibr" target="#b36">[36]</ref> -61.8% CNN-LSTM (2017) <ref type="bibr" target="#b20">[20]</ref> -83.0% CNN (2017) <ref type="bibr" target="#b20">[20]</ref> -82.0% FormulaNet (2017) <ref type="bibr" target="#b22">[22]</ref> -90.3% BidirDagLSTM-AttDagLSTM (this work) 81.0% 91.4%</p><p>datasets: Mizar 1 <ref type="bibr" target="#b12">[13]</ref> and Holstep 2 <ref type="bibr" target="#b20">[20]</ref>. The hyperparameters and network configurations for our approach can be found in the appendix (which is located in the supplementary material).</p><p>Mizar Dataset: Mizar [13] is a corpus of 57,917 theorems. Like <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b27">27]</ref>, we use only the subset of 32,524 theorems which have an associated ATP proof, as those have been paired with both positive and negative premises (i.e., axioms that do / do not entail a particular theorem) to train our approach. We randomly split the 32,524 theorems as 80% / 10% / 10% for training, development, and testing (yielding 417,763 / 51,877 / 52,880 individual premises). Following <ref type="bibr" target="#b28">[28]</ref>, each example given to the network consisted of a conjecture paired with the complete set of both positive and negative premises. The task was then to classify each individual premise as positive or negative.</p><p>Holstep Dataset: Holstep <ref type="bibr" target="#b20">[20]</ref> is a large corpus designed to test machine learning approaches on automated reasoning. Following prior work <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b22">22]</ref>, we use only the portion needed for proof step classification. That part has 9,999 conjectures for training and 1,411 conjectures for testing, where each conjecture is paired with an equal number of positive and negative proof steps (i.e., proof steps that were / were not part of the final proof for the associated conjecture). Using that data, we obtain 2,013,046 training examples and 196,030 testing examples, where each example is a triple with the proof step, conjecture, and a positive or negative label. We held out 10% of the training set to be used as a development set. We follow the binary classification problem setting of <ref type="bibr" target="#b22">[22]</ref> and <ref type="bibr" target="#b20">[20]</ref> where our approach classifies each proof step as either relevant or irrelevant to its paired conjecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Classification Experiments</head><p>Baselines: For premise selection on Mizar, we compare with two existing systems: the distributed formula representation of <ref type="bibr" target="#b27">[27]</ref> and the property-invariant formula representation of <ref type="bibr" target="#b28">[28]</ref>. For the proof step classification task on Holstep, we compare against 4 systems implemented in two prior works: 1) DeepWalk <ref type="bibr" target="#b36">[36]</ref> and FormulaNet <ref type="bibr" target="#b22">[22]</ref>, both of which were applied to Holstep in <ref type="bibr" target="#b22">[22]</ref>. 2) CNN-LSTM and CNN, both introduced in the original Holstep paper <ref type="bibr" target="#b20">[20]</ref>.</p><p>Main Results: <ref type="table" target="#tab_0">Table 1</ref> shows the performance for the version of our approach that incorporates the entire context surrounding a node into its embedding and jointly embeds premises / proof steps with the conjecture, i.e., a Bidirectional DAG LSTM with attention-enhanced DAG LSTM pooling. Overall, our system outperforms all state-of-the-art systems on both datasets using a standard evaluation on held-out test data. It outperforms by a large margin on Mizar (+4.5%, which is statistically significant with p &lt; 0.01) and by a moderate, but still statistically significant, margin on Holstep (+1.1% with p &lt; 0.01). In addition to the standard evaluation using a held-out test dataset reported on <ref type="table" target="#tab_0">Table 1</ref>, we also compare to <ref type="bibr" target="#b28">[28]</ref>, which introduced a GNN designed to process specifically first-order logic theories in conjunctive normal form. In their evaluation on Mizar, they split their data into only a train and test set and evaluated the model obtained at each epoch on their test set, reporting an accuracy of "around 80%" as the best performance across all test set evaluations. Following their setup, our best validation performance is 81.9%, a roughly 2% gain over <ref type="bibr" target="#b28">[28]</ref>.</p><p>The result in <ref type="table" target="#tab_0">Table 1</ref> confirms our hypothesis that a more holistic treatment of logical formulae can result in a more effective embedding process than simpler methods that, by their implementation, consider less structure and embed premises and conjectures independently. For Holstep, when controlling for node embedding type the R + pooling methods had better performance than max pooling. Interestingly, when controlling for pooling type, the difference between the MPNN and R + node embedding methods was not significant. Within approaches introduced here, those variants using AttDagPool did not significantly improve over those using DagPool. We suspect that this is due to the fundamental difference between proof step classification and premise selection. Intermediate proof steps are typically much larger and noisier than actual premises, which may have led to Holstep example pairs being independent (i.e., there were properties of an individual proof step without the conjecture that would give away the positive or negative label). This is partially supported by both <ref type="bibr" target="#b22">[22]</ref> and <ref type="bibr" target="#b20">[20]</ref>, who observed that their architectures performed just as well when classifying with only the proof step, rather than on both the proof step and conjecture (90.0% vs. 90.3% for FormulaNet and 83.0% vs. 83.0% for CNN-LSTM). On validation data, we also explored higher numbers of update rounds (i.e., the k parameter) for variants of our approach using an MPNN as the initial node embedder; however, like <ref type="bibr" target="#b22">[22]</ref> we found insignificant change beyond k = 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Premise Selection for Automated Theorem Proving</head><p>To show that our approach could be used to improve the performance of an actual theorem prover, we ran a traditional premise selection experiment with E <ref type="bibr" target="#b26">[26]</ref>. We first trained new models using our settings from the classification experiments, however, this time optimizing for binary classification between pairs of individual formulae. In addition to our Mizar training set from before, we also augmented our training data by adding randomly selected negative examples for each example from our original training set. For testing, we paired the conjecture of each of the 3,252 problems from our Mizar validation set with the complete set of statements from all chronologically preceding problems (as described in <ref type="bibr" target="#b18">[18]</ref>) in the union of our training and validation sets. For each problem, our model then ranked the premises with respect to each conjecture and returned the top k ∈ {16, 32, 64, 128, 256, 512, 1024, 2048, ∞} premises (where ∞ indicates including all premises).</p><p>E was run on each problem in auto-schedule mode (which tries several expert heuristics based on the given problem) with a time limit of 10 seconds per k, stopping at the first k where the problem was solved. To validate that our approach solves more problems than E would have by itself in the same amount of time, we also measured the performance of E when run with all premises (identical to k = ∞) for 90 seconds per problem. Out of 3,252 problems, E by itself was able to solve 918; however, using our approach as its premise selection mechanism, E was capable of solving 1484. In both settings, E had the same amount of time (90 seconds) per problem to find a proof, but with our approach it was able to solve 566 more problems (a 61.6% improvement) which is statistically significant with p &lt; 0.01.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Graph representation for a conjecture which regards the lattice of subgroups of a group</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>A depiction of the overall embedding process with an MPNN as the initial node embedder and DAG LSTM as the pooling method. In both boxes, arrows indicate flow of information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>vc should be considered the messages to be passed to h v , and h (t) v represents the node embedding for node v after t rounds of iteration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>learned matrices (different for each edge type). i and o represent input and output gates, while c andĉ are intermediate computations (memory cells)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Experimental results for Mizar and Holstep test sets, best result for both datasets in bold</figDesc><table><row><cell>Formula Embedding Method</cell><cell>Mizar Acc. Holstep Acc.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ablation study on Mizar and Holstep test sets; all variations implemented by this work. Statistically significant improvements over prior work marked in Statistically Sig. Node Embedding Pool Type k Mizar Acc. Statistically Sig. Holstep Acc. Statistically Sig.We present the results of our ablations inTable 2, with statistically significant (p &lt; 0.01) improvements over<ref type="bibr" target="#b27">[27]</ref> on Mizar and<ref type="bibr" target="#b22">[22]</ref> on Holstep marked explicitly. On Mizar, we can see that variants with attention-based pooling were the most performant by a large margin. When controlling for pooling type, the R + node embedders provided better performance than the R k node embedders. Similarly, when controlling for node embedding type, R + pooling methods provided improvement over max pooling.</figDesc><table><row><cell>MPNN MPNN MPNN</cell><cell>MaxPool DagPool AttDagPool 2 2 2</cell><cell>76.9% 77.4% 79.7%</cell><cell>√ √</cell><cell>90.5% 91.3% 91.3%</cell><cell>√ √</cell></row><row><cell>GCN GCN GCN DagLSTM DagLSTM BidirDagLSTM BidirDagLSTM</cell><cell>MaxPool DagPool AttDagPool 2 2 2 DagPool -AttDagPool -DagPool -AttDagPool -</cell><cell>74.7% 77.3% 79.8% 78.4% 80.7% 78.1% 81.0%</cell><cell>√ √ √ √ √ √</cell><cell>89.0% 90.9% 90.8% 91.4% 91.5% 91.4% 91.4%</cell><cell>√ √ √ √ √ √</cell></row><row><cell>Ablation Studies:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/JUrban/deepmath 2 http://cl-informatik.uibk.ac.at/cek/holstep/</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Network Configurations</head><p>For Holstep, our hyperparameters were chosen to be comparable to <ref type="bibr" target="#b22">[22]</ref>. In our model, node embeddings were 256-dimensional vectors and edge embeddings were 64-dimensional vectors. All feed-forward networks (each F (t)</p><p>A , F BD , and F CL ) followed mostly the same configuration, except for their input dimensionalities. Each had one hidden layer with dimensionality equal to the output layer (except for F CL where the dimensionality was half the input dimensionality). Every hidden layer for all feed-forward networks (except for F CL ) was followed by batch normalization <ref type="bibr" target="#b32">[32]</ref> and a ReLU. The final activation for F CL was a sigmoid; for all other feed-forward networks, the final activations were ReLUs. For the DAG LSTMs, the hidden states were 256-dimensional vectors. Each U</p><p>were learned 256 × 256 matrices and each of W i , W o , W f , W c , W a , and W g were learned 256 × 256 matrices. For Mizar, all above dimensionalities were halved to be comparable to <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b28">28]</ref>. For the attention-enhanced DAG LSTM, the multi-headed attention mechanism used two heads, with each W</p><p>mapping from the node state dimensionality to double the node state dimensionality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Training</head><p>Our models were constructed in PyTorch <ref type="bibr" target="#b37">[37]</ref> and trained with the Adam Optimizer <ref type="bibr" target="#b38">[38]</ref> with default settings. The loss function optimized for was binary cross-entropy. We trained each model for 5 epochs on Holstep and 30 epochs on Mizar, as validation performance did not improve with more training. Performance on the validation sets was evaluated after each epoch and the best performing model on validation was used for the single evaluation on the test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Hardware Setup</head><p>All experiments were run on Linux machines with 72-core Intel Xeon(R) 6140 CPUs @ 2.30 GHz and 750 GB RAM, and two Tesla P100 GPUs with 16 GB GPU memory.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Operating system verification-an overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerwin</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sadhana</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="27" to="69" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Comprehensive formal verification of an os microkernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerwin</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">June</forename><surname>Andronick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Elphinstone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Sewell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Kolanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gernot</forename><surname>Heiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer Systems (TOCS)</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="70" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The ioa language and toolset: Support for designing, analyzing, and building distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nancy</forename><forename type="middle">A</forename><surname>Garland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lynch</surname></persName>
		</author>
		<idno>MIT/LCS/TR- 762</idno>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
		<respStmt>
			<orgName>Laboratory for Computer Science . . .</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ironfleet: proving practical distributed systems correct</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hawblitzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Howell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manos</forename><surname>Kapritsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Lorch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Setty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th Symposium on Operating Systems Principles</title>
		<meeting>the 25th Symposium on Operating Systems Principles</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A verified compiler for a structured assembly language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Curzon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Curzon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TPHOLs</title>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page" from="253" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Formal verification of a realistic compiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Leroy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="107" to="115" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Microprocessor design verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Warren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hunt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Automated Reasoning</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="429" to="460" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A formal proof of the kepler conjecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gertrud</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat Dat</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cezary</forename><surname>Hoang Le Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Kaliszyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Magron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat</forename><surname>Mclaughlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thang Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Forum of Mathematics, Pi</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2017" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The tptp problem library and associated infrastructure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Sutcliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Automated Reasoning</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">337</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">First-orderized researchcyc: Expressivity and efficiency in a common-sense ontology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pace</forename><surname>Reagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Goolsbey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI workshop on contexts and ontologies: theory, practice and applications</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sine qua non for large theory reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kryštof</forename><surname>Hoder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Voronkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Automated Deduction</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="299" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">An introduction to the syntax and content of cyc</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Matuszek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Witbrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Cabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Deoliveira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
		<respStmt>
			<orgName>UMBC Computer Science and Electrical Engineering Department Collection</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mizar 40 for mizar 40</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cezary</forename><surname>Kaliszyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Urban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Autom. Reasoning</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="245" to="256" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The suggested upper merged ontology: A large ontology for the semantic web and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Pease</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Niles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Working notes of the AAAI-2002 workshop on ontologies and the semantic web</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="7" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Divvy: An atp meta-system based on axiom relevance ordering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Roederer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yury</forename><surname>Puzis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Sutcliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Automated Deduction</title>
		<imprint>
			<biblScope unit="page" from="157" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Overview and evaluation of premise selection techniques for large theory mathematics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kühlwein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeni</forename><surname>Twan Van Laarhoven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Tsivtsivadze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heskes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Automated Reasoning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="378" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Premise selection for mathematics by corpus analysis and kernel methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Alama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Heskes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kühlwein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeni</forename><surname>Tsivtsivadze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Urban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Automated Reasoning</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="191" to="213" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deepmath-deep sequence models for premise selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niklas</forename><surname>Alexander A Alemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Een</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Chollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Urban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2235" to="2243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Holist: An environment for machine learning of higher order logic theorem proving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kshitij</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Loos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Rabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stewart</forename><surname>Wilcox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="454" to="463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Holstep: A machine learning dataset for higher-order logic theorem proving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cezary</forename><surname>Kaliszyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Loos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cezary</forename><surname>Kaliszyk</surname></persName>
		</author>
		<title level="m">Deep network guided proof search. International Conference on Logic for Programming, Artificial Intelligence, and Reasoning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Premise selection for theorem proving by deep graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihe</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2786" to="2796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Graph representations for higher-order logic and theorem proving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Paliwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Loos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Rabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kshitij</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI 2020</title>
		<meeting>AAAI 2020</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1556" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Schulz</surname></persName>
		</author>
		<title level="m">System description: E 1.8. In International Conference on Logic for Programming Artificial Intelligence and Reasoning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="735" to="743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Premise selection with neural networks and distributed representation of features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrzej</forename><surname>Stanisław Kucik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Korovin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.10268</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Property invariant embedding for automated reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miroslav</forename><surname>Olšák</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cezary</forename><surname>Kaliszyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Urban</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.12073</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Saxton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Amos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<title level="m">Can neural networks understand logical entailment? International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Gamepad: A learning environment for theorem proving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Practical foundations of mathematics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Paul</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="volume">59</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
