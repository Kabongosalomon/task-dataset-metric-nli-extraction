<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachit</forename><surname>Menon</surname></persName>
							<email>sachit.menon@duke.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Duke University Durham</orgName>
								<address>
									<region>NC</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Damian</surname></persName>
							<email>alexandru.damian@duke.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Duke University Durham</orgName>
								<address>
									<region>NC</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijia</forename><surname>Hu</surname></persName>
							<email>shijia.hu@duke.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Duke University Durham</orgName>
								<address>
									<region>NC</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Ravi</surname></persName>
							<email>nikhil.ravi@duke.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Duke University Durham</orgName>
								<address>
									<region>NC</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Rudin</surname></persName>
							<email>cynthia.rudin@duke.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Duke University Durham</orgName>
								<address>
									<region>NC</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>* denotes equal contribution</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The primary aim of single-image super-resolution is to construct a high-resolution (HR) image from a corresponding low-resolution (LR) input. In previous approaches, which have generally been supervised, the training objective typically measures a pixel-wise average distance between the super-resolved (SR) and HR images. Optimizing such metrics often leads to blurring, especially in high variance (detailed) regions. We propose an alternative formulation of the super-resolution problem based on creating realistic SR images that downscale correctly. We present a novel super-resolution algorithm addressing this problem, PULSE (Photo Upsampling via Latent Space Exploration), which generates high-resolution, realistic images at resolutions previously unseen in the literature. It accomplishes this in an entirely self-supervised fashion and is not confined to a specific degradation operator used during training, unlike previous methods (which require training on databases of LR-HR image pairs for supervised learning). Instead of starting with the LR image and slowly adding detail, PULSE traverses the high-resolution natural image manifold, searching for images that downscale to the original LR image. This is formalized through the "downscaling loss," which guides exploration through the latent space of a generative model. By leveraging properties of high-dimensional Gaussians, we restrict the search space to guarantee that our outputs are realistic. PULSE thereby generates super-resolved images that both are realistic and downscale correctly. We show extensive experimental results demonstrating the efficacy of our approach in the domain of face super-resolution (also known as face hallucination). We also present a discussion of the limitations and biases of the method as currently implemented with an accompanying model card with relevant metrics. Our method outperforms state-of-the-art methods in perceptual quality at higher resolutions and scale factors than previously pos-sible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>* denotes equal contribution</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In this work, we aim to transform blurry, low-resolution images into sharp, realistic, high-resolution images. Here, we focus on images of faces, but our technique is generally applicable. In many areas (such as medicine, astronomy, microscopy, and satellite imagery), sharp, high-resolution images are difficult to obtain due to issues of cost, hardware restriction, or memory limitations <ref type="bibr" target="#b24">[24]</ref>. This leads to the capture of blurry, low-resolution images instead. In other cases, images could be old and therefore blurry, or even in a modern context, an image could be out of focus or a person could be in the background. In addition to being visually unappealing, this impairs the use of downstream analysis methods (such as image segmentation, action recognition, or disease diagnosis) which depend on having highresolution images <ref type="bibr" target="#b19">[20]</ref>  <ref type="bibr" target="#b23">[23]</ref>. In addition, as consumer laptop, phone, and television screen resolution has increased over recent years, popular demand for sharp images and video has surged. This has motivated recent interest in the computer vision task of image super-resolution, the creation of realistic high-resolution (henceforth HR) images that a given low-resolution (LR) input image could correspond to.</p><p>While the benefits of methods for image super-resolution are clear, the difference in information content between HR and LR images (especially at high scale factors) hampers efforts to develop such techniques. In particular, LR images inherently possess less high-variance information; details can be blurred to the point of being visually indistinguishable. The problem of recovering the true HR image depicted by an LR input, as opposed to generating a set of potential such HR images, is inherently ill-posed, as the size of the total set of these images grows exponentially with the scale factor <ref type="bibr" target="#b2">[3]</ref>. That is to say, many high-resolution images can correspond to the exact same low-resolution image.</p><p>Traditional supervised super-resolution algorithms train a model (usually, a convolutional neural network, or CNN) to minimize the pixel-wise mean-squared error (MSE) between the generated super-resolved (SR) images and the corresponding ground-truth HR images <ref type="bibr" target="#b14">[15]</ref>  <ref type="bibr" target="#b7">[8]</ref>. However, this approach has been noted to neglect perceptually relevant details critical to photorealism in HR images, such as texture <ref type="bibr" target="#b15">[16]</ref>. Optimizing on an average difference in pixelspace between HR and SR images has a blurring effect, encouraging detailed areas of the SR image to be smoothed out to be, on average, more (pixelwise) correct. In fact, in the case of mean squared error (MSE), the ideal solution is the (weighted) pixel-wise average of the set of realistic images that downscale properly to the LR input (as detailed later). The inevitable result is smoothing in areas of high variance, such as areas of the image with intricate patterns or textures. As a result, MSE should not be used alone as a measure of image quality for super-resolution.</p><p>Some researchers have attempted to extend these MSEbased methods to additionally optimize on metrics intended to encourage realism, serving as a force opposing the smoothing pull of the MSE term <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b7">8]</ref>. This essentially drags the MSE-based solution in the direction of the natural image manifold (the subset of R M Ã—N that represents the set of high-resolution images). This compromise, while improving perceptual quality over pure MSE-based solutions, makes no guarantee that the generated images are realistic. Images generated with these techniques still show signs of blurring in high variance areas of the images, just as in the pure MSE-based solutions.</p><p>To avoid these issues, we propose a new paradigm for super-resolution. The goal should be to generate realistic images within the set of feasible solutions; that is, to find points which actually lie on the natural image manifold and also downscale correctly. The (weighted) pixel-wise average of possible solutions yielded by the MSE does not generally meet this goal for the reasons previously described. We provide an illustration of this in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>Our method generates images using a (pretrained) generative model approximating the distribution of natural images under consideration. For a given input LR image, we traverse the manifold, parameterized by the latent space of the generative model, to find regions that downscale correctly. In doing so, we find examples of realistic images that downscale properly, as shown in 1.</p><p>Such an approach also eschews the need for supervised training, being entirely self-supervised with no 'training' needed at the time of super-resolution inference (except for the unsupervised generative model). This framework presents multiple substantial benefits. First, it allows the same network to be used on images with differing degradation operators even in the absence of a database of corresponding LR-HR pairs (as no training on such databases takes place). Furthermore, unlike previous methods, it does not require super-resolution task-specific network architectures, which take substantial time on the part of the researcher to develop without providing real insight into the problem; instead, it proceeds alongside the state-of-the-art in generative modeling, with zero retraining needed.</p><p>Our approach works with any type of generative model with a differentiable generator, including flow-based models, variational autoencoders (VAEs), and generative adver- sarial networks (GANs); the particular choice is dictated by the tradeoffs each make in approximating the data manifold. For this work, we elected to use GANs due to recent advances yielding high-resolution, sharp images <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>One particular subdomain of image super-resolution deals with the case of face images. This subdomain -known as face hallucination -finds application in consumer photography, photo/video restoration, and more <ref type="bibr" target="#b28">[28]</ref>. As such, it has attracted interest as a computer vision task in its own right. Our work focuses on face hallucination, but our methods extend to a more general context.</p><p>Because our method always yields a solution that both lies on the natural image manifold and downsamples correctly to the original low-resolution image, we can provide a range of interesting high-resolution possibilities e.g. by making use of the stochasticity inherent in many generative models: our technique can create a set of images, each of which is visually convincing, yet look different from each other, where (without ground truth) any of the images could plausibly have been the source of the low-resolution input.</p><p>Our main contributions are as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1.</head><p>A new paradigm for image super-resolution. Previous efforts take the traditional, ill-posed perspective of attempting to 'reconstruct' an HR image from an LR input, yielding outputs that, in effect, average many possible solutions. This averaging introduces undesirable blurring. We introduce new approach to superresolution: a super-resolution algorithm should create realistic high-resolution outputs that downscale to the correct LR input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>A novel method for solving the super-resolution task. In line with our new perspective, we propose a new algorithm for super-resolution. Whereas tradi-tional work has at its core aimed to approximate the LR â†’ HR map using supervised learning (especially with neural networks), our approach centers on the use of unsupervised generative models of HR data. Using generative adversarial networks, we explore the latent space to find regions that map to realistic images and downscale correctly. No retraining is required. Our particular implementation, using StyleGAN <ref type="bibr" target="#b12">[13]</ref>, allows for the creation of any number of realistic SR samples that correctly map to the LR input.</p><p>3. An original method for latent space search under high-dimensional Gaussian priors. In our task and many others, it is often desirable to find points in a generative model's latent space that map to realistic outputs. Intuitively, these should resemble samples seen during training. At first, it may seem that traditional log-likelihood regularization by the latent prior would accomplish this, but we observe that the 'soap bubble' effect (that much of the density of a high dimensional Gaussian lies close to the surface of a hypersphere) contradicts this. Traditional log-likelihood regularization actually tends to draw latent vectors away from this hypersphere and, instead, towards the origin. We therefore constrain the search space to the surface of that hypersphere, which ensures realistic outputs in higher-dimensional latent spaces; such spaces are otherwise difficult to search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>While there is much work on image super-resolution prior to the advent of convolutional neural networks (CNNs), CNN-based approaches have rapidly become state-of-the-art in the area and are closely relevant to our work; we therefore focus on neural network-based approaches here. Generally, these methods use a pipeline where a low-resolution (LR) image, created by downsampling a high-resolution (HR) image, is fed through a CNN with both convolutional and upsampling layers, generating a super-resolved (SR) output. This output is then used to calculate the loss using the chosen loss function and the original HR image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Current Trends</head><p>Recently, supervised neural networks have come to dominate current work in super-resolution. Dong et al. <ref type="bibr" target="#b8">[9]</ref> proposed the first CNN architecture to learn this non-linear LR to HR mapping using pairs of HR-LR images. Several groups have attempted to improve the upsampling step by utilizing sub-pixel convolutions and transposed convolutions <ref type="bibr" target="#b22">[22]</ref>. Furthermore, the application of ResNet architectures to super-resolution (started by SRResNet <ref type="bibr" target="#b15">[16]</ref>), has yielded substantial improvement over more traditional con-volutional neural network architectures. In particular, the use of residual structures allowed for the training of larger networks. Currently, there exist two general trends: one, towards networks that primarily better optimize pixel-wise average distance between SR and HR, and two, networks that focus on perceptual quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Loss Functions</head><p>Towards these different goals, researchers have designed different loss functions for optimization that yield images closer to the desired objective. Traditionally, the loss function for the image super-resolution task has operated on a per-pixel basis, usually using the L2 norm of the difference between the ground truth and the reconstructed image, as this directly optimizes PSNR (the traditional metric for the super-resolution task). More recently, some researchers have started to use the L1 norm since models trained using L1 loss seem to perform better in PSNR evaluation. The L2 norm (as well as pixel-wise average distances in general) between SR and HR images has been heavily criticized for not correlating well with human-observed image quality <ref type="bibr" target="#b15">[16]</ref>. In face super-resolution, the state-of-the-art for such metrics is FSRNet <ref type="bibr" target="#b7">[8]</ref>, which used a facial prior to achieve previously unseen PSNR.</p><p>Perceptual quality, however, does not necessarily increase with higher PSNR. As such, different methods, and in particular, objective functions, have been developed to increase perceptual quality. In particular, methods that yield high PSNR result in blurring of details. The information required for details is often not present in the LR image and must be 'imagined' in. One approach to avoiding the direct use of the standard loss functions was demonstrated in <ref type="bibr" target="#b25">[25]</ref>, which draws a prior from the structure of a convolutional network. This method produces similar images to the methods that focus on PSNR, which lack detail, especially in high frequency areas. Because this method cannot leverage learned information about what realistic images look like, it is unable to fill in missing details. Methods that try to learn a map from LR to HR images can try to leverage learned information; however, as mentioned, networks optimized on PSNR are still explicitly penalized for attempting to hallucinate details they are unsure about, thus optimizing on PSNR stills resulting in blurring and lack of detail.</p><p>To resolve this issue, some have tried to use generative model-based loss terms to provide these details. Neural networks have lent themselves to application in generative models of various types (especially generative adversarial networks-GANs-from <ref type="bibr" target="#b9">[10]</ref>), to image reconstruction tasks in general, and more recently, to super-resolution. Ledig et al. <ref type="bibr" target="#b15">[16]</ref> created the SRGAN architecture for single-image upsampling by leveraging these advances in deep generative models, specifically GANs. Their general methodology was to use the generator to upscale the low-resolution input image, which the discriminator then attempts to distinguish from real HR images, then propagate the loss back to both networks. Essentially, this optimizes a supervised network much like MSE-based methods with an additional loss term corresponding to how fake the discriminator believes the generated images to be. However, this approach is fundamentally limited as it essentially results in an averaging of the MSE-based solution and a GAN-based solution, as we discuss later. In the context of faces, this technique has been incorporated into FSRGAN, resulting in the current perceptual state-of-the-art in face super resolution at Ã—8 upscaling factors up to resolutions of 128 Ã— 128. Although these methods use a 'generator' and a 'discriminator' as found in GANs, they are trained in a completely supervised fashion; they do not use unsupervised generative models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Generative Networks</head><p>Our algorithm does not simply use GAN-style training; rather, it uses a truly unsupervised GAN (or, generative model more broadly). It searches the latent space of this generative model for latents that map to images that downscale correctly. The quality of cutting-edge generative models is therefore of interest to us.</p><p>As GANs have produced the highest-quality highresolution images of deep generative models to date, we chose to focus on these for our implementation. Here we provide a brief review of relevant GAN methods with highresolution outputs. Karras et al. <ref type="bibr" target="#b11">[12]</ref> presented some of the first high-resolution outputs of deep generative models in their ProGAN algorithm, which grows both the generator and the discriminator in a progressive fashion. Karras et al. <ref type="bibr" target="#b12">[13]</ref> further built upon this idea with StyleGAN, aiming to allow for more control in the image synthesis process relative to the black-box methods that came before it. The input latent code is embedded into an intermediate latent space, which then controls the behavior of the synthesis network with adaptive instance normalization applied at each convolutional layer. This network has 18 layers (2 each for each resolution from 4 Ã— 4 to 1024 Ã— 1024). After every other layer, the resolution is progressively increased by a factor of 2. At each layer, new details are introduced stochastically via Gaussian input to the adaptive instance normalization layers. Without perturbing the discriminator or loss functions, this architecture leads to the option for scale-specific mixing and control over the expression of various high-level attributes and variations in the image (e.g. pose, hair, freckles, etc.). Thus, StyleGAN provides a very rich latent space for expressing different features, especially in relation to faces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>We begin by defining some universal terminology necessary to any formal description of the super-resolution prob-lem. We denote the low-resolution input image by I LR . We aim to learn a conditional generating function G that, when applied to I LR , yields a higher-resolution super-resolved image I SR . Formally, let</p><formula xml:id="formula_0">I LR âˆˆ R mÃ—n . Then our desired function SR is a map R mÃ—n â†’ R M Ã—N where M &gt; m, N &gt; n. We define the super-resolved image I SR âˆˆ R M Ã—N I SR := SR(I LR ).<label>(1)</label></formula><p>In a traditional approach to super-resolution, one considers that the low-resolution image could represent the same information as a theoretical high-resolution image I HR âˆˆ R M Ã—N . The goal is then to best recover this particular I HR given I LR . Such approaches therefore reduce the problem to an optimization task: fit a function SR that minimizes</p><formula xml:id="formula_1">L := I HR âˆ’ I SR p p (2)</formula><p>where Â· p denotes some l p norm.</p><p>In practice, even when trained correctly, these algorithms fail to enhance detail in high variance areas. To see why this is, fix a low resolution image I LR . Let M be the natural image manifold in R M Ã—N , i.e., the subset of R M Ã—N that resembles natural realistic images, and let P be a probability distribution over M describing the likelihood of an image appearing in our dataset. Finally, let R be the set of images that downscale correctly, i.e., R = {I âˆˆ R N Ã—M : DS(I) = I LR }. Then in the limit as the size of our dataset tends to infinity, our expected loss when the algorithm outputs a fixed image I SR is</p><formula xml:id="formula_2">Mâˆ©R I HR âˆ’ I SR p p dP (I HR ).<label>(3)</label></formula><p>This is minimized when I SR is an l p average of I HR over M âˆ© R. In fact, when p = 2, this is minimized when</p><formula xml:id="formula_3">I SR = Mâˆ©R I HR dP (I HR ),<label>(4)</label></formula><p>so the optimal I SR is a weighted pixelwise average of the set of high resolution images that downscale properly. As a result, the lack of detail in algorithms that rely only on an l p norm cannot be fixed simply by changing the architecture of the network. The problem itself has to be rephrased. We therefore propose a new framework for single image super resolution. Let M, DS be defined as above. Then for a given LR image I LR âˆˆ R mÃ—n and &gt; 0, our goal is to find an image I SR âˆˆ M with</p><formula xml:id="formula_4">DS(I SR ) âˆ’ I LR p â‰¤ .<label>(5)</label></formula><p>In particular, we can let R âŠ‚ R N Ã—M be the set of images that downscale properly, i.e., Then we are seeking an image I SR âˆˆ Mâˆ©R . The set Mâˆ© R is the set of feasible solutions, because a solution is not feasible if it did not downscale properly and look realistic. It is also interesting to note that the intersections Mâˆ©R and in particular M âˆ© R 0 are guaranteed to be nonempty, because they must contain the original HR image (i.e., what traditional methods aim to reconstruct).</p><formula xml:id="formula_5">R = {I âˆˆ R N Ã—M : DS(I) âˆ’ I LR p p â‰¤ }. (6)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Downscaling Loss</head><p>Central to the problem of super-resolution, unlike general image generation, is the notion of correctness. Traditionally, this has been interpreted to mean how well a particular ground truth image I HR is 'recovered' by the application of the super-resolution algorithm SR to the lowresolution input I LR , as discussed in the related work section above. This is generally measured by some l p norm between I SR and the ground truth, I HR ; such algorithms only look somewhat like real images because minimizing this metric drives the solution somewhat nearer to the manifold. However, they have no way to ensure that I SR lies close to M. In contrast, in our framework, we never deviate from M, so such a metric is not necessary. For us, the critical notion of correctness is how well the generated SR image I SR corresponds to I LR .</p><p>We formalize this through the downscaling loss, to explicitly penalize a proposed SR image for deviating from its LR input (similar loss terms have been proposed in <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b25">[25]</ref>). This is inspired by the following: for a proposed SR image to represent the same information as a given LR image, it must downscale to this LR image. That is,</p><formula xml:id="formula_6">I LR â‰ˆ DS(I SR ) = DS(SR(I LR ))<label>(7)</label></formula><p>where DS(Â·) represents the downscaling function.</p><p>Our downscaling loss therefore penalizes SR the more its outputs violate this,</p><formula xml:id="formula_7">L DS (I SR , I LR ) := DS(I SR ) âˆ’ I LR p p .<label>(8)</label></formula><p>It is important to note that the downscaling loss can be used in both supervised and unsupervised models for superresolution; it does not depend on an HR reference image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Latent Space Exploration</head><p>How might we find regions of the natural image manifold M that map to the correct LR image under the downscaling operator? If we had a differentiable parameterization of the manifold, we could progress along the manifold to these regions by using the downscaling loss to guide our search. In that case, images found would be guaranteed to be high resolution as they came from the HR image manifold, while also being correct as they would downscale to the LR input.</p><p>In reality, we do not have such convenient, perfect parameterizations of manifolds. However, we can approximate such a parameterization by using techniques from unsupervised learning. In particular, much of the field of deep generative modeling (e.g. VAEs, flow-based models, and GANs) is concerned with creating models that map from some latent space to a given manifold of interest. By leveraging advances in generative modeling, we can even use pretrained models without the need to train our own network. Some prior work has aimed to find vectors in the latent space of a generative model to accomplish a task; see <ref type="bibr" target="#b1">[2]</ref> for creating embeddings and <ref type="bibr" target="#b5">[6]</ref> in the context of compressed sensing. (However, as we describe later, this work does not actually search in a way that yields realistic outputs as intended.) In this work, we focus on GANs, as recent work in this area has resulted in the highest quality image-generation among unsupervised models.</p><p>Regardless of its architecture, let the generator be called G, and let the latent space be L. Ideally, we could approximate M by the image of G, which would allow us to rephrase the problem above as the following: find a latent vector z âˆˆ L with</p><formula xml:id="formula_8">DS(G(z)) âˆ’ I LR p p â‰¤ .<label>(9)</label></formula><p>Unfortunately, in most generative models, simply requiring that z âˆˆ L does not guarantee that G(z) âˆˆ M; rather, such methods use an imposed prior on L. In order to ensure G(z) âˆˆ M, we must be in a region of L with high probability under the chosen prior. One idea to encourage the latent to be in the region of high probability is to add a loss term for the negative log-likelihood of the prior. In the case of a Gaussian prior, this takes the form of l 2 regularization. Indeed, this is how the previously mentioned work <ref type="bibr" target="#b5">[6]</ref> attempts to address this issue. However, this idea does not actually accomplish the goal. Such a penalty forces vectors towards 0, but most of the mass of a high-dimensional Gaussian is located near the surface of a sphere of radius âˆš d (see <ref type="bibr" target="#b26">[26]</ref>). To get around this, we observed that we could replace the Gaussian prior on R d with a uniform prior on âˆš dS dâˆ’1 . This approximation can be used for any method with high dimensional spherical Gaussian priors.</p><p>We can let L = âˆš dS dâˆ’1 (where S dâˆ’1 âŠ‚ R d is the unit sphere in d dimensional Euclidean space) and reduce the problem above to finding a z âˆˆ L that satisfies Equation <ref type="bibr" target="#b8">(9)</ref>. This reduces the problem from gradient descent in the entire latent space to projected gradient descent on a sphere.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We designed various experiments to assess our method. We focus on the popular problem of face hallucination, enhanced by recent advances in GANs applied to face generation. In particular, we use Karras et al.'s pretrained Face StyleGAN (trained on the Flickr Face HQ Dataset, or FFHQ) <ref type="bibr" target="#b12">[13]</ref>. We adapted the implementation found at <ref type="bibr" target="#b27">[27]</ref> in order to transfer the original StyleGAN-FFHQ weights and model from TensorFlow <ref type="bibr">[1]</ref> to PyTorch <ref type="bibr" target="#b18">[19]</ref>. For each experiment, we used 100 steps of spherical gradient descent with a learning rate of 0.4 starting with a random initialization. Each image was therefore generated in âˆ¼5 seconds on a single NVIDIA V100 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data</head><p>We evaluated our procedure on the well-known highresolution face dataset CelebA HQ. (Note: this is not to be confused with CelebA, which is of substantially lower resolution.) We performed these experiments using scale factors of 64Ã—, 32Ã—, and 8Ã—. For our qualitative comparisons, we upscale at scale factors of both 8Ã— and 64Ã—, i.e., from 16 Ã— 16 to 128 Ã— 128 resolution images and 1024 Ã— 1024 resolution images. The state-of-the-art for face super-resolution in the literature prior to this point was limited to a maximum of 8Ã— upscaling to a resolution of 128Ã—128, thus making it impossible to directly make quantitative comparisons at high resolutions and scale factors. We followed the traditional approach of training the supervised methods on CelebA HQ. We tried comparing with supervised methods trained on FFHQ, but they failed to generalize and yielded very blurry and distorted results when evaluated on CelebA HQ; therefore, in order to compare our method with the best existing methods, we elected to train the supervised models on CelebA HQ instead of FFHQ. <ref type="figure" target="#fig_4">Figure 5</ref> shows qualitative results to demonstrate the visual quality of the images from our method. We observe levels of detail that far surpass competing methods, as exemplified by certain high frequency regions (features like eyes or lips). More examples and full-resolution images are in the appendix. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Qualitative Image Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Quantitative Comparison</head><p>Here we present a quantitative comparison with state-ofthe-art face super-resolution methods. Due to constraints on the peak resolution that previous methods can handle, evaluation methods were limited, as detailed below.</p><p>We conducted a mean-opinion-score (MOS) test as is common in the perceptual super-resolution literature <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b13">14]</ref>. For this, we had 40 raters examine images upscaled by 6 different methods (nearest-neighbors, bicubic, FSRNet, FSRGAN, and our PULSE). For this comparison, we used a scale factor of 8 and a maximum resolution of 128 Ã— 128, despite our method's ability to go substantially higher, due to this being the maximum limit for the competing methods. After being exposed to 20 examples of a 1 (worst) rating exemplified by nearest-neighbors upsampling, and a 5 (best) rating exemplified by high-quality HR images, raters provided a score from 1-5 for each of the 240 images. All images fell within the appropriate = 1e âˆ’ 3 for the downscaling loss. The results are displayed in <ref type="table">Table 1</ref>.</p><p>PULSE outperformed the other methods and its score approached that of the HR dataset. Note that the HR's 3.74 av- erage image quality reflects the fact that some of the HR images in the dataset had noticeable artifacts. All pairwise differences were highly statistically significant (p &lt; 10 âˆ’5 for all 15 comparisons) by the Mann-Whitney-U test. The results demonstrate that PULSE outperforms current methods in generating perceptually convincing images that downscale correctly.</p><p>To provide another measure of perceptual quality, we evaluated the Naturalness Image Quality Evaluator (NIQE) score <ref type="bibr" target="#b17">[18]</ref>, previously used in perceptual super-resolution <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b29">29]</ref>. This no-reference metric extracts features from images and uses them to compute a perceptual index (lower is better). As such, however, it only yields meaningful results at higher resolutions. This precluded direct comparison with FSRNet and FSRGAN, which produce images of at most 128 Ã— 128 pixels.  <ref type="figure">Figure 6</ref>. (x32) We show the robustness of PULSE under various degradation operators. In particular, these are downscaling followed by Gaussian noise (std=25, 50), motion blur in random directions with length 100 followed by downscaling, and downscaling followed by salt-and-pepper noise with a density of 0.05.</p><p>We evaluated NIQE scores for each method at a resolution of 1024 Ã— 1024 from an input resolution of 16 Ã— 16, for a scale factor of 64. All images for each method fell within the appropriate = 1e âˆ’ 3 for the downscaling loss. The results are in <ref type="table" target="#tab_1">Table 2</ref>. PULSE surpasses even the CelebA HQ images in terms of NIQE here, further showing the perceptual quality of PULSE's generated images. This is possible as NIQE is a no-reference metric which solely considers perceptual quality; unlike reference metrics like PSNR, performance is not bounded above by that of the HR images typically used as reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Image Sampling</head><p>As referenced earlier, we initialize the point we start at in the latent space by picking a random point on the sphere. We found that we did not encounter any issues with convergence from random initializations. In fact, this provided us one method of creating many different outputs with highlevel feature differences: starting with different initializations. An example of the variation in outputs yielded by this process can be observed in <ref type="figure" target="#fig_2">Figure 3</ref>.</p><p>Furthermore, by utilizing a generative model with inherent stochasticity, we found we could sample faces with fine-level variation that downscale correctly; this procedure can be repeated indefinitely. In our implementation, we accomplish this by resampling the noise inputs that StyleGAN uses to fill in details within the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Robustness</head><p>The main aim of our algorithm is to perform perceptually realistic super-resolution with a known downscaling operator. However, we find that even for a variety of unknown downscaling operators, we can apply our method using bicubic downscaling as a stand-in for more substantial degradations applied-see <ref type="figure">Figure 6</ref>. In this case, we provide only the degraded low-resolution image as input. We find that the output downscales approximately to the true, nonnoisy LR image (that is, the bicubically downscaled HR) rather than to the degraded LR given as input. This is desired behavior, as we would not want to create an image that matches the additional degradations. PULSE thus implicitly denoises images. This is due to the fact that we restrict the outputs to only realistic faces, which in turn can only downscale to reasonable LR faces. Traditional supervised networks, on the other hand, are sensitive to added noise and changes in the domain and must therefore be explicitly trained with the noisy inputs (e.g., <ref type="bibr" target="#b3">[4]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Bias</head><p>While we initially chose to demonstrate PULSE using StyleGAN (trained on FFHQ) as the generative model for its impressive image quality, we noticed some bias when evaluated on natural images of faces outside of our test set. In particular, we believe that PULSE may illuminate some biases inherent in StyleGAN. We document this in a more structured way with a model card in <ref type="figure">Figure 8</ref>, where we also examine success/failure rates of PULSE across different subpopulations. We propose a few possible sources for this bias:</p><p>Bias inherited from latent space constraint: If StyleGAN pathologically placed people of color in areas of lower density in the latent space, bias would be introduced by PULSE's constraint on the latent space which is necessary to consistently generate high resolution images. To evaluate this, we ran PULSE with different radii for the hypersphere PULSE searches on, corresponding to different samples. This did not seem to have an effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Failure to converge:</head><p>In the initial code we released on GitHub, PULSE failed to return "no image found" when at the end of optimization it still did not find an image that downscaled correctly (within ). The concern could therefore be that it is harder to find images in the outputs of StyleGAN that downscale to people of color than to white people. To test this, we found a new dataset with better representation to evaluate success/failure rates on, "FairFace: Face Attribute Dataset for Balanced Race, Gender, and Age" <ref type="bibr" target="#b30">[30]</ref>. This dataset was labeled by third-party annotators on these fields. We sample 100 examples per subgroup and calculate the success/failure rate across groups with Ã—64 downscaling after running PULSE 5 times per image. The results of this experiment can be found in <ref type="table">Table 6</ref>. There is some variation in these percentages, but it does not seem to be the primary cause of what we observed. Note that this metric is lacking in that it only reports whether an image was found -which does not reflect the diversity of images found over many runs on many images, an important measure that was difficult to quantify.</p><p>Bias inherited from optimization: This would imply that the constrained latent space contains a wide range of images of people of color but that PULSE's optimization procedure does not find them. However, if this is the case then we should be able to find such images with enough random initializations in the constrained latent space. We ran this experiment and this also did not seem to have an effect.</p><p>Bias inherited from StyleGAN: Some have noted that it seems more diverse images can be found in an augmented latent space of StyleGAN per <ref type="bibr" target="#b1">[2]</ref>. However, this is not close to the set of images StyleGAN itself generates when trained on faces: for example, in the same paper, the authors display images of unrelated domains (such as cats) being embedded successfully as well. In our work, PULSE is constrained to images StyleGAN considers realistic face images (actually, a slight expansion of this set; see below and Appendix for an explanation of this).</p><p>More technically: in StyleGAN, they sample a latent vector z, which is fed through the mapping network to become a vector w, which is duplicated 18 times to be fed through the synthesis network. In <ref type="bibr" target="#b1">[2]</ref>, they instead find 18 different vectors to feed through the synthesis network that correspond to any image of interest (whether faces or otherwise). In addition, while each of these latent vectors would have norm â‰ˆ âˆš 512 when sampled, this augmented latent space allows them to vary freely, potentially finding points in the latent space very far from what would have been seen in training. Using this augmented latent space therefore removes any guarantee that the latent recovered corresponds to a realistic image of a face.</p><p>In our work, instead of duplicating the vector 18 times as StyleGAN does, we relax this constraint and encourage these 18 vectors to be approximately equal to each other so as to still generate realistic outputs (see Appendix). This relaxation means the set of images PULSE can generate should be broader than the set of images StyleGAN could produce naturally. We found that loosening any of Style-GAN's constraints on the latent space further generally led to unrealistic faces or images that were not faces.</p><p>Overall, it seems that sampling from StyleGAN yields white faces much more frequently than faces of people of color, indicating more of the prior density may be dedicated to white faces. Recent work by Salminen et al. <ref type="bibr" target="#b21">[21]</ref> describes the implicit biases of StyleGAN in more detail, which seem to confirm these observations. In particular, we note their analysis of the demographic bias of the outputs of the model:</p><p>Results indicate a racial bias among the generated pictures, with close to three-[fourths] (72.6%) of the pictures representing White people. Asian (13.8%) and Black (10.1%) are considerably less frequent, while Indians represent only a minor fraction of the pictures (3.4%).</p><p>This bias extends to any downstream application of Style-GAN, including the implementation of PULSE using Style-GAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Discussion and Future Work</head><p>Through these experiments, we find that PULSE produces perceptually superior images that also downscale correctly. PULSE accomplishes this at resolutions previously unseen in the literature. All of this is done with unsupervised methods, removing the need for training on paired datasets of LR-HR images. The visual quality of our images as well as MOS and NIQE scores demonstrate that our proposed formulation of the super-resolution problem corresponds with human intuition. Starting with a pre-trained GAN, our method operates only at test time, generating each image in about 5 seconds on a single GPU. However, we also note significant limitations when evaluated on natural images past the standard benchmark.</p><p>One reasonable concern when searching the output space of GANs for images that downscale properly is that while GANs generate sharp images, they need not cover the whole distribution as, e.g., flow-based models must. In our experiments using CelebA and StyleGAN, we did not observe any manifestations of this, which may be attributable to bias see Section 6 (The "mode collapse" behavior of GANs may exacerbate dataset bias and contribute to the results described in Section 6 and the model card, <ref type="figure">Figure 8</ref>.) Advances in generative modeling will allow for generative models with better coverage of larger distributions, which can be directly used with PULSE without modification.</p><p>Another potential concern that may arise when considering this unsupervised approach is the case of an unknown downscaling function. In this work, we focused on the most prominent SR use case: on bicubically downscaled images. In fact, in many use cases, the downscaling function is either known analytically (e.g., bicubic) or is a (known) function of hardware. However, methods have shown that the degradations can be estimated in entirely unsupervised fashions for arbitrary LR images (that is, not necessarily those which have been downscaled bicubically) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b31">31]</ref>. Through such methods, we can retain the algorithm's lack of supervision; integrating these is an interesting topic for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusions</head><p>We have established a novel methodology for image super-resolution as well as a new problem formulation. This opens up a new avenue for super-resolution methods along different tracks than traditional, supervised work with CNNs. The approach is not limited to a particular degradation operator seen during training, and it always maintains high perceptual quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Card -PULSE with StyleGAN FFHQ Generative Model Backbone</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Details</head><p>â€¢ PULSE developed by researchers at Duke University, 2020, v1.</p><p>-Latent Space Exploration Technique.</p><p>-PULSE does no training, but is evaluated by downscaling loss (equation 9) for fidelity to input low-resolution image.</p><p>-Requires pre-trained generator to parameterize natural image manifold. â€¢ StyleGAN developed by researchers at NVIDIA, 2018, v1.</p><p>-Generative Adversarial Network.</p><p>-StyleGAN trained with adversarial loss (WGAN-GP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intended Use</head><p>â€¢ PULSE was intended as a proof of concept for one-to-many super-resolution (generating multiple high resolution outputs from a single image) using latent space exploration. â€¢ Intended use of implementation using StyleGAN-FFHQ (faces) is purely as an art project -seeing fun pictures of imaginary people that downscale approximately to a low-resolution image. â€¢ Not suitable for facial recognition/identification. PULSE makes imaginary faces of people who do not exist, which should not be confused for real people. It will not help identify or reconstruct the original image. â€¢ Demonstrates that face recognition is not possible from low resolution or blurry images because PULSE can produce visually distinct high resolution images that all downscale correctly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Factors</head><p>â€¢ Similarly to <ref type="bibr" target="#b16">[17]</ref>: "based on known problems with computer vision face technology, potential relevant factors include groups for gender, age, race, and Fitzpatrick skin type." Additional factors include lighting, background content, hairstyle, pose, camera focal length, and accessories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metrics</head><p>â€¢ Evaluation metrics include the success/failure rate of PULSE using StyleGAN (when it does not find an image that downscales appropriately). Note that this metric is lacking in that it only reports whether an image was found -which does not reflect the diversity of images found over many runs on many images, an important measure that was difficult to quantify. In original evaluation experiments, the failure rate was zero. â€¢ To better evaluate the way that this metric varies across groups, we found a new dataset with better representation to evaluate success/failure rates on, "FairFace: Face Attribute Dataset for Balanced Race, Gender, and Age" <ref type="bibr" target="#b30">[30]</ref>. This dataset was labeled by third-party annotators on these fields. We sample 100 examples per subgroup and calculate the success/failure rate across groups with Ã—64 downscaling after running PULSE 5 times per image. We note that this is a small sample size per subgroup. The results of this analysis can be found in <ref type="table">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Data</head><p>â€¢ PULSE is never trained itself, it leverages a pretrained generator. â€¢ StyleGAN is trained on FFHQ <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Data</head><p>â€¢ CelebA HQ, test data split, chosen as a basic proof of concept. â€¢ MOS Score evaluated via ratings from third party annotators (Amazon MTurk) Ethical Considerations</p><p>â€¢ Evaluation data -CelebA HQ: Faces based on public figures (celebrities). Only image data is used (no additional annotations). However, we point out that this dataset has been noted to have a severe imbalance of white faces compared to faces of people of color (almost 90% white) <ref type="bibr" target="#b30">[30]</ref>. This leads to evaluation bias. As this has been the accepted benchmark in face super-resolution, issues of bias in this field may go unnoticed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Caveats and Recommendations</head><p>â€¢ FairFace appears to be a better dataset to use than CelebA HQ for evaluation purposes.</p><p>â€¢ Due to lack of available compute, we could not at this time analyze intersectional identities and the associated biases.</p><p>â€¢ For an in depth discussion of the biases of StyleGAN, see <ref type="bibr" target="#b21">[21]</ref>.</p><p>â€¢ Finally, again similarly to <ref type="bibr" target="#b16">[17]</ref>:</p><p>1. Does not capture race or skin type, which has been reported as a source of disproportionate errors.</p><p>2. Given gender classes are binary (male/not male), which we include as male/female. Further work needed to evaluate across a spectrum of genders.</p><p>3. An ideal evaluation dataset would additionally include annotations for Fitzpatrick skin type, camera details, and environment (lighting/humidity) details. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PULSE: Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Appendix A: Additional Figures</head><p>Here we provide further samples of the output of our super-resolution method for illustration in <ref type="figure" target="#fig_0">Figure 1</ref>. These results were obtained with Ã—8 scale factor from an input of resolution 16 Ã— 16. This highlights our method's capacity to illustrate detailed features that we did not have space to show in the main paper, such as noses and a wider variety of eyes. We also present additional examples depicting PULSE's robustness to additional degradation operators in <ref type="figure" target="#fig_1">Figure 2</ref> and some randomly selected generated samples in <ref type="figure" target="#fig_2">Figures 3, 4</ref>, 5, and 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Appendix B: Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">StyleGAN</head><p>In order to generate experimental results using our method, we had to pick a pretrained generative model to work with. For this we chose StyleGAN due to its state-ofthe-art performance on high resolution image generation.</p><p>StyleGAN consists of two components: first, a mapping network M : R 512 â†’ R 512 , a tiling function T : R 512 â†’ R 18Ã—512 , and a synthesis network S : R 18Ã—512 Ã— N â†’ R 1024Ã—1024 where N is collection of Euclidean spaces of varying dimensions representing the domains of each of the noise vectors fed into the synthesis network. To generate images, a vector z is sampled uniformly at random from the surface of the unit sphere in R 512 . This is transformed into another 512-dimensional vector by the mapping network, which is replicated 18 times by the tiling function T . The new 18 Ã— 512 dimensional vector is input to the synthesis network which uses it to generate a high-resolution, 1024 Ã— 1024 pixel image. More precisely, the synthesis network consists of 18 sequential layers, and the resolution of the generated image is doubled after every other layer; each of these 18 layers is re-fed into the 512-dimensional output of the mapping network, hence the tiling function. The synthesis network also takes as input noise sampled from the unit Gaussian, which it uses to stochastically add details to the generated image. Formally, Î· is sampled from the Gaussian prior on N , at which point the output is obtained by computing S(T (M (z)), Î·).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Latent Space Embedding</head><p>Experimentally, we observed that optimizing directly on z âˆˆ S 511 âŠ‚ R 512 yields poor results; this latent space is not expressive enough to map to images that downscale correctly. A logical next step would be to use the expanded 18 Ã— 512-dimensional latent space that the synthesis network takes as input, as noted by Abdal, Qin, and Wonka <ref type="bibr">[1]</ref>. By ignoring the mapping network, S can be applied to any vector in R 18Ã—512 , rather than only those consisting of a single 512-dimensional vector repeated 18 times. This expands the expressive potential of the network; however, by allowing the 18 512-dimensional input vectors to S to vary independently, the synthesis network is no longer constrained to the original domain of StyleGAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Cross Loss</head><p>For the purposes of super-resolution, such approaches are problematic because they void the guarantee that the algorithm is traversing a good approximation of M, the natural image manifold. The synthesis network was trained with a limited subset of R 18Ã—512 as input; the further the input it receives is from that subset, the less we know about the output it will produce. The downscaling loss, defined in the main paper, is alone not enough to guide PULSE to a realistic image (only an image that downscales correctly). Thus, we want to make some compromise between the vastly increased expressive power of allowing the input vectors to vary independently and the realism produced by tiling the input to S 18 times. Instead of optimizing on downscaling loss alone, we need some term in the loss discouraging straying too far in the latent space from the original domain.</p><p>To accomplish this, we introduce another metric, the "cross loss." For a set of vectors v 1 , ..., v k , we define the cross loss of v 1 , ..., v k to be</p><formula xml:id="formula_9">CROSS(v 1 , ..., v k ) = i&lt;j |v i âˆ’ v j | 2 2</formula><p>The cross loss imposes a penalty based on the Euclidean distance between every pair of vectors input to S. This can be considered a simple form of relaxation on the original constraint that the 18 input vectors be exactly equal.</p><p>When v 1 , ..., v k are sampled from a sphere, it makes more sense to compare geodesic distances along the sphere. This is the approach we used in generating our results. Let Î¸(v, w) denote the angle between the vectors v and w. We then define the geodesic cross loss to be</p><formula xml:id="formula_10">GEOCROSS(v 1 , ..., v k ) = i&lt;j Î¸(v i , v j ) 2 .</formula><p>Empirically, by allowing the 18 input vectors to S to vary while applying the soft constraint of the (geo)cross loss, we can increase the expressive potential of the network without large deviations from the natural image manifold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Approximating the input distribution of S</head><p>StyleGAN begins with a uniform distribution on S 511 âŠ‚ R 512 , which is pushed forward by the mapping network to a transformed probability distribution over R 512 . Therefore, another requirement to ensure that S([v 1 , ..., v 18 ], Î·) is a realistic image is that each v i is sampled from this pushforward distribution. While analyzing this distribution, we found that we could transform this back to a distribution on the unit sphere without the mapping network by simply applying a single linear layer with a leaky-ReLU activationan entirely invertible transformation. We therefore inverted this function to obtain a sampling procedure for this distribution. First, we generate a latent w from S 511 , and then apply the inverse of our transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Noise Inputs</head><p>The second parameter of S controls the stochastic variation that StyleGAN adds to an image. When the noise is set to 0, StyleGAN generates smoothed-out, detail-free images. The synthesis network takes 18 noise vectors at varying scales, one at each layer. The earlier noise vectors influence more global features, for example the shape of the face, while the later noise vectors add finer details, such as hair definition. Our first approach was to sample the noise vectors before we began traversing the natural image manifold, keeping them fixed throughout the process. In an attempt to increase the expressive power of the synthesis network, we also tried to perform gradient descent on both the latent input and the noise input to S simultaneously, but this tended to take the noise vectors out of the spherical shell from which they were sampled and produced unrealistic images. Using a standard Gaussian prior forced the noise vectors towards 0 as mentioned in the main paper. We therefore experimented with two approaches for the noise input:</p><p>1. Fixed noise: Especially when upsampling from 16 Ã— 16 to 1024 Ã— 1024, StyleGAN was already expressive enough to upsample our images correctly and so we did not need to resort to more complicated techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Partially trainable noise:</head><p>In order to slightly increase the expressive power of the network, we optimized on the latent and the first 5-7 noise vectors, allowing us to slightly modify the facial structure of the images we generated to better match the LR images while main-  taining image quality. This was the approach we used to generate the images presented in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Appendix C: Alternative Metrics</head><p>For completeness, we provide the metrics of PSNR and SSIM here. These results were obtained with Ã—8 scale factor from an input of resolution 16 Ã— 16. Note that we explicitly do not aim to optimize on this pixel-wise average distance from the high-resolution image, so these metrics do not have meaningful implications for our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Appendix D: Robustness</head><p>Traditional supervised approaches using CNNs are notoriously sensitive to tiny changes in the input domain, as any perturbations are propagated and amplified through the network. This caused some problems when trying to train FS-RNET and FSRGAN on FFHQ and then test them on Cele-bAHQ. However, PULSE never feeds an LR image through a convolutional network and never applies filters to the LR input images. Instead, the LR image is only used as a term in the downscaling loss. Because the generator is not capable of producing "noisy" images, it will seek an SR image that downscales to the closest point on the LR natural image manifold to the noisy LR input. This means that PULSE outputs an SR image that downscales to the projection of the noisy LR input onto the LR natural image manifold, and if the noise is not too strong, this should be close to the "true" unperturbed LR . This may explain why PULSE had no problems when applied to different domains, and why we could demonstrate robustness when the low resolution image was degraded with various types of noise.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>(x32) The input (top) gets upsampled to the SR image (middle) which downscales (bottom) to the original image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>FSRNet tends towards an average of the images that downscale properly. The discriminator loss in FSRGAN pulls it in the direction of the natural image manifold, whereas PULSE always moves along this manifold.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>We show here how visually distinct images, created with PULSE, can all downscale (represented by the arrows) to the same LR image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>While traveling from zinit to z f inal in the latent space L, we travel from Iinit âˆˆ M to I f inal âˆˆ M âˆ© R.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Comparison of PULSE with bicubic upscaling, FSRNet, and FSRGAN. In the first image, PULSE adds a messy patch in the hair to match the two dark diagonal pixels visible in the middle of the zoomed in LR image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 1 .</head><label>1</label><figDesc>Further comparison of PULSE with bicubic upscaling, FSRNet, and FSRGAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 2 .</head><label>2</label><figDesc>(x32) Additional robustness results for PULSE under additional degradation operators (these are downscaling followed by Gaussian noise (std=25, 50), motion blur in random directions with length 100 followed by downscaling, and downscaling followed by saltand-pepper noise with a density of 0.05.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 .</head><label>3</label><figDesc>(64x) Sample 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 .Figure 5 .Figure 6 .</head><label>456</label><figDesc>(64x) Sample 2 (64x) Sample 3 (64x) Sample 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>NIQE Score for various algorithms at 1024Ã—1024. Lower is better.</figDesc><table><row><cell>HR</cell><cell cols="3">Nearest Bicubic PULSE</cell></row><row><cell>3.90</cell><cell>12.48</cell><cell>7.06</cell><cell>2.47</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Success rates (frequency with which PULSE finds an image in the outputs of the generator that downscales correctly) of PULSE with StyleGAN-FFHQ across various groups, evaluated on FairFace. See "Failure to converge" in Section 6 for full explanation of this analysis and its limitations.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Race</cell><cell></cell><cell></cell><cell></cell><cell>Gender</cell></row><row><cell cols="7">Black East Asian Indian Latino/Hispanic Middle Eastern Southeast Asian White</cell><cell>Female Male</cell></row><row><cell>79.2%</cell><cell>87.0%</cell><cell>87.4%</cell><cell>90.2%</cell><cell>87.0%</cell><cell>87.4%</cell><cell>83.4%</cell><cell>91.4% 88.6%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>.menon,alexandru.damian,nikhil.ravi,shijia.hu,cynthia.rudin}@duke.edu</figDesc><table><row><cell>Sachit Menon*, Alexandru Damian*, Nikhil Ravi, Shijia Hu, Cynthia Rudin Duke University Durham, NC</cell></row></table><note>{sachit</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">MartÃ­n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th {USENIX} symposium on operating systems design and implementation ({OSDI} 16)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Im-age2StyleGAN: How to embed images into the Style-GAN latent space?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameen</forename><surname>Abdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yipeng</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Limits on superresolution and how to break them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="372" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">New techniques for preserving global structure and denoising with low information loss in single-image superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijie</forename><surname>Bei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Damian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachit</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Rudin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="874" to="881" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The 2018 PIRM challenge on perceptual image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yochai</forename><surname>Blau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roey</forename><surname>Mechrez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Michaeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihi</forename><surname>Zelnik-Manor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV) Workshops</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Compressed sensing using generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Bora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajil</forename><surname>Jalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><forename type="middle">G</forename><surname>Dimakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning (ICML)</title>
		<meeting>the 34th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">To learn image super-resolution, use a gan to learn how to do image degradation first</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="185" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fsrnet: End-to-end learning face superresolution with facial priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2492" to="2501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multiframe example-based super-resolution using locally directional self-similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokhwa</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inhye</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joonki</forename><surname>Paik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Consumer Electronics</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="353" to="358" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Progressive growing of GANs for improved quality, stability, and variation. CoRR, abs/1710.10196</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Appeared at the 6th Annual International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A stylebased generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Progressive face super-resolution via attention to facial landmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deokyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minseon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gihyun</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dae-Shik</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th British Machine Vision Conference (BMVC)</title>
		<meeting>the 30th British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Photo-realistic single image superresolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>HuszÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4681" to="4690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zaldivar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parker</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vasserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Spitzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on fairness, accountability, and transparency</title>
		<meeting>the conference on fairness, accountability, and transparency</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="220" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Making a completely blind image quality analyzer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="212" />
			<date type="published" when="2013-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, highperformance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<meeting>the International Conference on Medical Image Computing and Computer-Assisted Intervention</meeting>
		<imprint>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Analyzing demographic bias in artificially generated facial pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joni</forename><surname>Salminen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soon-Gyo</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shammur</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><forename type="middle">J</forename><surname>Jansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-04" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Twostream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Super resolution applications in modern digital image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanjot</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jagroop Singh</forename><surname>Sidhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Applications</title>
		<imprint>
			<biblScope unit="volume">150</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="975" to="8887" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep image prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Vershynin</surname></persName>
		</author>
		<title level="m">Random Vectors in High Dimensions, page 3869. Cambridge Series in Statistical and Probabilistic Mathematics</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Pytorch implementation of the stylegan generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Viehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lernapparat</surname></persName>
		</author>
		<ptr target="https://github.com/lernapparat/lernapparat/" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A comprehensive survey to face hallucination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nannan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinbo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="30" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">ESRGAN: Enhanced super-resolution generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV) Workshops</title>
		<meeting>the European Conference on Computer Vision (ECCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fair-gan+: Achieving fair data generation and classification through generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Big Data (Big Data)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1401" to="1406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Unsupervised degradation learning for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongwei</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghua</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04240</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Im-age2StyleGAN: How to embed images into the StyleGAN latent space?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameen</forename><surname>Abdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yipeng</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
