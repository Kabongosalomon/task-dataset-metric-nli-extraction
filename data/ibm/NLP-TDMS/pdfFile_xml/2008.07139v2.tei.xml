<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AID: Pushing the Performance Boundary of Human Pose Estimation with Information Dropping Augmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">XForwardAI Technology Co</orgName>
								<address>
									<settlement>Ltd, Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
							<email>zhengzhu@ieee.org</email>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">XForwardAI Technology Co</orgName>
								<address>
									<settlement>Ltd, Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalong</forename><surname>Du</surname></persName>
							<email>dalong.du@xforwardai.com</email>
							<affiliation key="aff0">
								<orgName type="institution">XForwardAI Technology Co</orgName>
								<address>
									<settlement>Ltd, Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">AID: Pushing the Performance Boundary of Human Pose Estimation with Information Dropping Augmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Both appearance cue and constraint cue are vital for human pose estimation. However, there is a tendency in most existing works to overfitting the former and overlook the latter. In this paper, we propose Augmentation by Information Dropping (AID) to verify and tackle this dilemma. Alone with AID as a prerequisite for effectively exploiting its potential, we propose customized training schedules, which are designed by analyzing the pattern of loss and performance in training process from the perspective of information supplying. In experiments, as a model-agnostic approach, AID promotes various state-ofthe-art methods in both bottom-up and top-down paradigms with different input sizes, frameworks, backbones, training and testing sets. On popular COCO human pose estimation test set, AID consistently boosts the performance of different configurations by around 0.6 AP in top-down paradigm and up to 1.5 AP in bottom-up paradigm. On more challenging CrowdPose dataset, the improvement is more than 1.5 AP. As AID successfully pushes the performance boundary of human pose estimation problem by considerable margin and sets a new state-of-the-art, we hope AID to be a regular configuration for training human pose estimators. The source code will be publicly available for further research.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human pose estimation serves many visual understanding tasks such as video surveillance <ref type="bibr" target="#b23">[24]</ref> and action recognition <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b49">50]</ref>. In recent years, research community has witnessed a significant advance from single person <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b43">44]</ref> to multi-person pose estimation <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b7">8]</ref>, where the key engine consists of the network architecture evolution <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b1">2]</ref>, unbiased data processing <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b15">16]</ref> and effective grouping strategies <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>In this paper, we pay attention to the possible overfitting problem, which is a conjecture raised in rethinking the relationship between manually labeling methods and the model training supervisions. The base information that human use for keypoint locating is the appearance cues. This inspires the pioneers to use response map, whose center is exactly located at the keypoints, as the supervision in the human pose estimation training process. The response map supervision is intuitive and has been proved effective in most existing works <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b7">8]</ref>. Besides, another cue is the constraints like the keypoint relationship in human pose or the interaction between human and its surrounding environment. Constraint cues enable one locating the keypoints under some challenging situations where appearance cues are absent or not sufficient, such as occlusion or ambiguity between left and right knees. Although the powerful neural networks have potential to learn from the training data, the constraint cue is still too hard for detectors to learn. By contrast, the appearance cue is intuitively easier for acquiring with convolutional neural network. When the appearance cue is always present and there are no penalty on the neglecting of constraint cue, we suspect that the algorithms only with response map supervision have a tendency to overfitting the appearance cue.</p><p>Based on the aforementioned analysis, we introduce information dropping methods to verify this conjecture and indirectly force the neural network to focus on the constraint cue learning. Information dropping is a well known method for regularization and has been widely used in many other problems <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b35">36]</ref>. By dropping information in images, the neural networks can learn discriminative features, resulting in a notable increase of model robustness. Inspired by this, we randomly drop the appearance information of a keypoint and maintain the response map supervision, in purpose of preventing the trained estimators from overfitting the appearance cues and making it pay more attention to the constraints. We call this Augmentation by Information Dropping(AID).</p><p>Although AID is theoretically agreed with the purpose</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random Erase</head><p>Cutout HaS Grid Mask Ours Origin <ref type="figure">Figure 1</ref>. The illustration of different information dropping methods. Random Erase and Cutout perform single-area information dropping while Hide-and-Seek(HaS) and GridMask perform multi-area information dropping. All of them have a certain probability of dropping the appearance information of keypoints. of paying more attention to the constraint cues, the effect is negligible or even negative if we use the common training schedule <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b33">34]</ref>. With the observation of the loss and performance in training process, we find that AID makes the standard learning process much more challenging. Specifically, the appearance information shortage caused by AID challenges the early training process, confusing the network just like letting a child learn quantum mechanics. To address this problem, two customized training schedules are proposed in this paper to provide the prerequisite access for higher performance human pose estimation with AID.</p><p>In experiments, we apply AID to the state-of-the-art methods in both top-down and bottom-up paradigms. Without bells and whistles, AID successfully pushes the performance boundary of human pose estimation problem by considerable and stable margin in different input sizes, frameworks, backbones, training and testing sets. On challenging COCO <ref type="bibr" target="#b25">[26]</ref> human pose estimation test set, AID consistently boosts the performance of various configurations by around 0.6 AP in top-down paradigm and up to 1.5 AP in bottom-up paradigm. On more challenging CrowdPose <ref type="bibr" target="#b22">[23]</ref> dataset, the improvement is more than 1.5 AP. The experimental results not only verify the potential shortcoming in the state-of-the-arts with response map supervision, but also prove the general effectiveness of AID in performance improvement. Based on the strong results, we recommend AID to be a common configuration for training human pose estimators, which is the same as random flip, random scale and random rotation. In addition with ablation study on information dropping methods and train schedules, we offer some guidelines about performing AID.</p><p>The main contributions of this paper can be summarized as follows:</p><p>1. This paper pioneers the diagnosis of the appearance cue overfitting problem in human pose estimation and introduces Augmentation by Information Dropping (AID) to verify and address it.</p><p>2. The inefficiency of AID in previous work is analyzed from the viewpoint of information shortage in early training process. And the proposed customized training schedules in this paper are the prerequisite of performance improvement with AID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>With thorough experiments, we showcase that AID successfully pushes the performance boundary of human pose estimation problem and sets a new state-ofthe-art baseline for it. Thus, we hope AID to be a regular configuration for training human pose estimators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Human Pose Estimation</head><p>Bottom-up methods detect identity-free keypoints for all the persons at first, and then group them into person instances. Most bottom-up methods focus on the grouping problem. OpenPose <ref type="bibr" target="#b2">[3]</ref> adds another branch to learn pairwise relationships (part affinity fields) between keypoints for grouping. AssociativeEmbedding <ref type="bibr" target="#b28">[29]</ref> groups the keypoints just according to the embedding vector which is learned alone with heatmaps. <ref type="bibr" target="#b21">[22]</ref> proposes to learn the Part Intensity Field, aiming at precisely locating small instance. MultiPoseNet <ref type="bibr" target="#b20">[21]</ref> simultaneously achieves human detection and pose estimation, and proposes PRN to group the keypoints by the bounding box of each people. At the cost of high computation, HigherHRNet <ref type="bibr" target="#b7">[8]</ref> maintains highresolution feature maps which effectively improve the precision of the predictions mainly by reducing the systemic error which is stated in UDP <ref type="bibr" target="#b15">[16]</ref>. <ref type="bibr" target="#b10">[11]</ref> replaces the postprocessing grouping with differentiable hierarchical graph grouping to achieve end-to-end learning for multi-person pose estimation.</p><p>Top-down methods achieve multi-person pose estimation by the two-stages process: detecting the bounding box of persons by a person detector and perceiving individual keypoint locations within these boxes. The ar-chitecture of backbones is the main consideration in this paradigm. CPN <ref type="bibr" target="#b6">[7]</ref> and MSPN <ref type="bibr" target="#b24">[25]</ref> are the leading methods on COCO Keypoint Challenge in 2017 and 2018 respectively, with the main idea of refining keypoint prediction with cascade networks. As a follower, RSN <ref type="bibr" target="#b1">[2]</ref> designs Res-Steps-Net unit and pose refine machine to learn delicate local representations specific for MSPN network architecture. SimpleBasline <ref type="bibr" target="#b42">[43]</ref> proposes a simple but effective paradigm by adding a few deconvolutional layers to enlarge the resolution of output features. HRNet <ref type="bibr" target="#b36">[37]</ref> maintains high-resolution representations through the whole architecture, achieving state-of-the-art performance on public dataset. Mask R-CNN <ref type="bibr" target="#b13">[14]</ref> achieves a good balance between performance and inference speed by building an end-to-end framework. PoseFix <ref type="bibr" target="#b27">[28]</ref> is designed as a postprocessing module that learns to modify the mistake in existing methods. Analogously, Graph-PCNN <ref type="bibr" target="#b18">[19]</ref> designs an extra refine stage which revises the feature for localization and takes the relationship between keypoints into consideration. Recently, some other works pay attention to the data processing aspect of human pose estimation. DARK <ref type="bibr" target="#b46">[47]</ref> achieves high precision decoding by designing a distribution-aware method. UDP <ref type="bibr" target="#b15">[16]</ref> diagnoses the bias data processing in existing methods and form a higher and more reliable baseline for human pose estimation problem, being the strong basic of the champion solution on COCO Keypoint Challenge in 2020.</p><p>Some previous works explicitly utilize the constraint cue in neural network construction or postprocessing. By using predefined pose graph and complex neural network architecture, <ref type="bibr" target="#b47">[48]</ref> designs Cascade Prediction Fusion and Pose Graph Neural Network to exploit underlying contextual information. OpenPose <ref type="bibr" target="#b2">[3]</ref> builds a model that contains two branches to predict keypoint heatmaps and pairwise relationships (part affinity fields) between them. The part affinity fields explicitly learn the constraint cue and are used in grouping process. Whether this constraint cue can promote the response map predicting or not has not been studied. When using the constraint cue, the aforementioned methods only focus on the model of human body, and few jobs consider the interaction between target and environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Information dropping</head><p>As an effective way for regularization, information dropping has served as a common training strategy for many tasks such as person re-identification <ref type="bibr" target="#b26">[27]</ref>, face recognition <ref type="bibr" target="#b34">[35]</ref>, classification <ref type="bibr" target="#b14">[15]</ref> and object detection <ref type="bibr" target="#b8">[9]</ref>. Information dropping for deep learning derives from random erasing <ref type="bibr" target="#b48">[49]</ref> and Cutout <ref type="bibr" target="#b9">[10]</ref>. Recently, hide-and-seek (HaS) <ref type="bibr" target="#b35">[36]</ref> and GridMask <ref type="bibr" target="#b5">[6]</ref> put forward two multi-area information dropping strategies respectively, achieving better regularization effect in classification problem.</p><p>To the best of our knowledge, augmentation by infor-mation dropping has not been a popular paradigm in human pose estimation problem and is absent in most stateof-the-arts. <ref type="bibr" target="#b19">[20]</ref> pioneers keypoint masking training in purpose of imitating different occlusion situation, however the improvement is negligible. <ref type="bibr" target="#b33">[34]</ref> performs thorough experiments to prove that occlusion augmentation is not necessary for human pose estimation, as it provides no improvement or even degrades the performance under some situations. The previous works use the same training schedule for fair comparison when perform ablation study on augmentation with information dropping or information disturbing. We will analyze this from the viewpoint of information supplying in training process, which results in a conclusion that the best schedules are different in training with or without information dropping augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">AID for Human Pose Estimation</head><p>The idea of information dropping is to randomly drop appearance information of some specific annotated keypoints while maintaining the response map supervision, keeping the training process away from overfitting. As illustrated in <ref type="figure">Figure 1</ref>, random erase <ref type="bibr" target="#b48">[49]</ref> and Cutout <ref type="bibr" target="#b9">[10]</ref> achieve this by dropping a single continuous area centered at a random position in the image plane. By contrast, HaS <ref type="bibr" target="#b35">[36]</ref> and Grid-Mask <ref type="bibr" target="#b5">[6]</ref> perform multi-area information dropping. HaS firstly splits the image into small patches and then drops some of them under a certain probability. GridMask drops appearance information according to a regular mask constructed with uniformly distributed squares. All aforementioned methods have a certain probability of dropping the appearance information of keypoints, but have different effect on the performance. According to preliminary experiment results, Cutout is the superior methods for top-down paradigm while HaS for bottom-up paradigm. We offer more details about ablation study in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training Schedule for AID</head><p>Optimisation schedule is of significance for training high performance pose estimation networks with AID. Empirically, directly applying AID in training process even degrades the performance of pose estimator. By performing ablation study, we observe the variation of loss and performance in training process as illustrated in <ref type="figure" target="#fig_0">figure 2</ref>. The train loss is much higher when training with AID and the performance is lagging in early training process but gradually catch up with in the following. Base on this observation, we argue that the shortage of appearance information caused by AID disturbs the early study and postpones the saturation. Thus training human pose estimators with AID requires a longer schedule.</p><p>Here we propose two simple but effective ways to tackle this dilemma. One is to double the training schedule, leaving enough time for the network to conquer the difficulty. And the other is to split the training process into two stages: the training process starts with a common schedule without AID as the previous works, followed by an extra refinement schedule as long as the first one with AID. The main advantage of second approach is that we can reuse the existing well training models in previous works to save computational resource. Empirically, the two training schedules mentioned above have the similar effect and an algorithm can gain a proper promotion from either of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">COCO</head><p>Implementation details. Our model is trained on COCO train subset, which is equipped about 57,000 images and 150,000 person instances. We evaluate the trained models on the val set and test-dev set, containing about 5,000 images and 20,000 images, respectively. The AP evaluation metric is reported based on Object Keypoint Similarity (OKS). We use UDPv1 <ref type="bibr" target="#b15">[16]</ref> as the data-processing guider and set the training configuration strictly following the HRNet-UDPv1 <ref type="bibr" target="#b15">[16]</ref> for all architectures. State-ofthe-art backbones are used in experiments including Sim-pleBaseline <ref type="bibr" target="#b42">[43]</ref>, HRNet <ref type="bibr" target="#b36">[37]</ref> and RSN <ref type="bibr" target="#b1">[2]</ref> for top-down paradigm, HRNet <ref type="bibr" target="#b36">[37]</ref> and HigherHRNet <ref type="bibr" target="#b7">[8]</ref> for bottom-up paradigm. During inference, HTC <ref type="bibr" target="#b4">[5]</ref> detector is used to detect human instances for top-down paradigm as UDP <ref type="bibr" target="#b15">[16]</ref>.</p><p>With multi-scale test, the 80-class and person AP on COCO val set <ref type="bibr" target="#b25">[26]</ref> are 52.9 and 65.1, respectively. We report the performance of single model, and only flipping test strategy is used.</p><p>Results on the val set. The results of proposed method and state-of-the-arts are listed in <ref type="table" target="#tab_0">Table 1</ref>. For top-down paradigm, we report the performance improvement when AID is applied to most state-of-the-art architectures including RSN <ref type="bibr" target="#b1">[2]</ref>, SimpleBaseline <ref type="bibr" target="#b42">[43]</ref> and HRNet <ref type="bibr" target="#b36">[37]</ref>. The improvement is steady around 0.6 AP among different network architectures. With configurations of HRNet-W32-384×288 and HRNet-W48-256×192, we show that the effect of AID is consistently around +0.6 AP among different network sizes or input sizes. Based on higher baseline with HRNet-W48plus-384×288 configuration, the proposed AID still brings in 0.6 AP improvement. The consistency in performance improvement proves both the widely existing overfitting problem and the universal effectiveness of the proposed augmentation method. For bottom-up paradigm, we take HigherHRNet <ref type="bibr" target="#b7">[8]</ref> as baseline. AID promotes different configurations by more than 1.1 AP. With multi-scale testing the improvement is up to 1.5 AP.</p><p>It is worth noting that AID not only brings steady improvement on the primary metric AP, but also boosts the performance of the algorithms on all other metrics. As AID creates more invisible keypoints and intuitively benefits the perception of them, in addition to study how AID effects the performance on visible and invisible keypoints respectively, we split the original val set into two subsets: val-vis only contains visible keypoints and val-invis only contains invisible keypoints. Based on the predefined metrics in COCO <ref type="bibr" target="#b25">[26]</ref>, two primary metrics AP-vis and AP-invis are reported in <ref type="table" target="#tab_0">Table 1</ref> for the two subsets. The experimental results show that, under most configurations, AID not only promotes the performance on the invisible keypoints, but also benefits the perception of the visible keypoints.</p><p>Results on the test-dev set. Although the baseline is unprecedentedly high, the proposed AID still boosts the performance of this configuration by 0.5 AP to 78.7 AP, indicating that more training data effectively improves the performance of trained models but can not tackle the overfitting problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">CrowdPose</head><p>Compared with COCO <ref type="bibr" target="#b25">[26]</ref>, CrowdPose <ref type="bibr" target="#b22">[23]</ref> contains more crowded scenes where constraint cues are more required. We take HigherHRNet <ref type="bibr" target="#b7">[8]</ref> as baseline and maintain their configurations for training and testing. To be specific, the networks are trained on train and val sets with 12k images in total, and the results are reported on test set with 8k images. As listed in <ref type="table" target="#tab_5">Table 3</ref>, AID improves HigherHRNet-W32-512×512 configuration by 1.7 AP to 67.3 AP and HigherHRNet-W48-640×640 configuration by 1.5 AP to 68.2 AP. Beside the primary metric, the improvements are also showcased in all the other metrics with considerable margin. What's more, owing to the insufficiency of appearance cue in crowded scenes, the improvements in CrowdPose dataset are more than that in COCO dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Information Dropping Methods</head><p>We perform information dropping method Cutout <ref type="bibr" target="#b9">[10]</ref>, HaS <ref type="bibr" target="#b35">[36]</ref> and GridMask <ref type="bibr" target="#b5">[6]</ref> with the implementations from their  official projects. As the hyper-parameters is vital for the effectiveness, we adjust them with principle of keeping the initial loss close with each other to provide similar regularization effect. And this offer us a quick access to searching the superior hyper-parameters for each of them. With limited hyper-parameter searching experiments, we report the performance of the best configuration in <ref type="table">Table 4</ref>.</p><p>We summarize two discoveries here: (a) Performance of different information dropping methods is various in bottom-up paradigm while is close in top-down paradigm. (b) The best method for information dropping is different in different paradigms: HaS for bottom-up with large superiority, while Cutout for top-down with small superiority.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Training Schedule</head><p>In this subsection, we use top-down paradigm with HRNet-W32-256×192 configurations and ground-truth human boxes. To explore the effect of training schedule on AID, we firstly design three different training schedules: S1. Normal training schedule from HRNet <ref type="bibr" target="#b36">[37]</ref> with a base learning rate of 1e-3 and is dropped to 1e-4 and 1e-5 at the 170th and 200th epochs, repectively. The training process is terminated within 210 epochs.</p><p>S2. Double the length of the schedule S1. The learning rate is dropped at 380th and 410th epochs, respectively. The training process is terminated within 420 epochs.</p><p>S3. Repeat the schedule S1 twice with different configurations, i.e. first 210 epochs are trained without AID, and applying AID on the subsequent. Based on the pre-defined training schedules, five experimental configurations are constructed as listed in <ref type="table" target="#tab_6">Table 5</ref>. The performance and the corresponding training loss on COCO val AP metric is illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>. Compared E1 with E2, where the same standard training schedule S1 is used and the variable is whether or not to use AID, the training loss is higher when training with AID. The performance of E2 is poorer than E1 in the early training process, indicating that appearance information is vital in early training process and AID would disturb the study the of appearance feature. However, E1 and E2 have similar performance at the end of this training schedule. This means that, AID will not provide positive effect with the standard training schedule. Compare E3 with E4, where a longer schedule is adopted. The performance of E4 with AID starts surpassing E3 at around 250 epoch. And this superiority gradually grows in the subsequent training process. Compare E3 with E1 and E4 with E2, a longer schedule enables the algorithms learning more useful information when AID is used, but makes the algorithms overfitting the training data when AID is absent. Compared E4 with E5, Schedule2 and Schedule3 offer similar improvements, which means that we can reuse the pre-trained models from the previous works and boost their performance by applying another fine tuning process with AID.</p><formula xml:id="formula_0">ID E1 E2 E3 E4 E5 schedule S1 S1 S2 S2 S3 AID OFF ON OFF ON OFF/ON</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Qualitative Comparison</head><p>To qualitatively showcase the efficiency of the proposed method, <ref type="figure" target="#fig_1">Figure 3</ref> visualizes some detection results under challenging situations, where the appearance cue is not sufficient and constraint cue is necessary for keypoint location. The results produced by models trained with AID are more reasonable and precise than that without AID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>In this paper, we expose and remedy the possible overfitting problem in human pose estimation by proposing Augmentation by Information Dropping. In response to bias in existing jobs and from the perspective of information supplying, we offer a reasonable explanation for the invalid of AID with standard training schedule and propose customized training schedule to effectively exploit the potential of the proposed information dropping augmentation. As a result, AID offers fundamental breakthrough in robust human pose estimation and consistently boosts the performance of state-of-the-arts by a considerable margin. Future works will focus on proper neural network architecture for constraint cue learning and more efficient training schedule or more effective information dropping formats for AID.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>The training loss and performance of different configurations. Training loss with AID is much higher than that without. Trained with AID, the performance of the models falls behind in early phase but gradually catch up with the baseline(UDPv1) and becomes superior in the later. AID degrades the performance in early stage and postpones the saturation in training process. However it effectively pushes the performance boundary of human pose estimation when the training reaches saturation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>The visualization of some predicted results under challenging situations. From left to right are ground truths, predicted results with information dropping augmentation and predicted results without information dropping augmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparisons on COCO val set. AID consistently boosts the performance of the state-of-the-arts by around 0.6 AP in top-down paradigm and up to 1.5 AP in bottom-up paradigm. HRNet-W48plus: A modification of HRNet-W48 with deeper network structure.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>Input size</cell><cell>AP</cell><cell>AP 50</cell><cell>AP 75</cell><cell>AP M</cell><cell>AP L</cell><cell>AR</cell><cell cols="2">AP-vis AP-invis</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Bottom-up methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">HigherHRNet [8] HRNet-W32</cell><cell cols="2">512 × 512 64.4</cell><cell>-</cell><cell>-</cell><cell>57.1</cell><cell>75.6</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>UDPv1 [16]</cell><cell>HRNet-W32</cell><cell cols="2">512 × 512 67.0</cell><cell>86.2</cell><cell>72.0</cell><cell>60.7</cell><cell>76.7</cell><cell>71.6</cell><cell>71.2</cell><cell>59.9</cell></row><row><cell>UDPv1+AID</cell><cell>HRNet-W32</cell><cell cols="2">512 × 512 68.4 (+1.4)</cell><cell>88.1</cell><cell>74.9</cell><cell>62.7</cell><cell>77.1</cell><cell>73.0</cell><cell>72.6</cell><cell>60.6</cell></row><row><cell cols="4">HigherHRNet [8] HigherHRNet-W32 512 × 512 67.1</cell><cell>86.2</cell><cell>73.0</cell><cell>61.5</cell><cell>76.1</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>UDPv1 [16]</cell><cell cols="3">HigherHRNet-W32 512 × 512 67.8</cell><cell>86.2</cell><cell>72.9</cell><cell>62.2</cell><cell>76.4</cell><cell>72.4</cell><cell>72.2</cell><cell>59.6</cell></row><row><cell>UDPv1+AID</cell><cell cols="3">HigherHRNet-W32 512 × 512 69.0 (+1.2)</cell><cell>88.0</cell><cell>74.9</cell><cell>64.0</cell><cell>76.9</cell><cell>73.8</cell><cell>73.2</cell><cell>60.8</cell></row><row><cell cols="4">HigherHRNet [8] HigherHRNet-W48 640 × 640 69.9</cell><cell>87.2</cell><cell>76.1</cell><cell>65.4</cell><cell>76.4</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>UDPv1 [16]</cell><cell cols="3">HigherHRNet-W48 640 × 640 69.9</cell><cell>87.3</cell><cell>76.2</cell><cell>65.9</cell><cell>76.2</cell><cell>74.4</cell><cell>74.1</cell><cell>60.6</cell></row><row><cell>UDPv1+AID</cell><cell cols="3">HigherHRNet-W48 640 × 640 71.0 (+1.1)</cell><cell>88.2</cell><cell>77.3</cell><cell>67.4</cell><cell>77.1</cell><cell>75.5</cell><cell>75.2</cell><cell>62.0</cell></row><row><cell></cell><cell cols="6">Bottom-up methods with multi-scale test as in HigherHRNet [8]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>UDPv1 [16]</cell><cell>HRNet-W32</cell><cell cols="2">512 × 512 70.4</cell><cell>88.2</cell><cell>75.8</cell><cell>65.3</cell><cell>77.6</cell><cell>74.7</cell><cell>74.5</cell><cell>62.3</cell></row><row><cell>UDPv1 +AID</cell><cell>HRNet-W32</cell><cell cols="2">512 × 512 71.1 (+0.7)</cell><cell>88.9</cell><cell>77.2</cell><cell>66.7</cell><cell>77.8</cell><cell>75.5</cell><cell>75.4</cell><cell>62.4</cell></row><row><cell cols="4">HigherHRNet [8] HigherHRNet-W32 512 × 512 69.9</cell><cell>87.1</cell><cell>76.0</cell><cell>65.3</cell><cell>77.0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>UDPv1 [16]</cell><cell cols="3">HigherHRNet-W32 512 × 512 70.2</cell><cell>88.1</cell><cell>76.2</cell><cell>65.4</cell><cell>77.4</cell><cell>74.5</cell><cell>74.6</cell><cell>61.2</cell></row><row><cell>UDPv1+AID</cell><cell cols="3">HigherHRNet-W32 512 × 512 71.3 (+1.1)</cell><cell>89.0</cell><cell>77.4</cell><cell>66.9</cell><cell>77.7</cell><cell>75.6</cell><cell>75.7</cell><cell>61.5</cell></row><row><cell cols="4">HigherHRNet [8] HigherHRNet-W48 640 × 640 72.1</cell><cell>88.4</cell><cell>78.2</cell><cell>67.8</cell><cell>78.3</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>UDPv1 [16]</cell><cell cols="3">HigherHRNet-W48 640 × 640 71.5</cell><cell>88.3</cell><cell>77.3</cell><cell>67.9</cell><cell>77.2</cell><cell>75.9</cell><cell>76.1</cell><cell>61.7</cell></row><row><cell>UDPv1+AID</cell><cell cols="3">HigherHRNet-W48 640 × 640 73.0 (+1.5)</cell><cell>89.2</cell><cell>79.3</cell><cell>69.2</cell><cell>78.6</cell><cell>77.0</cell><cell>77.2</cell><cell>63.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Top-down methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>UDPv1 [16]</cell><cell>ResNet-50</cell><cell cols="2">256 × 192 74.6</cell><cell>91.0</cell><cell>81.8</cell><cell>70.9</cell><cell>81.1</cell><cell>80.1</cell><cell>78.3</cell><cell>67.4</cell></row><row><cell>+AID</cell><cell>ResNet-50</cell><cell cols="2">256 × 192 75.3 (+0.7)</cell><cell>91.5</cell><cell>82.8</cell><cell>71.7</cell><cell>81.8</cell><cell>80.9</cell><cell>79.0</cell><cell>68.2</cell></row><row><cell>UDPv1 [16]</cell><cell>2xRSN-50</cell><cell cols="2">256 × 192 77.7</cell><cell>91.7</cell><cell>84.7</cell><cell>74.3</cell><cell>84.2</cell><cell>83.3</cell><cell>81.0</cell><cell>70.8</cell></row><row><cell>+AID</cell><cell>2xRSN-50</cell><cell cols="2">256 × 192 78.2 (+0.5)</cell><cell>92.1</cell><cell>84.7</cell><cell>74.6</cell><cell>84.6</cell><cell>83.4</cell><cell>81.5</cell><cell>70.7</cell></row><row><cell>UDPv1 [16]</cell><cell>HRNet-W32</cell><cell cols="2">256 × 192 77.2</cell><cell>91.6</cell><cell>84.2</cell><cell>73.7</cell><cell>83.7</cell><cell>82.5</cell><cell>80.7</cell><cell>69.4</cell></row><row><cell>+AID</cell><cell>HRNet-W32</cell><cell cols="2">256 × 192 77.8 (+0.6)</cell><cell>92.1</cell><cell>84.5</cell><cell>74.1</cell><cell>84.1</cell><cell>82.8</cell><cell>81.1</cell><cell>70.3</cell></row><row><cell>UDPv1 [16]</cell><cell>HRNet-W48</cell><cell cols="2">256 × 192 77.8</cell><cell>92.0</cell><cell>84.2</cell><cell>74.4</cell><cell>84.1</cell><cell>83.0</cell><cell>81.3</cell><cell>70.3</cell></row><row><cell>+AID</cell><cell>HRNet-W48</cell><cell cols="2">256 × 192 78.4 (+0.6)</cell><cell>92.3</cell><cell>84.9</cell><cell>75.1</cell><cell>84.6</cell><cell>83.4</cell><cell>81.7</cell><cell>70.8</cell></row><row><cell>UDPv1 [16]</cell><cell>HRNet-W32</cell><cell cols="2">384 × 288 77.9</cell><cell>91.7</cell><cell>83.9</cell><cell>74.1</cell><cell>84.5</cell><cell>83.1</cell><cell>81.4</cell><cell>70.5</cell></row><row><cell>+AID</cell><cell>HRNet-W32</cell><cell cols="2">384 × 288 78.7 (+0.8)</cell><cell>92.2</cell><cell>85.0</cell><cell>75.0</cell><cell>85.1</cell><cell>83.6</cell><cell>81.9</cell><cell>71.4</cell></row><row><cell>UDPv1 [16]</cell><cell>HRNet-W48plus</cell><cell cols="2">384 × 288 78.5</cell><cell>91.9</cell><cell>84.9</cell><cell>74.6</cell><cell>85.2</cell><cell>83.6</cell><cell>81.7</cell><cell>71.5</cell></row><row><cell>+AID</cell><cell>HRNet-W48plus</cell><cell cols="2">384 × 288 79.1 (+0.6)</cell><cell>92.2</cell><cell>85.3</cell><cell>75.4</cell><cell>85.7</cell><cell>84.1</cell><cell>82.3</cell><cell>71.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>reports the performance boost of AID on COCO test-dev set. The results show similar improvement compared with val set, indicating the superior generalization property of AID. In addition, we use extra data from AI Challenger<ref type="bibr" target="#b41">[42]</ref> to verify the effect of AID under the condition of using more training data. With extra training data, the configuration of HRNet-W48plus-384×288-UDPv1 scores 78.2 AP on test-dev set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>The improvement of AP on COCO test-dev set when the proposed AID is applied to the state-of-the-art methods. * means extra data is used. HRNet-W48plus: A modification of HRNet-W48 with deeper network structure.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>The improvement of AP on CrowdPose test set when AID is applied. * means multi-scale testing is used.</figDesc><table><row><cell>Method</cell><cell cols="2">Baseline Cutout</cell><cell>HaS</cell><cell>GridMask</cell></row><row><cell>Bottom-up</cell><cell>67.8</cell><cell>68.1</cell><cell>69.0</cell><cell>68.2</cell></row><row><cell>Top-down</cell><cell>77.2</cell><cell>77.8</cell><cell>77.7</cell><cell>77.6</cell></row><row><cell cols="5">Table 4. Comparison of different information dropping methods.</cell></row><row><cell cols="5">Results are primary metric AP conducted on COCO val set with</cell></row><row><cell cols="5">configuration HRNet-W32-256×192 for top-down paradigm and</cell></row><row><cell cols="5">HigherHRNet-W32-512×512 for bottom-up paradigm.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Configurations of different training schedules for ablation study.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pictorial structures revisited: People detection and articulated pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning delicate local representations for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhao</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxiong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binyi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angang</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erjin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hybrid task cascade for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Gridmask data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengguang</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.04086</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Higherhrnet: Scale-aware representation learning for bottom-up human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Occlusion robust pedestrian detector in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Stan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Differentiable hierarchical graph grouping for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xie</forename><surname>Enze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Wenhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ouyang</forename><surname>Wanli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Ping</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rmpe: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuqin</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Piotr Dollar, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The devil is in the details: Delving into unbiased data processing for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A coarsefine network for keypoint localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoli</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deepercut: A deeper, stronger, and faster multi-person pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Graph-pcnn: Two stage human pose estimation with graph pose refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Errui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-scale structure-aware network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lipeng</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ching</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multiposenet: Fast multi-person pose estimation using pose residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salih</forename><surname>Karagoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Akbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pifpaf: Composite fields for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Kreiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Bertoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Crowdpose: Efficient crowded scenes pose estimation and a new benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">State-aware re-identification feature for multitarget multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiabin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binyi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuming</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00148</idno>
		<title level="m">Rethinking on multi-stage networks for human pose estimation</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bag of tricks and a strong baseline for deep person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youzhi</forename><surname>Hao Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenqi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Posefix: Model-agnostic general human pose refinement network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Associative embedding: End-to-end learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Personlab: Person pose estimation and instance segmentation with a bottom-up, part-based, geometric embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Chris Bregler, and Kevin Murphy. Towards accurate multi-person pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nori</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Tilting at windmills: Data augmentation for deep pose estimation does not help with occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Pytel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">C</forename><surname>Osman Semih Kayhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gemert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.10451</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Towards universal representation learning for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Hide-and-seek: A data augmentation technique for weakly-supervised localization and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><forename type="middle">Kumar</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aron</forename><surname>Sarmasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautam</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.02545</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jonathan J Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1799" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shih-En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Ai challenger: A large-scale dataset for going deeper in image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shipei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.06475</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning feature pyramids for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning feature pyramids for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">A context-and-spatial aware network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05355</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Distribution-aware coordinate representation for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Human pose estimation with spatial contextual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.01760</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Convolutional relation network for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiagang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">370</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="109" to="117" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Action machine: Toward person-centric action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiagang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1633" to="1637" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
