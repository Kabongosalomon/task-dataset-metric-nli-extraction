<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dance with Flow: Two-in-One Stream Action Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaojiao</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cees</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Dance with Flow: Two-in-One Stream Action Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The goal of this paper is to detect the spatio-temporal extent of an action. The two-stream detection network based on RGB and flow provides state-of-the-art accuracy at the expense of a large model-size and heavy computation. We propose to embed RGB and optical-flow into a single twoin-one stream network with new layers. A motion condition layer extracts motion information from flow images, which is leveraged by the motion modulation layer to generate transformation parameters for modulating the low-level RGB features. The method is easily embedded in existing appearance-or two-stream action detection networks, and trained end-to-end. Experiments demonstrate that leveraging the motion condition to modulate RGB features improves detection accuracy. With only half the computation and parameters of the state-of-the-art two-stream methods, our two-in-one stream still achieves impressive results on UCF101-24, UCFSports and J-HMDB.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This paper strives for the spatio-temporal detection of human actions in video, which is a crucial ability for selfdriving cars, autonomous care robots, and advanced video search engines. The leading approach for this challenging problem relies on fast detectors at the frame level <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b36">37]</ref>, which are then linked <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b36">37]</ref> or tracked <ref type="bibr" target="#b47">[48]</ref> over time. <ref type="bibr">Kalogeiton et al. [21]</ref> and Singh et al. <ref type="bibr" target="#b35">[36]</ref> further showed it is advantageous to stack the features from subsequent frames before predicting action class scores and determining the enclosing tube. Most of the state-of-the-art action detectors exploit a two-stream architecture <ref type="bibr" target="#b34">[35]</ref>, one for RGB and one for optical-flow, which are individually trained before fusion. However, the double computation and parameter demand of two-stream methods does not lead to double accuracy compared to a single stream. We propose to embed RGB and optical-flow into a single stream for action detection.</p><p>We are inspired by progress on feature normalization, especially conditional normalization <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18]</ref>, which has been successfully employed to visual question answer-  <ref type="figure">Figure 1</ref>: Two-in-one stream. We propose to embed RGB and optical-flow into a single stream for spatio-temporal action detection. Besides efficiency gains, it helps recognizing whether the dancer in the current frame is standing up or sitting down without considering the future. By utilizing information from flow images, the dancer is given a moving direction, up or down, better indicating the action.</p><p>ing <ref type="bibr" target="#b4">[5]</ref>, visual reasoning <ref type="bibr" target="#b29">[30]</ref>, image style transfer <ref type="bibr" target="#b16">[17]</ref> and super-resolution <ref type="bibr" target="#b46">[47]</ref>. Peretz et al. <ref type="bibr" target="#b29">[30]</ref> propose a feature-wise linear modulation layer which enables a recurrent neural network over an input question to influence convolutional neural network computation over an image. It demonstrates that features are capable to be modulated via a simple feature-wise affine transformation based on conditioning information. However, as their modulation layer is agnostic to spatial location, it is unsuited for action detection. In <ref type="bibr" target="#b46">[47]</ref>, Wang et al. developed a spatial feature transform layer, which is conditioned on categorical semantic probability maps, to modulate a super-resolution network. Encouraged by these works, we propose a motion condition layer and a motion modulation layer to adjust an RGBstream for spatio-temporal action detection.</p><p>We make the following contributions in this paper. We propose to embed RGB and optical-flow into a single stream for spatio-temporal action detection. It reduces the computational costs of conventional two-stream detection networks by half while maintaining its high accuracy. We in-troduce the two-in-one stream with motion condition layer and motion modulation layer, which learns video representations of appearance-stream features conditioned on optical-flow. As shown in <ref type="figure">Figure 1</ref>, the motion condition will guide the model to pay more attention on what moves, rather than the static background. The method is easily embedded in existing appearance-or two-stream action detection networks, and trained end-to-end, leading to new stateof-the-art on UCF101-24, UCFSports and J-HMDB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The spatio-temporal detection of human actions in video has a long tradition in computer vision, e.g. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b21">22]</ref>. Early success came from detection based on exhaustive cuboid search, efficient feature representations, and SVM-based learning, <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b51">52]</ref>. This was later extended with more flexible sequences of bounding boxes <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b50">51]</ref>, or spatiotemporal proposals <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b44">45]</ref>, together with engineered appearance and motion features, most notably the dense trajectories <ref type="bibr" target="#b27">[28]</ref>. The past few years, architectures integrating detection and deep representation learning have been leading <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50]</ref>, typically combining appearance and flow streams <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b36">37]</ref>. We follow this tradition.</p><p>The two-stream network was first introduced by Simonyan and Zisserman in <ref type="bibr" target="#b34">[35]</ref>. Their convolutional architecture included a separate RGB-stream and a flow-stream, which were combined by late fusion, for SVM-based action classification. In <ref type="bibr" target="#b8">[9]</ref>, Feichtenhofer et al. investigated a number of ways to fuse the RGB and flow streams in order to best take advantage of their fused representation for action classification. While we concentrate on action detection in the paper, we are interested in RGB and flow as well, but rather than combining the two streams in a late fusion, we prefer a single stream.</p><p>Gkioxari and Malik <ref type="bibr" target="#b12">[13]</ref> introduced a two-stream architecture with R-CNN detectors in action detection. They fused features from the last layer of an RGB-and a flowstream, and then trained action specific SVM classifiers. A Viterbi algorithm <ref type="bibr" target="#b39">[40]</ref> was adopted to link the detection boxes per frame into a tube. Weinzaepfel et al. <ref type="bibr" target="#b47">[48]</ref> also used a two-stream R-CNN detector but replaced the linking by a tracking-by-detection method. Both methods are not end-to-end trainable and restricted to trimmed videos.</p><p>End-to-end two-stream detectors based on faster-RCNN were proposed in <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b33">34]</ref>. In <ref type="bibr" target="#b28">[29]</ref>, Peng and Schmid performed region of interest pooling and score fusion to incorporate an RGB-stream and a flow-stream. In <ref type="bibr" target="#b15">[16]</ref>, Hou et al. extended 2D region of interest pooling to 3D tube-ofinterest pooling with 3D convolutions, which directly generate tubelets for action detection. Singh et al. adopted a two-stream single-shot-multibox detector (SSD) <ref type="bibr" target="#b26">[27]</ref> for realizing real time detection in <ref type="bibr" target="#b36">[37]</ref>. Singh et al. <ref type="bibr" target="#b35">[36]</ref> also introduced a transition matrix to generate a set of action proposals on pairs of frames. Kalogeiton et al. <ref type="bibr" target="#b20">[21]</ref> proposed to exploit temporal continuity by taking as much as six frames as input for their single-shot multibox detector, leading to state-of-the-art results. In this paper, we take the single-shot multibox detector network as our backbone, using single <ref type="bibr" target="#b36">[37]</ref> or multiple <ref type="bibr" target="#b20">[21]</ref> frames as input, but rather than separating the streams for RGB and flow we introduce a single two-in-one stream.</p><p>Li et al. <ref type="bibr" target="#b25">[26]</ref> proposed an action detector using an LSTM architecture with motion-based attention. Our two-in-one stream not only takes motion as attention, which helps to locate actions, but also uses motion to modulate RGB features which helps to better classify actions. Moreover, our method is easily embedded in existing appearance-or twostream action detection and classification networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Two-in-One Network</head><p>We define the RGB-stream network D rgb θ trained on single frame for spatio-temporal action detection as:</p><formula xml:id="formula_0">(L rgb , S rgb ) = D rgb θ (I rgb )<label>(1)</label></formula><p>where I rgb ∈ R H×W ×3 is a single RGB frame of height H and width W which is the input for the network D rgb θ . L rgb ∈ R Q×4 and S rgb ∈ R Q×(P +1) are Q box locations and corresponding box classification scores for P action classes and a background class. θ represents the parameters of the learned network. Similarly, we define a flow-stream network on single frame for spatio-temporal action detection as:</p><formula xml:id="formula_1">(L of , S of ) = D of θ (I of )<label>(2)</label></formula><p>I of ∈ R H×W ×2 is a single optical flow image with x and y components of the velocity respectively in two channels. The two-stream method includes training the two networks D rgb θ and D of θ independently, and fuses the results (L rgb , S rgb ) and (L of , S of ).</p><p>Motion condition layer. In our method, I of is regarded as a motion map with the same resolution as the corresponding RGB image I rgb . We take I of as prior information Ψ when applying an RGB-stream network D rgb θ to estimate where and what actions may occur. Then we formulate our two-in-one network as a condition network:</p><formula xml:id="formula_2">(L , S ) = D θ (I rgb |Ψ) = D θ (I rgb |MC(I of )) (3) Ψ = MC(I of ) = MC((I ofx , I ofy ))<label>(4)</label></formula><p>means two-in-one stream, MC(·) is a mapping function to generate simple features from the flow images. So the two-in-one stream D θ learns a model conditioned on motion information by a motion condition layer.</p><p>Motion modulation layer. We introduce a motion modulation (M 2 ) layer to modify the features learned from ... RGB images. An M 2 layer is able to influence the appearance network by incorporating motion and weighting the action area. We first learn a pair of affine transformation parameters (β, γ) from the prior flow condition Ψ by a function F : Ψ −→ (β, γ). Concretely, the two-in-one network is further expressed as:</p><formula xml:id="formula_3">(β, γ) = F(Ψ), (L , S ) = D θ (I rgb |β, γ)<label>(5)</label></formula><p>In order to modulate the appearance network, we apply a transform function M 2 (·) with the learned transformation parameters (β, γ) to the RGB features F rgb .</p><formula xml:id="formula_4">M 2 (F rgb ) = β F rgb + γ<label>(6)</label></formula><p>is an element-wise multiplication operation. The RGB feature maps F rgb has the same dimensions with parameters β and γ. The flow information represented by (β, γ) influences the appearance network by both feature-wise and spatial-wise manipulations. The complete network with the motion condition layer and the motion modulation layer is shown in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>Network architecture. Due to sparsity of flow images, we adopt simple convolutional layers to extract low-level motion condition information. 1 × 1 convolutional layers attempt to keep the spatial pixel-wise motion vectors. The motion condition then inputs to a motion modulation (M 2 ) layer in which it is separately mapped to a pair of transformation parameters β and γ. Two groups of 1 × 1 convolutional layers are independently adopted for generating each of the parameters β and γ. The low-level RGB features from the appearance network are adjusted by β and γ. The motion modulation layer is capable to be added to any bottom layer of the appearance network, including conv1, conv2, conv3 and conv4. All of them share the motion condition layer. The whole network is end-to-end trainable.</p><p>Feature visualization. In order to intuitively understand the method, we show the generated feature maps from the appearance network before and after modulation by motion condition in <ref type="figure" target="#fig_2">Figure 3</ref>. We randomly select some feature maps from the motion condition layer in the first row. The features are low-level and sparse, which are taken as prior conditions. From the second row to the last row, we show the corresponding scale (β) and shift (γ) maps generated from conditions, RGB features without modulation and features modulated by β and γ. It is interesting to see the difference between the features without and with modulation in <ref type="figure" target="#fig_2">Figure 3</ref>. For example the modified features of the actor areas in feature maps 0 and 43, after modulation, especially for the female ice skater, which is blended into the background on the regular RGB stream. On the 28-th feature map, a feature response is even hard to see on both actors before modulation. Feature maps 10 and 127 show the change in x-direction features and y-direction features. The flow condition pushes the model to focus on moving actors.</p><p>Training loss. In order to demonstrate the generalization and flexibility of the proposed method, we embed the motion condition layer and the motion modulation layer in a single-frame appearance stream and a multi-frame appear- ance stream. The basic loss function is derived from the one for object detection <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b30">31]</ref>. Defining x p ij = {1, 0} as an indicator for matching the i-th default box to the j-th ground truth box of action category p. The overall loss function contains the localization (loc) loss and the confidence (conf ) loss:</p><formula xml:id="formula_5">L(x, c, l, g) = 1 N (L loc (x, l, g) + L conf (x, c)) (7)</formula><p>with N representing the number of matched default boxes. c represents multiple classes confidences. l and g are the predicted box and the ground truth box.</p><p>The confidence loss applies the softmax loss as below:</p><formula xml:id="formula_6">L conf (x, c) = − N i∈P os x p ij log(ĉ p i ) − i∈N eg log(ĉ 0 i ) c p i = exp(c p i ) p exp(c p i )<label>(8)</label></formula><p>The localization loss applies a smooth L1 loss <ref type="bibr" target="#b11">[12]</ref> between the predicted box and the ground truth box. The network regresses to offsets for the center (cx, cy) of the default box(d) and for its width (w) and height (h).</p><p>L loc (x, l, g) = N i∈P os m∈{cx,cy,w,h}</p><formula xml:id="formula_7">x k ij smooth L1 (l m i −ĝ m j ) g cx j = (g cx j − d cx i )/d w iĝ cy j = (g cy j − d cy i )/d h î g w j = log( g w j d w i )ĝ h j = log( g h j d h i )<label>(9)</label></formula><p>For the multi-frame appearance stream, we follow Kalogeiton et al. <ref type="bibr" target="#b20">[21]</ref> to train the network.</p><p>Two-in-one two-stream. Our method emphasizes to utilize RGB and optical flow information in one stream. Furthermore, it is possible to follow the standard practice of two-stream action detection. We train a two-in-one detector conditioned on flow images, and a separate flow detector which only takes as input the flow images. For a single-frame two-in-one two-stream, we use average fusion method to merge the results from each stream following <ref type="bibr" target="#b36">[37]</ref>. And for multi-frame two-stream, the late fusion <ref type="bibr" target="#b8">[9]</ref> is a better choice <ref type="bibr" target="#b20">[21]</ref>.</p><p>Linking. Once the frame-level detections or tubelet detections are achieved, we link them to build action tubes. We adopt the linking method described in <ref type="bibr" target="#b36">[37]</ref> for framelevel detections and the method in <ref type="bibr" target="#b20">[21]</ref> for tubelet detections.</p><p>Code is available at https://github.com/jiaozizhao/Twoin-One-ActionDetection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets, Metrics &amp; Implementation</head><p>Datasets. We perform experiments on three action detection datasets. UCF101-24 <ref type="bibr" target="#b38">[39]</ref> is a subset of UCF101. It contains 24 sport classes in 3207 untrimmed videos. Each video contains a single action category. Multiple action instances with the same class, but different spatial and temporal boundaries may occur. We use the revised annotations for UCF101-24 from <ref type="bibr" target="#b36">[37]</ref>. UCF-Sports [32] contains 10 sport classes in 150 trimmed videos. We follow <ref type="bibr" target="#b23">[24]</ref> to divide the training and test splits. J-HMDB [20] contains 21 action categories in 928 trimmed videos. We report the average results on three splits.</p><p>Metrics. Following <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b47">48]</ref>, we utilize video mean Average Precision (mAP ) to evaluate action detection accuracy. We calculate an average of per-frame Intersectionover-Union (IoU) across time between tubes. A detection is correct if it's IoU with the ground truth tube is greater than a threshold and its action label is correctly assigned. We compute the average precision for each class and report the average over all classes.</p><p>Implementation. We adopt a real-time single shot multibox detector (SSD) network <ref type="bibr" target="#b26">[27]</ref> as the backbone. We  <ref type="table">Table 1</ref>: Two-in-one vs. baselines for action detection on UCF101-24 and action classification on UCF101. Two-in-one with motion modulation works well for both action detection and action classification.</p><p>insert the developed motion layers into two state-of-the-art appearance SSD networks, one based on single frame <ref type="bibr" target="#b36">[37]</ref> and the other based on multiple frames <ref type="bibr" target="#b20">[21]</ref>. We use VGG-16 pre-trained weights on ImageNet as model initialization.</p><p>The input size is 300x300 for both of them. We follow <ref type="bibr" target="#b20">[21]</ref> to use 6 continuous frames as input to the multi-frame SSD. The initial learning rate is set to 0.001 for the single-frame network and 0.0001 for the multi-frame network on all the three datasets and changed by applying step decay strategy. We trained a flow-stream, an RGB-stream and our two-inone stream for 13.2, 13.2 and 15.5 hours, respectively. Alternatively, we considered to use appearance information to modulate flow stream. However, it does not work well. It appears difficult to modulate features from flow images which are sparse, using RGB images which are more dense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>All the ablation studies are performed on UCF101-24. We only report mAP at the most challenging high IoU thresholds 0.5:0.95 (with step 0.05). Initially, in order to maintain the spatial pixel-wise motion vectors, we apply 1x1 convolution kernels to all layers in the motion condition layer and the motion modulation layer. We use layer parameter stride to control the size of β and γ. Then the motion modulation layer is applied to conv1 of SSD. Flow images are generated using the method in <ref type="bibr" target="#b1">[2]</ref>, which we refer to as BroxFlow.</p><p>Two-in-one vs. baselines. We compare the two-in-one stream to its corresponding RGB-stream, flow-stream and two-stream in <ref type="table">Table 1</ref>. Runtime and # param. are also reported for comparing the efficiency. Our single two-inone stream exceeds a single RGB-stream by 1.5%. Notably, two-in-one even outperforms the corresponding two-stream with only half the computation cost and # param..</p><p>We also consider action classification, on UCF101. We follow <ref type="bibr" target="#b45">[46]</ref>, with ResNet152 as backbone. The Top 1 accuracy and efficiency shown in <ref type="table">Table 1</ref> illustrate our strategy also works for action classification and generalizes beyond SSD with VGG16. For training, our two-in-one stream converges at the 100-th epoch, but the RGB-and flow-     Our motion modulation strategy works better for the detection task, which needs localization representations that are translation-variant, compared to the classification task which favors translation invariance.</p><p>Where to add the modulation layer? The motion condition layer is leveraged to generate low-level motion features as flow images are more sparse. We add the motion   modulation layer to the bottom convolutional layers with low-level RGB features. We conduct two experiments on which layer to add the modulation. We compare the accuracy and # param. after applying a modulation layer to conv1, conv2, conv3 and conv4 in <ref type="figure" target="#fig_4">Figure 4 (a)</ref>. Accuracy decreases and # param. increases slightly for deeper layers. Next we add the modulation layers to multiple convolutional layers simultaneously in <ref type="figure" target="#fig_4">Figure 4 (b)</ref>. Applying multiple modulation layers does not change the results much. Thus, we prefer to use a single modulation layer. Note that accuracy drops for deeper layers as we use 1x1 convolution kernels to process flow images, leading to smaller receptive field for deeper layers. How to design the condition layer? To further improve the method, we consider whether the 1x1 convolution kernel for the motion condition layer is the best choice. Besides keeping the spatial pixel-wise motion, it may need to consider some context of motion to better fit the RGB features. We adopt the 3x3 convolution kernels to the last layer of the condition network. <ref type="figure" target="#fig_6">Figure 5</ref> demonstrates that considering motion context boosts the accuracy for all layers. As a bigger receptive field is used, the conv2 model achieves the best results, about 1.5% improvement compared to 1x1 convolution kernels. The run time hardly increases for deeper layers, and is still 0.04 sec per frame. The # param. are <ref type="bibr">26.85, 26.92, 27</ref>.01 and 27.19 M respectively of conv1, conv2, conv3 and conv4. Considering the trade-off between the results and parameters, we believe conv2 provides the best accuracy/efficiency trade-off.</p><p>What flow? As we leverage flow information as prior conditions, we wonder how the model is influenced by flow images. Here we adopt flow images generated by three different methods (seen in <ref type="figure" target="#fig_7">Figure 6</ref>) and evaluate how  <ref type="figure">Figure 7</ref>: Generalization ability. Accuracy comparison on: (a) UCF101-24, (b) UCFSports, (c) J-HMDB, with different methods. Two-in-one stream even outperforms twostream on UCF101-24 and UCFSports. Two-in-one stream fused with a flow-stream obtains the best accuracy on all three datasets.</p><p>our strategies work. We use BroxFlow <ref type="bibr" target="#b1">[2]</ref> (accurate flow method), Flownet <ref type="bibr" target="#b5">[6]</ref> (deep network method) and a realtime but less accurate optical flow method <ref type="bibr" target="#b22">[23]</ref> (RealTime-Flow). From <ref type="table" target="#tab_3">Table 2</ref>, it is concluded that no matter which kind of flow images are applied, our two-in-one stream outperforms RGB-streams and corresponding two-streams. We also note that the more accurate the flow images, the more improvement the two-in-one stream obtains. Even when using the somewhat noisy RealTimeFlow images, the twoin-one stream still improves the RGB-stream. However, a two-stream based on RealTimeFlow obtains almost the same accuracy as the RGB-stream, which illustrates that two-stream depends on the the quality of flow images. Our two-in-one stream is more robust to the quality of flow images. Moreover, we report the flow computation in seconds/frame for the three kinds of flow methods: BroxFlow (0.098), FlowNet (0.183) and RealTimeFlow (0.014). Re-alTimeFlow only needs 0.014 seconds to generate one flow image, at the expense of a slightly lower mAP. Generalization ability. To stress the generalization ability of our proposal, we compare the results on three different datasets. Following the conclusions of our ablation so far, we use the BroxFlow image for generating condition and apply a 3x3 kernel to the last layer of the motion condition layer. The motion modulation layer is only leveraged for the conv2 layer of the appearance stream. We report results in <ref type="figure">Figure 7</ref>.</p><p>Obviously, the proposed two-in-one stream performs better than other one-stream networks. It is noteworthy that our two-in-one stream even outperforms traditional twostream networks on UCF101-24 and UCFSport by 2% with only half the parameters of a two-stream network. On J-HMDB, two-in-one is 3% higher than RGB-stream but 3% lower than two-stream. We look into J-HMDB and find that most videos in the dataset have neighbouring repeated frames. For fair comparison, we just download the Brox-Flow images used in <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b36">37]</ref>. However, the provided Brox-Flow image between the two repeated RGB frames is not 0, as it should be, but similar to the last flow frame. The issue affects our two-in-one stream due to the fact that we need  correct flow image as the condition of the corresponding RGB frame. We expect that two-in-one will present better results on J-HMDB after correcting the flow images. As expected, adding a separate flow-stream to our two-in-one stream gives the best accuracy on all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Qualitative Analysis</head><p>The motion condition layer and the motion modulation layer are beneficial to generate better video representations for spatio-temporal action detection. But how do the layers make a difference to the appearance network? To understand this behavior, we visualize in <ref type="figure" target="#fig_9">Figure 8</ref>, the detection results of an RGB-stream network and a two-in-one network. Also, we visualize the gradient-weighted class activation heatmaps <ref type="bibr" target="#b52">[53]</ref> for better understanding how the motion conditions influence the behavior of the appearance network. We choose a challenging case of cliff diving here. The image resolution is low and the actor is quite tiny. The cluttered background obviously increases the difficulty to Saha et al. <ref type="bibr" target="#b33">[34]</ref> Behl et al. <ref type="bibr" target="#b0">[1]</ref> Singh et al. <ref type="bibr" target="#b36">[37]</ref> This paper: two-in-one This paper: two-in-one two-stream multi-frame Saha et al. <ref type="bibr" target="#b32">[33]</ref> Kalogeiton et al. <ref type="bibr" target="#b20">[21]</ref> This paper: two-in-one This paper: two-in-one two-stream detect actions. We manually overlay green dashed boxes to indicate the locations of the actor and zoom in to highlight where the action is happening. The second row shows that the RGB-stream fails to detect any actions. From the corresponding heatmaps, it is apparent that the appearance network pays more attention to the background than to the actions. There are only weak responses on the action positions. We manually overlay red dashed boxes to highlight the position of the actor on the heatmaps. From the heatmaps for the two-in-one network in the last row, we clearly see it is capable to balance the activation on actions and background. The responses on action positions are strengthened. As expected, the two-in-one stream performs better than the RGB-stream. It outputs correct detections for cliff diving on all the frames (forth row ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison to the State-of-the-art</head><p>Accuracy. For fair comparisons, we just use the original images as in all the state-of-the-arts, without camera motion removal. We compare the mAP at variable IoU thresholds in <ref type="table">Table 3</ref>. Considering the most challenging high IoU thresholds 0.5:0.95, we observe that for the single-frame setting, our two-in-one stream achieves even better results than existing two-stream methods on UCF101-24 and UCF-  <ref type="table">Table 3</ref>: Accuracy comparison to the state-of-the-art. Bold means top accuracy and italic means second top accuracy. For the high overlap setting of mAP @IoU =0.5:0.95, our two-in-one stream works well in both a single-frame and multipleframe network for all three datasets. When we add an additional flow-stream to obtain a two-in-one two stream we further improve accuracy.</p><p>Sports. For instance, two-in-one stream outperforms Singh et al. <ref type="bibr" target="#b36">[37]</ref> with the same SSD detector by more than 1% and Peng and Schmid <ref type="bibr" target="#b28">[29]</ref> with a Faster-RCNN detector by an absolute 12% on UCF101-24. As analyzed previously, two-in-one stream performs modest on J-HMDB because of the data issue of the provided BroxFlow images. When we combine two-in-one into a regular two-stream network by fusing with a flow-stream, it produces good results on all three datasets. Compared to two-in-one stream, it gets about 5% improvement on J-HMDB. Moreover, when feeding our two-in-one network variants with multiple frames, as suggested by Kalogeiton et al. <ref type="bibr" target="#b20">[21]</ref>, our two-in-one stream outperforms the two-stream <ref type="bibr" target="#b20">[21]</ref> a little on UCF101-24 and UCFSports with only half computation and the number of parameters. Our two-in-one stream fused with a flow stream further boosts the results, outperforming the very recent work of Singh et al. <ref type="bibr" target="#b35">[36]</ref>.</p><p>Efficiency. Besides good detection accuracy, our method has the advantage of a reduced inference time and less # param.. Here we compare our methods from the efficiency aspect to the state-of-the-art on UCF101-24. We test our models on one NVIDIA GTX 1080 GPU. The trade-off between accuracy and inference time, as well as parameters are visualized in <ref type="figure" target="#fig_11">Figure 9</ref>. Among the single-frame methods, our two-in-one stream has the fastest run time with 0.04s per frame, two times faster than <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b36">[37]</ref> and much faster than <ref type="bibr" target="#b33">[34]</ref> and <ref type="bibr" target="#b28">[29]</ref> (about 0.5s per frame). Moreover, the # param. of our two-in-one stream is smallest, about 26.93 M. While our two-in-one accuracy is even better than the two-stream methods by <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37]</ref>. Combining our two-in-one stream with a standard flow-stream gains an accuracy improvement at the expense of more computation and parameters. Our two-in-one alternative even outperforms <ref type="bibr" target="#b20">[21]</ref> a little in accuracy with only half the parameters. The two-in-one two stream further improves the result with almost similar inference time, but slightly more parameters. We conclude that two-in-one stream networks provide a good accuracy/efficiency trade-off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose an effective and efficient two-in-one stream network for spatio-temporal action detection. It takes flow images as prior motion condition when training an RGBstream network. The network's motion condition layer and motion modulation layer address two issues in action detection: frame-level RGB images lack motion information and (static) background-context may dominant the learned representation. Our two-in-one stream achieves state-ofthe-art accuracy at high IoU thresholds, using only half of the parameters and computation of two-stream alternatives. Besides motion, we believe that other information such as depth-maps or infrared images may help locate the actors, and can be exploited as additional prior conditions for training two-in-one streams.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Two-in-one network architecture. The motion condition layer (pink cube) maps flow images to prior condition information. The condition inputs to the motion modulation layer (purple cube) to generate transformation parameters which are used to modulate RGB features (F rgb ). The network has half the computation and parameters of a two-stream equivalent, while obtaining better action detection accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Feature maps. Visualization of the motion condition maps, scale maps, shift maps, RGB features without modulation and features with modulation. The modulated features focus more on moving actors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Where to add the modulation layer? Accuracy on UCF101-24 and # param. with varying: (a) single modulated layer, and (b) multiple modulated layers. A single modulation layer at conv1 gets the best result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>How to design the condition layer? Comparing accuracy on UCF101-24 when applying 1x1 conv or 3x3 conv to the last layer of the motion condition layer. The 3x3 conv performs better. stream converge at 200-th and 300-th epoch, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>What flow? Examples of flow images generated by different flow methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>RGBstream Results: no detections (confidence scores &lt; 0.5) (b) RGBstream Heatmaps: low activation on actor (c) Twoinone Results: correct detections (cliff diving scores &gt; 0.5) (d) Twoinone Heatmaps: high activation on actor</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Visualization of detection and heatmaps on conv4 layers from RGB-stream network in (a) (b) and twoin-one stream network in (c) (d). We add the green dashed boxes to indicate the action. The two-in-one stream has higher activation on actions, resulting in correct detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>Efficiency comparison to the state-of-the-art. Accuracy vs. (a) inference time (second per frame) and (b) # param. (M) on UCF101-24. Our two-in-one stream best balances accuracy and efficiency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>What flow? No matter what flow images are applied on UCF101-24, our two-in-one stream outperforms the corresponding flow-, RGB-and two-stream. We obtain the best result with BroxFlow.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments Supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DOI/IBC) contract number D17PC00343. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing endorsements, either expressed or implied, of IARPA, DOI/IBC, or the U.S. Government.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Incremental tube construction for human action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harkirat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Behl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gurkirt</forename><surname>Sapienza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suman</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Cuzzolin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">High accuracy optical flow estimation based on a theory for warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrés</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Papenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Crossdataset action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangliang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Human detection using oriented histograms of flow and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navneet</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Modulating early visual processing by language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Harm De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jérémie</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Videocapsulenet: A simplified network for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A learned representation for artistic style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kudlur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Exploring the structure of a real-time, arbitrary neural artistic stylization network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">João</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.10066</idno>
		<title level="m">A better baseline for ava</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Finding action tubes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ava: A video dataset of spatio-temporally localized atomic visual actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhui</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeqing</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanna</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generic tubelet proposals for action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mostafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Tube convolutional neural network (t-cnn) for action detection in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Tubelets: Unsupervised action proposals from spatiotemporal super-voxels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihir</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Bouthemy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">IJCV</biblScope>
			<biblScope unit="page" from="287" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Towards understanding action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hueihan</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Action tubelet detector for spatiotemporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicky</forename><surname>Kalogeiton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A spatio-temporal descriptor based on 3d-gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Klaser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Marszałek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fast optical flow using dense inverse search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Till</forename><surname>Kroeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Discriminative figurecentric models for joint action localization and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recurrent tubelet proposal and recognition networks for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Videolstm convolves, attends and flows for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirill</forename><surname>Gavrilyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihir</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">166</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="41" to="50" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient action localization with approximately normalized fisher vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-region twostream r-cnn for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Film: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harm De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Action mach a spatio-temporal maximum average correlation height filter for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mikel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javed</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Amtnet: Action-micro-tube regression by end-to-end trainable deep architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suman</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gurkirt</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Cuzzolin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep learning for detecting multiple space-time action tubes in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suman</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gurkirt</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Sapienza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cuzzolin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Tramnettransition matrix network for efficient action tube proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gurkirt</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suman</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Cuzzolin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Online real-time multiple spatiotemporal action localisation and prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gurkirt</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suman</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Sapienza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cuzzolin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Predicting the where and what of actors and actions through online action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haroon</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">On-line viterbi algorithm and its relationship to random walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Broňa</forename><surname>Rastislavšrámek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Brejová</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinař</surname></persName>
		</author>
		<idno type="arXiv">arXiv:0704.0062</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Actor-centric relation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Spatiotemporal deformable part models for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yicong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Max-margin structured output regression for spatio-temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Video event detection: From subvolume localization to spatio-temporal path search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Apt: Action localization proposals from dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihir</forename><surname>Jan C Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ella</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Towards good practices for very deep two-stream convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.02159</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Recovering realistic texture in image super-resolution by deep spatial feature transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning to track for spatio-temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Spatiotemporal action detection with cascade proposal and location anticipation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fast action proposals for human action detection and search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Discriminative video pattern search for efficient action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1728" to="1743" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
