<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantically Conditioned Dialog Response Generation via Hierarchical Disentangled Self-Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
							<email>wenhuchen@cs.ucsb.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
							<email>jianshuchen@tencent.comqinpengda@bupt.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">AI Lab</orgName>
								<address>
									<settlement>Bellevue</settlement>
									<region>Tencent, WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengda</forename><surname>Qin</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
							<email>xyan@cs.ucsb.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
							<email>william@cs.ucsb.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Semantically Conditioned Dialog Response Generation via Hierarchical Disentangled Self-Attention</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T15:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantically controlled neural response generation on limited-domain has achieved great performance.</p><p>However, moving towards multi-domain large-scale scenarios are shown to be difficult because the possible combinations of semantic inputs grow exponentially with the number of domains. To alleviate such scalability issue, we exploit the structure of dialog acts to build a multi-layer hierarchical graph, where each act is represented as a rootto-leaf route on the graph. Then, we incorporate such graph structure prior as an inductive bias to build a hierarchical disentangled self-attention network, where we disentangle attention heads to model designated nodes on the dialog act graph. By activating different (disentangled) heads at each layer, combinatorially many dialog act semantics can be modeled to control the neural response generation. On the large-scale Multi-Domain-WOZ dataset, our model can yield a significant improvement over the baselines on various automatic and human evaluation metrics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Conversational artificial intelligence <ref type="bibr" target="#b23">(Young et al., 2013)</ref> is one of the critical milestones in artificial intelligence. Recently, there have been increasing interests in industrial companies to build task-oriented conversational agents <ref type="bibr" target="#b8">Li et al., 2017;</ref><ref type="bibr" target="#b13">Rojas-Barahona et al., 2017)</ref> to solve pre-defined tasks such as restaurant or flight bookings, etc (see <ref type="figure" target="#fig_0">Figure 1</ref> for an example dialog from MultiWOZ <ref type="bibr" target="#b0">(Budzianowski et al., 2018)</ref>). Traditional agents are built based on slotfilling techniques, which requires significant human handcraft efforts. And it is hard to generate naturally sounding utterances in a generalizable and scalable manner. Therefore, different semantically controlled neural language generation models have been developed <ref type="bibr" target="#b19">(Wen et al., 2015</ref><ref type="bibr" target="#b17">(Wen et al., , 2016a</ref><ref type="bibr" target="#b3">Dusek and Jurcícek, 2016)</ref> to replace the traditional systems, where an explicit semantic representation (dialog act) are used to influence the RNN generation. The canonical approach is proposed in <ref type="bibr" target="#b19">(Wen et al., 2015)</ref> to encode each individual dialog act as a unique vector and use it as an extra input feature into the cell of long short-term memory (LSTM) to influence the generation. As pointed in <ref type="bibr" target="#b18">(Wen et al., 2016b)</ref>, these models though achieving good performance on limited domains, suffer from scalability problem as the possible dialog acts grow combinatorially with the number of domains.</p><p>In order to alleviate such issue, we propose a hierarchical graph representation by leveraging the structural property of dialog acts. Specifically, we first build a multi-layer tree to represent the entire dialog act space based on their interrelationships. Then, we merge the tree nodes with the same semantic meaning to construct an acyclic multi-layered graph, where each dialog act is interpreted as a root-to-leaf route on the graph. Such graph representation of dialog acts not only grasps the inter-relationships between different acts but also reduces the exponential representation cost to almost linear, which will also endow it with greater generalization ability. Instead of simply feeding such vectorized representation as an external feature vector to the neural networks, we propose to incorporate such a structure act as an inductive prior for designing the neural architecture, which we name as hierarchical disentangled self-attention network (HDSA). In <ref type="figure" target="#fig_1">Figure 2</ref>, we show how the dialog act graph structure is explicitly encoded into model architecture. Specifically, HDSA consists of multiple layers of disentangled self-attention modules <ref type="bibr">(DSA)</ref>. Each DSA has multiple switches to set the on/off state for its heads, and each head is bound for modeling a designated node in the dialog act graph. At the train- ing stage, conditioned on the given dialog acts and the target output sentences, we only activate the heads in HDSA corresponding to the given acts (i.e., the path in the graph) to activate the heads with their designated semantics. At test time, we first predict the dialog acts and then use them to activate the corresponding heads to generate the output sequence, thereby controlling the semantics of the generated responses without handcrafting rules. As depicted in <ref type="figure" target="#fig_1">Figure 2</ref>, by gradually activating nodes from domain → action → slot, the model is able to narrow its response down to specifically querying the user about the color and type of the taxi, which provides both strong controllability and interpretability. Experiment results on the large-scale Multi-WOZ dataset <ref type="bibr" target="#b0">(Budzianowski et al., 2018)</ref> show that our HDSA significantly outperforms other competing algorithms. <ref type="bibr" target="#b25">1</ref> In particular, the proposed hierarchical dialog act representation effectively <ref type="bibr" target="#b25">1</ref> The code and data are released in https://github. com/wenhuchen/HDSA-Dialog improves the generalization ability on the unseen test cases and decreases the sample complexity on seen cases. In summary, our contributions include: (i) we propose a hierarchical graph representation of dialog acts to exploit their inter-relationships, which greatly reduces the sample complexity and improves generalization, (ii) we propose to incorporate the structure prior in semantic space to design HDSA to explicitly model the semantics of neural generation, and outperforms baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work &amp; Background</head><p>Canonical task-oriented dialog systems are built as pipelines of separately trained modules: (i) user intention classification <ref type="bibr" target="#b14">(Shi et al., 2016;</ref><ref type="bibr" target="#b5">Goo et al., 2018)</ref>, which is for understanding human intention. (ii) belief state tracker <ref type="bibr">Mrksic et al., 2017a,b;</ref><ref type="bibr" target="#b24">Zhong et al., 2018;</ref>, which is used to track user's query constraint and formulate DB query to retrieve entries from a large database. (iii) dialog act prediction , which is applied to classify the system action. (iv) response generation (Rojas- <ref type="bibr" target="#b13">Barahona et al., 2017;</ref><ref type="bibr" target="#b18">Wen et al., 2016b;</ref><ref type="bibr" target="#b8">Li et al., 2017;</ref><ref type="bibr" target="#b7">Lei et al., 2018)</ref> to realize language surface form given the semantic constraint. In order to handle the massive number of entities in the response, Rojas-Barahona et al. I recommend Little Seoul, which has a Low price.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Post-Processing</head><p>I recommend &lt;Res.Name&gt;, which has a &lt;Res.Price&gt; price. <ref type="figure">Figure 3</ref>: Illustration of the neural dialog system. We decompose it into two parts: the lower part describes the dialog state tracking and DB query, and the upper part denotes the Dialog Action Prediction and Response Generation. In this paper, we are mainly interested in improving the performance of the upper part. 2016), CamRes767 (Rojas- <ref type="bibr" target="#b13">Barahona et al., 2017)</ref> and KVRET <ref type="bibr" target="#b4">(Eric et al., 2017)</ref>, etc. However, a recently introduced multi-domain and large-scale dataset MultiWOZ <ref type="bibr" target="#b0">(Budzianowski et al., 2018)</ref> poses great challenges to these approaches due to the large number of slots and complex ontology. Dealing with such a large semantic space remains a challenging research problem.</p><p>We follow the nomenclature proposed in Rojas- <ref type="bibr" target="#b13">Barahona et al. (2017)</ref> to visualize the overview of the pipeline system in <ref type="figure">Figure 3</ref>, and then decompose it into two parts: the lower part (blue rectangle) contains state tracking and symbolic DB execution, the upper part consists of dialog act prediction and response generation conditioned on the state tracking and DB results. In this paper, we are particularly interested in the upper part (act prediction and response generation) by assuming the ground truth belief state and DB records are available. More specifically, we set out to investigate how to handle the large semantic space of dialog acts and leverage it to control the neural response generation. Our approach encodes the history utterances into distributed representations to predict dialog acts and then uses the predicted dialog acts to control neural response generation. The key idea of our model is to devise a more compact structured representation of the dialog acts to reduce the exponential growth issue and then incorporate the structural prior for the semantic space into the neural architecture design. Our proposed HDSA is inspired by the linguistically-inform self-attention <ref type="bibr" target="#b15">(Strubell et al., 2018)</ref>, which combines multi-head self-attention with multi-task NLP tasks to enhance the linguistic awareness of the model. In contrast, our model disentangles different heads to model different se-mantic conditions in a single task, which provides both better controllability and interpretability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dialog Act Representation</head><p>Dialog acts are defined as the semantic condition of the language sequence, comprising of domains, actions, slots, and values.</p><p>Tree Structure The dialog acts have universally hierarchical property, which is inherently due to the different semantic granularity. Each dialog act can be seen as a root-to-leaf path as depicted in <ref type="figure" target="#fig_2">Figure 4</ref> 2 . Such tree structure can capture the kinship between dialog acts, i.e. "restaurant-inform-location" has stronger similarity with "restaurant-inform-name" than "hotelrequest-address". The canonical approach to encode dialog acts is by concatenating the one-hot representation at each tree level into a flat vector like SC-LSTM <ref type="bibr" target="#b19">(Wen et al., 2015;</ref><ref type="bibr" target="#b0">Budzianowski et al., 2018)</ref> (details are in in Github 3 ). However, such representation impedes the cross-domain transfer between different slots and the cross-slot transfer between different values (e.g the "recommend" under restaurant domain is different from "recommend" under hospital domain). As a result, the sample complexity can grow combinatorially as the potential dialog act space expands in large-scale real-life dialog systems, where the potential domains and actions can grow dramatically. To address such issue, we propose a more compact graph representation. Graph Structure The tree-based representation cannot capture the cross-branch relationship like "restaurant-inform-location" vs. "hotel-informlocation", leading to a huge expansion of the tree. Therefore, we propose to merge the cross-branch nodes that share the same semantics to build a compact acyclic graph in the right part of <ref type="bibr">Figure 4 4</ref> . Formally, we let A denote the set of all the original dialog acts. And for each act a ∈ A, we use H(a) = {b 1 , · · · , b i , · · · , b L } to denote its L-layer graph form, where b i is its one-hot representation in the i th layer of the graph. For example, a dialog act "hotel-inform-name" has a compact graph representation H(a) = {b 1 :</p><formula xml:id="formula_1">[1, 0, 0], b 2 : [1, 0], b 3 : [1, 0, 0, 0, 0]}.</formula><p>More formally, let H 1 · · · H L denote the number of nodes at the layer of 1, · · · , L, respectively. Ideally, the total representation cost can be dramatically decreased from O( L i=1 H i ) tree-based representation to H 0 = L i=1 H i in our graph representation. Due to the page limit, we include the full dialog act graph and its corresponding semantics in the Appendix. When multiple dialog acts H(a) 1 , · · · , H(a) k are involved in the single response, we propose to aggregate them as A = BitOR(H(a) 1 , · · · , H(a) k ) as the H 0dimensional graph representation, where BitOR denotes the bit-wise OR operator 5 .</p><p>Generalization Ability Compared to the treebased representation, the proposed graph representation under strong cross-branch overlap can greatly lower the sample complexity. Hence, it leads to great advantage under sparse training instances. For example, suppose the ex-act dialog act "hotel-recommend-area" never appears in the training set. Then, at test time when used for response generation, the flat representation will obviously fail. In contrast, with our hierarchical representation, "hotel", "recommend" and "area" may have appeared separately in other instances (e.g., "recommend" appears in "attraction-recommend-name"). Its graph representation could still be well-behaved and generalize well to the unseen (or less frequent) cases due to the strong compositionality. <ref type="figure" target="#fig_3">Figure 5</ref> gives an overview of our dialog system. We now proceed to discuss its components below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Model</head><p>Dialog Act Predictor We first explain the utterance encoder module, which uses a neural network f ACT to encode the dialog history (i.e., concatenation of previous utterances from both the user and the system turns x 1 , · · · , x m ), into distributed token-wise representations u 1 , · · · , u m with its overall representationū as follows:</p><formula xml:id="formula_2">u, u1, · · · , um = fACT (x1, · · · , xm)<label>(1)</label></formula><p>where f ACT can be CNN, LSTM, Transformer, etc,ū, u 1 , · · · , u m ∈ R D are the representation. The overall featureū is used to predict the hierarchical representation of dialog act. That is, we output a vector P θ (A) ∈ R H 0 , whose i th component gives the probability of the i th node in the dialog act graph being activated:</p><formula xml:id="formula_3">P θ (A) = f θ (ū, v kb , v bf ) = σ(V T a tanh(Wuū + W b [v kb ; v bf ] + b))<label>(2)</label></formula><p>where V a ∈ R D×H 0 is the attention matrix, the weights W u , W b , b are the learnable parameters to project the input to R D space, and σ is the Sigmoid function. Here, we follow Budzianowski . For convenience, we use θ to collect all the parameters of the utterance encoder and action predictor. At training time, we propose to maximize the cross-entropy objective L(θ) as follows:</p><formula xml:id="formula_4">L(θ) =A · log(f θ (ū, v kb , v bf )+ (1 − A) · log(1 − f θ (ū, v kb , v bf ))<label>(3)</label></formula><p>where · denotes the inner product between two vectors. At test time, we predict the dialog actŝ</p><formula xml:id="formula_5">A = {I(P θ (A) i &gt; T )|1 ≤ i ≤ H 0 },</formula><p>where T is the threshold and I is the indicator function.</p><p>Disentangled Self-Attention Recently, the selfattention-based Transformer model has achieved state-of-the-art performance on various NLP tasks such as machine translation <ref type="bibr" target="#b16">(Vaswani et al., 2017)</ref>, and language understanding <ref type="bibr" target="#b2">(Devlin et al., 2018;</ref><ref type="bibr" target="#b12">Radford et al., 2018)</ref>. The success of the Transformer is partly attributed to the multi-view representation using multi-head attention architecture. Unlike the standard transformer which concatenates vectors from different heads into one vector, we propose to uses a switch to activate certain heads and only pass through their information to the next level (depicted in the right of <ref type="figure" target="#fig_3">Figure 5</ref>). Hence, we are able to disentangle the H attention heads to model H different semantic functionalities, and we refer to such module as the disentangled self-attention (DSA). Formally, we follow the canonical Transformer <ref type="bibr" target="#b16">(Vaswani et al., 2017)</ref> to define the Scaled Dot-Product Attention function given the input query/key/value features Q, K, V ∈ R n×D as:</p><formula xml:id="formula_6">Attention(Q, K, V ) = softmax( QK T √ D )V<label>(4)</label></formula><p>where n denotes the sequence length of the input, Q, K, V denotes query, key and value. Here, we use H different self attention functions with their independent parameterization to compute the multi-head representation G i as follows:</p><formula xml:id="formula_7">gi = Attention(QW Q i , KW K i , V W V i ) Gi = fP F F (fLM (fMLP (fAT T (gi, u1:m)))<label>(5)</label></formula><p>where the input matrices Q, K, V are computed from the input token embedding x 1:n ∈ R n×D , and D denotes the dimension of the embedding. The i th head adopts its own parameters W Q i ,</p><formula xml:id="formula_8">W K i , W V i ∈ R D× D H to compute the output g i ∈ R n× D H .</formula><p>We shrink the dimension at each head to D/H to reduce the computation cost as suggested in <ref type="bibr" target="#b16">Vaswani et al. (2017)</ref>. We first use the cross-attention network f AT T to incorporate the encoded dialog history u 1:m , and then we apply a position-wise feed forward neural network f P F F , a layer normalization f LM , and a linear projection layer f M LP to obtain G i ∈ R n×D . These layers are shared across different heads. The main innovation of our architecture lies in disentangling the heads. That is, instead of concatenating G i to obtain the layer output like the standard Transformer, we employ a binary switch vector s = (α 1 , . . . , α H ) ∈ {0, 1} H to control H different heads and aggregate them as a n × D output matrix G = H i=1 α i G i . Specifically, the j-th row of G, denoted as C j ∈ R D , can be understood as the output corresponding to the j-th input token y j in the response. This approach is similar to a gating function to selectively pass desired information. By manipulating the attention-head switch s, we can better control the information flow inside the self-attention module. We illustrate the gated summation over multi-heads in <ref type="figure" target="#fig_4">Figure 6</ref>. Hierarchical DSA When the dialog system involves more complex ontology, the semantic space can grow rapidly. In consequence, a single-layer disentangled self-attention with a large number of heads is difficult to handle the complexity. Therefore, we further propose to stack multiple DSA layers to better model the huge semantic space with strong compositionality. As depicted in <ref type="figure">Figure 3</ref>, the lower layers are responsible for grasping coarse-level semantics and the upper layers are responsible for capturing fine-level semantics. Such progressive generation bears a strong similarity with human brains in constructing precise responses. In each DSA layer, we feed the utterance encoding u 1:m and last layer output C 1:n as the input to obtain the newer output matrix G. We collect the output O 1:n = C 1:n from the last DSA layer to compute the joint probability over a observed sequence y 1:n , which can be decomposed as a series of product over the probabilities: 6 P β (y1:n|u1:m, s1:L) = where W v ∈ R D×V and b v ∈ R V are the projection weight and bias onto a vocabulary of size V , l ∈ {1, · · · , n} is the index, sof tmax denotes the softmax operation, s 1:L denotes the set of the attention switches s 1 , · · · , s L over the L layers, and β denotes all the decoder parameters.</p><p>Recall that the graph structure of dialog acts is explicitly encoded into HDSA as a prior, where each head in HDSA is set to model a designated semantic node on the graph. In consequence, the hierarchical representation A can be used to control the head switch s 1:L . At training time, the model parameters β are optimized from the training data triple (y 1:n , u 1:m , A) to maximize the likelihood of ground truth acts and responses given the dialog history. Formally, we propose to maximize the following objective function as follows:</p><formula xml:id="formula_9">L(β) = log P β (y1:n|u1:m, s1:L = A)</formula><p>At test time, we propose to use the predicted dialog actÂ to control the language generation. The errors can be seen as coming from two sources, one is from inaccurate dialog act prediction, the other is from imperfect response generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Dataset To evaluate our proposed methods, we use the recently proposed MultiWOZ dataset <ref type="bibr" target="#b0">(Budzianowski et al., 2018)</ref> as the benchmark, which was specifically designed to cover the challenging multi-domain and large-scale dialog managements (see the summary in <ref type="table" target="#tab_2">Table 1</ref>). This new benchmark involves a much larger dialog action space due to the inclusion of multiple domains and complex database backend. We represent the 625 potential dialog acts into a three-layered hierarchical graph that with a total 44 nodes (see Appendix for the complete graph). We follow <ref type="bibr" target="#b0">Budzianowski et al. (2018)</ref>    Dialog Act Prediction We first train dialog act predictors using different neural networks to compare their performances. The experimental results (measured in F1 scores) are reported in <ref type="table" target="#tab_3">Table 2</ref>.</p><p>Experimental results show that fine-tuning the pretrained BERT <ref type="bibr" target="#b2">(Devlin et al., 2018)</ref> can lead to significantly better performance than the other models. Therefore, we will use it as the dialog act prediction model in the following experiments. Instead of jointly training the predictor and the response generator, we simply fix the trained predictor when learning the generator P β (y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Automatic Evaluation</head><p>We follow <ref type="bibr" target="#b0">Budzianowski et al. (2018)</ref> to use delexicalized-BLEU <ref type="bibr" target="#b11">(Papineni et al., 2002)</ref>, inform rate and request success as three basic metrics to compare the delexicalized generation against the delexicalized reference. We further propose Entity F1 <ref type="bibr" target="#b13">(Rojas-Barahona et al., 2017)</ref> to evaluate the entity coverage accuracy (including all slot values, days, numbers, and reference, etc), and restore-BLEU to compare the restored generation against the raw reference. The evaluation metrics are detailed in the supplementary material. Before diving into the experiments, we first list all the models we experiment with as follows:</p><p>1. Without Dialog Act, we use the official code 7 :</p><p>(i) LSTM <ref type="bibr" target="#b0">(Budzianowski et al., 2018)</ref>: it uses history as the attention context and applies belief state and KB results as side inputs. (ii) Transformer <ref type="bibr" target="#b16">(Vaswani et al., 2017)</ref>: it uses stacked Transformer architecture with dialog history as source attention context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>With Sparse Tree Dialog Act, we feed the treebased representation as an external vectors into different architectures. (i) SC-LSTM <ref type="bibr" target="#b19">(Wen et al., 2015)</ref>: it feeds the sparse dialog act to the semantic gates to control the generation process. (ii) Transformer-in: it appends the sparse dialog act vector to input word embedding (iii) Transformer-out: it appends the sparse dialog act vector to the last layer output, before the softmax function.</p><p>3. With Compact Graph Dialog Act (Predicted), we use the proposed graph representation for dialog acts and use it to control the natural language generation. (i) Transformer-in/out: it uses the flattened graph representation and feeds it as an external embedding feature.</p><p>(ii) Straight DSA: it uses the flattened graph representation and model it with a one-layer DSA followed with two layers of self-attention.</p><p>(iii) 2-layer HDSA: it adopts the partial action/slot levels of hierarchical graph representation, used as an ablation study. (iv) 3-layer HDSA: it adopts the full 3-layered hierarchical graph representation, used for the main model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">With</head><p>Graph Dialog Act (Groundtruth): it uses the ground truth dialog acts as input to see the performance upper bound of the proposed response generator architecture.</p><p>In order to make these models comparable, we design different hidden dimensions to make their total parameter size comparable. We demonstrate  the performance of different models in <ref type="table" target="#tab_5">Table 3</ref>, and briefly conclude with the following points: (i) by feeding the sparse tree representation to input/output layer (Transformer-in/out), the model is not able to capture the large semantics space of dialog acts with sparse training instances, which unsurprisingly leads to restricted performance gain against without dialog act input. (ii) the graph dialog act is essential in reducing the sample complexity, the replacement can lead to significant and consistent improvements across different models. (iii) the hierarchical graph structure prior is an efficient inductive bias; the structureaware HDSA can better model the compositional semantic space of dialog acts to yield a decent gain over Transformer-in/out with flattened input vector. (vi) our approaches yield significant gain (10+%) on the Inform/Request success rate, which reflects that the explicit structured representation of dialog act is very effective in guiding dialog response in accomplishing the desired tasks. (v) the generator is greatly hindered by the predictor accuracy, by feeding the ground truth acts, the proposed HDSA is able to achieve an additional gain of 7.0 in BLEU and 21% in Entity F1.</p><p>Generalization Ability To better understand the performance gain of the hierarchical graph-based representation, we design synthetic tests to examine its generalization ability. Specifically, we divide the dialog acts into five categories based on their frequency of appearance in the training data: very few shot (1-100 times), few shot (100-500 times), medium shot (500-2K times), many shot (2K-5K times), and very many shot (5K+ times). We compute the average BLEU score of the turns within each frequency category and plot the result in <ref type="figure">Figure 7</ref>. First, by comparing Transformer-in with compact Graph-Act against Transformer-in with sparse Tree-Act, we observe that for few number shots, the graph act significantly boosts the performance, which reflects our conjecture to lower sample complexity and generalize better to unseen (or less frequent) cases. Furthermore, by comparing Graph-Act Transformerin with HDSA, we observe that HDSA ahieves better results by exploiting the hierarchical structure in dialog act space.  <ref type="figure">Figure 7</ref>: The BLEU scores for dialog acts with different number of shots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Human Evaluation</head><p>Response Quality Owing to the low consistency between automatic metrics and human perception on conversational tasks, we also recruit trustful judges from Amazon Mechanical Turk  (AMT) (with prior approval rate &gt;95%) 8 to perform human comparison between the generated responses from HDSA and SC-LSTM. Three criteria are adopted: (i) relevance: the response correctly answers the recent user query. (ii) coherence: the response is coherent with the dialog history. (iii) consistency: the generated sentence is semantically aligned with ground truth. During the evaluation, each AMT worker is presented two responses separately generated from HDSA and SC-LSTM, as well the ground truth dialog history. Each HIT assignment has 5 comparison problems, and we have a total of 200 HIT assignments to distribute. In the end, we perform statistical analysis on the harvested results after rejecting the failure cases and display the statistics in <ref type="table" target="#tab_8">Table 4</ref>. From the results, we can observe that our model significantly outperforms SC-LSTM in the coherence, i.e., our model can better control the generation to maintain its coherence with the dialog history.</p><p>Semantic Controllability In order to quantitatively compare the controllability of HDSA, Graph-Act Tranformer-in, and SC-LSTM, we further design a synthetic NLG experiment, where we randomly pick 50 dialog history as the context from test set, and then randomly select 3 dialog acts and their combinations as the semantic condition to control the model's responses generation. We demonstrate an example in the supplementary to visualize the evaluation procedure. Quantitatively, we hire human workers to rate (measured in match, partially match, and totally mismatch) whether the model follows the given semantic condition to generate coherent sentences. The experimental results are reported in the bottom half of <ref type="table" target="#tab_8">Table 4</ref>, which demonstrate that both the com-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>Graph Representation as Transfer Learning The proposed graph representation works well under the cases where the set of domain slotvalue pairs have significant overlaps, like Restaurant, Hotel, where the knowledge is easy to transfer. Under occasions where such exact overlap is scarce, we propose to use group similar concepts together as hypernym and use one switch to control the hypernym, which can generalize the proposed method to the broader domain.</p><p>Compression vs. Expressiveness A trade-off that we found in our structure-based encoding scheme is that: when multiple dialog acts are involved with overlaps in the action layer, ambiguity will happen under the graph representation. For example, the two dialog acts "restaurant-informprice" and "hotel-inform-location" are merged as "[restaurant, hotel] → [inform] → [price, location]", the current compressed representation is unable to distinguish them with "hotel-informprice" or "restaurant-inform-location". Though these unnatural cases are very rare in the given dataset without hurting the performance per se, we plan to address such pending expressiveness problem in the future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>In this paper, we propose a new semanticallycontrolled neural generation framework to resolve the scalability and generalization problem of existing models. Currently, our proposed method only considers the supervised setting where we have annotated dialog acts, and we have not investigated the situation where such annotation is not available. In the future, we intend to infer the dialog acts from the annotated responses and use such noisy data to guide the response generation.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An example dialog from MultiWOZ dataset, where the upper rectangle includes the dialog history, the tables at the bottom represent the external database, and the lower rectangle contains the dialog action and the language surface form that we need to predict.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The left part is the graph representation of the dialog acts, where each path in the graph denotes a unique dialog act. The right part denotes our proposed HDSA, where the orange nodes are activated while the others are blocked. (For details, refer toFigure 5)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>The left figure describes the tree representation of the dialog acts, and the right figure denotes the obtained graph representation from the left after merging the cross-branch nodes that have the same semantics. The Hierarchical form is used in our main model HDSA, Falttented is used for baseline models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>The left figure describes the dialog act predictor and HDSA, and the right figure describes the details of DSA. The predicted hierarchical dialog acts are used to control the switch in HDSA at each layer. Here we use L = 3 layers, the head numbers at each layer are H = (4, 3, 6) heads, the hierarchical graph representation A=[[0, 1, 0, 0], [0, 1, 0], [0, 0, 1, 1, 0, 0]]. We use m to denote the dialog history length and n for response. et al. (2018); Rojas-Barahona et al. (2017) to use one-hot vector v kb and v bf for representing the DB records and belief state (see the original papers for details)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>The disentangled multi-head attention, with a sequence length of 3, 3 different heads are used with hidden dimension 7. The switch only enables the information flow from the 1st and 3rd head.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>y l |y 0:l−1 , u1:m, s1:L) p β (y l |y 0:l−1 , u1:m, s1:L) = sof tmax(WvO l + bv)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Illustration of different evaluation metrics, in the delexicalized and non-delexicalized form.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Illustration of Human Evaluation Interface. Dialog Act History: I'm looking for a restaurant in the centre. inform-area ✔ There is a restaurant in the [restaurant.area] part of town. request-price ✔ What price range are you looking for? request-price ✖ inform-area I have a restaurant in the [restaurant.area], what food style are you looking for?Figure 11: Illustration of an example in controlling response generation given dialog act condition. Check mark means pass and cross mark means fail.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 12 :</head><label>12</label><figDesc>Illustration of entire dialog graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Summary of the MultiWOZ dataset.</figDesc><table><row><cell cols="4">to select 1000 dialogs as the test set and 1000</cell></row><row><cell cols="4">dialogs as the development set. And we mainly</cell></row><row><cell cols="4">focus on the context-to-response problem, with</cell></row><row><cell cols="4">the dialog act prediction being a preliminary</cell></row><row><cell cols="4">task. The best HDSA uses three DSA layers with</cell></row><row><cell cols="4">10/7/27 heads to separately model the semantics</cell></row><row><cell cols="4">of domain, actions and slot (dummy head is</cell></row><row><cell cols="4">included to model "none" node). Adam (Kingma</cell></row><row><cell cols="4">and Ba, 2014) with a learning rate of 10 −3 is</cell></row><row><cell cols="4">used to optimize the objective. A beam size of 2</cell></row><row><cell cols="4">is adopted to search the hypothesis space during</cell></row><row><cell cols="4">decoding with vocabulary size of 3,130. Also, by</cell></row><row><cell cols="4">small-scale search, we fix the threshold T = 0.4</cell></row><row><cell cols="2">due to better empirical results.</cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell cols="3">Precision Recall F1</cell></row><row><cell cols="2">Bi-directional LSTM 72.4</cell><cell>70.5</cell><cell>71.4</cell></row><row><cell>Word-CNN</cell><cell>72.8</cell><cell>70.3</cell><cell>71.5</cell></row><row><cell>3-layer Transformer</cell><cell>73.3</cell><cell>72.6</cell><cell>73.1</cell></row><row><cell>12-layer BERT</cell><cell>77.5</cell><cell>77.4</cell><cell>77.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Accuracy of Dialog Act Prediction</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Empirical Results on MultiWOZ Response Generation, we experiment with three forms of dialog act, namely none, one-hot and hierarchical.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Many Shot Sentence BLEU Tree-Act Transformer-in Graph-Act Transformer-in HDSA</head><label></label><figDesc></figDesc><table><row><cell>30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>25</cell><cell></cell><cell></cell><cell></cell><cell>24.6</cell><cell>25.2</cell><cell>25.5</cell><cell>25.1</cell><cell>25.4</cell><cell>25.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell>20.9</cell><cell></cell><cell></cell><cell></cell></row><row><cell>20</cell><cell></cell><cell></cell><cell>19.1</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>16.8</cell><cell>17.3</cell><cell></cell><cell></cell><cell></cell></row><row><cell>15</cell><cell>14.4</cell><cell>14.1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>11.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>10</cell><cell></cell><cell>9.7</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>5.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Very Few Shot</cell><cell>Few Shot</cell><cell>Medium Shot</cell><cell cols="3">Many Shot</cell><cell>Very</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Experimental results of two human evaluations for HDSA vs. SC-LSTM vs. Transformer-in. The top table gives the response quality evaluation and the bottom table demonstrates the controllability evaluation results in section 5.2.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">we add dummy node "none" to transform those non-leaf acts into leaf act to normalize all acts into triplet; for example "hotel-inform" is converted into "hotel-inform-none" 3 https://github.com/andy194673/ nlg-sclstm-multiwoz/blob/master/ resource/woz3/template.txt</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We call it graph because now one child node can have multiple parents, which violates the tree's definition.5  For example, two acts, H(a)1 = [<ref type="bibr" target="#b25">[1,</ref> 0, 0],<ref type="bibr" target="#b25">[1,</ref> 0]] and H(a)2 = [[1, 0, 0], [0, 1]], are aggregated into A = [<ref type="bibr" target="#b25">[1,</ref> 0, 0],<ref type="bibr" target="#b25">[1,</ref><ref type="bibr" target="#b25">1]</ref>].</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">We follow the standard approach in Transformer to use a mask to make O l depend only on y 0:l−1 during training. And during test time, we decode sequentially from left-to-right.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">https://github.com/budzianowski/ multiwoz</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">https://www.mturk.com/ pact dialog act representation and the hierarchical structure prior are essential for controllability.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Acknowledgements</head><p>We really appreciate the efforts of the anonymous reviews and cherish their valuable comments, they have helped us improve the paper a lot. We are gratefully supported by a Tencent AI Lab Rhino-Bird Gift Fund. We are also very thankful for the public available dialog dataset released by University of Cambridge and PolyAI.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Details of Model Implementation</head><p>Here we detailedly explain the model implementation of the baselines and our proposed HDSA model. In the encoder side, we use a three-layered transformer with input embedding size of 64 and 4 heads, the dimension of query/value/key are all set to 16, in the output layer, the results of 4 heads are concatenated to obtain a 64-dimensional vector, which is the first broadcast into 256-dimension and then back-projected to 64-dimension. By stacking three layers of such architecture, we obtain at the end the series of 64-dimensional vectors. Following BERT, we use the first symbol as the sentence-wise representation u, and compute its matching score against all the tree node to predict the representation of dialog actsÂ. In the decoder, we adopt take as input any length features x 1 , · · · , x n , each with dimension of 64, in the first layer, since we have 10 heads, the dimension for each head is 6, thus the key, query feature dimensions are fixed to 6, the second layer with dimension of 9, the third with dimension of 2. The value feature is all fixed to 16, which is equivalent to the encoder side. After self-attention, the position-wise feed-forward neural network projects each feature back to 64 dimensions, which is further projected to 3.1K vocabulary dimension to model word probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Automatic Evaluation</head><p>We simply demonstrate an example of our automatic evaluation metrics in <ref type="figure">Figure 9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Baseline Implementation</head><p>Here we visualize how we feed the dialog act input in as an embedding into the transformer to control the sequence generation process as <ref type="figure">Figure 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Human Evaluation Interface</head><p>To better understand the human evaluation procedure, we demonstrate the user interface in <ref type="figure">Figure</ref> 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Controllability Evaluation</head><p>To better understand the results, we depict an example in <ref type="figure">Figure 11</ref>, where 3 different dialog acts are picked as the semantic condition to constrain the response generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Enumeration of all the Dialog Acts</head><p>Here we first enumerate the node semantics of the graph representation as follows:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multiwoz -A largescale multi-domain wizard-of-oz dataset for taskoriented dialogue modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pawel</forename><surname>Budzianowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo-Hsiang</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iñigo</forename><surname>Casanueva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Osman Ramadan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gasic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-10-31" />
			<biblScope unit="page" from="5016" to="5026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Xlnbt: A cross-lingual neural belief tracking framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="414" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A contextaware natural language generator for dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Dusek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Jurcícek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<meeting><address><addrLine>Los Angeles, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-09" />
			<biblScope unit="page" from="185" to="190" />
		</imprint>
	</monogr>
	<note>Proceedings of the SIGDIAL 2016 Conference</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Key-value retrieval networks for task-oriented dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihail</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lakshmi</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Charette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue</title>
		<meeting>the 18th Annual SIGdial Meeting on Discourse and Dialogue<address><addrLine>Saarbrücken, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-15" />
			<biblScope unit="page" from="37" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Slot-gated modeling for joint slot filling and intent prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Chih-Wen Goo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Kai</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Li</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Chieh</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keng-Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT, New Orleans</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT, New Orleans<address><addrLine>Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-01" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="753" to="757" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sequicity: Simplifying task-oriented dialogue systems with single sequence-to-sequence architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqiang</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xisen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018-07-15" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1437" to="1447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">End-to-end taskcompletion neural dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli Ç</forename><surname>Elikyilmaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017-11-27" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="733" to="743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural belief tracker: Data-driven dialogue state tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diarmuidó</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><forename type="middle">J</forename><surname>Young</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1163</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017-07-30" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1777" to="1788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semantic specialization of distributional word vector spaces using monolingual and cross-lingual constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vulic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diarmuidó</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Leviant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><forename type="middle">J</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="309" to="324" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/languageunsupervised/language" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A network-based end-to-end trainable task-oriented dialogue system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><forename type="middle">Maria</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><forename type="middle">J</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vandyke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter<address><addrLine>Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017-04-03" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="438" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep LSTM based feature mapping for query classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>San Diego California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-12" />
			<biblScope unit="page" from="1501" to="1511" />
		</imprint>
	</monogr>
	<note>NAACL HLT 2016</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Linguistically-informed self-attention for semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-10-31" />
			<biblScope unit="page" from="5027" to="5038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Conditional generation and snapshot learning in neural dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><forename type="middle">Maria</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><forename type="middle">J</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11-01" />
			<biblScope unit="page" from="2153" to="2162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-domain neural network language generation for spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><forename type="middle">Maria</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><forename type="middle">J</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>San Diego California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-12" />
			<biblScope unit="page" from="120" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semantically conditioned lstm-based natural language generation for spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><forename type="middle">J</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-17" />
			<biblScope unit="page" from="1711" to="1721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Latent intention dialogue models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishu</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><forename type="middle">J</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-11" />
			<biblScope unit="page" from="3732" to="3741" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The dialog state tracking challenge series: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Raux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">D&amp;D</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="4" to="33" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The dialog state tracking challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Raux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 14th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<meeting><address><addrLine>Metz, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="page" from="404" to="413" />
		</imprint>
	</monogr>
	<note>Proceedings of the SIGDIAL 2013 Conference</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pomdp-based statistical spoken dialog systems: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gašić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1160" to="1179" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Global-locally self-attentive dialogue state tracker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Domain-Layer 10 choices: &apos;restaurant&apos;, &apos;hotel&apos;, &apos;attraction</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Action-Layer 7 choices: &apos;inform&apos;, &apos;request&apos;, &apos;recommend&apos;, &apos;book&apos;, &apos;select&apos;, &apos;sorry</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<title level="m">Slot-Layer 27 choices: &apos;pricerange</title>
		<imprint/>
	</monogr>
	<note>internet&apos;, &apos;day&apos;, &apos;arriveby&apos;, &apos;departure&apos;, &apos;destination&apos;, &apos;leaveat&apos;, &apos;duration&apos;, &apos;trainid&apos;, &apos;people&apos;, &apos;department&apos;, &apos;stay&apos;</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<idno>Count:1} Delexicalized BLEU: 12.3</idno>
		<title level="m">Then we enumerate the entire graph as follows: Entity F1: 57.1% Prediction: {Res.Name:1, Res.Price:1, Hotel.Name:1} Reference: {Res.Name:1, Res.Price:1, Res.Stars:1</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Groundtruth: Among the &lt;Count&gt; candidates, &lt;Res.Name&gt; is good with both &lt;Res.Price&gt; price and &lt;Res.Stars&gt; review. DB: {Res.Name: Little Seoul</title>
		<idno>15$/person} Restored BLEU: 11.5</idno>
	</analytic>
	<monogr>
		<title level="m">Prediction: I would recommend &lt;Res.Name&gt; with &lt;Res.Price&gt; price in the &lt;Res.Location&gt; near &lt;Hotel.Name&gt;</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Prediction: I would recommend Little Seoul, which has a low price in the south near &lt;Hotel.Name&gt;. Groundtruth: Among the 4 candidates , Little Seoul is a good with both low price and 4-star review</title>
		<imprint/>
	</monogr>
	<note type="report_type">Post-Processing</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
