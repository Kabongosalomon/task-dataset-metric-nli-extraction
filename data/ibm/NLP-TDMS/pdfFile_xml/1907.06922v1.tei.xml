<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Human Pose Estimation for Real-World Crowded Scenarios</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Golda</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Vision and Fusion Lab</orgName>
								<orgName type="institution">Karlsruhe Institute of Technology KIT c/o Technologiefabrik</orgName>
								<address>
									<addrLine>Haid-und-Neu-Strasse 7</addrLine>
									<postCode>76131</postCode>
									<settlement>Karlsruhe</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Fraunhofer Institute for Optronics</orgName>
								<orgName type="department" key="dep2">System Technologies and Image Exploitation IOSB Fraunhoferstrasse</orgName>
							</affiliation>
							<affiliation key="aff2">
								<address>
									<postCode>76131</postCode>
									<settlement>Karlsruhe</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Kalb</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Fraunhofer Institute for Optronics</orgName>
								<orgName type="department" key="dep2">System Technologies and Image Exploitation IOSB Fraunhoferstrasse</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arne</forename><surname>Schumann</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Fraunhofer Institute for Optronics</orgName>
								<orgName type="department" key="dep2">System Technologies and Image Exploitation IOSB Fraunhoferstrasse</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">JÃ¼rgen</forename><surname>Beyerer</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Vision and Fusion Lab</orgName>
								<orgName type="institution">Karlsruhe Institute of Technology KIT c/o Technologiefabrik</orgName>
								<address>
									<addrLine>Haid-und-Neu-Strasse 7</addrLine>
									<postCode>76131</postCode>
									<settlement>Karlsruhe</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Fraunhofer Institute for Optronics</orgName>
								<orgName type="department" key="dep2">System Technologies and Image Exploitation IOSB Fraunhoferstrasse</orgName>
							</affiliation>
							<affiliation key="aff2">
								<address>
									<postCode>76131</postCode>
									<settlement>Karlsruhe</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Human Pose Estimation for Real-World Crowded Scenarios</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human pose estimation has recently made significant progress with the adoption of deep convolutional neural networks and many applications have attracted tremendous interest in recent years. However, many of these applications require pose estimation for human crowds, which still is a rarely addressed problem. For this purpose this work explores methods to optimize pose estimation for human crowds, focusing on challenges introduced with larger scale crowds like people in close proximity to each other, mutual occlusions, and partial visibility of people due to the environment. In order to address these challenges, multiple approaches are evaluated including: the explicit detection of occluded body parts, a data augmentation method to generate occlusions and the use of the synthetic generated dataset JTA <ref type="bibr" target="#b2">[3]</ref>. In order to overcome the transfer gap of JTA originating from a low pose variety and less dense crowds, an extension dataset is created to ease the use for real-world applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Articulated human pose estimation has been a commonly addressed problem in computer vision for many years. It has attracted broad interest for its many applications such as video surveillance, action recognition, human computer interaction and motion capturing. Recent advances in human pose estimation have been achieved by harnessing the power of deep convolutional neural networks (CNNs) and large-scale pose estimation datasets like COCO <ref type="bibr" target="#b6">[7]</ref> and MPII <ref type="bibr" target="#b0">[1]</ref>. Up until now most approaches focus on pose estimation for images with few people in uncrowded scenarios. However, in many applications it is inevitable to han-* Both authors contributed equally to this work. <ref type="figure">Figure 1</ref>: Example of a scene captured by a surveillance camera. Such situation is characterized by heterogeneous levels of crowdedness and lots of occlusions and ambiguities at more crowded spots. The drawn poses show the difficulty of the task. dle larger groups or crowds of people which also introduces many new challenges, see <ref type="figure">Fig. 1</ref>. Such challenges include partially occluded people, mutual occlusions by humans and low resolution of persons due to sensor limitations or distance to the camera. One prominent application is video based surveillance of public places, where pose estimation can be utilized to analyze crowd behavior. To be more clear at this point, our notion of crowded situations is different compared to the field of e.g. crowd counting. Whilst in the latter case most of the time only the area around the head of single persons is visible, we regard highly crowded situations as cases in which occlusions between persons are not permanently present but still dominate the situation. Pose information encoded as body joint keypoints is a low dimensional description and hence well suited for real-time analysis. This could enable to detect and prevent critical or dangerous situations by alarming security forces, if nec-essary. However, to the best of our knowledge there exists very little work on human pose estimation for crowded surveillance scenarios.</p><p>Thus, in this work we evaluate several aspects of stateof-the-art pose detection methods with respect to the characteristics of crowded surveillance imagery. Specifically, we focus on three distinct aspects in the pipeline of modern pose detection methods. We first evaluate the impact of a recently proposed occlusion data augmentation method on highly crowded data. We then propose our own architecture modification for explicitly modeling occluded keypoints within a CNN pose detector. And finally, we generate a new simulated dataset, which is specifically suited to evaluate recent pose detection methods under crowded surveillance scenario conditions. Our resulting surveillance pose recognition model, as well as the generated dataset will be made available to the community to spark further research. This work is divided as follows. Starting with this introduction, this work then presents related literature for human pose estimation as well as the few existing works on human pose estimation for crowds. The subsequent section concentrates on the methods chosen for our experiments, wWihich includes data augmentation methods, the adaption of an enhanced architecture for detection of occluded keypoints and the use of synthetic training data. Finally, the described methods are evaluated separately and in a combined model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Top-Down and Bottom-Up Methods</head><p>Existing methods for human pose estimation can be divided into two opposing approach classes: bottom-up and top-down methods. Methods of the former class attempt to first locate all person keypoints (e.g. joints, eyes, ears) over the entire input image. These keypoints are then grouped to single persons, which is done by solving an assignment problem. In contrast, top-down methods start from person detections and identify keypoints within the detection bounding boxes, following the assumption that each bounding box contains at most one keypoint per keypoint class. Each of them comes with different advantages and disadvantages. Although bottom-up methods seem to be better suited for crowded scenes since they process whole images and their runtime is thus less dependent of the actual number of persons, literature shows that top-down methods like <ref type="bibr" target="#b5">[6]</ref> and <ref type="bibr" target="#b10">[11]</ref> perform comparably well on publicly available datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Simple Baselines for Human Pose Estimation</head><p>The model introduced in <ref type="bibr" target="#b10">[11]</ref> is the recent winner of the PoseTrack ECCV 2018 1 challenge in the category for multi-person pose tracking. It provides a simplistic baseline model based on a top-down pose estimation network and a novel method for tracking detected poses across multiple frames. The main difference to other state-of-the-art methods like CPN <ref type="bibr" target="#b1">[2]</ref> and the hourglass network <ref type="bibr" target="#b7">[8]</ref> lies in the method for generating high resolution feature maps using transposed convolutions. The method proposed by <ref type="bibr" target="#b10">[11]</ref> refines the common top-down pipeline by additionally using a second set of bounding boxes, which is determined by optical flow from previous frames. Therefore, the keypoints in the current frame are propagated to the next frame using the optical flow between the frames. With the help of the propagated keypoints, a bounding box proposal is generated. Non-maximum suppression is then used to select the best set of bounding boxes. Furthermore, the authors apply a flow-based pose similarity measure S flow to the greedy matching algorithm, instead of using Intersection-over-Union (IoU).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Further Work on Crowd Pose Estimation</head><p>As previously stated, human pose estimation in crowded scenarios is a rarely addressed problem. However, some recent works focus on this subject. Li et al. <ref type="bibr" target="#b5">[6]</ref> introduce a new benchmark for evaluating pose estimation methods for this problem and proposed a method where human bounding box proposals obtained by human detector are fed into joint-candidate single person pose estimator (JCSPPE). JC-SPPE locates the joint candidates with different response scores on the heatmap. Then the joint association algorithm takes these results and builds a person-joint connection graph. Finally, a graph matching problem is solved to find the best joint association result with a global maximum joints association algorithm. With their method the authors show robustness to interference in crowded scenes. Yet, their definition of "crowdedness" differs slightly from common situations in surveillance applications. Since the collected data originates from the COCO dataset, it has a relatively low person per image count. Furthermore, the perspective the images were taken from often differs strongly from typical views of surveillance cameras, which in most cases are mounted in higher spots leading to a steeper viewing angle. Furthermore, the work of Fabbri et al. <ref type="bibr" target="#b2">[3]</ref> is also of great interest for this kind of problem. Using Grand Theft Auto V, a widely known computer game with an active modding community, they created a synthetic dataset of humans annotated with highly accurate keypoint information along-side with a tool that allows to easily generate own synthetic datasets. Both, <ref type="bibr" target="#b2">[3]</ref> and <ref type="bibr" target="#b5">[6]</ref> proposed interesting datasets, which we used for our experiments, whereas <ref type="bibr" target="#b2">[3]</ref> is more similar to surveillance situations than the dataset introduced by <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>This section explores methods to optimize pose estimation for crowd applications. For this purpose we adapt the single-person pose estimator proposed by Xiao et al. <ref type="bibr" target="#b10">[11]</ref> using the ResNet50 <ref type="bibr" target="#b4">[5]</ref> network as backbone. The method is a two-staged top-down approach that first localizes every person using state-of-the-art object detection methods and then performs single-person pose estimation for every detection. This choice is motivated by the simplicity of the model and its performance compared to other state-of-theart approaches.</p><p>For adaptation to crowded scenes, data augmentation methods are first discussed which improve the performance of the baseline model on both uncrowded and crowded scenarios. Next, the baseline architecture is extended in order to explicitly detect occluded keypoints with the help of the occluded keypoint annotation provided with the JTA dataset. Finally, the earlier mentioned JTA dataset is examined for the challenges of real world crowd applications and extended by a small dataset which was created specifically to alleviate some of these problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Synthetic Generated Occlusions</head><p>As SÃ¡rÃ¡ndi et al. <ref type="bibr" target="#b9">[10]</ref> have shown, synthetic generated occlusions are an effective augmentation method to reduce occlusion-induced errors in pose estimation, while also improving results on data without any occlusions. Since the amount of real-world data that is currently available for pose estimation of human crowds is very limited, the question arises whether these augmentation methods are also helpful in the task of pose estimation for human crowds. In order to answer this question, two different kinds of synthetic occlusions are examined: occlusions by random objects and occlusions by person instances. For both augmentation methods the instance segmentation labels provided with the COCO dataset are used to get a high variety of objects and persons. Similar to the approach of SÃ¡rÃ¡ndi et al. <ref type="bibr" target="#b9">[10]</ref> randomly picked objects from the COCO dataset are cut out from their original images using the provided segmentation maps. These cutouts are then added onto the bounding boxes of people in the training image. The size and the position of the cutout is chosen at random, where the size can range from 8% to 70% of the size of the person's bounding box. Furthermore, the occlusion flag of a keypoint is adjusted accordingly, if it is occluded by the cutout.</p><p>While the segmentation labels for person instances are also available, they need to be handled separately, because they introduce different challenges than the occlusion by objects. For example, if a person is added exactly on top of another person, the labeled keypoints are likely to be occluded by the added person's body which can lead to faulty training data. To counterbalance this issue, two different methods are evaluated to add additional persons to the detections. The first one is instead of adding full person instances to the image only body parts are added onto the scene. Secondly, placement of cutouts is restricted to the outer bounds of the target persons bounding box to avoid confusing the top-down pose recognizer with two fully valid person images inside one bounding box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Occlusion Detection Networks</head><p>In order to utilize the additional information provided by the occlusion flags in the JTA dataset, the baseline architecture is extended by integrating a branch exclusively for the detection of occluded keypoints. The idea behind that is that the detection of occluded and visible keypoints are connected but different tasks. Furthermore, this should enable the network to learn a better representation of the human body structure that is more robust at handling occlusions and provide explicit additional occlusion information for possible subsequent methods in an image analysis pipeline. Two architectures are proposed with an additional branch to detect the occluded keypoints, see <ref type="figure" target="#fig_1">Fig. 3</ref>. The first proposed architecture, OccNet (Occlusion Net), splits after two transposed convolutions so a joint representation can be learned in the previous layers. The second architecture OccNetCB (Occlusion Net Cross Branch) splits after only one transposed convolution, however the output from both layers are shared to one another. So that both branches can utilize information extracted by the other one. JTA also provides the distinction between self-occluded and occluded keypoints, during training self-occluded keypoints are treated in the same way as visible keypoints. The Occlusion Detection Networks output two sets of heatmaps per pose, one for visible keypoints and the other one for occluded keypoints. The mean squared error loss of the baseline model of Xiao et al. <ref type="bibr" target="#b10">[11]</ref> is adjusted by adding a second set of heatmaps for occluded keypoints and a weighting factor Î± to account for the lesser share of occluded keypoints in the training data. The proposed loss can be defined as:</p><formula xml:id="formula_0">L o = 1 n n i=1 P vis i â G vis i 2 + Î± P occ i â G occ i 2 (1)</formula><p>where G i and P i denote the respective ground-truth and predicted heatmap of the corresponding keypoint i. Due to its definition, the loss explicitly penalizes keypoints predicted by the wrong branch, which is achieved by setting the ground truth heatmaps for visible keypoints to zero in the occluded branch and vice versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">JTA-Ext</head><p>The synthetic dataset JTA introduced by Fabbri et al. <ref type="bibr" target="#b2">[3]</ref> is currently the largest publicly available pose tracking dataset with a larger number of people within an image. However, the adaption of the dataset for real-world crowd applications introduces some challenges, mainly: the visual difference of synthetic images of a video game, limited pose variety and less crowded scenes in comparison to CrowdPose. Since JTA focuses primarily on urban settings, the images almost exclusively show people walking, which leads to far less variety of poses. In order to quantify this observation, <ref type="figure">Fig. 5</ref> shows 2d histograms of the location of specific keypoint types. It is evident that the distribution of the JTA keypoints are much more concentrated in comparison to the approximately uniformly distributed Crowd-Pose dataset, which leads to the problem a model purely trained on JTA has a strong bias of walking and standing people. We addressed this issue by extending the existing dataset and including all activities provided with the JTA-Mod like sitting, doing yoga, doing push-ups, cheering and fighting. In this work, the proposed extended JTA version is referred to as JTA-Ext 2 . Another goal in creating an extension for the JTA dataset was to increase the share of highly crowded scenarios. In order to gauge the "crowdedness" of the datasets we utilized the CrowdIndex proposed by Li et al. <ref type="bibr" target="#b5">[6]</ref>. The different CrowdIndex distributions are presented in <ref type="figure" target="#fig_0">Fig. 2</ref>. While <ref type="bibr" target="#b5">[6]</ref> sampled the dataset to achieve a uniform CrowdIndex distribution, JTA has a bias on less crowded images. The proposed extension is aimed to increases the amount of strongly crowded images in the dataset. The final extended JTA dataset consists of 58 additional video sequences with a total of 812,742 poses distributed over 46,350 frames. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We conduct experiments on two different pose estimation datasets: the recently released real-world dataset CrowdPose <ref type="bibr" target="#b5">[6]</ref> and the synthetical dataset JTA <ref type="bibr" target="#b2">[3]</ref>. The CrowdPose dataset <ref type="bibr" target="#b5">[6]</ref> contains about 20,000 images and a total of 80,000 human poses with 14 labeled keypoints. In the following experiments the approaches will be trained on the training set of CrowdPose and will be evaluated on the test set of CrowdPose which includes 8,000 images. JTA is a synthetic dataset for pedestrian pose estimation and tracking which Fabbri et al.  <ref type="figure">Fig. 4a</ref> belongs to the class of hard cases and <ref type="figure">Fig. 4b</ref> to the medium cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Metrics</head><p>Following the procedure introduced in <ref type="bibr" target="#b5">[6]</ref>, the adjusted Object Keypoints Similarity (OKS) of CrowdPose is used. The OKS is used to describe the similarity of two poses and their keypoints by introducing a way to measure overlaps between keypoints in a manner as the IoU provides for bounding boxes. Afterwards, the results are reported on three different partitions of the dataset which cluster images based on their CrowdIndex C = min{ 1  <ref type="figure">Fig. 4</ref> shows two examples for crowded images. Although the metric describes crowded images from a shallow viewing angle in an appropriate way, it does not really capture the crowdedness of images taken from a higher viewing point. Due to this property, persons close to each other show a lower overlap of bounding boxes compared to image material recorded with lower located cameras. Consequently, this results in a CrowdIndex of 0.91 and 0.69 for <ref type="figure">Fig. 4a</ref> and <ref type="figure">Fig. 4b</ref> respectively, which suggests that the left image is more difficult.</p><formula xml:id="formula_1">N N i=1 N i a N i b , 1.0}.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Implementation</head><p>For the baseline model a TensorFlow re-implementation 3 of the model of Xiao et al. <ref type="bibr" target="#b10">[11]</ref> is used with a pre-trained ResNet50 backbone. For the input images the same resolution of 256Ã192 is used while the resolution of the keypoint heatmaps is 64 Ã 48 as proposed by <ref type="bibr" target="#b10">[11]</ref>. While groundtruth boxes are used during training of the models, the evaluation requires person detections for a fair comparison to 3 https://github.com/mks0601/TF-SimpleHumanPose other approaches. Therefore, the person detection results of the YOLOv3 <ref type="bibr" target="#b8">[9]</ref> model integrated in AlphaPose is used for all the following experiments 4 on CrowdPose. For our used loss function we set Î± to 1.5. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Synthetic Generation of Occlusions</head><p>For comparison between the different cutout methods and the effect these methods have in the crowded scenario the Baseline-50 model is trained on the CrowdPose training set and evaluated respective test set. During training the Adam Optimizer is utilized with a learning rate of 10 â3 which is dropped at epochs 90 and 120 by a factor of 10, similar to the training schedule proposed by Xiao et al. <ref type="bibr" target="#b10">[11]</ref>. All the models were trained for a total number of 140 epochs with a mini-batch size of 64. Additionally, to testing every single cutout method separately, combinations of objects and the different person cutout methods are evaluated. Therefore, two different setups are proposed. The first is, to add cutouts independently that means it is possible, that both cutout methods are applied simultaneously. The second one is, that the cutout methods are selected randomly with a probability of 50% without both occurring at the same time. The quantitative results in <ref type="table" target="#tab_0">Table 1</ref> show that all the proposed cutout methods improve the accuracy across all crowding levels, while the relative performance improvement increases as the scenes are more crowded. The object cutout method produces the best results across all crowding levels on the CrowdPose test set, but the increase in performance <ref type="figure">Figure 5</ref>: 2d histograms of selected keypoint types of CrowdPose and JTA. It is evident that the keypoint distribution of CrowdPose is more spread out than the distributions of JTA. However, the distributions of JTA are multivariate, because the persons in JTA face the camera just as often as they do not, whereas the persons in CrowdPose are primarily facing the camera. This is also represented in the average poses for the respective datasets.</p><p>in comparison to the body cutout methods stagnates as the images are more crowded. Another interesting observation is that the full body cutouts degrade the performance compared to adding only body parts. The reason behind that could be, that by avoiding the center of the image for the full body cutout, the degree of occlusion is lower than on the body part cutout, where the position is not limited. Applying two different cutout methods simultaneously leads to a decrease in accuracy. This can be attributed to the fact that the combination of two different cutouts within in image occlude too much of the person's body, prohibiting the network to see unobstructed people during training. Which would also explain that the accuracy increases again when only one of the two cutout method is applied per image. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Detection of Occluded Keypoints</head><p>In order to study the effects of explicitly detecting occluded keypoints in crowded pose estimation, the Baseline-50 is compared to the proposed OccNet and OccNetCB architectures. Since the occluded joint detection networks need reliable ground truth annotations for visibility flags, the JTA dataset is chosen for training. All three architectures are trained for 15 epochs with a mini-batch size of 64 on the entire JTA training set. Again a learning rate of 10 â3 is chosen which is dropped at epochs 8 and 10 by a factor of 10. For testing a subset of the JTA test sequences is used. Also it is important to note that during evaluation the ground-truth human detections are used to ensure that occluded body parts are included in the boxes. The results in <ref type="table" target="#tab_1">Table 2</ref> show that the distinction between occluded and visible keypoints does not improve the accuracy, because the proposed methods achieve similar accuracy on the JTA test set. Nevertheless, the qualitative results presented in <ref type="figure">Fig. 6</ref> show, that the proposed OccNet architectures learned to distinguish between visible and occluded <ref type="figure">Figure 6</ref>: Visualized results of the OccNetCB. Detected occluded keypoints are marked in red and visible keypoints in green. The darker shade of keypoints denotes uncertain keypoints (i.e. with a likelihood lower than 0.7). The two images on the left show persons from JTA and the images on the right show selected images from CrowdPose <ref type="bibr" target="#b5">[6]</ref>. keypoints, even on selected real-world images. That proves, that the distinction between occluded and visible keypoints is working, but simply does not provide a benefit to pose estimation in crowded scenarios. Therefore, the results also imply that the Baseline-50 model is sufficient to predict the location of occluded keypoints implicitly by inferring the location of visible keypoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Synthetic Training Data</head><p>Next, it is tested whether the use of synthetic data can provide comparable results on real-world datasets and if the extension of JTA can help to close the transfer-gap to a realworld dataset like CrowdPose. Therefore, the results of the Baseline-50 model trained on JTA, JTA-Ext and CrowdPose are compared to each other on the CrowdPose test set. The results are presented in <ref type="table" target="#tab_2">Table 3</ref>. While it is apparent that the extension of JTA helped to improve the results, the accuracy severely lacks behind a model trained on CrowdPose. However, <ref type="table" target="#tab_3">Table 4</ref> shows that if the models are finetuned on CrowdPose is the AP increases across all crowding levels compared to a model purely trained on CrowdPose. Under these circumstances synthetic data has proven again to be a valuable addition to existing real-world training data, but should not be used as sole foundation for real-world applications.  <ref type="figure">7</ref>: Qualitative results. The left column reports results generated by our Baseline-50 method, the right column of our final model. The latter detects more keypoints and delivers legit estimates for occluded ones (see first and second row). Furthermore, at smaller scale more poses can be detected (third row). The crops have heights of 254px, 239px, and 97px for the top, middle and last row respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Combined Experiments</head><p>Previous experiments explored approaches to improve the baseline architecture and evaluated them separately for the task of crowded pose estimation. The following experiment combines these approaches to compete with current approaches on the CrowdPose dataset. In contrast to previous experiments a ResNet101 backbone is used to provide a fair comparison to the approach of <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b5">[6]</ref>. Additionally, the model is pre-trained on JTA for 8 epochs with a learning rate of 10 â3 which is dropped at epochs 3 and 8 by a factor of 10. During training a mini-batch size of 80 was chosen and the object cutout method is utilized. For re-training on CrowdPose the same learning schedule and batch size as described in previous experiments was set.  in which the models are purely trained on the CrowdPose training set. As stated above the same human detector of <ref type="bibr" target="#b5">[6]</ref> is used to ensure a fair comparison between the different methods. Our proposed approach shows significant improvement compared to the adapted baseline approach of <ref type="bibr" target="#b10">[11]</ref> and also provides comparable results to Li et al. <ref type="bibr" target="#b5">[6]</ref> on the easy and medium crowding levels. However, on extremely crowded images <ref type="bibr" target="#b5">[6]</ref> surpasses the final model on CrowdPose by 4.3% AP. This can be attributed to their joint candidate matching method designed especially for handling these kinds of crowded scenarios. However, to our best knowledge there are no further methods reporting results on CrowdPose. A qualitative comparison of the improvements of our combined model in relation to the Baseline-50 model is depicted in <ref type="figure">Fig. 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we examined the impact of different extensions to the training of human pose estimators for crowded scenes. We show that significant improvements were accomplished by utilizing simple data augmentation techniques like adding COCO <ref type="bibr" target="#b6">[7]</ref> objects to the input images from crowded scenarios thus simulating occlusions. On the other hand, we observe that adding person instances or body parts is less effective and also requires additional attention to avoid creating erroneous poses in a top-down method. Our results further prove that JTA <ref type="bibr" target="#b2">[3]</ref> is a valuable addition to the training data for crowd-level pose estimation, especially since no large datasets are available for pose estimation in crowded scenarios. The created extension of JTA, which includes a higher variety of poses and denser crowds, further improves the accuracy on CrowdPose. However, solely relying on synthetic training data is not sufficient, due to the domain gap between synthetic and real-world data, originating from limited pose variety and visual limitations of a real-time rendering engine. Finally, the combination of the investigated approaches improved the baseline model and achieves results comparable to state-of-the-art methods on the real-world dataset CrowdPose.</p><p>In future work, we will especially focus on the problem arising by the gap between synthetic and real-world data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>CrowdIndex distributions. CrowdPose and JTA differ significantly regarding their CrowdIndex distributions. JTA-Ext was created to diminish this difference and create a distribution closer to uniform distributed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Our two architecture extensions: OccNet (top), OccNetCB (bottom). The Occlusion Detection Networks with an additional branch to detect occluded keypoints and OccNetCB with interconnections between the visible and occluded branch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>[ 3 ]Figure 4 :</head><label>34</label><figDesc>collected by extracting videos from the video game Grand Theft Auto V. The annotations for human poses in JTA include 22 keypoints, a tracking id and visibility flags. The visibility flags distinguish whether a keypoints is visible, occluded or selfoccluded. In order to compare the approaches across different datasets the annotations of JTA were matched to the format of CrowdPose by simply discarding and reordering the keypoints. For the evaluation on the JTA dataset only a subset of the test set is used which includes only every fif-teenth frame of the JTA test-sequences. Furthermore, JTA-Ext is used for training which includes the extension of the JTA dataset and the original training set of JTA. Two images showing crowded situations. Based on the CrowdIndex</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Thereby, N is the number of persons in the image, N i b is the number of keypoints within the bounding box of person i corresponding to the person and N i a is the number of keypoints inside bounding box belonging to all persons j = i. These levels are denoted as easy [0.0, 0.1), medium [0.1, 0.8), and hard [0.8, 1.0].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>The results of Xiao et al. and Li et al. are taken from [6],</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results of the different proposed cutout augmentation methods on the CrowdPose test set. While all approaches improve the performance over all crowding levels, the object cutout augmentation provides the best result, with an increase in 1.1% AP compared to the Baseline-50.</figDesc><table><row><cell>Cutout Method</cell><cell>AP</cell><cell cols="3">AP Easy AP Med AP Hard</cell></row><row><cell>-</cell><cell>61.9</cell><cell>72.7</cell><cell>63.1</cell><cell>49.1</cell></row><row><cell>Objects</cell><cell>63.0</cell><cell>73.4</cell><cell>64.1</cell><cell>50.5</cell></row><row><cell>Body Parts</cell><cell>62.8</cell><cell>73.0</cell><cell>63.9</cell><cell>50.4</cell></row><row><cell>Full Body</cell><cell>62.6</cell><cell>73.0</cell><cell>63.8</cell><cell>50.0</cell></row><row><cell>Parts and Obj.</cell><cell>61.3</cell><cell>71.7</cell><cell>62.5</cell><cell>48.7</cell></row><row><cell>Full and Obj.</cell><cell>61.5</cell><cell>71.6</cell><cell>62.7</cell><cell>49.0</cell></row><row><cell>Parts or Obj.</cell><cell>62.9</cell><cell>73.0</cell><cell>64.0</cell><cell>50.2</cell></row><row><cell>Full or Obj.</cell><cell>62.8</cell><cell>73.1</cell><cell>63.9</cell><cell>50.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Overview of the results of the occluded joint detection networks in comparison to the baseline architecture on the subset of the JTA test set. OccNetCB provides results similar to the Baseline-50 model, whereas OccNet even degrades in performance.</figDesc><table><row><cell cols="5">Architecture AP AP Easy AP Med AP Hard</cell></row><row><cell>Baseline-50</cell><cell>88.1</cell><cell>90.8</cell><cell>86.5</cell><cell>85.7</cell></row><row><cell>OccNet</cell><cell>87.8</cell><cell>90.7</cell><cell>86.3</cell><cell>85.5</cell></row><row><cell>OccNetCB</cell><cell>88.0</cell><cell>90.7</cell><cell>86.5</cell><cell>85.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison of the results of the Baseline-50 model trained on JTA, JTA-Ext and CrowdPose, tested on the CrowdPose test set.Training Set AP AP Easy AP Med AP Hard</figDesc><table><row><cell>CrowdPose</cell><cell>61.9</cell><cell>72.7</cell><cell>63.1</cell><cell>49.1</cell></row><row><cell>JTA</cell><cell>26.6</cell><cell>36.6</cell><cell>27.0</cell><cell>17.1</cell></row><row><cell>JTA-Ext</cell><cell>28.8</cell><cell>39.8</cell><cell>29.2</cell><cell>18.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>The results of the Baseline-50 model pre-trained on JTA, JTA-Ext and fine-tuned on CrowdPose in comparison to a model purely trained on CrowdPose. The CrowdPose test set is used for evaluation.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Figure</cell></row><row><cell cols="5">Pre-Training Set AP AP Easy AP Med AP Hard</cell></row><row><cell>-</cell><cell>61.9</cell><cell>72.7</cell><cell>63.1</cell><cell>49.1</cell></row><row><cell>JTA</cell><cell>63.3</cell><cell>73.7</cell><cell>64.4</cell><cell>50.6</cell></row><row><cell>JTA-Ext</cell><cell>63.0</cell><cell>73.1</cell><cell>64.3</cell><cell>50.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Results on the CrowdPose test set including the proposed split of<ref type="bibr" target="#b5">[6]</ref> in different crowding levels. Since the dataset is quite young, only few work reports results on its test set.</figDesc><table><row><cell>Method</cell><cell cols="4">AP AP Easy AP Med AP Hard</cell></row><row><cell cols="2">Xiao et al. [11] 60.8</cell><cell>71.4</cell><cell>61.2</cell><cell>51.2</cell></row><row><cell>Li et al. [6]</cell><cell>66.6</cell><cell>75.7</cell><cell>66.3</cell><cell>57.4</cell></row><row><cell>Ours</cell><cell>65.5</cell><cell>75.2</cell><cell>66.6</cell><cell>53.1</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://posetrack.net/workshops/eccv2018/ posetrack_eccv_2018_results.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Dataset available on https://github.com/thomasgolda/ Human-Pose-Estimation-for-Real-World-Crowded-</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">The code and the used weights of the used YOLOv3 provided by Xiu et al.<ref type="bibr" target="#b11">[12]</ref> and Fang et al.<ref type="bibr" target="#b3">[4]</ref> are available at: https://github.com/MVIG-SJTU/AlphaPose/tree/ 7eea793e5d497e7947bb8e6ce547d98c6a0059bf</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2D Human Pose Estimation: New Benchmark and State of the Art Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1711.07319</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning to detect and track visible and occluded body joints in a virtual world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fabbri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lanzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Palazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vezzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">RMPE: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Crowdpose</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00324</idno>
		<title level="m">Efficient crowded scenes pose estimation and a new benchmark</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno>abs/1405.0312</idno>
		<title level="m">Microsoft COCO: common objects in context. CoRR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<idno>abs/1603.06937</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Yolov3: An incremental improvement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno>abs/1804.02767</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">How robust is 3d human pose estimation to occlusion?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>SÃ¡rÃ¡ndi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Linder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">O</forename><surname>Arras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS Workshop -Robotic Co-workers 4</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">0</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pose Flow: Efficient online pose tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In BMVC</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
