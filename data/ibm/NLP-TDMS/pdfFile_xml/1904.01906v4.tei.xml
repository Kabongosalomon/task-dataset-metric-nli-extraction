<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">What Is Wrong With Scene Text Recognition Model Comparisons? Dataset and Model Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeonghun</forename><surname>Baek</surname></persName>
							<email>jh.baek@navercorp.com</email>
							<affiliation key="aff0">
								<orgName type="department">Clova AI Research</orgName>
								<orgName type="institution">NAVER/LINE Corp</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geewook</forename><surname>Kim</surname></persName>
							<email>geewook@sys.i.kyoto-u.ac.jpcoallaoh@linecorp.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Kyoto University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyeop</forename><surname>Lee</surname></persName>
							<email>junyeop.lee@navercorp.com</email>
							<affiliation key="aff0">
								<orgName type="department">Clova AI Research</orgName>
								<orgName type="institution">NAVER/LINE Corp</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungrae</forename><surname>Park</surname></persName>
							<email>sungrae.park@navercorp.com</email>
							<affiliation key="aff0">
								<orgName type="department">Clova AI Research</orgName>
								<orgName type="institution">NAVER/LINE Corp</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
							<email>dongyoon.han@navercorp.com</email>
							<affiliation key="aff0">
								<orgName type="department">Clova AI Research</orgName>
								<orgName type="institution">NAVER/LINE Corp</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
							<email>sangdoo.yun@navercorp.com</email>
							<affiliation key="aff0">
								<orgName type="department">Clova AI Research</orgName>
								<orgName type="institution">NAVER/LINE Corp</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong</forename><forename type="middle">Joon</forename><surname>Oh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Clova AI Research</orgName>
								<orgName type="institution">NAVER/LINE Corp</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwalsuk</forename><surname>Lee</surname></persName>
							<email>hwalsuk.lee@navercorp.com</email>
							<affiliation key="aff0">
								<orgName type="department">Clova AI Research</orgName>
								<orgName type="institution">NAVER/LINE Corp</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">What Is Wrong With Scene Text Recognition Model Comparisons? Dataset and Model Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many new proposals for scene text recognition (STR) models have been introduced in recent years. While each claim to have pushed the boundary of the technology, a holistic and fair comparison has been largely missing in the field due to the inconsistent choices of training and evaluation datasets. This paper addresses this difficulty with three major contributions. First, we examine the inconsistencies of training and evaluation datasets, and the performance gap results from inconsistencies. Second, we introduce a unified four-stage STR framework that most existing STR models fit into. Using this framework allows for the extensive evaluation of previously proposed STR modules and the discovery of previously unexplored module combinations. Third, we analyze the module-wise contributions to performance in terms of accuracy, speed, and memory demand, under one consistent set of training and evaluation datasets. Such analyses clean up the hindrance on the current comparisons to understand the performance gain of the existing modules. Our code is publicly available 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Reading text in natural scenes, referred to as scene text recognition (STR), has been an important task in a wide range of industrial applications. The maturity of Optical Character Recognition (OCR) systems has led to its successful application on cleaned documents, but most traditional OCR methods have failed to be as effective on STR tasks due to the diverse text appearances that occur in the real world and the imperfect conditions in which these scenes are captured.</p><p>To address these challenges, prior works <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b18">19]</ref> have proposed multi-stage pipelines, where each stage is a deep neural network addressing a specific challenge. For example, Shi et al. <ref type="bibr" target="#b23">[24]</ref> have suggested using a recurrent neural network to address the varying number of characters in a given input, and a connectionist temporal classification loss <ref type="bibr" target="#b5">[6]</ref> to identify the number of characters. Shi et al. <ref type="bibr" target="#b24">[25]</ref> have proposed a transformation module that normalizes the input into a straight text image to reduce the representational burden for downstream modules to handle curved texts.</p><p>However, it is hard to assess whether and how a newly proposed module improves upon the current art, as some papers have come up with different evaluation and testing environments, making it difficult to compare reported numbers at face value <ref type="table">(Table 1</ref>). We observed that 1) the training datasets and 2) the evaluation datasets deviate amongst various methods, as well. For example, different works use a different subset of the IC13 dataset as part of their evaluation set, which may cause a performance disparity of more than 15%. This kind of discrepancy hinders the fair comparison of performance between different models.</p><p>Our paper addresses these types of issues with the following main contributions. First, we analyze all training and evaluation datasets commonly used in STR papers. Our analysis reveals the inconsistency of using the STR datasets and its causes. For instance, we found 7 missing examples in IC03 dataset and 158 missing examples in IC13 dataset as well. We investigate several previous works on the STR datasets and show that the inconsistency causes incomparable results as shown in <ref type="table">Table 1</ref>. Second, we introduce a unifying framework for STR that provides a common perspective for existing methods. Specifically, we divide the STR model into four different consecutive stages of operations: transformation (Trans.), feature extraction (Feat.), sequence modeling (Seq.), and prediction (Pred.). The framework provides not only existing methods but their possible variants toward an extensive analysis of module-wise con-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Year <ref type="table" target="#tab_7">Train data  IIIT SVT  IC03  IC13  IC15  SP CT  Time params  3000 647 860 867 857 1015 1811 2077 645 288</ref>   <ref type="table">Table 1</ref>: Performance of existing STR models with their inconsistent training and evaluation settings. This inconsistency hinders the fair comparison among those methods. We present the results reported by the original papers and also show our re-implemented results under unified and consistent setting. At the last row, we also show the best model we have found, which shows competitive performance to state-of-the-art methods. MJ, ST, C, and, PRI denote MJSynth <ref type="bibr" target="#b10">[11]</ref>, SynthText <ref type="bibr" target="#b7">[8]</ref>, Character-labeled <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b29">30]</ref>, and private data <ref type="bibr" target="#b2">[3]</ref>, respectively. Top accuracy for each benchmark is shown in bold.</p><p>tribution. Finally, we study the module-wise contributions in terms of accuracy, speed, and memory demand, under a unified experimental setting. With this study, we assess the contribution of individual modules more rigorously and propose previously overlooked module combinations that improves over the state of the art. Furthermore, we analyzed failure cases on the benchmark dataset to identify remaining challenges in STR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Dataset Matters in STR</head><p>In this section, we examine the different training and evaluation datasets used by prior works, and then their discrepancies are addressed. Through this analysis, we highlight how each of the works differs in constructing and using their datasets, and investigate the bias caused by the inconsistency when comparing performance between different works ( <ref type="table">Table 1</ref>). The performance gaps due to dataset inconsistencies are measured through experiments and discussed in §4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Synthetic datasets for training</head><p>When training a STR model, labeling scene text images is costly, and thus it is difficult to obtain enough labeled data for. Alternatively using real data, most STR models have used synthetic datasets for training. We first introduce two most popular synthetic datasets used in recent STR papers:</p><p>• MJSynth (MJ) <ref type="bibr" target="#b10">[11]</ref> is a synthetic dataset designed for STR, containing 8.9 M word box images. The word box generation process is as follows: 1) font rendering, 2) border and shadow rendering, 3) background coloring, 4) composition of font, border, and background, 5) applying projective distortions, 6) blending with realworld images, and 7) adding noise. <ref type="figure" target="#fig_0">Figure 1a</ref> shows some examples of MJSynth,</p><p>• SynthText (ST) <ref type="bibr" target="#b7">[8]</ref> is another synthetically generated dataset and was originally designed for scene text detection. An example of how the words are rendered onto scene images is shown in <ref type="figure" target="#fig_0">Figure 1b</ref>. Even though SynthText was designed for scene text detection task, it has been also used for STR by cropping word boxes. SynthText has 5.5 M training data once the word boxes are cropped and filtered for non-alphanumeric characters.</p><p>Note that prior works have used diverse combinations of MJ, ST, and or other sources <ref type="table">(Table 1)</ref>. These inconsistencies call into question whether the improvements are due to the contribution of the proposed module or to that of a better or larger training data. Our experiment in §4.2 describes the influence of the training datasets to the final performance on the benchmarks. We further suggest that future STR researches clearly indicate the training datasets used and com- pare models using the same training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Real-world datasets for evaluation</head><p>Seven real-world STR datasets have been widely used for evaluating a trained STR model. For some benchmark dataset, different subsets of the dataset may have been used in each prior work for evaluation <ref type="table">(Table 1)</ref>. These difference in subsets result in inconsistent comparison.</p><p>We introduce the datasets by categorizing them into regular and irreguglar datsets. The benchmark datasets are given the distinction of being "regular" or "irregular" datasets <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b4">5]</ref>, according to the difficulty and geometric layout of the texts. First, regular datasets contain text images with horizontally laid out characters that have even spacings between them. These represent relatively easy cases for STR:</p><p>• IIIT5K-Words (IIIT) <ref type="bibr" target="#b20">[21]</ref> is the dataset crawled from Google image searches, with query words that are likely to return text images, such as "billboards", "signboard", "house numbers", "house name plates", and "movie posters". IIIT consists of 2,000 images for training and 3,000 images for evaluation,</p><p>• Street View Text (SVT) <ref type="bibr" target="#b28">[29]</ref> contains outdoor street images collected from Google Street View. Some of these images are noisy, blurry, or of low-resolution. SVT consists of 257 images for training and 647 images for evaluation,</p><p>• ICDAR2003 (IC03) <ref type="bibr" target="#b19">[20]</ref>    <ref type="table" target="#tab_9">(IIIT5k, SVT, IC03, IC13)</ref> and irregular (IC15, SVTP, CUTE) real-world datasets.</p><p>• ICDAR2013 (IC13) <ref type="bibr" target="#b13">[14]</ref> inherits most of IC03's images and was also created for the ICDAR 2013 Robust Reading competition. It contains 848 images for training and 1,095 images for evaluation, where pruning words with non-alphanumeric characters results in 1,015 images. Again, researchers have used two different versions for evaluation: 857 and 1,015 images. The 857-image set is a subset of the 1,015 set where words shorter than 3 characters are pruned.</p><p>Second, irregular datasets typically contain harder corner cases for STR, such as curved and arbitrarily rotated or distorted texts <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b4">5]</ref>:</p><p>• ICDAR2015 (IC15) <ref type="bibr" target="#b12">[13]</ref> was created for the ICDAR 2015 Robust Reading competitions and contains 4,468 images for training and 2,077 images for evaluation. The images are captured by Google Glasses while under the natural movements of the wearer. Thus, many are noisy, blurry, and rotated, and some are also of low resolution. Again, researchers have used two different versions for evaluation: 1,811 and 2,077 images. Previous papers <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b1">2]</ref> have only used 1,811 images, discarding non-alphanumeric character images and some extremely rotated, perspective-shifted, and curved images for evaluation. Some of the discarded word boxes can be found in the supplementary materials,</p><p>• SVT Perspective (SP) <ref type="bibr" target="#b21">[22]</ref> is collected from Google Street View and contains 645 images for evaluation. Many of the images contain perspective projections due to the prevalence of non-frontal viewpoints, • CUTE80 (CT) <ref type="bibr" target="#b22">[23]</ref> is collected from natural scenes and contains 288 cropped images for evaluation. Many of these are curved text images.</p><p>Notice that, <ref type="table">Table 1</ref> provides us a critical issue that prior works evaluated their models on different benchmark datasets. Specifically, the evaluation has been conducted on different versions of benchmarks in IC03, IC13 and IC15. In IC03, 7 examples can cause a performance gap by 0.8% that is a huge gap when comparing those of prior performances. In the case of IC13 and IC15, the gap of the example numbers is even bigger than those of IC03.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">STR Framework Analysis</head><p>The goal of the section is introducing the scene text recognition (STR) framework consisting of four stages, derived from commonalities among independently proposed STR models. After that, we describe the module options in each stage.</p><p>Due to the resemblance of STR to computer vision tasks (e.g. object detection) and sequence prediction tasks, STR has benefited from high-performance convolutional neural networks (CNNs) and recurrent neural networks (RNNs). The first combined application of CNN and RNN for STR, Convolutional-Recurrent Neural Network (CRNN) <ref type="bibr" target="#b23">[24]</ref>, extracts CNN features from the input text image, and reconfigures them with an RNN for robust sequence prediction. After CRNN, multiple variants <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b2">3]</ref> have been proposed to improve performance. For rectifying arbitrary text geometries, as an example, transformation modules have been proposed to normalize text images <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b16">17]</ref>. For treating complex text images with high intrinsic dimensionality and latent factors (e.g. font style and cluttered background), improved CNN feature extractors have been incorporated <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b3">4]</ref>. Also, as people have become more concerned with inference time, some methods have even omitted the RNN stage <ref type="bibr" target="#b2">[3]</ref>. For improving character sequence prediction, attention based decoders have been proposed <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>The four stages derived from existing STR models are as follows:</p><p>1. Transformation (Trans.) normalizes the input text image using the Spatial Transformer Network (STN <ref type="bibr" target="#b11">[12]</ref>) to ease downstream stages. 2. Feature extraction (Feat.) maps the input image to a representation that focuses on the attributes relevant for character recognition, while suppressing irrelevant features such as font, color, size, and background. 3. Sequence modeling (Seq.) captures the contextual information within a sequence of characters for the next stage to predict each character more robustly, rather than doing it independently. 4. Prediction (Pred.) estimates the output character sequence from the identified features of an image.</p><p>We provide <ref type="figure" target="#fig_2">Figure 3</ref> for an overview and all the architectures we used in this paper are found in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Transformation stage</head><p>The module of this stage transforms the input image X into the normalized imageX. Text images in natural scenes come in diverse shapes, as shown by curved and tilted texts. If such input images are fed unaltered, the subsequent feature extraction stage needs to learn an invariant representation with respect to such geometry. To reduce this burden, thin-plate spline (TPS) transformation, a variant of the spatial transformation network (STN) <ref type="bibr" target="#b11">[12]</ref>, has been applied with its flexibility to diverse aspect ratios of text lines <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b17">18]</ref>. TPS employs a smooth spline interpolation between a set of fiducial points. More precisely, TPS finds multiple fiducial points (green '+' marks in <ref type="figure" target="#fig_2">Figure 3</ref>) at the upper and bottom enveloping points, and normalizes the character region to a predefined rectangle. Our framework allows for the selection or de-selection of TPS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Feature extraction stage</head><p>In this stage, a CNN abstract an input image (i.e., X or X) and outputs a visual feature map V = {v i }, i = 1, . . . , I (I is the number of columns in the feature map). Each column in the resulting feature map by a feature extractor has a corresponding distinguishable receptive field along the horizontal line of the input image. These features are used to estimate the character on each receptive field.</p><p>We study three architectures of VGG <ref type="bibr" target="#b25">[26]</ref>, RCNN <ref type="bibr" target="#b15">[16]</ref>, and ResNet <ref type="bibr" target="#b9">[10]</ref>, previously used as feature extractors for STR. VGG in its original form consists of multiple convolutional layers followed by a few fully connected layers <ref type="bibr" target="#b25">[26]</ref>. RCNN is a variant of CNN that can be applied recursively to adjust its receptive fields depending on the character shapes <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b27">28]</ref>. ResNet is a CNN with residual connections that eases the training of relatively deeper CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Sequence modeling stage</head><p>The extracted features from Feat. stage are reshaped to be a sequence of features V . That is, each column in a feature map v i ∈ V is used as a frame of the sequence. However, this sequence may suffer the lack of contextual information. Therefore, some previous works use Bidirectional LSTM (BiLSTM) to make a better sequence H = Seq.(V) after the feature extraction stage <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b3">4]</ref>. On the other hand, Rosetta <ref type="bibr" target="#b2">[3]</ref> removed the BiLSTM to reduce computational complexity and memory consumption. Our framework allows for the selection or de-selection of BiLSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Prediction stage</head><p>In this stage, from the input H, a module predict a sequence of characters, (i.e., Y = y 1 , y 2 , . . . ). By summing up previous works, we have two options for prediction: (1) Connectionist temporal classification (CTC) <ref type="bibr" target="#b5">[6]</ref> and <ref type="formula">(2)</ref> attention-based sequence prediction (Attn) <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b3">4]</ref>. CTC allows for the prediction of a non-fixed number of a sequence even though a fixed number of the features are given. The key methods for CTC are to predict a character at each column (h i ∈ H) and to modify the full character sequence into a non-fixed stream of characters by deleting repeated characters and blanks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b23">24]</ref>. On the other hand, Attn automatically captures the information flow within the input sequence to predict the output sequence <ref type="bibr" target="#b0">[1]</ref>. It enables an STR model to learn a character-level language model representing output class dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment and Analysis</head><p>This section contains the evaluation and analysis of all possible STR module combinations (2×3×2×2= 24 in total) from the four-stage framework in §3, all evaluated under the common training and evaluation dataset constructed from the datasets listed in §2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation detail</head><p>As we described in §2, training and evaluation datasets influences the measured performances of STR models significantly. To conduct a fair comparison, we have fixed the choice of training, validation, and evaluation datasets. STR training and model selection We use an union of MJSynth 8.9 M and SynthText 5.5 M (14.4 M in total) as our training data. We adopt the AdaDelta <ref type="bibr" target="#b30">[31]</ref> optimizer, whose decay rate is set to ρ = 0.95. The training batch size is 192, and the number of iterations is 300 K. Gradient clipping is used at magnitude 5. All parameters are initialized with He's method <ref type="bibr" target="#b8">[9]</ref>. We use the union of the training sets IC13, IC15, IIIT, and SVT as the validation data, and validated the model after every 2000 training steps to select the model with the highest accuracy on this set. Notice that, the validation set does not contain the IC03 train data because some of them were duplicated in the evaluation dataset of IC13. The total number of duplicated scene images is 34, and they contain 215 word boxes. Duplicated examples can be found in the supplementary materials. Evaluation metrics In this paper, we provide a thorough analysis on STR combinations in terms of accuracy, time, and memory aspects altogether. For accuracy, we measure the success rate of word predictions per image on the 9 real-world evaluation datasets involving all subsets of the benchmarks, as well as a unified evaluation dataset (8,539 images in total); 3,000 from IIIT, 647 from SVT, 867 from IC03, 1015 from IC13, 2,077 from IC15, 645 from SP, and 288 from CT. We only evaluate on alphabets and digits. For each STR combination, we have run five trials with different initialization random seeds and have averaged their accuracies. For speed assessment, we measure the per-image average clock time (in millisecond) for recognizing the given texts under the same compute environment, detailed below. For memory assessment, we count the number of trainable floating point parameters in the entire STR pipeline. Environment: For a fair speed comparison, all of our evaluations are performed on the same environment: an Intel Xeon(R) E5-2630 v4 2.20GHz CPU, an NVIDIA TESLA P40 GPU, and 252GB of RAM. All experiments are performed with NAVER Smart Machine Learning (NSML) platform <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Analysis on training datasets</head><p>We investigate the influence of using different groups of the training datasets to the performance on the benchmarks. As we mentioned in §2.1, prior works used different sets of the training datasets and left uncertainties as to the contributions of their models to improvements. To unpack this issue, we examined the accuracy of our best model from §4.3 with different settings of the training dataset. We obtained 80.0% total accuracy by using only MJSynth, 75.6% by using only SynthText, and 84.1% by using both. The combination of MJSynth and SynthText improved accuracy by more than 4.1%, over the individual usages of MJSynth and SynthText. A lesson from this study is that the performance results using different training datasets are incomparable, and such comparisons fail to prove the contribution of the model, which is why we trained all models with the same training dataset, unless mentioned otherwise.</p><p>Interestingly, training on 20% of MJSynth (1.8M) and 20% of SynthText (1.1M) together (total 2.9M ≈ the half of SynthText) provides 81.3% accuracy -better performance than the individual usages of MJSynth or SynthText. MJSynth and SynthText have different properties because they were generated with different options, such as distortion and blur. This result showed that the diversity of training data can be more important than the number of training examples, and that the effects of using different training datasets is more complex than simply concluding more is better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Analysis of trade-offs for module combinations</head><p>Here, we focus on the accuracy-speed and accuracymemory trade-offs shown in different combinations of modules. We provide the full table of results in the supplementary materials. See <ref type="figure" target="#fig_4">Figure 4</ref> for the trade-off plots of all 24 combinations, including the six previously proposed STR models (Stars in <ref type="figure" target="#fig_4">Figure 4</ref>). In terms of the accuracy-time trade-off, Rosetta and STAR-net are on the frontier and the other four prior models are inside of the frontier. In terms     of the accuracy-memory trade-off, R2AM is on the frontier and the other five of previously proposed models are inside of the frontier. Module combinations along the trade-off frontiers are labeled in ascending ascending order of accuracy (T1 to T5 for accuracy-time and P1 to P5 for accuracymemory).</p><p>Analysis of combinations along the trade-off frontiers.</p><p>As shown in  <ref type="table" target="#tab_11">Table 4b</ref>, P1 is the model with the least amount of memory consumption, and from P1 to P5 the trade-off between memory and accuracy takes place. As in the accuracy-speed trade-off, we observe a single module shift at each step up to P5, where the changed modules are: Attn, TPS, BiL-STM, and ResNet. They sequentially increase the accuracy at the cost of memory. Compared to VGG used in T1, we observe that RCNN in P1-P4 is lighter and gives a good accuracy-memory trade-off. RCNN requires a small number of unique CNN layers that are repeatedly applied. We observe that transformation, sequential, and prediction modules are not significantly contributing to the memory consumption (1.9M→7.2M parameters). While being lightweight overall, these modules provide accuracy improvements (75.4%→82.3%). The final change, ResNet, on the other hand, increases the accuracy by 1.7% at the cost of increased memory consumption from 7.2M to 49.6M floating point parameters. Thus, a practitioner concerned about memory consumption can be assured to choose specialized transformation, sequential, and prediction modules relatively freely, but should refrain from the use of heavy feature extractors like ResNets.</p><p>The most important modules for speed and memory. We have identified the module-wise impact on speed and memory by color-coding the scatter plots in <ref type="figure" target="#fig_4">Figure 4</ref> according to module choices. The full set of color-coded plots is in the supplementary materials. Here, we show the scatter plots with the most speed-and memory-critical modules, namely the prediction and feature extraction modules, respectively, in <ref type="figure" target="#fig_5">Figure 5</ref>.</p><p>There are clear clusters of combinations according to the prediction and feature modules. In the accuracy-speed trade-off, we identify CTC and Attn clusters (the addition of Attn significantly slows the overall STR model). On the other hand, for accuracy-memory trade-off, we observe that the feature extractor contributes towards memory most significantly. It is important to recognize that the most significant modules for each criteria differ, therefore, practitioners under different applications scenarios and constraints should look into different module combinations for the best trade-offs depending on their needs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Module analysis</head><p>Here, we investigate the module-wise performances in terms of accuracy, speed, and memory demand. For this analysis, the marginalized accuracy of each module is calculated by averaging out the combination including the module in <ref type="table" target="#tab_7">Table 2</ref>. Upgrading a module at each stage requires additional resources, time or memory, but provides performance improvements. The table shows that the performance improvement in irregular datasets is about two times that of regular benchmarks over all stages. when comparing accuracy improvement versus time usage, a sequence of ResNet, BiLSTM, TPS, and Attn is the most efficient upgrade order of the modules from a base combination of None-VGG-None-CTC. This order is the same order  of combinations for the accuracy-time frontiers (T1→T5). On the other hand, an accuracy-memory perspective finds RCNN, Attn, TPS, BiLSTM and ResNet as the most efficient upgrading order for the modules, like the order of the accuracy-memory frontiers (P1→P5). Interestingly, the efficient order of modules for time is reverse from those for memory. The different properties of modules provide different choices in practical applications. In addition, the module ranks in the two perspectives are the same as the order of the frontier module changes, and this shows that each module contributes to the performances similarly under all combinations.</p><p>Qualitative analysis Each module contributes to identify text by solving targeted difficulties of STR tasks, as described in §3. <ref type="figure" target="#fig_8">Figure 7</ref> shows samples that are only correctly recognized when certain modules are upgraded (e.g. from VGG to ResNet backbone). Each row shows a module upgrade at each stage of our framework. Presented samples are failed before the upgrade, but becomes recognizable afterward. TPS transformation normalizes curved and perspective texts into a standardized view. Predicted results show dramatic improvements especially for "POLICE" in a circled brand logo and "AIRWAYS" in a perspective view of a storefront sign. Advanced feature extractor, ResNet, results in better representation power, improving on cases with heavy background clutter "YMCA", "CITYARTS") and unseen fonts ("NEUMOS"). BiLSTM leads to better context modeling by adjusting the receptive field; it can ignore unrelatedly cropped characters ("I" at the end of "EXIT", "C" at the end of "G20"). Attention including implicit characterlevel language modeling finds missing or occluded character, such as "a" in "Hard", "t" in "to", and "S" in "HOUSE". These examples provide glimpses to the contribution points of the modules in real-world applications.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Failure case analysis</head><p>We investigate failure cases of all 24 combinations. As our framework derived from commonalities among proposed STR models, and our best model showed competitive performance with previously proposed STR models, the presented failure cases constitute a common challenge for the field as a whole. We hope our analysis inspires future works in STR to consider addressing those challenges. <ref type="bibr">Among 8,</ref><ref type="bibr">539</ref> examples in the benchmark datasets ( §2), 644 images (7.5%) are not correctly recognized by any of the 24 models considered. We found six common failure cases as shown in <ref type="figure" target="#fig_7">Figure 6</ref>. The followings are discussion about the challenges of the cases and suggestion future research directions. Calligraphic fonts: font styles for brands, such as "Coca Cola", or shop names on streets, such as "Cafe", are still in remaining challenges. Such diverse expression of characters requires a novel feature extractor providing generalized visual features. Another possible approach is regularization because the model might be over-fitting to the font styles in a training dataset. Vertical texts: most of current STR models assumes horizontal text images, and thus structurally could not deal with vertical texts. Some STR models <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b4">5]</ref> exploit vertical information also, however, vertical texts are not clearly covered yet. Further research would be needed to cover vertical texts. Special characters: since current benchmarks do not evaluate special characters, existing works exclude them during training. This results in failure prediction, misleading the model to treat them as alphanumeric characters. We suggest training with special characters. This has resulted in a boost from 87.9% to 90.3% accuracy on IIIT. Heavy occlusions: current methods do not extensively exploit contextual information to overcome occlusion. Future researches may consider superior language models to maximally utilize context. Low resolution: existing models do not explicitly handle low resolution cases; image pyramids or super-resolution modules may improve performance. Label noise: We found some noisy (incorrect) labels in the failure examples. We examined all examples in the benchmark to identify the ratio of the noisy labels. All benchmark datasets contain noisy labels and the ratio of mislabeling without considering special character was 1.3%, mislabeling with considering special character was 6.1%, and mislabeling with considering case-sensitivity was 24.1%.</p><p>We make all failure cases available in our Github repository, hoping that they will inspire further researches on corner cases of the STR problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>While there has been great advances on novel scene text recognition (STR) models, they have been compared on inconsistent benchmarks, leading to difficulties in determining whether and how a proposed module improves the STR baseline model. This work analyzes the contribution of the existing STR models that was hindered under inconsistent experiment settings before. To achieve this goal, we have introduced a common framework among key STR methods, as well as consistent datasets: seven benchmark evaluation datasets and two training datasets (MJ and ST). We have provided a fair comparison among the key STR methods compared, and have analyzed which modules brings about the greatest accuracy, speed, and size gains. We have also provided extensive analyses on module-wise contributions to typical challenges in STR as well as the remaining failure cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Contents</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B : Dataset Matters in STR -examples</head><p>• We illustrate examples of the problematic datasets described in §2 and §4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C : STR Framework -verification</head><p>• We verify our STR module implementations for our framework by reproducing four existing STR models, namely CRNN <ref type="bibr" target="#b23">[24]</ref>, RARE <ref type="bibr" target="#b24">[25]</ref>, GRCNN <ref type="bibr" target="#b27">[28]</ref>, and FAN (w/o Focus Net) <ref type="bibr" target="#b3">[4]</ref>.</p><p>Appendix D : STR Framework -architectural details</p><p>• We describe the architectural details of all modules in our framework described in §3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix E : STR Framework -full experimental results</head><p>• We provide the comprehensive results of our experiments described in §4, and discuss them in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix F : Additional Experiments</head><p>• We provide 3 experiments; fine-tuning, varying training dataset size and test on COCO dataset <ref type="bibr" target="#b26">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dataset Matters in STR -examples</head><p>IC03 -7 missing word boxes in 860 evaluation dataset.</p><p>The original IC03 evaluation dataset has 1,110 images, but prior works have conducted additional filtering, as described in §2. All papers have ignored all words that are either too short (less than 3 characters) or ones that contain non-alphanumeric characters. Although all papers have supposedly applied the same data filtering method and should have reduced the evaluation set from 1,110 images to 867 images, the reported example numbers are different: either the expected 867 images or a further reduced 860 images. We identified the missing examples as shown in <ref type="figure" target="#fig_9">Figure 8</ref>. IC15 -Filtered examples in evaluation dataset. The IC15 dataset originally contains 2,077 examples for its evaluation set, however prior works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b1">2]</ref> have filtered it down to 1,811 examples and have not given unambiguous specifications between them for deciding on which example to discard. To resolve this ambiguity, we have contacted one of the authors, who shared the specific dataset used for the evaluation. This information is made available with the source code on Github. A few sample images that have been filtered out of the IC15 evaluation dataset is shown in <ref type="figure" target="#fig_10">Figure 9</ref>. IC03 and IC13 -Duplicated images between IC03 training dataset and IC13 evaluation dataset. <ref type="figure" target="#fig_0">Figure 10</ref> shows two images from the subset given by the intersection between the IC03 training dataset and the IC13 evaluation dataset. In our investigation, a total of 34 duplicated scene   images have been found, amounting to 215 duplicate word boxes, in total. Therefore, when one assesses the performance of a model on the IC13 evaluation data, he/she should be mindful of these overlapping data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. STR Framework -verification</head><p>To show the correctness of our implemented module for our framework, we reproduce the performances of existing models that can be re-built by our framework. Specifically, we compare the results of our implementation of CRNN, RARE, GRCNN, and FAN (w/o Focus Net) <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b3">4]</ref> from those of publicly reported by the authors. We implemented each module as described in their original papers, and also we followed the training and evaluation pipelines of their original papers to train the individual models. <ref type="table" target="#tab_9">Table 3</ref> shows the results. Our implementation has overall similar performance with reported result in their paper, which <ref type="table" target="#tab_9">Model   Train Data  IIIT SVT  IC03  IC13  IC15  SP  CT  3000 647</ref> 860 867 857 1015 1811 2077 645 288 CRNN <ref type="bibr" target="#b23">[24]</ref> reported in paper MJ 78.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. STR Framework -architectural details</head><p>In this appendix, we describe each module of our framework in terms of its concept and architectural specifications. We first introduce common notations used in this appendix and then explain the modules of each stage; Trans., Feat., Seq., and Pred. Notations For a simple expression for a neural network architecture, we denote 'c', 'k', 's' and 'p' for the number of the output channel, the size of kernel, the stride, and the padding size respectively. BN, Pool, and FC denote the batch normalization layer, the max pooling layer, and the fully connected layer, respectively. In the case of convolution operation with the stride of 1 and the padding size of 1, 's' and 'p' are omitted for convenience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Transformation stage</head><p>The module of this stage transforms the input image X into the normalized imageX. We explained the concept of TPS <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b17">18]</ref> in §3.1, but here we deliver its mathematical background and the implementation details. TPS transformation: TPS generates a normalized image that shows a focused region of an input image. To build this pipeline, TPS consists of a sequence of processes; finding a text boundary, linking the location of the pixels in the boundary to those of the normalized image, and generating a normalized image by using the values of pixels and the linking information. Such processes are called as localization network, grid generator, and image sampler, respectively. Conceptually, TPS employs a smooth spline interpolation between a set of F fiducial points that represented a focused boundary of text in an image. Here, F indicates the constant number of fiducial points.</p><p>The localization network explicitly calculates x, ycoordinates of F fiducial points on an input image, X. The grid generator provides a mapping function from the identified regions by the localization network to the normalized image. The mapping function can be parameterized by a matrix T ∈ R 2×(F +3) , which is computed by</p><formula xml:id="formula_0">T = ∆ −1 C C 0 3×2 (1) where ∆ C ∈ R (F +3)×(F +3)</formula><p>is a matrix determined only byC, thus also a constant:</p><formula xml:id="formula_1">∆C =   1 F ×1C R 0 0 1 1×F 0 0C   (2)</formula><p>where the element of i-th row and j-th column of R is d 2 ij ln d ij , d ij is the euclidean distance betweenc i andc j . The pixels of grid on the normalized imageX is denoted byP = {p i } i=1,...,N , wherep i = [x i ,ỹ i ] is the x,ycoordinates of the i-th pixel, N is the number of pixels. For every pixelp i onX, we find the corresponding point p i = [x i , y i ] on X, by applying the transformation:</p><formula xml:id="formula_2">p i = T [1,x i ,ỹ i , r i1 , . . . , r iF ]<label>(3)</label></formula><formula xml:id="formula_3">r if = d 2 if ln d if<label>(4)</label></formula><p>where d if is the euclidean distance between pixelp i and the f -th base fiducial pointc f . By iterating Eq. 3 over all points inP, we generate a grid P = {p i } i=1,...,N on the input image X. Finally, the image sampler produces the normalized image by interpolating the pixels in the input images which are determined by the grid generator. TPS-Implementation: TPS requires the localization network calculating fiducial points of an input image. We designed the localization network by following most of the components of prior work <ref type="bibr" target="#b24">[25]</ref>, and added batch normalization layers and adaptive average pooling to stabilize the  training of the network. <ref type="table" target="#tab_11">Table 4</ref> shows the details of our architecture. In our implementation, the localization network has 4 convolution layers, each followed by a batch normalization layer and 2 x 2 max-pooling layer. The filter size, padding size, and stride are 3, 1, 1 respectively, for all convolutional layers. Following the last convolutional layer is an adaptive average pooling layer (APool in <ref type="table" target="#tab_11">Table 4</ref>). After that, two fully connected layers are following: 512 to 256 and 256 to 2F. Final output is 2F dimensional vector which corresponds to the value of x, y-coordinates of F fiducial points on input image. Activation functions for all layers are the ReLU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Feature extraction stage</head><p>In this stage, a CNN abstract an input image (i.e., X or X) and outputs a feature map V = {v i }, i = 1, . . . , I (I is the number of columns in the feature map). VGG: we implemented VGG <ref type="bibr" target="#b25">[26]</ref> which is used in CRNN <ref type="bibr" target="#b23">[24]</ref> and RARE <ref type="bibr" target="#b24">[25]</ref>. We summarized the architecture in <ref type="table" target="#tab_13">Table 5</ref>. The output of VGG is 512 channels × 24 columns. Recurrently applied CNN (RCNN): As a RCNN module, we implemented a Gated RCNN (GRCNN) <ref type="bibr" target="#b27">[28]</ref> which is a variant of RCNN that can be applied recursively with a gating mechanism. The architectural details of the module are shown in <ref type="table" target="#tab_14">Table 6</ref>. The output of RCNN is 512 channels × 26 columns. Residual Network (ResNet): As a ResNet <ref type="bibr" target="#b9">[10]</ref> module, we implemented the same network which is used in FAN <ref type="bibr" target="#b3">[4]</ref>. It has 29 trainable layers in total. The details of the network is shown in   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. Sequence modeling stage</head><p>Some previous works used Bidirectional LSTM (BiL-STM) to make a contextual sequence H = Seq.(V) after the Feat. stage <ref type="bibr" target="#b23">[24]</ref>. BiLSTM: We implemented 2-layers BiLSTM <ref type="bibr" target="#b6">[7]</ref> which is used in CRNN <ref type="bibr" target="#b23">[24]</ref>. In the followings, we explain a BiL-STM layer used in our framework: A l th BiLSTM layer identifies two hidden states, h (t),f i and h (t),b i ∀t, calculated through time sequence and its reverse. Following <ref type="bibr" target="#b23">[24]</ref>, we additionally applied a FC layer between BiLSTM layers to determine one hidden state,ĥ    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4. Prediction stage</head><p>A prediction module produces the final prediction output from the input H, (i.e., Y = y 1 , y 2 , . . . ), which is a sequence of characters. We implemented two modules: Connectionist Temporal Classification (CTC) <ref type="bibr" target="#b5">[6]</ref> based and Attention mechanism (Attn) based Pred. module. In our experiments, we make the character label set C which include 36 alphanumeric characters. For the CTC, additional blank token is added to the label set due to the characteristics of the CTC. For the Attn, additional end of sentence (EOS) token is added to the label set due to the characteristics of the Attn. That is, the number of character set C is 37. Connectionist Temporal Classification (CTC): CTC takes a sequence H = h 1 , . . . , h T , where T is the sequence length, and outputs the probability of π, which is defined as <ref type="bibr" target="#b4">(5)</ref> where y t πt is the probability of generating character π t at each time step t. After that, the mapping function M which maps π to Y by removing repeated characters and blanks. For instance, M maps "aaa--b-b-c-ccc-c--" onto "abbccc", where '-' is blank token. The conditional probability is defined as the sum of probabilities of all π that are mapped by M onto Y , which is</p><formula xml:id="formula_4">p(π|H) = T t=1 y t πt</formula><formula xml:id="formula_5">p(Y |H) = π:M (π)=Y p(π|H)<label>(6)</label></formula><p>At testing phase, the predicted label sequence is calculated by taking the highest probability character π t at each time step t, and map the π onto Y :</p><formula xml:id="formula_6">Y * ≈ M (arg max π p(π|H))<label>(7)</label></formula><p>Attention mechanism (Attn): We implemented one layer LSTM attention decoder <ref type="bibr" target="#b0">[1]</ref> which is used in FAN, AON, and EP <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b1">2]</ref>. The details are as follows: at t-step, the decoder predicts an output y t as</p><formula xml:id="formula_7">y t = softmax(W o s t + b o )<label>(8)</label></formula><p>where W 0 and b 0 are trainable parameters. s t is the decoder LSTM hidden state at time t as</p><formula xml:id="formula_8">s t = LSTM(y t−1 , c t , s t−1 )<label>(9)</label></formula><p>and c t is a context vector, which is computed as the weighted sum of H = h 1 , ...h I from the former stage as</p><formula xml:id="formula_9">c t = I i=1 α ti h i<label>(10)</label></formula><p>where α ti is called attention weight and computed by</p><formula xml:id="formula_10">α ti = exp(e ti ) I k=1 exp(e tk )<label>(11)</label></formula><p>where</p><formula xml:id="formula_11">e ti = v tanh(W s t−1 + V h i + b)<label>(12)</label></formula><p>and v, W , V and b are trainable parameters. The dimension of LSTM hidden state was set as 256.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5. Objective function</head><p>Denote the training dataset by T D = {X i , Y i }, where X i is the training image and Y i is the word label. The training conducted by minimizing the objective function that negative log-likelihood of the conditional probability of word label.</p><formula xml:id="formula_12">O = − Xi,Yi∈T D log p(Y i |X i )<label>(13)</label></formula><p>This function calculates a cost from an image and its word label, and the modules in the framework are trained end-toend manner.   None TPS <ref type="figure" target="#fig_0">Figure 11</ref>: Color-coded version of <ref type="figure" target="#fig_4">Figure 4</ref> in §4.3, according to the transformation stage. Each circle represents the performance for each different combination of STR modules, while the each cross represents the average performance among STR combinations without TPS (black) or with TPS (magenta). Choosing to add TPS or not does not seem to give a noticeable advantage in performance when looking at the performances of each STR combination. However, the average accuracy does increase when using TPS compared to when it is unused, at the cost of longer inference times and a slight increase in the number of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. STR Framework -full experimental results</head><p>We report the full results of our experiments in <ref type="table" target="#tab_18">Table 8</ref>. FLOPS in <ref type="table" target="#tab_18">Table 8</ref> is approximately calculated, the detail is in our GitHub issue 2 . In addition, <ref type="figure" target="#fig_0">Figure 11-14</ref> show two types of trade-off plots of 24 combinations in respect of accuracy versus time and accuracy versus memory. In      <ref type="figure" target="#fig_4">Figure 4</ref> in §4.3, according to prediction stage. Each circle represents the performance for each different combination of STR modules, while the each cross represents the average performance among STR combinations with CTC (cyan) or with Attn (blue). The choice between CTC and Attn gives the largest and clearest inference time increase for each percentage of accuracy gained. The same cannot be said about the increase in the number of parameters with respect to accuracy increase, as Attn gains about 2 percentage points with minimal memory usage increase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Additional Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1. Fine-tuning on real datasets</head><p>We have fine-tuned our best model on the union of training sets IIIT, SVT, IC13, and IC15 (in-distribution), the held-out subsets of evaluation datasets of real scene text images. Other evaluation datasets, IC03, SP, and CT (outdistribution), do not have held-out subset for training; SP and CT have not training sets and some training images of IC03 have been found in IC13 evaluation dataset, as mentioned in §4.1, thus it is not appropriate to fine-tuning on IC03 training set.</p><p>Our model has been fine-tuned for 10 epochs. The table 9 shows the results. By fine-tuning on the real data, the accuracy on in-distribution subset (the union of evaluation datasets IIIT, SVT, IC13, and IC15) and on all benchmark data have improved by 2.2 pp and 1.5 pp, respectively. Meanwhile, the fine-tuned performance on the outdistribution subset (the union of evaluation datasets IC03, SP, and CT) has decreased by 1.3 pp. We conclude that finetuning over real data is effective when the real-data is close to the test-time distribution. Otherwise, fine-tuning over real data may do more harm than good.  <ref type="table">Table 9</ref>: Accuracy change with fine-tuning on real datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2. Accuracy with varying training dataset size</head><p>We have evaluated the accuracy of all 24 STR models against varying training dataset size. Training dataset consists of MJSynth 8.9 M and SynthText 5.5 M (14.4 M in total), same setting as in §4.1. We report the full results of varying training dataset size in <ref type="table" target="#tab_21">Table 10</ref>. In addition, <ref type="figure" target="#fig_0">Figure 15-18</ref> show averaged accuracy plots. Each plot is colorcoded in terms of each module, which helps to grasp the tendency of each module.</p><p>From the <ref type="table" target="#tab_21">Table 10</ref> and the plot of average of all models in <ref type="figure" target="#fig_0">Figure 15</ref>-18, we observe that the average of all 24 models tends to have higher accuracy with more data.</p><p>In <ref type="figure" target="#fig_0">Figure 15</ref>, we observe that the curves of without TPS do not get saturated at 100% training data size; more training data are certainly likely to improve them. The curves of TPS show saturated performances at 80% training data. We conjecture this is because TPS usually normalizes the input images and the last 20% of training dataset would be normalized by TPS, rather than improve accuracy. Thus other kinds of datasets, which will not simply be normalized by TPS trained with 80% training dataset, would be needed to better accuracy.  , as mentioned in §4.1. Note that, for each STR combination, we have run only one trial and thus the result could be slightly different from the <ref type="table" target="#tab_18">Table 8</ref>.</p><p>In <ref type="figure" target="#fig_0">Figure 16</ref>, we observe that the curves of ResNet do not get saturated at 100% training data size. The averages of VGG and RCNN, on the other hand, show saturated performances at 60% and 80% training data, respectively. We conjecture this is because VGG and RCNN have lower capacity than ResNet and they have already reached their performance limits at the current amount of training data.</p><p>In <ref type="figure" target="#fig_0">Figure 17</ref>, we observe that the curves of BiLSTM do not get saturated at 100% training data size. The curves of without BiLSTM show saturated performances at 80% training data. We conjecture this is because using BiLSTM has higher capacity than without BiLSTM and thus using BiLSTM still has room for improving accuracy with more training data.</p><p>In <ref type="figure" target="#fig_0">Figure 18</ref>, we observe that the curves of Attn do not get saturated at 100% training data size. The curves of CTC show saturated performances at 80% training data. Again, we conjecture this is because using Attn has higher capacity than CTC and thus using Attn still has room for improving accuracy with more training data.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3. Evaluation on COCO-Text dataset</head><p>We have evaluated the models on COCO-Text dataset <ref type="bibr" target="#b26">[27]</ref>, another good benchmark derived from MS COCO containing complex and low-resolution scene images. COCO-Text contains many special characters, heavy noises, and occlusions; it is generally considered more challenging than the seven benchmarks considered so far. <ref type="figure" target="#fig_0">Figure 19</ref> shows the accuracy-time and accuracy-space trade-off plots for 24 STR methods on COCO-Text. Except that the overall accuracy is lower, the relative orders amongst methods are largely preserved compared to <ref type="figure" target="#fig_4">Figure 4</ref>. Fine-tuning models with COCO-Text training set has improved the averaged accuracy (24 models) from 42.4% to 58.2%, a relatively big jump that is attributable to the unusual data distribution for COCO-Text. Evaluation and analysis over COCO-Text are beneficial, especially to address remaining corner cases for STR. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Samples of MJSynth and SynthText used as training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Examples of regular</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Visualization of an example flow of scene text recognition. We decompose a model into four stages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>-VGG-BiLSTM-CTC RARE: TPS-VGG-BiLSTM-Attn R2AM: None-RCNN-None-Attn GRCNN: None-RCNN-BiLSTM-CTC</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Two types of trade-offs exhibited by STR module combinations. Stars indicate previously proposed models and circular dots represent new module combinations evaluated by our framework. Red solid curves indicate the trade-off frontiers found among the combinations. Tables under each plot describe module combinations and their performance on the tradeoff frontiers. Modules in bold denote those that have been changed from the combination directly before it; those modules improve performance over the previous combination while minimizing the added time or memory cost. 0 5 10 15 20 25 30</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Color-coded version ofFigure 4, according to the prediction (left) and feature extraction (right) modules. They are identified as the most significant factors for speed and memory, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Samples of failure cases on all combinations of our framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Challenging examples for the STR combinations without a specific module. All STR combinations without the notated modules failed to recognize text in the examples, but upgrading the module solved the problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>7 missing examples of IC03-860 evaluation dataset. The examples represented by the red-colored word boxes of the two real scene images are included in IC03-867, but not in IC03-860.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Images that were filtered out of the IC15 evaluation dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 :</head><label>10</label><figDesc>Duplicated scene images. These are example images that have been found in both the IC03 training dataset and the IC13 evaluation dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>The coordinates are denoted by C = [c 1 , . . . , c F ] ∈ R 2×F , whose f -th column c f = [x f , y f ] contains the coordinates of the f -th fiducial point.C represents pre-defined top and bottom locations on the normalized image,X.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>(l) t , by using the two identified</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>.</head><label></label><figDesc>The dimensions of all hidden states including the FC layer was set as 256.None indicates not to use any Seq. modules upon the output of the Feat. modules, that is, H = V .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 11 -</head><label>11</label><figDesc><ref type="bibr" target="#b13">14</ref>, all the combination are color-coded in terms of each module, which helps to grasp the effectiveness of each module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 12 :</head><label>12</label><figDesc>Color-coded version ofFigure 4in §4.3, according to the feature extraction stage. Each circle represents the performance for each different combination of STR modules, while the each cross represents the average performance among STR combinations using VGG (green), RCNN (orange), or ResNet (violet). VGG gives the lowest accuracy on average for the lowest amount of inference time required, while RCNN achieves higher accuracy over VGG by taking the longest time for inferencing and the lowest memory usage out of the three. ResNet, exhibits the highest accuracy at the cost of using significantly more memory than the other modules. In summary, if the system to be implemented is constrained by memory, RCNN offers the best trade-off, and if the system requires high accuracy, ResNet should be used. The time difference between the three modules are so small in practice that it should be considered only in the most extreme of circumstances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 13 :</head><label>13</label><figDesc>Color-coded version ofFigure 4in §4.3, according to the sequence modeling stage. Each circle represents the performance for each different combination of STR modules, while the each cross represents the average performance among STR combinations without BiLSTM (gray) or with BiLSTM (gold). Using BiLSTM seems to have a similar effect to using TPS, and vice versa, except BiLSTM gives a larger accuracy boost on average with similar inference time or parameter size concessions compared to TPS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 14 :</head><label>14</label><figDesc>Color-coded version of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 15 :</head><label>15</label><figDesc>Averaged accuracy with varying training dataset size. Each plot represents the average performance among STR combinations without TPS (brown), with TPS (magenta), or all models (black)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 16 :</head><label>16</label><figDesc>Averaged accuracy with varying training dataset size. Each plot represents the average performance among STR combinations using VGG (green), RCNN (orange), ResNet (violet), or all models (black)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 17 :</head><label>17</label><figDesc>Averaged accuracy with varying training dataset size. Each plot represents the average performance among STR combinations without BiLSTM (gray), with BiLSTM (gold), or all models (black)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 18 :</head><label>18</label><figDesc>Averaged accuracy with varying training dataset size. Each plot represents the average performance among STR combinations with CTC (cyan), with Attn (blue), or all models (black)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 19 :</head><label>19</label><figDesc>-VGG-BiLSTM-CTC RARE: TPS-VGG-BiLSTM-Attn R2AM: None-RCNN-None-Attn GRCNN: None-RCNN-BiLSTM-CTC Rosetta: None-ResNet-None-CTC STAR-Net:TPS-ResNet-BiLSTM-CTC COCO-Text accuracy version of Figure 4 in §4.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>ms/image ×10 6</figDesc><table><row><cell></cell><cell>CRNN [24]</cell><cell>2015 MJ</cell><cell cols="4">78.2 80.8 89.4 −</cell><cell cols="3">− 86.7 −</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>160</cell><cell>8.3</cell></row><row><cell></cell><cell>RARE [25]</cell><cell>2016 MJ</cell><cell cols="6">81.9 81.9 90.1 − 88.6 −</cell><cell>−</cell><cell cols="3">− 71.8 59.2</cell><cell>&lt;2</cell><cell>−</cell></row><row><cell></cell><cell>R2AM [16]</cell><cell>2016 MJ</cell><cell cols="4">78.4 80.7 88.7 −</cell><cell cols="3">− 90.0 −</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>2.2</cell><cell>−</cell></row><row><cell>Reported results</cell><cell cols="12">STAR-Net [18] 2016 MJ+PRI GRCNN [28] 2017 MJ ATR [30] 2017 PRI+C FAN [4] 2017 MJ+ST+C 87.4 85.9 − 94.2 − 93.3 70.6 − 83.3 83.6 89.9 − − 89.1 − − 73.5 − 80.8 81.5 91.2 − − − − − − − − − − − − − − − 75.8 69.3 − − Char-Net [17] 2018 MJ 83.6 84.4 91.5 − 90.8 − − 60.0 73.5 − AON [5] 2018 MJ+ST 87.0 82.8 − 91.5 − − − 68.2 73.0 76.8</cell><cell>− − − − − −</cell><cell>− − − − − −</cell></row><row><cell></cell><cell>EP [2]</cell><cell>2018 MJ+ST</cell><cell cols="8">88.3 87.5 − 94.6 − 94.4 73.9 −</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell></row><row><cell></cell><cell>Rosetta [3]</cell><cell>2018 PRI</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell></row><row><cell></cell><cell>SSFL [19]</cell><cell>2018 MJ</cell><cell cols="6">89.4 87.1 − 94.7 94.0 −</cell><cell>−</cell><cell cols="3">− 73.9 62.5</cell><cell>−</cell><cell>−</cell></row><row><cell>Our experiment</cell><cell cols="2">CRNN [24] RARE [25] R2AM [16] STAR-Net [18] 2016 MJ+ST 2015 MJ+ST 2016 MJ+ST 2016 MJ+ST GRCNN [28] 2017 MJ+ST Rosetta [3] 2018 MJ+ST Our best model MJ+ST</cell><cell cols="10">82.9 81.6 93.1 92.6 91.1 89.2 69.4 64.2 70.0 65.5 86.2 85.8 93.9 93.7 92.6 91.1 74.5 68.9 76.2 70.4 83.4 82.4 92.2 92.0 90.2 88.1 68.9 63.6 72.1 64.9 87.0 86.9 94.4 94.0 92.8 91.5 76.1 70.3 77.5 71.7 84.2 83.7 93.5 93.0 90.9 88.8 71.4 65.8 73.6 68.1 84.3 84.7 93.4 92.9 90.9 89.0 71.2 66.0 73.8 69.2 87.9 87.5 94.9 94.4 93.6 92.3 77.6 71.8 79.2 74.0</cell><cell>4.4 23.6 24.1 10.9 10.7 4.7 27.6</cell><cell>8.3 10.8 2.9 48.7 4.6 44.3 49.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4a ,</head><label>4a</label><figDesc>T1 takes the minimum time by not including any transformation or sequential module. Moving from T1 to T5, the following modules are introduced in order (indicated as bold): ResNet, BiLSTM, TPS, and Attn. Note that from T1 to T5, a single module changes at a time. Our framework provides a smooth shift of methods that gives the least performance trade-off depending on the application scenario. They sequentially increase the complexity of the overall STR model, resulting in increased performance at the cost of computational efficiency. ResNet, BiLSTM, and TPS introduce relatively moderate overall slow down (1.3ms→10.9ms), while greatly boosting accuracy (69.5%→82.9%). The final change, Attn, on the other hand, only improves the accuracy by 1.1% at a huge cost in efficiency(27.6 ms).As for the accuracy-memory trade-offs shown in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 2 :</head><label>2</label><figDesc>Study of modules at the four stages with respect to total accuracy, inference time, and the number of parameters. The accuracies are acquired by taking the mean of the results of the combinations including that module. The inference time and the number of parameters are measured individually.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 3 :</head><label>3</label><figDesc>Sanity checking our experimental platform by reproducing the existing four STR models: CRNN [</figDesc><table><row><cell>24], RARE [25],</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 4 :</head><label>4</label><figDesc>Architecture of the localization network in TPS. The localization network extracts the location of the text line, that is, the xand y-coordinates of the fiducial points F within the input image.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 .</head><label>7</label><figDesc>The output of ResNet is 512 channels ×</figDesc><table><row><cell>Layers</cell><cell cols="2">Configurations</cell><cell>Output</cell></row><row><cell>Input</cell><cell cols="2">grayscale image</cell><cell>100 × 32</cell></row><row><cell>Conv1</cell><cell>c: 64</cell><cell cols="2">k: 3 × 3 100 × 32</cell></row><row><cell>Pool1</cell><cell cols="2">k: 2 × 2 s: 2 × 2</cell><cell>50 × 16</cell></row><row><cell>Conv2</cell><cell>c: 128</cell><cell cols="2">k: 3 × 3 50 × 16</cell></row><row><cell>Pool2</cell><cell cols="2">k: 2 × 2 s: 2 × 2</cell><cell>25 × 8</cell></row><row><cell>Conv3</cell><cell>c: 256</cell><cell>k: 3 × 3</cell><cell>25 × 8</cell></row><row><cell>Conv4</cell><cell>c: 256</cell><cell>k: 3 × 3</cell><cell>25 × 8</cell></row><row><cell>Pool3</cell><cell cols="2">k: 1 × 2 s: 1 × 2</cell><cell>25 × 4</cell></row><row><cell>Conv5</cell><cell>c: 512</cell><cell>k: 3 × 3</cell><cell>25 × 4</cell></row><row><cell>BN1</cell><cell>-</cell><cell></cell><cell>25 × 4</cell></row><row><cell>Conv6</cell><cell>c: 512</cell><cell>k: 3 × 3</cell><cell>25 × 4</cell></row><row><cell>BN2</cell><cell>-</cell><cell></cell><cell>25 × 4</cell></row><row><cell>Pool4</cell><cell cols="2">k: 1 × 2 s: 1 × 2</cell><cell>25 × 2</cell></row><row><cell>Conv7</cell><cell cols="2">c: 512 s: 1 × 1 p: 0 × 0 k: 2 × 2</cell><cell>24 × 1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 5 :</head><label>5</label><figDesc>Architecture of VGG.</figDesc><table><row><cell>Layers</cell><cell cols="2">Configurations</cell><cell>Output</cell></row><row><cell>Input</cell><cell cols="2">grayscale image</cell><cell>100 × 32</cell></row><row><cell>Conv1</cell><cell>c: 64</cell><cell>k: 3 × 3</cell><cell>100 × 32</cell></row><row><cell>Pool1</cell><cell>k: 2 × 2</cell><cell>s: 2 × 2</cell><cell>50 × 16</cell></row><row><cell>GRCL1</cell><cell cols="2">c :64, k :3 × 3 × 5</cell><cell>50 × 16</cell></row><row><cell>Pool2</cell><cell>k: 2 × 2</cell><cell>s: 2 × 2</cell><cell>25 × 8</cell></row><row><cell>GRCL2</cell><cell cols="2">c :128, k :3 × 3 × 5</cell><cell>25 × 8</cell></row><row><cell>Pool3</cell><cell>k: 2 × 2 s: 1 × 2</cell><cell>p: 1 × 0</cell><cell>26 × 4</cell></row><row><cell>GRCL3</cell><cell cols="2">c :256, k :3 × 3 × 5</cell><cell>26 × 4</cell></row><row><cell>Pool4</cell><cell>k: 2 × 2 s: 1 × 2</cell><cell>p: 1 × 0</cell><cell>27 × 2</cell></row><row><cell>Conv2</cell><cell>c: 512 s: 1 × 1</cell><cell>k: 3 × 3 p: 0 × 0</cell><cell>26 × 1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 6 :</head><label>6</label><figDesc>Architecture of RCNN.</figDesc><table><row><cell>26 columns.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 7 :</head><label>7</label><figDesc>Architecture of ResNet.</figDesc><table /><note>hidden states, h</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>CRNN. 2 R2AM. 3 GRCNN. 4 Rosetta. 5 RARE. 6 STAR-Net. 7 our best model.</figDesc><table><row><cell>#</cell><cell>Trans.</cell><cell>Feat.</cell><cell>Seq.</cell><cell>Pred.</cell><cell>IIIT SVT 3000 647</cell><cell cols="3">IC03 860 867 857 1015 1811 2077 645 288 Total IC13 IC15 SP CT Acc. Time params FLOPS ms ×10 6 ×10 9</cell></row><row><cell>1 2 3 1 4</cell><cell></cell><cell>VGG</cell><cell>None BiLSTM</cell><cell cols="3">CTC 76.2 73.8 86.7 86.0 84.8 81.9 56.6 52.4 56.6 49.9 69.5 Attn 80.1 78.4 91.0 90.5 88.5 86.3 63.0 58.3 66.0 56.1 74.6 CTC 82.9 81.6 93.1 92.6 91.1 89.2 69.4 64.2 70.0 65.5 78.4 Attn 84.3 83.8 93.7 93.1 91.9 90.0 70.8 65.4 71.9 66.8 79.7</cell><cell>1.3 19.0 4.4 21.2</cell><cell>5.6 6.6 8.3 9.1</cell><cell>1.2 1.6 1.4 1.6</cell></row><row><cell>5 6 2 7 3 8</cell><cell>None</cell><cell>RCNN</cell><cell>None BiLSTM</cell><cell cols="3">CTC 80.9 78.5 90.5 89.8 88.4 85.9 65.1 60.5 65.8 60.3 75.4 Attn 83.4 82.4 92.2 92.0 90.2 88.1 68.9 63.6 72.1 64.9 78.5 CTC 84.2 83.7 93.5 93.0 90.9 88.8 71.4 65.8 73.6 68.1 79.8 Attn 85.7 84.8 93.9 93.4 91.6 89.6 72.7 67.1 75.0 69.2 81.0</cell><cell>7.7 24.1 10.7 27.4</cell><cell>1.9 2.9 4.6 5.5</cell><cell>1.6 2.0 1.8 2.0</cell></row><row><cell>9 4 10 11 12</cell><cell></cell><cell>ResNet</cell><cell>None BiLSTM</cell><cell cols="3">CTC 84.3 84.7 93.4 92.9 90.9 89.0 71.2 66.0 73.8 69.2 80.0 Attn 86.1 85.7 94.0 93.6 91.9 90.1 73.5 68.0 74.5 72.2 81.5 CTC 86.2 86.0 94.4 94.1 92.6 90.8 73.6 68.0 76.0 72.2 81.9 Attn 86.6 86.2 94.1 93.7 92.8 91.0 75.6 69.9 76.4 72.6 82.5</cell><cell>4.7 22.2 7.8 25.0</cell><cell>44.3 45.3 47.0 47.9</cell><cell>10.1 10.5 10.3 10.5</cell></row><row><cell>13 14 15 16 5</cell><cell></cell><cell>VGG</cell><cell>None BiLSTM</cell><cell cols="3">CTC 80.0 78.0 90.1 89.7 88.7 87.5 65.1 60.6 65.5 57.0 75.1 Attn 82.9 82.3 92.0 91.7 90.5 89.2 69.4 64.2 73.0 62.2 78.5 CTC 84.6 83.8 93.3 92.9 91.2 89.4 72.4 66.8 74.0 66.8 80.2 Attn 86.2 85.8 93.9 93.7 92.6 91.1 74.5 68.9 76.2 70.4 82.0</cell><cell>4.8 21.0 7.6 23.6</cell><cell>7.3 8.3 10.0 10.8</cell><cell>1.6 2.0 1.8 2.0</cell></row><row><cell>17 18 19 20</cell><cell>TPS</cell><cell>RCNN</cell><cell>None BiLSTM</cell><cell cols="3">CTC 82.8 81.7 92.0 91.6 89.5 88.4 69.8 64.6 71.3 61.2 78.3 Attn 85.1 84.0 93.1 93.1 91.5 90.2 72.4 66.8 75.6 64.9 80.6 CTC 85.1 84.3 93.5 93.1 91.4 89.6 73.4 67.7 74.4 69.1 80.8 Attn 86.3 85.7 94.0 94.0 92.8 91.1 75.0 69.2 77.7 70.1 82.3</cell><cell>10.9 26.4 14.1 30.1</cell><cell>3.6 4.6 6.3 7.1</cell><cell>1.9 2.3 2.1 2.3</cell></row><row><cell>21 22 23 6 24 7</cell><cell></cell><cell>ResNet</cell><cell>None BiLSTM</cell><cell cols="3">CTC 85.0 85.7 94.0 93.6 92.5 90.8 74.6 68.8 75.2 71.0 81.5 Attn 87.1 87.1 94.3 93.9 93.2 91.8 76.5 70.6 78.9 73.2 83.3 CTC 87.0 86.9 94.4 94.0 92.8 91.5 76.1 70.3 77.5 71.7 82.9 Attn 87.9 87.5 94.9 94.4 93.6 92.3 77.6 71.8 79.2 74.0 84.0</cell><cell>8.3 25.6 10.9 27.6</cell><cell>46.0 47.0 48.7 49.6</cell><cell>10.5 10.9 10.7 10.9</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 8 :</head><label>8</label><figDesc>The full experimental results for all 24 STR combinations. Top accuracy for each benchmark is shown in bold. For each STR combination, we have run five trials with different initialization random seeds and have averaged their accuracies.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head></head><label></label><figDesc>None VGG None CTC 66.1 68.1 69.2 68.8 68.8 2 Attn 71.5 73.0 74.2 74.7 74.6 3 1 BiLSTM CTC 75.8 77.6 77.7 77.9 78.6 4 Attn 75.6 77.9 79.3 79.3 79.7 5 CRNN. 2 R2AM. 3 GRCNN. 4 Rosetta. 5 RARE. 6 STAR-Net. 7 our best model.</figDesc><table><row><cell>#</cell><cell>Trans.</cell><cell>Feat.</cell><cell>Seq.</cell><cell>Pred.</cell><cell>20</cell><cell>Training dataset size (%) 40 60 80</cell><cell>100</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>6 2 7 3 8</cell><cell></cell><cell>RCNN</cell><cell>None BiLSTM</cell><cell cols="4">CTC 69.7 71.2 72.0 75.5 74.7 Attn 76.2 77.5 77.3 78.1 78.2 CTC 77.1 78.8 79.6 80.0 79.7 Attn 77.9 79.6 80.3 80.5 81.3</cell></row><row><cell>9 4 10 11 12</cell><cell></cell><cell>ResNet</cell><cell>None BiLSTM</cell><cell cols="4">CTC 75.9 77.8 78.8 78.9 80.7 Attn 78.0 80.3 80.5 81.6 81.5 CTC 78.9 80.7 80.8 81.3 81.7 Attn 79.2 81.0 81.9 82.3 82.6</cell></row><row><cell>13 14 15 16 5</cell><cell></cell><cell>VGG</cell><cell>None BiLSTM</cell><cell cols="4">CTC 73.8 74.9 75.4 75.5 75.2 Attn 75.9 78.3 78.8 78.5 78.7 CTC 77.9 79.2 79.9 79.5 80.1 Attn 79.6 81.1 81.7 82.0 81.9</cell></row><row><cell>17 18 19 20</cell><cell>TPS</cell><cell>RCNN</cell><cell>None BiLSTM</cell><cell cols="4">CTC 77.8 78.5 76.8 78.6 78.1 Attn 79.2 79.8 80.5 80.4 80.7 CTC 78.7 80.7 81.2 80.8 80.9 Attn 80.4 81.8 82.2 82.5 83.1</cell></row><row><cell>21 22 23 6 24 7</cell><cell></cell><cell>ResNet</cell><cell>None BiLSTM</cell><cell cols="4">CTC 80.7 80.7 80.8 81.7 81.9 Attn 80.7 81.6 82.6 83.0 83.3 CTC 80.7 81.8 82.6 83.0 83.2 Attn 81.3 82.7 83.2 84.0 84.1</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 10 :</head><label>10</label><figDesc>The full experimental results of varying training dataset size for all 24 STR combinations. Each value represent Total accuracy (%)</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/clovaai/ deep-text-recognition-benchmark/issues/125</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>The authors would like to thank Jaeheung Surh for helpful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Edit probability for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanzhan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuigeng</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rosetta: Large scale system for text detection and recognition in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fedor</forename><surname>Borisyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viswanath</forename><surname>Sivakumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="71" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Focusing attention: Towards accurate text recognition in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanzhan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunlu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuigeng</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Aon: Towards arbitrarily-oriented text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanzhan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangliu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuigeng</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A novel connectionist system for unconstrained handwriting recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Bertolami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bunke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TPAMI</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Synthetic data for text localisation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankush</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Synthetic data and artificial neural networks for natural scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Deep Learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Icdar 2015 competition on robust reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimosthenis</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluis</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anguelos</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suman</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masakazu</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><forename type="middle">Ramaseshan</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1156" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimosthenis</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seiichi</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masakazu</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluis</forename><surname>Gomez I Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Sergi Robles Mestre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mas</surname></persName>
		</author>
		<title level="m">Icdar 2013 robust reading competition. In ICDAR</title>
		<meeting><address><addrLine>David Fernandez Mota, Jon Almazan Almazan, and Lluis Pere De Las Heras</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1484" to="1493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongjoo</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heungseok</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soeun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunwoo</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngil</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngkwan</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.09957</idno>
		<title level="m">Meet the mlaas platform with a real-world case study</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recursive recurrent nets with attention modeling for ocr in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="2231" to="2239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Charnet: A character-aware neural network for distorted scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaofeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwan-Yee K</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Star-net: A spatial attention residue network for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaofeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kwan-Yee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Synthetically supervised feature learning for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Wassell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Icdar 2003 robust reading competitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Panaretos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Sosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirley</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="682" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Scene text recognition using higher order language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anand</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recognizing text with perspective distortion in natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Palaiahnakote</forename><surname>Trung Quy Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangxuan</forename><surname>Shivakumara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chew Lim</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="569" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A robust arbitrary text detection system for natural scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anhar</forename><surname>Risnumawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Palaiahankote</forename><surname>Shivakumara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chew Lim</forename><surname>Chee Seng Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESWA</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="8027" to="8048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TPAMI</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Robust scene text recognition with automatic rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengyuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Coco-text: Dataset and benchmark for text detection and recognition in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Matera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.07140</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Gated recurrent convolution neural network for ocr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">End-to-end scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1457" to="1464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to read irregular text with attention mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dafang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lee</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Adadelta: an adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
