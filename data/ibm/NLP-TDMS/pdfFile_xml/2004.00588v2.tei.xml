<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Better Sign Language Translation with STMC-Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kayo</forename><surname>Yin</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Read</surname></persName>
							<email>jesse.read@polytechnique.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">LIX, Ecole Polytechnique Institut Polytechnique de Paris</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Better Sign Language Translation with STMC-Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sign Language Translation (SLT) first uses a Sign Language Recognition (SLR) system to extract sign language glosses from videos. Then, a translation system generates spoken language translations from the sign language glosses. This paper focuses on the translation system and introduces the STMC-Transformer which improves on the current state-of-the-art by over 5 and 7 BLEU respectively on gloss-to-text and video-to-text translation of the PHOENIX-Weather 2014T dataset. On the ASLG-PC12 corpus, we report an increase of over 16 BLEU.</p><p>We also demonstrate the problem in current methods that rely on gloss supervision. The videoto-text translation of our STMC-Transformer outperforms translation of GT glosses. This contradicts previous claims that GT gloss translation acts as an upper bound for SLT performance and reveals that glosses are an inefficient representation of sign language. For future SLT research, we therefore suggest an end-to-end training of the recognition and translation models, or using a different sign language annotation scheme.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Communication holds a central position in our daily lives and social interactions. Yet, in a predominantly aural society, sign language users are often deprived of effective communication. Deaf people face daily issues of social isolation and miscommunication to this day <ref type="bibr">(Souza et al., 2017)</ref>. This paper is motivated to provide assistive technology that allow Deaf people to communicate in their own language.</p><p>In general, sign languages developed independently of spoken language and do not share the grammar of their spoken counterparts <ref type="bibr" target="#b36">(Stokoe, 1960)</ref>. For this, Sign Language Recognition (SLR) systems on their own cannot capture the underlying grammar and complexities of sign language, and Sign Language Translation (SLT) faces the additional challenge of taking into account the unique linguistic features during translation. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, current SLT approaches involve two steps. First, a tokenization system generates glosses from sign language videos. Then, a translation system translates the recognized glosses into spoken language. Recent work <ref type="bibr" target="#b26">(Orbay and Akarun, 2020;</ref><ref type="bibr" target="#b42">Zhou et al., 2020)</ref> has addressed the first step, but there has been none improving the translation system. This paper aims to fill this research gap by leveraging recent success in Neural Machine Translation (NMT), namely Transformers.</p><p>Another limit to current SLT models is that they use glosses as an intermediate representation of sign language. We show that having a perfect continuous SLR system will not necessarily improve SLT results. We introduce the STMC-Transformer model performing video-to-text translation that surpasses translation of ground truth glosses, which reveals that glosses are a flawed representation of sign language.</p><p>The contributions of this paper can be summarized as:</p><p>1. A novel STMC-Transformer model for video-to-text translation surpassing GT glosses translation contrary to previous assumptions 2. The first successful application of Transformers to SLT achieving state-of-the-art results in both gloss to text and video to text translation on PHOENIX-Weather 2014T and ASLG-PC12 datasets 3. The first usage of weight tying, transfer learning, and ensemble learning in SLT and a comprehensive series of baseline results with Transformers to underpin future research</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>Despite considerable advancements made in machine translation (MT) between spoken languages, sign language processing falls behind for many reasons. Unlike spoken language, sign language is a multidimensional form of communication that relies on both manual and non-manual cues which presents additional computer vision challenges <ref type="bibr" target="#b1">(Asteriadis et al., 2012)</ref>. These cues may occur simultaneously whereas spoken language follows a linear pattern where words are processed one at a time. Signs also vary in both space and time and the number of video frames associated to a single sign is not fixed either.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Sign Language Glossing</head><p>Glossing corresponds to transcribing sign language word-for-word by means of another written language. Glosses differ from translation as they merely indicate what each part in a sign language sentence mean, but do not form an appropriate sentence in the spoken language. While various sign language corpus projects have provided different guidelines for gloss annotation <ref type="bibr" target="#b9">(Crasborn et al., 2007;</ref><ref type="bibr" target="#b15">Johnston, 2013)</ref>, there is no universal standard which hinders the easy exchange of data between projects and consistency between different sign language corpora. Gloss annotations are also an imprecise representation of sign language and can lead to an information bottleneck when representing the multi-channel sign language by a single-dimensional stream of glosses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Sign Language Recognition</head><p>SLR consists of identifying isolated single signs from videos. Continuous sign language recognition (CSLR) is a relatively more challenging task that identifies a sequence of running glosses from a running video. Works in SLR and CSLR, however, only perform visual recognition and ignore the underlying linguistic features of sign language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Sign Language Translation</head><p>As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, the SLT system takes CSLR as a first step to tokenize the input video into glosses. Then, an additional step translates the glosses into a valid sentence in the target language.</p><p>SLT is novel and difficult compared to other translation problems because it involves two steps: extract meaningful features from a video of a multi-cue language accurately then generate translations from an intermediate gloss representation, instead of translation from the source language directly. 3 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sign Language Recognition</head><p>Early approaches for SLR rely on hand-crafted features <ref type="bibr" target="#b38">(Tharwat et al., 2014;</ref><ref type="bibr" target="#b40">Yang, 2010)</ref> and use Hidden Markov Models <ref type="bibr" target="#b11">(Forster et al., 2013)</ref> or Dynamic Time Warping <ref type="bibr" target="#b22">(Lichtenauer et al., 2008)</ref> to model sequential dependencies. More recently, 2D convolutional neural networks (2D-CNN) and 3D convolutional neural networks (3D-CNN) effectively model spatio-temporal representations from sign language videos <ref type="bibr" target="#b10">(Cui et al., 2017;</ref><ref type="bibr" target="#b25">Molchanov et al., 2016)</ref>. Most existing work on CSLR divides the task into three sub-tasks: alignment learning, single-gloss SLR, and sequence construction <ref type="bibr" target="#b41">Zhang et al., 2014</ref>) while others perform the task in an end-to-end fashion using deep learning <ref type="bibr" target="#b14">(Huang et al., 2015;</ref><ref type="bibr" target="#b5">Camgoz et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sign Language Translation</head><p>SLT was formalized in <ref type="bibr" target="#b6">Camgoz et al. (2018)</ref> where they introduce the PHOENIX-Weather 2014T dataset and jointly use a 2D-CNN model to extract gloss-level features from video frames, and a seq2seq model to perform German sign language translation. Subsequent works on this dataset <ref type="bibr" target="#b26">(Orbay and Akarun, 2020;</ref><ref type="bibr" target="#b42">Zhou et al., 2020)</ref> all focus on improving the CSLR component in SLT. A contemporaneous paper <ref type="bibr" target="#b7">(Camgoz et al., 2020)</ref> also obtains encouraging results with multi-task Transformers for both tokenization and translation, however their CSLR performance is sub-optimal, with a higher Word Error Rate than baseline models.</p><p>Similar work has been done on Korean sign language by <ref type="bibr" target="#b20">Ko et al. (2019)</ref> where they estimate human keypoints to extract glosses, then use seq2seq models for translation. <ref type="bibr" target="#b0">Arvanitis et al. (2019)</ref> use seq2seq models to translate ASL glosses of the ASLG-PC12 dataset <ref type="bibr" target="#b27">(Othman and Jemni, 2012)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Neural Machine Translation</head><p>Neural Machine Translation (NMT) employs neural networks to carry out automated text translation. Recent methods typically use an encoder-decoder architecture, also known as seq2seq models.</p><p>Earlier approaches use recurrent <ref type="bibr" target="#b16">(Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b37">Sutskever et al., 2014)</ref> and convolutional networks <ref type="bibr" target="#b17">(Kalchbrenner et al., 2016;</ref><ref type="bibr" target="#b12">Gehring et al., 2017)</ref> for the encoder and the decoder. However, standard seq2seq networks are unable to model long-term dependencies in large input sentences without causing an information bottleneck. To address this issue, recent works use attention mechanisms <ref type="bibr" target="#b2">(Bahdanau et al., 2015;</ref><ref type="bibr" target="#b24">Luong et al., 2015)</ref> that calculates context-dependent alignment scores between encoder and decoder hidden states. <ref type="bibr" target="#b39">Vaswani et al. (2017)</ref> introduces the Transformer, a seq2seq model relying on self-attention that obtains state-of-the-art results in NMT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Model architecture</head><p>For translation from videos to text, we propose the STMC-Transformer network illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Spatial-Temporal Multi-Cue (STMC) Network</head><p>Our work is the first to use STMC networks <ref type="bibr" target="#b42">(Zhou et al., 2020)</ref> for SLT. A spatial multi-cue (SMC) module with a self-contained pose estimation branch decomposes the input video into spatial features of multiple visual cues (face, hand, full-frame and pose). Then, a temporal multi-cue (TMC) module with stacked TMC blocks and temporal pooling (TP) layers calculates temporal correlations within (inter-cue) and between cues (intra-cue) at different time steps, which preserves each unique cue while exploring their relation at the same time. The inter-cue and intra-cue features are each analyzed by Bi-directional Long Short-Term Memory (BiLSTM) <ref type="bibr" target="#b37">(Sutskever et al., 2014)</ref> and Connectionist Temporal Classification (CTC) <ref type="bibr" target="#b13">(Graves et al., 2006)</ref> units for sequence learning and inference.</p><p>This architecture efficiently processes multiple visual cues from sign language video in collaboration with each other, and achieves state-of-the-art performance on three SLR benchmarks. On the PHOENIX-Weather 2014T dataset, it achieves a Word Error Rate of 21.0 for the SLR task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Transformer</head><p>For translation, we train a two-layered Transformer to maximize the log-likelihood</p><formula xml:id="formula_0">(x i ,y i )∈D log P (y i |x i , θ)</formula><p>where D contains gloss-text pairs (x i , y i ).</p><p>Two layers, compared to six in most spoken language translation, is empirically shown to be optimal in Section 6.1, likely because our datasets are limited in size. We refer to the original Transformer paper <ref type="bibr" target="#b39">(Vaswani et al., 2017)</ref> for more architecture details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Datasets</head><p>German Sign Gloss German American Sign Gloss English  PHOENIX-Weather 2014T <ref type="bibr" target="#b6">(Camgoz et al., 2018)</ref> This dataset is extracted from weather forecast airings of the German tv station PHOENIX. This dataset consists of a parallel corpus of German sign language videos from 9 different signers, gloss-level annotations with a vocabulary of 1,066 different signs and translations into German spoken language with a vocabulary of 2,887 different words. It contains 7,096 training pairs, 519 development and 642 test pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ASLG-PC12 (Othman and Jemni, 2012)</head><p>This dataset is constructed from English data of Project Gutenberg that has been transformed into ASL glosses following a rule-based approach. This corpus with 87,709 training pairs allows us to evaluate Transformers on a larger dataset, where deep learning models usually require lots of data. It also allows us to compare performance across different sign languages. However, the data is limited since it does not contain sign language videos, and is less complex due to being created semi-automatically. We make our data and code publicly available 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments and Discussions</head><p>Our models are built using PyTorch <ref type="bibr" target="#b29">(Paszke et al., 2019)</ref> and Open-NMT <ref type="bibr" target="#b19">(Klein et al., 2017)</ref>. We configure Transformers with word embedding size 512, gloss level tokenization, sinusoidal positional encoding, 2,048 hidden units and 8 heads. For optimization, we use Adam <ref type="bibr" target="#b18">(Kingma and Ba, 2014)</ref> with β 1 = 0.9, β 2 = 0.998, Noam learning rate schedule, 0.1 dropout, and 0.1 label smoothing. We evaluate on the dev set each half-epoch and employ early stopping with patience 5. During decoding, generated unk tokens are replaced by the source token having the highest attention weight. This is useful when unk symbols correspond to proper nouns that can be directly transposed between languages <ref type="bibr" target="#b19">(Klein et al., 2017)</ref>. We perform a series of experiments to find the optimal setup for this novel application. We equally experiment with various techniques often used in classic NMT to SLT such as transfer learning, weight tying and ensembling to improve model performance.</p><p>For evaluation we use BLEU <ref type="bibr" target="#b28">(Papineni et al., 2002)</ref>, ROUGE <ref type="bibr" target="#b23">(Lin, 2004)</ref> and METEOR <ref type="bibr" target="#b3">(Banerjee and Lavie, 2005)</ref>. For BLEU, we report BLEU-1,2,3,4 scores and as ROUGE score we report the ROUGE-L F1 score. These metrics allow us to directly compare directly with previous works. METEOR is calculated in addition as it demonstrates higher correlation with human evaluation than BLEU on several MT tasks. All reported results unless otherwise specified are averaged over 10 runs with different random seeds.</p><p>We organize our experiments into two groups:</p><p>1. Gloss2Text (G2T) in which we translate GT gloss annotations to simulate perfect tokenization on both PHOENIX-Weather 2014T and ASLG-PC12 2. Sign2Gloss2Text (S2G2T) where we perform video-to-text translation on PHOENIX-Weather 2014T with the STMC-Transformer</p><formula xml:id="formula_1">6.1 Gloss2Text (G2T)</formula><p>G2T is a text-to-text translation task that is novel and challenging compared to classic translation tasks between spoken languages because of the high linguistic variance between source and target sentences, scarcity of resources, and information loss or imprecision in the source sentence itself.</p><p>For ASLG-PC12, many ASL glosses are English words with an added prefix so during data preprocessing we remove all such prefixes. We also set all glosses that appear less than 5 times during training as unk to reduce vocabulary size.  <ref type="table">Table 2</ref>: Statistics of the ASLG-PC12 dataset before and after preprocessing. <ref type="table">Table 2</ref> shows that the source and target corpora in ASLG-PC12 are more similar to each other with many shared vocabulary and a relatively high BLEU-4 score on raw data. This allows us to compare Transformer performance on a larger and less challenging dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model size</head><p>The original Transformer in <ref type="bibr" target="#b39">(Vaswani et al., 2017)</ref> uses 6 layers for the encoder and decoder for NMT. However, our task differs from a standard MT task between two spoken languages so we first train Transformers with 1, 2, 4 and 6 encoder-decoder layers. Networks are trained with batch size 2,048 and initial learning rate 1.  To choose the best model, we mainly take into account BLEU-4 as it is currently the most widely used metric in MT. We do find that our final model outperforms the other models across all metrics. <ref type="table" target="#tab_4">Table  3</ref> shows that on PHOENIX-Weather 2014T, using 2 layers obtains the highest BLEU-4. Because our dataset is much smaller than spoken language datasets, larger networks may be disadvantaged. Moreover, a smaller model has the advantage of taking up less memory and computation time. Repeating the same experiment on ASLG-PC12, we also find 2 layers to be the optimal model size. ASLG-PC12 is larger but less complex which may also explain why smaller networks are more suitable. We carry out the rest of our experiments using 2 enc-dec layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Embedding schemes</head><p>Press and Wolf <ref type="formula">(2017)</ref> shows that tying the input and output embeddings while training language models may provide better performance. Our decoder is in fact a language model conditioned on the encoding of the source sentence and previous outputs, we can tie the decoder embeddings by using a shared weight matrix for the input and output word embeddings.</p><p>In addition, models are often initialized with pre-trained embeddings for transfer learning. These embeddings are typically trained in an unsupervised manner on a large corpus of text in the desired language. We perform experiments on PHOENIX-Weather 2014T using two popular word embeddings: GloVe 3 <ref type="bibr" target="#b30">(Pennington et al., 2014)</ref>, and fastText <ref type="bibr" target="#b4">(Bojanowski et al., 2017)</ref>. To the best of our knowledge, weight-tying or pre-trained embeddings have never been employed in SLT. <ref type="bibr">GloVe (de)</ref>    <ref type="table" target="#tab_6">Table 4</ref> shows there is only one matching token between German glosses and the pre-trained embeddings, while over 90% of the words in the German text appear in both pre-trained embeddings. We therefore initialize pre-trained embeddings on the decoder only, and keep random initialization for the encoder. The embedding layers are fine-tuned during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dev Set</head><p>Test Set   <ref type="table" target="#tab_8">Table 5</ref> shows that the new embedding schemes do not improve performance on PHOENIX-Weather 2014T. It may be because pre-trained embeddings are shown to be more effective when used on the encoding layer <ref type="bibr" target="#b34">(Qi et al., 2018)</ref>. Another possible reason is the difference between the domain of our dataset and of the corpus the embeddings were trained on. We therefore keep random initialization of word embeddings for experiments on PHOENIX-Weather 2014T. Using this setting, we run a parameter search over the learning rate and warm-up steps, and we use initial learning rate 0.5 with 3,000 warm-up steps for the remaining experiments. Details of the parameter search are included in Appendix A.1.</p><p>Both GloVe and fastText English vectors have a reasonable overlap with the vocabulary of ASL glosses as well as the English targets <ref type="table" target="#tab_6">(Table 4</ref>). Therefore on ASLG-PC12 we load pre-trained embeddings on only the decoder, as well as on both the encoder and decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dev Set</head><p>Test Set  <ref type="table">Table 6</ref>: G2T performance comparison using different embedding schemes on ASLG-PC12. <ref type="table">Table 6</ref> shows that fastText pre-trained embeddings for the decoder improves performance, and tied decoder embeddings with random initialization gives the best performance. Weight tying is more suited on this dataset likely because it acts as regularization and combats overfitting, while the previous dataset is more complex and therefore less prone to overfitting. For the remaining experiments, we use tied decoder embeddings, initial learning rate 0.2 and 8,000 warm-up steps. A naive method for decoding is greedy search, where the model simply chooses the word with the highest probability at each time step. However, this approach may become sub-optimal in the context of the entire sequence. Beam search addresses this by expanding all possible candidates at each time step and keeping a number of most likely sequences, or the beam width. Large beam widths do not always result in better performance and take more space in memory and decoding time. We search and find the optimal beam width value to be 4 on PHOENIX-Weather 2014T and 5 on ASLG-PC12.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Beam width</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ensemble decoding</head><p>Ensemble methods combine multiple models to improve performance. We propose ensemble decoding, where we combine the output of different models by averaging their prediction distributions. We chose 9 models from our experiments that gave the highest BLEU-4 during testing on PHOENIX-Weather 2014T. The number of models is chosen empirically, as using fewer models will lead to less ensembling but too many weaker models may lessen the quality of the ensemble model. These models are of the same architecture, but are initialized with different seeds and trained using different batch sizes and/or learning rates. These models give a BLEU-4 on testing between 22.92 and 23.41 individually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dev Set</head><p>Test Set    <ref type="table" target="#tab_11">7</ref> gives a performance comparison on PHOENIX-Weather 2014T of the recurrent seq2seq model by <ref type="bibr" target="#b6">Camgoz et al. (2018)</ref>, Transformer trained concurrently by <ref type="bibr" target="#b7">Camgoz et al. (2020)</ref>, our single model, and ensemble model. We also provide the scores on the gloss annotations to illustrate the difficulty of this task.</p><p>Without any additional training, ensembling improves testing performance by over 1 BLEU-4. Also, we report an improvement of over 5 BLEU-4 on the state-of-the-art. A single Transformer also gives an improvement of over 4 BLEU-4 more than the state-of-the-art, which shows the advantage of Transformers for SLT, as shown also in <ref type="bibr" target="#b7">Camgoz et al. (2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dev Set</head><p>Test Set  We also use 5 of the best models from our experiments on ASLG-PC12 in an ensemble. Individually, these models obtain between 81.72 and 82.41 BLEU-4 on testing. <ref type="table" target="#tab_14">Table 8</ref> shows that the performance of our single Transformer surpasses the recurrent seq2seq model by <ref type="bibr" target="#b0">Arvanitis et al. (2019)</ref> by over 16 BLEU-4. The ensemble model reports an improvement of 0.46 BLEU-4 over the single model. There is relatively less increase from ensembling possibly because there is less variance across different models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">German Sign2Gloss2Text (S2G2T)</head><p>In S2G2T, both gloss recognition from videos and its translation to text are performed automatically. <ref type="bibr" target="#b6">Camgoz et al. (2018)</ref> claims the previous G2T setup to be an upper bound for translation performance, since it simulates having a perfect recognition system. However, this claim assumes that the ground truth gloss annotations give a full understanding of sign language, which ignores the information bottleneck in glosses. <ref type="bibr" target="#b7">Camgoz et al. (2020)</ref> hypothesizes that it is therefore possible to surpass G2T performance without using GT glosses, which we confirm in this section.</p><p>We perform experiments on the PHOENIX-Weather 2014T dataset as it contains parallel video, gloss and text data. On the other hand, the ASLG-PC12 corpus does not have sign language video information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dev Set</head><p>Test Set  <ref type="table">Table 9</ref>: SLT performance using STMC for CSLR. The first set of rows correspond to the current stateof-the-arts included for comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2G →G2T</head><p>To begin, we use the best performing model for German G2T to translate glosses predicted by a trained STMC network. In <ref type="table">Table 9</ref> we can see that despite no additional training for translation, this model already obtains a relatively high score that beats the current state-of-the-art by over 5 BLEU-4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recurrent seq2seq networks</head><p>For comparison, we also train and evaluate STMC used with recurrent seq2seq networks for translation. The translation models are composed of four stacked layers of Gated Recurrent Units (GRU) <ref type="bibr" target="#b8">(Chung et al., 2014)</ref>, with either Luong <ref type="bibr" target="#b24">(Luong et al., 2015)</ref> or Bahdanau <ref type="bibr" target="#b2">(Bahdanau et al., 2015)</ref> attention.</p><p>In <ref type="table">Table 9</ref>, recurrent seq2seq models obtain slightly better performance with Luong attention. Surprisingly, these models outperform previous models of similar architecture that translate GT glosses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformer</head><p>For the STMC-Transformer, we train Transformer models with the same architecture as in G2T. Parameter search yields an initial learning rate 1 with 3,000 warm-up steps and beam size 4. We empirically find using the 8 best models in ensemble decoding to be optimal. These models individually obtain between 23.51 and 24.00 BLEU-4.</p><p>Again, we observe that STMC-Transformer outperforms the previous system with ground truth glosses and Transformer. While STMC performs imperfect CSLR, its gloss predictions may be more useful than ground-truth annotations during SLT and are more readily analyzed by the Transformer. Again, the ground truth glosses represent merely a simplified intermediate representation of the actual sign language, so it is not entirely unexpected that translating ground truth glosses does not give the best performance.</p><p>STMC-Transformer also outperforms Transformers that translate GT glosses. While STMC performs imperfect CSLR, its gloss predictions may be better processed by the Transformer. Glosses are merely a simplified intermediate representation of the actual sign language so they may not be optimal. This result also reveals, training the recognition model to output more accurate glosses will not necessarily improve translation.</p><p>Both our STMC-Transformer and STMC-RNN also outperform <ref type="bibr" target="#b7">Camgoz et al. (2020)</ref>'s model. Their best model jointly train Transformers for recognition and translation, however it obtains 24.49 WER on recognition whereas STMC obtains a better WER of 21.0, which suggests their model may be weaker in processing the videos.</p><p>Moreover, Transformers outperform recurrent networks in this setup as well and STMC-Transformer improves the state-of-the-art for video-to-text translation by 7 BLEU-4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Qualitative comparison</head><p>Example outputs of the G2T and S2G2T models <ref type="table" target="#tab_1">(Table 10)</ref> show that the translations are of generally good quality, even with low BLEU scores. Most translations may have slight differences in word choice that do not change the overall meaning of the sentence or make grammatical errors, which suggests BLEU is not a good representative of human useful features for SLT. As for the comparison between the G2T and S2G2T networks, there does not seem to be a clear pattern between cases where S2G2T outperforms G2T and vice versa. One thing to note, though, is that the PHOENIX-Weather 2014T is restricted to the weather forecast domain, and a SLT dataset with a wider domain would be required to fully assess the performance of our model in more general real-life settings.</p><p>We also provide sample G2T outputs on the ASLG-PC12 corpus in Appendix A.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions and Future Work</head><p>In this paper, we proposed Transformers for SLT, notably the STMC-Transformer. Our experiments demonstrate how Transformers obtain better SLT performance than previous RNN-based networks. We also achieve new state-of-the-art results on different translation tasks on the PHOENIX-Weather 2014T and ASLG-PC12 datasets.</p><p>A key finding is we obtain better performance by using a STMC network for tokenization instead of translating GT glosses. This calls into question current methods that use glosses as an intermediate representation, since reference glosses themselves are suboptimal.</p><p>End-to-end training without gloss supervision is one promising step, though <ref type="bibr" target="#b7">Camgoz et al. (2020)</ref>'s end-to-end model does not yet surpass their joint training model. As future work, we suggest continuing work on end-to-end training of the recognition and translation models, so the recognition model learns an intermediate representation that optimizes translation, or using a different sign language annotation scheme that has less information loss.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Experiments on German G2T learning rate</head><p>A learning rate that is too low results in a notably slower convergence, but setting the learning rate too high risks leading the model to diverge. To prevent the model from diverging, we apply the Noam learning rate schedule where the learning rate increases linearly during the first training steps, or the warmup stage, then decreases proportionally to the inverse square root of the step number. The number of warmup steps is a parameter that has shown to influence Transformer performance <ref type="bibr" target="#b31">(Popel and Bojar, 2018)</ref> therefore we first run a parameter search over the number of warmup steps before finding the optimal initial learning rate. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Qualitative G2T Results on ASLG-PC12</head><p>Because quantitative metrics provide only a limited evaluation of translation performance, manual evaluation by viewing the translation outputs directly may give a better assessment of the quality of translations. <ref type="table" target="#tab_1">Table 11</ref> provides examples of SLT output on the ASLG-PC12 dataset. Here we can see how ASL glosses include prefixes that are not necessary to encapture the meaning of the phrase, which we have removed during data pre-processing before training. With a BLEU-4 testing score of 82.87, most predictions by our system are very close to the target English phrases and are able to convey the same meaning. We have also selected translation examples with lower BLEU-4 score and we can see that common errors include mistranslation of numbers and proper nouns. These are likely corner cases with infrequent examples during training.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Sign language translation pipeline 1 . * *Work carried out while atÉcole Polytechnique. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>STMC-Transformer network for SLT. PE: Positional Encoding, MHA: Multihead Attention, FF: Feed Forward.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>G2T decoding on RWTH-PHEONIX-WEATHER 2014T using different beam width.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>G2T performance on RWTH-PHEONIX-WEATHER 2014T with different warmup steps. Initial learning rate is fixed to 0G2T performance on RWTH-PHEONIX-WEATHER 2014T with various initial learning rate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Statistics of the RWTH-PHOENIX-Weather 2014T and ASLG-PC12 datasets. Out-of- vocabulary (OOV) words are those that appear in the development and testing sets, but not in the training set. Singletons are words that appear only once during training.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>G2T performance comparison of Transformers on PHOENIX-Weather 2014T with different number of enc-dec layers.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>German and English pre-trained embeddings statistics</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>G2T performance comparison using different embedding schemes on PHOENIX-Weather 2014T.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table /><note>G2T on PHEONIX-WEATHER 2014T final results.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>G2T on ASLG-PC12 final results</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>during the day eleven degrees in vogtland and twenty one degrees in upper rhine .) during the day eleven degrees in vogtland and twenty degrees in vogtland .)</figDesc><table><row><cell></cell><cell></cell><cell>BLEU-4</cell></row><row><cell cols="2">REF:ähnliches wetter auch am donnerstag .</cell></row><row><cell></cell><cell>(similar weather on thursday .)</cell></row><row><cell>G2T:</cell><cell>GLEICH WETTER AUCH DONNERSTAG</cell></row><row><cell></cell><cell>(SAME WEATHER ON THURSDAY)</cell></row><row><cell></cell><cell>ahnliches wetter auch am donnerstag .</cell><cell>100.00</cell></row><row><cell></cell><cell>(similar weather on thursday .)</cell></row><row><cell cols="2">S2G2T: GLEICH WETTER DONNERSTAG</cell></row><row><cell></cell><cell>(SAME WEATHER THURSDAY)</cell></row><row><cell></cell><cell>ahnliches wetter dann auch am donnerstag .</cell><cell>48.89</cell></row><row><cell></cell><cell>(similar weather then on thursday .)</cell></row><row><cell>REF:</cell><cell>der wind weht meist schwach aus unterschiedlichen richtungen .</cell></row><row><cell></cell><cell>(the wind usually blows weakly from different directions .)</cell></row><row><cell>G2T:</cell><cell>WIND SCHWACH UNTERSCHIED KOMMEN</cell></row><row><cell></cell><cell>(WIND WEAK DIFFERENCE COME)</cell></row><row><cell></cell><cell>der wind weht meist nur schwach aus unterschiedlichen richtungen .</cell><cell>65.80</cell></row><row><cell></cell><cell>(the wind usually blows only weakly from different directions .)</cell></row><row><cell cols="2">S2G2T: WIND SCHWACH UNTERSCHIED</cell></row><row><cell></cell><cell>(WIND WEAK DIFFERENCE)</cell></row><row><cell></cell><cell>der wind weht schwach aus unterschiedlichen richtungen .</cell><cell>61.02</cell></row><row><cell></cell><cell>(the wind is blowing weakly from different directions .)</cell></row><row><cell>REF:</cell><cell>sonnig geht es auch ins wochenende samstag ein herrlicher tag mit temperaturen bis siebzehn grad hier im westen .</cell></row><row><cell></cell><cell>(the weekend is also sunny and saturday is a wonderful day with temperatures up to seventeen degrees here in the west .)</cell></row><row><cell>G2T:</cell><cell>WOCHENENDE SONNE SAMSTAG SCHOEN TEMPERATUR BIS SIEBZEHN GRAD REGION</cell></row><row><cell></cell><cell>(WEEKEND SUN SATURDAY NICE TEMPERATURE UNTIL SEVENTEEN DEGREE REGION)</cell></row><row><cell></cell><cell>und am wochenende da scheint die sonne bei temperaturen bis siebzehn grad .</cell><cell>13.49</cell></row><row><cell></cell><cell>(and on the weekend the sun shines at temperatures up to seventeen degrees .)</cell></row><row><cell cols="2">S2G2T: WOCHENENDE SONNE SAMSTAG TEMPERATUR BIS SIEBZEHN GRAD REGION</cell></row><row><cell></cell><cell>(WEEKEND SUN SATURDAY TEMPERATURE UNTIL SEVENTEEN DEGREE REGION)</cell></row><row><cell></cell><cell>am wochenende scheint die sonne bei temperaturen bis siebzehn grad .</cell><cell>12.55</cell></row><row><cell></cell><cell>(on the weekend sun shines at temperatures up to seventeen degrees .)</cell></row><row><cell>REF:</cell><cell>es gelten entsprechende warnungen des deutschen wetterdienstes .</cell></row><row><cell></cell><cell>(appropriate warnings from the german weather service apply .)</cell></row><row><cell>G2T:</cell><cell>IX SCHON WARNUNG DEUTSCH WETTER DIENST STURM KOENNEN</cell></row><row><cell></cell><cell>(IX ALREADY WARNING GERMAN WEATHER SERVICE STORM CAN)</cell></row><row><cell></cell><cell>es bestehen entsprechende unwetterwarnungen des deutschen wetterdienstes .</cell><cell>38.26</cell></row><row><cell></cell><cell>(severe weather warnings from the german weather service exist .)</cell></row><row><cell cols="2">S2G2T: DANN IX SCHON WARNUNG DEUTSCH WETTER STURM KOENNEN</cell></row><row><cell></cell><cell>es gelten entsprechende warnungen des deutschen wetterdienstes .</cell><cell>100.00</cell></row><row><cell></cell><cell>(THEN IX ALREADY WARNING GERMAN WEATHER STORM CAN)</cell></row><row><cell></cell><cell>(appropriate warnings from the german weather service apply .)</cell></row><row><cell>REF:</cell><cell>richtung osten ist es meist sonnig .</cell></row><row><cell></cell><cell>(it is mostly sunny towards the east .)</cell></row><row><cell>G2T:</cell><cell>OST MEISTENS SONNE</cell></row><row><cell></cell><cell>(MOST EAST SUN)</cell></row><row><cell></cell><cell>im osten bleibt es meist sonnig .</cell><cell>43.47</cell></row><row><cell></cell><cell>(in the east it mostly stays sunny .)</cell></row><row><cell cols="2">S2G2T: OST REGION MEISTENS SONNE</cell></row><row><cell></cell><cell>im osten ist es meist sonnig .</cell><cell>80.91</cell></row><row><cell></cell><cell>(MOST REGION EAST SUN)</cell></row><row><cell></cell><cell>(in the east it is mostly sunny .)</cell></row><row><cell>REF:</cell><cell>am tag elf grad im vogtland und einundzwanzig grad am oberrhein .</cell></row><row><cell>G2T:</cell><cell>AM-TAG ELF VOGEL LAND</cell></row><row><cell></cell><cell>(IN-THE-DAY ELEVEN BIRD LAND)</cell></row><row><cell></cell><cell>elf grad am oberrhein .</cell><cell>18.74</cell></row><row><cell></cell><cell>(eleven degrees in upper rhine .)</cell></row><row><cell cols="2">S2G2T: ELF VOGEL ZWANZIG</cell></row><row><cell></cell><cell>(ELEVEN BIRD TWENTY)</cell></row><row><cell></cell><cell>am tag elf grad im vogtland und zwanzig grad im vogtland .</cell><cell>54.91</cell></row></table><note>((</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 10 :</head><label>10</label><figDesc>Qualitative comparison of G2T and S2G2T on RWTH-PHEONIX-WEATHER 2014T. Glosses are capitalized. REF refers to the reference German translation.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Gloss annotation from https://www.handspeak.com/translate/index.php?id=288</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/kayoyin/transformer-slt</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://deepset.ai/german-word-embeddings</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The Titan X Pascal used for this research was donated by the NVIDIA Corporation. The authors would also like to thank Jean-Baptiste Rémy for the helpful discussions and feedback with various aspects of this work, and Hao Zhou for sharing the details of her previous work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Translation of sign language glosses to text using sequence-to-sequence attention models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Arvanitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Constantinos</forename><surname>Constantinopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Kosmopoulos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="296" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Non-manual cues in automatic sign language recognition. Personal and Ubiquitous Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stylianos</forename><surname>Asteriadis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Caridakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Karpouzis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">METEOR: An automatic metric for MT evaluation with improved correlation with human judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satanjeev</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization</title>
		<meeting>the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Subunets: End-to-end hand shape and continuous sign language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Camgoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hadfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3075" to="3084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural sign language translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Camgoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hadfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7784" to="7793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sign language transformers: Joint end-to-end sign language recognition and translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Necati Cihan Camgoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hadfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Aglar Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1412.3555</idno>
	</analytic>
	<monogr>
		<title level="m">NIPS 2014 Deep Learning and Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sharing sign language data online: Experiences from the echo project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Onno</forename><surname>Crasborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johanna</forename><surname>Mesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dafydd</forename><surname>Waters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annika</forename><surname>Nonhebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Corpus Linguistics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>Els van der kooij, Bencie Woll, and Brita Bergman</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for continuous sign language recognition by staged optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runpeng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1610" to="1618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improving continuous sign language recognition: Speech recognition techniques and system design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Forster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Oberdörfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannick</forename><forename type="middle">L</forename><surname>Gweth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SLPAT</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sign language recognition using 3d convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiping</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Auslan corpus annotation guidelines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Johnston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Aäron van den Oord, Alex Graves, and Koray Kavukcuoglu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno>abs/1610.10099</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Neural machine translation in linear time</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">OpenNMT: Opensource toolkit for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Senellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural sign language translation based on human keypoint estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangki</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyedong</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Choongsang</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">2683</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Re-sign: Re-aligned end-to-end sequence modelling with deep recurrent cnn-hmms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepehr</forename><surname>Zargaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3416" to="3424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sign language recognition by combining statistical dtw and independent classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeroen</forename><surname>Lichtenauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emile</forename><surname>Hendriks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><surname>Reinders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2040" to="2046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004-07" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Online detection and classification of dynamic hand gestures with recurrent 3d convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shalini</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Tyree</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Neural sign language translation by learning tokenization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alptekin</forename><surname>Orbay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lale</forename><surname>Akarun</surname></persName>
		</author>
		<idno>abs/2002.00479</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Achraf</forename><surname>Othman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Jemni</surname></persName>
		</author>
		<title level="m">English-asl gloss parallel corpus 2012: Aslg-pc12</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002-07" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
	<note>Junjie Bai, and Soumith Chintala</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Training tips for the transformer model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Popel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Prague Bulletin of Mathematical Linguistics</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">110</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">The deaf community and culture at a crossroads: Issues and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janet</forename><forename type="middle">L</forename><surname>Pray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">King</forename><surname>Jordan</surname></persName>
		</author>
		<idno type="PMID">20730674</idno>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="168" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Using the output embedding to improve language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-04" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="157" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">When and why are pre-trained word embeddings useful for neural machine translation?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devendra</forename><surname>Sachan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarguna</forename><surname>Padmanabhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="529" to="535" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Main difficulties and obstacles faced by the deaf community in health access: an integrative literature review</title>
	</analytic>
	<monogr>
		<title level="j">Revista CEFAC</title>
		<editor>Maria Fernanda Neves Silveira de Souza, Amanda Miranda Brito Araújo, Luiza Fernandes Fonseca Sandes, Daniel Antunes Freitas, Wellington Danilo Soares, Raquel Schwenck de Mello Vianna, and Arlen Almeida Duarte de Sousa</editor>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="395" to="405" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sign language structure: an outline of the visual communication systems of the american deaf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stokoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of deaf studies and deaf education</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="3" to="37" />
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Sift-based arabic sign language recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaa</forename><surname>Tharwat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tarek</forename><surname>Gaber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Shahin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basma</forename><surname>Refaat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aboul Ella Hassanien</forename><surname>Ali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 1st Afro-European Conference for Industrial Advancement</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-11-17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Chinese sign language recognition based on video sequence appearance modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q X</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th IEEE Conference on Industrial Electronics and Applications</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1537" to="1542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A threshold-based hmm-dtw approach for continuous sign language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference Proceeding Series</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Spatial-temporal multi-cue network for continuous sign language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13009" to="13016" />
		</imprint>
	</monogr>
	<note type="report_type">BLEU-4</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">i am particularly grateful for the european parliament&apos;s driving role where the baltic sea cooperation is concerned</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asl: X-I Be Desc-Particularly Desc-Grateful For European Parliament X-Poss Drive Role Where Baltic</forename><surname>Sea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Be</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gt</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page">0</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">ASL: DESC-REFORE , DESC-MUCH WORK NEED TO BE DO IN ORDER TO DESC-FURR SIMPLIFY RULE GT: therefore , much work needs to be done in order to further simplify the rules</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page">0</biblScope>
		</imprint>
	</monogr>
	<note>Pred: i am particularly grateful for the european parliament&apos;s driving role where the baltic sea cooperation is concerned</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Pred: therefore , much work needs to be done in order to further simplify the rules</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">along the union&apos;s southern and eastern borders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asl: This Pressure Be Desc-Particularly Desc-Great Along Union X-Poss Desc-Sourn And Desc-Eastern Border</forename><surname>Gt</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page">0</biblScope>
		</imprint>
	</monogr>
	<note>this pressure is particularly great</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Pred: this pressure is particularly great along the union&apos;s southern and eastern borders</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">GT: more women die from the aggression directed against them than die from cancer</title>
	</analytic>
	<monogr>
		<title level="m">ASL: MORE WOMAN DIE FROM AGGRESSION DESC-DIRECT AGAINST X-Y THAN DIE FROM CANCER</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">ASL: X-IT FUEL WAR IN CAMBODIUM IN 1990 AND X-IT BE ENEMY DEMOCRACY GT: it fuelled the war in cambodia in the 1990s and it is the enemy of democracy</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">89</biblScope>
		</imprint>
	</monogr>
	<note>Pred: more women die from aggression directed against them than die from cancer</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Pred: it fuel war in the cambodium in 1990 and it is an enemy of democracy</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">GT: then the chief investigator himself is targeted and the house of cards collapses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asl: Desc-N Chief Investigator X-Himself</forename><surname>Be</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>House Card</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collapse</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">AMENDMENT THANK X-YOU MR GT: otherwise we have to vote on the corresponding part of amendment thank you mrs ţicȃu , we take due note of your observation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X-We Take Desc-Due Note X-You</forename><surname>Asl: U</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Observation</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">93</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Pred: mr president , we took due note of your observation</title>
	</analytic>
	<monogr>
		<title level="m">Examples of ASL translation with varying BLEU-4 scores</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
