<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Milking CowMask for Semi-Supervised Image Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>French</surname></persName>
							<email>g.french@uea.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing Sciences</orgName>
								<orgName type="institution">University of East Anglia</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
							<email>avitalo@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing Sciences</orgName>
								<orgName type="institution">University of East Anglia</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing Sciences</orgName>
								<orgName type="institution">University of East Anglia</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brain</forename><surname>Tean</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing Sciences</orgName>
								<orgName type="institution">University of East Anglia</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
							<email>salimans@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing Sciences</orgName>
								<orgName type="institution">University of East Anglia</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing Sciences</orgName>
								<orgName type="institution">University of East Anglia</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brain</forename><surname>Tean</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing Sciences</orgName>
								<orgName type="institution">University of East Anglia</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Milking CowMask for Semi-Supervised Image Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Consistency regularization is a technique for semi-supervised learning that underlies a number of strong results for classification with few labeled data. It works by encouraging a learned model to be robust to perturbations on unlabeled data. Here, we present a novel mask-based augmentation method called CowMask. Using it to provide perturbations for semi-supervised consistency regularization, we achieve a state-of-the-art result on ImageNet with 10% labeled data, with a top-5 error of 8.76% and top-1 error of 26.06%. Moreover, we do so with a method that is much simpler than many alternatives. We further investigate the behavior of CowMask for semi-supervised learning by running many smaller scale experiments on the SVHN, CIFAR-10 and CIFAR-100 data sets, where we achieve results competitive with the state of the art, indicating that CowMask is widely applicable. We open source our code at https://github.com/google-research/google-research/tree/ master/milking_cowmask.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Training accurate deep neural network based image classifiers requires large quantities of training data. While images are often readily available in many problem domains, producing ground truth annotations is usually a laborious and expensive task that can act as a bottleneck. Semi-supervised learning offers the tantalising possibility of reducing the amount of annotated data required by learning from a dataset that is only partially annotated.</p><p>Semi-supervised learning algorithms based on consistency regularization <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17]</ref> have proved to be simple while effective, yielding a number of state of the art results over the last few years. Consistency regularization is driven by encouraging consistent predictions for unsupervised samples under stochastic augmentation. Using CutOut <ref type="bibr" target="#b8">[9]</ref> -in which a rectangular region of an image is masked to zero -as the augmentation has proved to be highly effective, making significant contributions to the effectiveness of rich augmentation strategies <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>In this paper, we introduce a simple masking strategy that we call CowMask, whose shapes and appearance are more varied than the rectangular masks used by CutOut and RandErase <ref type="bibr" target="#b32">[33]</ref>. When used to erase parts of an image in a similar fashion to RandErase, CowMask outperforms rectangular masks in the majority of semi-supervised image classifications tasks that we tested.</p><p>We extend the Interpolation Consistency Training (ICT) algorithm <ref type="bibr" target="#b25">[26]</ref> to use mask-based mixing, using both rectangular masks as in CutMix <ref type="bibr" target="#b28">[29]</ref> and CowMask. Both CutMix and CowMask exhibit strong semi-supervised learning performance, with CowMask outperforming rectangular mask based mixing in the majority of cases. CowMask based mixing achieves a state-of-the-art result 1 on semi-supervised Imagenet, and results comparable to state-of-the-art on multiple small image datasets, all without the need for previously proposed complex training procedures.</p><p>In Section 2 we discuss related work that forms the basis of our approach, alongside other semisupervised learning algorithms for comparison. In Section 3 we present CowMask, the novel ingredient to our semi-supervised learning algorithm, that is described in Section 4. We present our experiments and results in Section 5. Finally we discuss our work and conclude in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Semi-supervised classification</head><p>A variety of semi-supervised deep neural network image classification approaches have been proposed over the last several years, including the use of auto-encoders <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b17">18]</ref>, GANs <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b7">8]</ref>, curriculum learning <ref type="bibr" target="#b3">[4]</ref> and self-supervised learning <ref type="bibr" target="#b30">[31]</ref>.</p><p>Many recent approaches are based on consistency regularization <ref type="bibr" target="#b16">[17]</ref>, a simple approach exemplified by the π-model <ref type="bibr" target="#b13">[14]</ref> and the Mean Teacher model <ref type="bibr" target="#b24">[25]</ref>. Two loss terms are minimized; standard crossentropy loss and consistency loss for supervised and unsupervised samples respectively. Consistency loss measures the difference between predictions resulting from differently perturbed variants of an unsupervised sample. The π-model perturbs samples twice using stochastic augmentation and minimises the squared difference between class probability predictions. The Mean Teacher model builds on the π-model by using two networks; a teacher and a student. The student is trained using gradient descent as normal while the weights of the teacher are an exponential moving average of those of the student. The consistency loss term measures the difference in predictions between the student and the teacher under different stochastic augmentation.</p><p>A variety of types of perturbation have been explored. Sajjadi et al. <ref type="bibr" target="#b19">[20]</ref> employed richer data augmentation including affine transformations, while <ref type="bibr" target="#b13">[14]</ref> and <ref type="bibr" target="#b24">[25]</ref> used standard augmentation strategies such as random crop and noise for small image datasets. Virtual Adversarial Training (VAT) uses adversarial perturbations that maximise the consistency loss term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Mixing regularization</head><p>Recent works have demonstrated that blending pairs of images and corresponding ground truths can act as an effective regularizer. MixUp <ref type="bibr" target="#b31">[32]</ref> draws a blending factor from the Beta distribution that is used to interpolate images and ground truth labels. Interpolation Consistency Training (ICT) <ref type="bibr" target="#b25">[26]</ref> extends this approach to work in a semi-supervised setting by combining it with the Mean Teacher model. The teacher network is used to predict class probabilities for a pair of images A and B and MixUp is used to blend the images and the teachers' predictions. The predictions of the student for the blended image are encouraged to be as close as possible to the blended teacher predictions.</p><p>MixMatch <ref type="bibr" target="#b1">[2]</ref> guesses labels for unsupervised samples by sharpening the averaged predictions from multiple rounds of standard augmentation and blends images and corresponding labels (ground truth for supervised samples, guesses for unsupervised) using MixUp <ref type="bibr" target="#b31">[32]</ref>. The blended images and corresponding guessed labels are used to compute consistency loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Rich augmentation</head><p>AutoAugment <ref type="bibr" target="#b5">[6]</ref> and RandAugment <ref type="bibr" target="#b6">[7]</ref> are rich augmentation schemes that combine a number of image operations provided by the Pillow library <ref type="bibr" target="#b14">[15]</ref>. AutoAugment learns an augmentation policy for a specific dataset using re-inforcement learning, requiring a large amount of computation to do so. RandAugment on the other hand has two hyper-parameters that are chosen via grid search; the number of operations to apply and a magnitude.</p><p>Unsupervised data augmentation (UDA) <ref type="bibr" target="#b27">[28]</ref> adds employs a combination of CutOut <ref type="bibr" target="#b8">[9]</ref> and Ran-dAugment <ref type="bibr" target="#b6">[7]</ref> in a semi-supervised setting achieving state-of-the-art results in small image benchmarks such as CIFAR-10. Their approach encourages consistency between the predictions for the original un-modified image and the same image with RandAugment applied.</p><p>ReMixMatch <ref type="bibr" target="#b0">[1]</ref> builds on MixMatch by adding distribution alignment and rich data augmentation using CTAugment or RandAugment (depending on the dataset). CTAugment is a variant of AutoAug-ment that learns an augmentation policy during training, and RandAugment is a pre-defined set of 15 forms of augmentations with concrete scales. It is worth noting that ReMixMatch uses predictions from standard 'weak' augmentation as guessed target probabilities for unsupervised samples and encourages predictions arising from multiple applications of the richer CTAugment to be close to the guessed target probabilities. The authors found that using rich augmentation for guessing target probabilities (a la MixMatch) resulted in unstable training.</p><p>FixMatch <ref type="bibr" target="#b22">[23]</ref> is a simple semi-supervised learning approach that uses standard 'weak' augmentation to predict pseudo-labels for unsupervised samples. The same samples are richly augmented using CTAugment and cross-entropy loss is computed using the pseudo-labels. Confidence thresholding <ref type="bibr" target="#b10">[11]</ref> masks the unsupervised cross-entropy loss to zero for samples whose predicted confidence is below 95%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Mask-based regularization</head><p>Erasing a rectangular region of an image by replacing it with zeros -as in Cutout <ref type="bibr" target="#b8">[9]</ref> -or noise -as in RandErase <ref type="bibr" target="#b32">[33]</ref> -has proved to be an effective augmentation strategy that yields improvements in supervised image classification.</p><p>CutOut has proved to be highly effective in semi-supervised classification scenarios. The UDA authors <ref type="bibr" target="#b27">[28]</ref> report impressive results, while the FixMatch authors <ref type="bibr" target="#b22">[23]</ref> report that CutOut alone is as effective as the combination of the other 14 image operations used in CTAugment.</p><p>CutMix <ref type="bibr" target="#b28">[29]</ref> replaces the blending factor in MixUp with a rectangular mask and uses it to mix pairs of images, effectively cutting and pasting a rectangle from one image onto another. This yielded significant supervised classification performance gains. CutMix was applied by French et al.as part of a consistency regularization based semi-supervised semantic segmentation algorithm <ref type="bibr" target="#b9">[10]</ref>. </p><formula xml:id="formula_0">s = std_dev(x s ) τ = m + √ 2 · erf −1 (2p − 1) · s {Compute threshold τ } c = x s ≤ τ {Threshold filtered noise} Return c 3 CowMask</formula><p>Our exploration of mask-based augmentation for consistency regularization is motivated by the strong performance of Cutout <ref type="bibr" target="#b8">[9]</ref> shown in ablation studies in UDA <ref type="bibr" target="#b27">[28]</ref> and FixMatch <ref type="bibr" target="#b22">[23]</ref>. Furthermore, French et al. show that semantic segmentation problems exhibit a challenging data distribution where the cluster assumption -identified in prior work <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26</ref>] as important to the success of consistency regularization -does not apply. In spite of this, they obtain strong results using CutMix, suggesting mask-based mixing as a promising avenue.</p><p>Here, we propose CowMask; a simple approach to generating the masks shown on the right of Algorithm 1, so called due to its' Friesian cow-like appearance. We note that the concurrent work FMix <ref type="bibr" target="#b12">[13]</ref> uses an inverse Fourier transform to generate masks with a similar visual appearance.</p><p>Briefly, a CowMask is generated by applying Gaussian filtering of scale σ to normally distributed noise. A threshold τ is chosen such that a proportion p of the smooth noise pixels are below τ . Pixels with a value below τ are assigned a value of 1, or 0 otherwise. The scale of the mask features is controlled by σ -as seen in the examples on the right of Algorithm 1 -and is drawn from a log-uniform distribution in the range (σ min , σ max ). The proportion p of pixels with a value of 1 is drawn from a uniform distribution in the range (p min , p max ). The procedure for generating a CowMask is provided in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Semi-Supervised Learning Method</head><p>We adopt the Mean Teacher <ref type="bibr" target="#b24">[25]</ref> framework as the basis of our approach. We use two networks; the student f θ (·) and the teacher g φ (·), both of which generate class probability vectors. The student is trained by gradient descent as normal. After every update to the student, the weights of the teacher are updated to be an exponential moving average of those of the student using φ = φα + θ(1 − α). The EMA momentum α controls the trade-off between the stability and the speed at which the teacher follows the student.</p><p>Our training set consists of a set of supervised samples S consisting of input images s and corresponding target labels t, and a set of unsupervised samples U consisting only of input images u. Given a labelled dataset we select the supervised subset randomly such that it maintains the class balance of the overall dataset 2 as is standard practice in the literature. All available samples are used as unsupervised samples. Our models f θ are then trained to minimize a combined loss:</p><formula xml:id="formula_1">L = L S (f θ (s), t) + ωL U (f θ (u), g φ (u))</formula><p>where we use standard cross entropy loss for the supervised loss L S (·) and consistency loss for the unsupervised loss L U (·) that is modulated by the unsupervised loss weight ω.</p><p>We explore two different types of mask-based consistency regularization: mask-based erasure and mask-based mixing. In mask-based erasure we perturb our input data by erasing the part of the input image corresponding to a randomly sampled mask. In mask-based mixing we blend two input images together, with the blending weights given by the sampled mask. We follow the nomenclature of Cutout and CutMix, using the terms CowOut and CowMix to refer to CowMask based erasure and mixing respectively.</p><formula xml:id="formula_2">Algorithm 2 CowOut erasure-based unsuper- vised loss Require: unlabeled image x, CowMask m Require: teacher model g φ Require: student model f θ Require: confidence threshold ψ x = std_aug(x) {standard augmentation} z = stop_gradient(g φ (x)) {teacher pred.} q = max i z[i] ≥ ψ {confidence mask} ∼ N (0, I) {generate noise image} x m =x * m + * (1 − m) {apply mask} y m = f θ (x m ) {student prediction} d = q * ||y m − z|| 2 2 {cons. loss} Return d Algorithm 3 CowMix mixing-based unsuper- vised loss Require: unlabeled images x a , x b Require: CowMask m Require: teacher model g φ Require: student model f θ Require: confidence threshold ψ x a = std_aug(x a ) {standard augmentation} x b = std_aug(x b ) z a = stop_gradient(g φ (x a )) {teacher pred.} z b = stop_gradient(g φ (x b )) c a = max i z a [i] {confidence of prediction} c b = max i z b [i] x m =x a * m +x b * (1 − m) {mix images} p = mean(m) {scalar mean of mask} z m = z a * p + z b * (1 − p) {mix tea. preds.} c m = c a * p + c b * (1 − p) {mix confidences} q = mean(c m ≥ ψ) {mean of conf. mask} y m = f θ (x m ) {stu. pred. on mixed image} d = q||y m − z m || 2 2 {cons. loss} Return d</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Mask-based augmentation by erasure</head><p>Mask-based erasure can function as an augmentation that can be added to the standard augmentation scheme used for the dataset at hand, with one caveat. Similar to prior work <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b22">23]</ref> we found it necessary to split our augmentation into a 'weak' standard augmentation scheme (e.g. crop and flip) and a 'strong' rich scheme; RandAugment in the case of the prior works mentioned or CowOut in our work. Weakly augmented samples are passed to the teacher network, generating predictions that are used as pseudo-targets that the student is encouraged to match for strongly augmented variants of the same samples. Using 'strong' erasure augmentation to generate pseudo-targets resulted in unstable training.</p><p>The π-model <ref type="bibr" target="#b13">[14]</ref> and the Mean Teacher model <ref type="bibr" target="#b24">[25]</ref> both use a Gaussian ramp-up function to modulate the effect of consistency loss during the early stages of training. Reinforcing the random predictions of an untrained network was found to harm performance. In place of a ramp-up we opt to use confidence thresholding <ref type="bibr" target="#b10">[11]</ref>. Consistency loss is masked to zero for samples for which the teacher networks' predictions are below a specified threshold. FixMatch <ref type="bibr" target="#b22">[23]</ref> uses confidence thresholding for similar reasons.</p><p>Our procedure for computing unsupervised consistency loss based on erasure is provided in Algorithm 2. For our small image experiments we found that the best value for the unsupervised weight factor ω is 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Mask-based mixing</head><p>Alternatively, we can construct an unsupervised consistency loss by mask-based mixing of images in place of erasure. Our approach for mixing image pairs using masks is essentially that of interpolation consistency training (ICT) <ref type="bibr" target="#b25">[26]</ref>. ICT works by passing the original image pair to the teacher network, the blended image to the student, and encouraging the student networks' prediction to match the blended teacher predictions. Where ICT draws per-pair blending factors a beta distribution, we mix images using a mask, and mix probability predictions with the mean of that mask (the proportion of pixels with a value of 1).</p><p>Confidence thresholding required adaptation for use with mix-based regularization. Rather than applying confidence thresholding to the blended teacher probability predictions we opted to blend the confidence values before thresholding as this gave slightly better results. Further improvements resulted from modulating the consistency loss by the proportion of samples in the batch whose predictions cross the confidence threshold, rather masking the loss for each sample individually.</p><p>The procedure for computing unsupervised mix consistency loss is provided in Algorithm 3. We found that a higher weight ω was appropriate for mix consistency loss; we used a value of 30 for our small image experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and results</head><p>We first evaluate CowMix for semi-supervised consistency regularization on the challenging ImageNet dataset, where we match the state of the art. Next, we examine CowOut and CowMix further and compare with previously proposed methods by trying multiple versions of our approach combined with multiple models on three small image datasets: CIFAR-10, CIFAR-100 and SVHN. The training regimes used for both ImageNet and the small image datasets are sufficiently similar that we used the same codebase for all of our experiments.</p><p>Our results are obtained by using the teacher network for evaluation. We report our results as error rates presented as the mean ± 1 standard deviation computed from the results of 5 runs, each of which uses a different subset of samples as the supervised set. Supervised sets are consistent for all experiments for a given dataset and number of supervised samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">ImageNet 2012</head><p>We contrast the following scenarios: a supervised baseline using 10% of the dataset, semi-supervised training with the same 10% of labelled examples using CowMix consistency regularization on all unlabeled examples, and fully supervised training with all 100% labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Setup</head><p>We used the ResNet-152 architecture. We adopted a training regime as similar as possible to a standard ImageNet ResNet training protocol. We used a batch size of 1024 and SGD with Nesterov Momentum <ref type="bibr" target="#b23">[24]</ref> set to 0.9 and weight decay (via L2 regularization) set to 0.00025. Our standard augmentation scheme consists of inception crop, random horizontal flip and colour jitter, as in <ref type="bibr" target="#b24">[25]</ref>.We found that the standard learning rate of 0.1 resulted in unstable training, but were able to stabilise it by reducing the learning rate to 0.04 <ref type="bibr" target="#b24">[25]</ref>. We found that our approach benefits from training for longer than in supervised settings, so we doubled the number of training epochs to 180 and stretched the learning rate schedule by a factor of 2, reducing the learning rate at epochs 60, 120 and 160 and reduced it by a factor of 0.2 rather than 0.1. We used a teacher EMA momentum α of 0.999.</p><p>We obtained our CowMix results using a mix loss weight of 100 and and a confidence threshold of 0.5. We drew the CowMask σ scale parameter from the range (32, 128).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Results</head><p>Our ImageNet results are presented in <ref type="table">Table 1</ref>. We match the S 4 L MOAM <ref type="bibr" target="#b30">[31]</ref> top-5 error result and beat their top-1 error result, with a simple end-to-end approach and a significantly smaller model. By comparison the S 4 L MOAM result is obtained using a 3-stage training and fine-tuning procedure. The recent SimCLR <ref type="bibr" target="#b4">[5]</ref> approach (concurrent work) uses self-supervised contrastive learning followed by a fine tuning stage. They beat our result when using a much larger model. We tested our approach with wider models (e.g. ResNet-50×2) but obtained better results from the deeper and commonly used ResNet-152.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approach</head><p>Architecture Params. Top-5 err.</p><p>Top-1 err. Mean Teacher <ref type="bibr" target="#b24">[25]</ref> ResNeXt-152 62M 9.11% ± 0.12 -UDA <ref type="bibr" target="#b27">[28]</ref> ResNet-50 24M 11.2% 31.22% FixMatch <ref type="bibr" target="#b22">[23]</ref> ResNet-50 24M 10.87 ± 0.28% 28.54 ± 0.52% S 4 L Full (MOAM) <ref type="bibr">[</ref>  <ref type="table">Table 1</ref>: Results on ImageNet with 10% labels. Note that S 4 L involves three steps with different training procedures, while CowMix involves a single training run. SimCLR is able to beat CowMix, but only when using a very large model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Small image experiments</head><p>Alongside CowOut and CowMix we implemented and evaluated Mean Teacher, CutOut/RandErase and CutMix, and we compare our method against these using the CIFAR-10, CIFAR-100, and SVHN datasets.</p><p>We note the following differences between our implementation and those of CutOut and CutMix: 1. Our boxes are chosen so that they entirely fit within the bounds of the mask, whereas CutOut and CutMix use a fixed or random size respectively and centre the box anywhere within the mask, with some of the box potentially being outside the bounds of the mask. 2. CutOut uses a fixed size box, CutMix randomly chooses an area but constrains the aspect ratio to be that of the mask, we choose both randomly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Setup</head><p>For the small image experiments we use a 27M parameter Wide ResNet 28-96x2d with shake-shake regularization <ref type="bibr" target="#b11">[12]</ref>. We note that as a result of a mistake in our implementation we used a 3 × 3   <ref type="table">Table 3</ref>: Results on SVHN test set, error rates as mean ± stdev of 5 independent runs. convolution rather than a 1 × 1 in the residual shortcut connections that either down-sample or change filter counts, resulting in a slightly higher parameter count.</p><p>The standard Wide ResNet training regime <ref type="bibr" target="#b29">[30]</ref> is very similar to that used for ImageNet. We used the optimizer, but with weight decay of 0.0005 and a batch size of 256. As before, the standard learning rate of 0.1 had to be reduced to ensure stability, this time to 0.05. The small image experiments also benefit from training for longer; 300 epochs instead of the standard 200 used in supervised settings. The adaptations made to the Wide ResNet learning rate schedule were nearly identical to those made to the ImageNet schedule. We doubled its length and reduced the learning rate by a factor of 0.2 rather than 0.1. We did however remove the last step; the learning rate is reduced at epochs 120 and 240 rather than epochs 60, 120 and 160 as used in supervised settings. For erasure experiments we used a teacher EMA momentum α of 0.99 and for mixing experiments we used 0.97.</p><p>When using CowOut and CowMix we obtained the best results when the CowMask scale parameter σ is drawn from the range <ref type="bibr" target="#b3">(4,</ref><ref type="bibr" target="#b15">16)</ref>. We note that this corresponds to a range of ( 1 8 , 1 2 ) relative to the 32 × 32 image size and that the σ range used in our ImageNet experiments bears a nearly identical relationship to the 224 × 224 image size used there. For erasure experiments using CowOut we obtained the best results when drawing p; the proportion of pixels that are retained from the range (0.25, 1). Intuitively it makes sense to retain at least 25% of the image pixels as encouraging the network to predict the same result for an image and a blank space is unlikely to be useful. For mixing experiments using CowMix we obtained the best results when drawing p from the range (0.2, 0.8).</p><p>We performed hyper-parameter tuning on the CIFAR-10 dataset using 1,000 supervised samples and evaluating on 5,000 training samples held out as a validation set. The best hyper-parameters found were used as-is for CIFAR-100 and SVHN.  <ref type="table">Table 4</ref>: Results on CIFAR-100 test set, error rates as mean ± stdev of 5 independent runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Results</head><p>Our results for CIFAR-10, CIFAR-100 and SVHN datasets are presented in Tables 2, 4 and 3 respectively. Considering the techniques we explore we find that mix-based regularization outperforms erasure based regularization, irrespective of the mask generation method used.</p><p>We would like to note that our 27M parameter model is larger than the 1.5M parameter models used for the majority of results in other works, so we cannot make an apples-to-apples comparison in these cases. Our CIFAR-10 results are competitive with recent work, except in small data regimes of less than 500 samples where EnAET <ref type="bibr" target="#b26">[27]</ref> and FixMatch <ref type="bibr" target="#b22">[23]</ref> outperform CowMix. Our CIFAR-100 and SVHN results are competitive with recent approaches but are not state of the art. We note that we did not tune our hyper-parameters for these datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>We explain the effectiveness of CowMix by considering the effects of CowMask and mixing based semi-supervised learning separately.</p><p>DeVries et al. <ref type="bibr" target="#b8">[9]</ref> established that Cutout -that uses a box shaped mask similar to RandErase -encourages the network to utilise a wider variety of features in order to overcome the varying combinations of parts of an image being present or masked out. In comparison to a rectangular mask the more flexibly shaped CowMask provides more variety and has less correlation between regions of the mask. This increases in the range of combinations of image regions being left intact or erased enhances its effect.</p><p>The MixUp <ref type="bibr" target="#b31">[32]</ref> and CutMix <ref type="bibr" target="#b28">[29]</ref> regularizers demonstrated that encouraging network predictions vary smoothly between two images as they are mixed -using either interpolation or mask-based mixing -improved supervised performance, with mask-based mixing offerring the biggest gains. We adapted CutMix -in a similar fashion to ICT -for semi-supervised learning and showed that mask based mixing yields significant gains when used as an unsupervised regularizer. CowMix adds the benefits of flexibly shaped masks into the mix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>We presented and evaluated CowMask for use in semi-supervised consistency regularization, achieving a new state of the art on semi-supervised Imagenet, with a much simpler method than in previously proposed approaches, using standard networks and training procedures. We examined both erasurebased and mixing-based augmentation using CowMask, and find that the mix-based variant -which we call CowMix -is particularly effective for semi-supervised learning. Further experiments on small image data sets SVHN, CIFAR-10, and CIFAR-100 demonstrate that CowMask is widely applicable.</p><p>Research on semi-supervised learning is moving fast, and many new approaches have been proposed over the last year alone that use mask-based perturbation. In future work we would like to further explore the use of CowMask in combination with these other recently proposed methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>Manual annotation is a laborious and expensive task. This can act as a bottleneck, slowing or preventing the adoption of machine learning systems. This is particularly likely to affect organisations such as small businesses and non-commercial entities, for which the financial cost of annotation could act as a hindrance. Semi-supervised learing offers the potential of reducing this bottleneck, making machine learning accessible to those with less access to resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mask based erasure</head><p>Our mask-based augmentation by erasure algorithm is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mask based mixing</head><p>Our mask-based augmentation by mixing algorithm is illustrated in <ref type="figure" target="#fig_2">Figure 2</ref>. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1</head><label>1</label><figDesc>CowMask generation algorithm, with example CowMasks on right with p = 0.5 and σ ∈ {8, 16, 32}. Require: mask size H × W Require: scale range (σ min , σ max ) Require: proportion range (p min , p max ) Require: inverse error function erf −1 σ ∼ logU(σ min , σ max ) {Randomly choose sigma} p ∼ U(p min , p max ) {Randomly choose proportion} x ∼ N H×W (0, 1) {Per-pixel Gaussian noise} x s = gaussian_filter_2d(x, σ) {Filter noise} m = mean(x s ) {Compute mean and std-dev}</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of the unsupervised mask based erasure consistency loss component of semisupervised image classification. Blue arrows carry image or mask content and grey arrows carry probability vectors. Note that confidence thresholding is not illustrated here.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of the unsupervised masked based mixing loss component of semi-supervised image classification. Blue arrows carry image or mask content, grey arrows carry probability vectors and yellow carry scalars. Please note that confidence thresholding is not illustrated here.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results on CIFAR-10 test set, error rates as mean ± std − dev of 5 independent runs.</figDesc><table><row><cell>Labeled samples</cell><cell>40</cell><cell>100</cell><cell>250</cell><cell>500</cell><cell>1000</cell><cell>2000</cell><cell>4000</cell><cell>ALL</cell></row><row><cell></cell><cell></cell><cell cols="5">Other work: uses smaller Wide ResNet 28-2 model with 1.5M parameters</cell><cell></cell><cell></cell></row><row><cell>EnAET</cell><cell></cell><cell>16.92%</cell><cell>3.21% ± 0.21</cell><cell>3.05%</cell><cell>2.92%</cell><cell>2.84%</cell><cell>2.69%</cell><cell></cell></row><row><cell>UDA</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2.55% ± 0.99</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MixMatch</cell><cell></cell><cell></cell><cell>3.78% ± 0.26</cell><cell>3.64% ± 0.46</cell><cell>3.27% ± 0.31</cell><cell>3.04% ± 0.13</cell><cell>2.89% ± 0.06</cell><cell></cell></row><row><cell>ReMixMatch</cell><cell>3.55% ± 3.87</cell><cell>3.10% ± 0.50</cell><cell></cell><cell>2.83% ± 0.30</cell><cell></cell><cell>2.42% ± 0.09</cell><cell></cell><cell></cell></row><row><cell>FixMatch (RA)</cell><cell>3.96% ± 2.17</cell><cell></cell><cell>2.48% ± 0.38</cell><cell></cell><cell>2.28% ± 0.11</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Other work: uses 26M parameter models</cell><cell></cell><cell></cell><cell></cell></row><row><cell>EnAET</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2.42%</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="5">Our results: uses 27M parameter Wide ResNet 28-96x2d with shake-shake</cell><cell></cell><cell></cell></row><row><cell>Supervised</cell><cell></cell><cell>71.24% ± 5.40</cell><cell>37.02% ± 6.15</cell><cell>18.85% ± 1.49</cell><cell>11.71% ± 0.55</cell><cell>8.23% ± 0.38</cell><cell>6.01% ± 0.46</cell><cell>2.82% ± 0.08</cell></row><row><cell></cell><cell cols="3">Augmentation / erasure based regularization</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mean teacher</cell><cell></cell><cell>62.16% ± 10.92</cell><cell>8.23% ± 4.62</cell><cell>3.84% ± 0.15</cell><cell>3.75% ± 0.10</cell><cell>3.61% ± 0.15</cell><cell>3.47% ± 0.12</cell><cell>2.73% ± 0.04</cell></row><row><cell>RandErase</cell><cell></cell><cell>52.55% ± 22.03</cell><cell>7.61% ± 1.71</cell><cell>6.17% ± 1.25</cell><cell>4.81% ± 0.46</cell><cell>3.66% ± 0.15</cell><cell>3.21% ± 0.22</cell><cell>2.36% ± 0.04</cell></row><row><cell>CowOut</cell><cell></cell><cell>66.66% ± 19.71</cell><cell>12.11% ± 1.82</cell><cell>5.94% ± 0.38</cell><cell>4.36% ± 0.29</cell><cell>3.59% ± 0.25</cell><cell>3.04% ± 0.04</cell><cell>2.42% ± 0.09</cell></row><row><cell></cell><cell cols="2">Mix based regularization</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CutMix</cell><cell></cell><cell>9.54% ± 2.53</cell><cell>5.62% ± 0.93</cell><cell>4.32% ± 0.52</cell><cell>3.79% ± 0.41</cell><cell>3.26% ± 0.27</cell><cell>2.92% ± 0.09</cell><cell>2.29% ± 0.09</cell></row><row><cell>CowMix</cell><cell></cell><cell>9.73% ± 4.01</cell><cell>3.59% ± 0.30</cell><cell>3.80% ± 0.32</cell><cell>3.72% ± 0.60</cell><cell>3.13% ± 0.11</cell><cell>2.90% ± 0.19</cell><cell>2.18% ± 0.06</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>80% ± 0.22 49.24% ± 0.40 36.04% ± 0.26 18.82% ± 0.22 Augmentation / erasure based regularization Mean teacher 76.97% ± 0.99 38.90% ± 0.48 30.04% ± 0.60 17.81% ± 0.17 RandErase 70.48% ± 1.05 35.61% ± 0.40 28.21% ± 0.16 16.71% ± 0.29 CowOut 68.86% ± 0.78 38.82% ± 0.44 27.54% ± 0.29 16.46% ± 0.22 Mix based regularization CutMix 64.11% ± 2.63 30.15% ± 0.58 24.08% ± 0.25 16.54% ± 0.18 CowMix 57.27% ± 1.34 29.25% ± 0.47 23.61% ± 0.30 15.73% ± 0.15</figDesc><table><row><cell># Labels</cell><cell>1000</cell><cell>5000</cell><cell>10000</cell><cell>ALL</cell></row><row><cell></cell><cell cols="3">Other work: uses 1.5M parameters Wide ResNet 28-2</cell><cell></cell></row><row><cell>EnAET</cell><cell>58.73%</cell><cell>31.83%</cell><cell cols="2">26.93% ± 0.21 20.55%</cell></row><row><cell>MixMatch</cell><cell></cell><cell></cell><cell>25.88% ± 0.30</cell><cell></cell></row><row><cell>FixMatch</cell><cell></cell><cell></cell><cell>22.60% ± 0.12</cell><cell></cell></row><row><cell></cell><cell cols="3">Other work: uses 26M parameter models</cell><cell></cell></row><row><cell>EnAET</cell><cell></cell><cell></cell><cell>22.92%</cell><cell>16.87%</cell></row><row><cell></cell><cell cols="3">Our results: 27M param WRN 28-96x2d</cell><cell></cell></row><row><cell>Supervised</cell><cell>78.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The concurrent SimCLR<ref type="bibr" target="#b4">[5]</ref> work achieves a better result using a larger architecture Preprint. Under review.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We use StratifiedShuffleSplit from Scikit-Learn<ref type="bibr" target="#b2">[3]</ref> </note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SUPPLEMENTARY MATERIAL -Illustration of computation</head><p>Here we provide illustrations that give a simplified overview of our approaches.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Remixmatch: Semi-supervised learning with distribution alignment and augmentation anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09785</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5050" to="5060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">API design for machine learning software: experiences from the scikit-learn project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Buitinck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Louppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Niculae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaques</forename><surname>Grobler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Layton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaël</forename><surname>Varoquaux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML PKDD Workshop: Languages for Data Mining and Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="108" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Curriculum labeling: Self-paced pseudo-labeling for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paola</forename><surname>Cascante-Bonilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuwen</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.06001</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="113" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Randaugment: Practical data augmentation with no separate search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13719</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Good semi-supervised learning that requires a bad gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6510" to="6520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno>abs/1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Semisupervised semantic segmentation needs strong, high-dimensional perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Mackiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Finlayson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.01916</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Self-ensembling for visual domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Mackiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Gastaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07485</idno>
		<title level="m">Shake-shake regularization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Understanding and enhancing mixed sample data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonia</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Painter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahesan</forename><surname>Niranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Prügel-Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Hare</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.12047</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Temporal ensembling for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredrik</forename><surname>Lundh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Clark</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Smooth neighbors on teacher graphs for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yucen</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8896" to="8905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Realistic evaluation of semi-supervised learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semisupervised learning with ladder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Rasmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikko</forename><surname>Honkala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapani</forename><surname>Raiko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3546" to="3554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mutual exclusivity loss for semisupervised deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehran</forename><surname>Javanmardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Tasdizen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">23rd IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Regularization with stochastic transformations and perturbations for deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehran</forename><surname>Javanmardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Tasdizen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1163" to="1171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A DIRT-T approach to unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokazu</forename><surname>Narui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Fixmatch: Simplifying semi-supervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07685</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1139" to="1147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1195" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Interpolation consistency training for semi-supervised learning. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">EnAET: Self-trained ensemble autoencoding transformations for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Kihara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09265</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12848</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Unsupervised data augmentation. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<editor>Edwin R. Hancock Richard C. Wilson and William A. P. Smith</editor>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2016-09" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="87" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">S4l: Self-supervised semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1476" to="1485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
