<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Single Image Super-Resolution via a Holistic Attention Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Niu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Northeastern University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilei</forename><surname>Wen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Xidian University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">SKLOIS</orgName>
								<orgName type="institution">IIE</orgName>
								<address>
									<region>CAS</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Ren</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">SKLOIS</orgName>
								<orgName type="institution">IIE</orgName>
								<address>
									<region>CAS</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangde</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Northeastern University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianping</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Northeastern University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuzhen</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Xidian University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff4">
								<orgName type="laboratory">ANU 6 AI Labs</orgName>
								<address>
									<settlement>Didi Chuxing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Cao</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">SKLOIS</orgName>
								<orgName type="institution">IIE</orgName>
								<address>
									<region>CAS</region>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
								<orgName type="institution">Cyberspace Security Research Center</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Shen</surname></persName>
						</author>
						<title level="a" type="main">Single Image Super-Resolution via a Holistic Attention Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Super-Resolution</term>
					<term>Holistic Attention</term>
					<term>Layer Attention</term>
					<term>Channel- Spatial Attention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Informative features play a crucial role in the single image super-resolution task. Channel attention has been demonstrated to be effective for preserving information-rich features in each layer. However, channel attention treats each convolution layer as a separate process that misses the correlation among different layers. To address this problem, we propose a new holistic attention network (HAN), which consists of a layer attention module (LAM) and a channel-spatial attention module (CSAM), to model the holistic interdependencies among layers, channels, and positions. Specifically, the proposed LAM adaptively emphasizes hierarchical features by considering correlations among layers. Meanwhile, CSAM learns the confidence at all the positions of each channel to selectively capture more informative features. Extensive experiments demonstrate that the proposed HAN performs favorably against the state-ofthe-art single image super-resolution approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Single image super-resolution (SISR) is an important task in computer vision and image processing. Given a low-resolution image, the goal of super-resolution (SR) is to generate a high-resolution (HR) image with necessary edge structures and texture details. The advance of SISR will immediately benefit many application fields, such as video surveillance and pedestrian detection.</p><p>SRCNN <ref type="bibr" target="#b2">[3]</ref> is an unprecedented work to tackle the SR problem by learning the mapping function from LR input to HR output using convolutional neural networks (CNNs). Afterwards, numerous deep CNN-based methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref> have been proposed in recent years and generate a significant progress. The superior reconstruction performance of CNNs based methods are mainly from deep architecture and residual learning <ref type="bibr" target="#b6">[7]</ref>. Networks with very deep layers have Equal contribution † Corresponding author arXiv:2008.08767v1 [eess.IV] 20 Aug 2020 larger receptive fields and are able to provide a powerful capability to learn a complicated mapping between the LR input and the HR counterpart. Due to the residual learning, the depth of the SR networks are going to deeper since residual learning could efficiently alleviate the gradient vanishing and exploding problems.</p><p>Though significant progress have been made, we note that the texture details of the LR image often tend to be smoothed in the super-resolved result since most existing CNN-based SR methods neglect the feature correlation of intermediate layers. Therefore, generating detailed textures is still a non-trivial problem in the SR task. Although the results obtained by using channel attention <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b1">2]</ref> retain some detailed information, these channel attention-based approaches struggle in preserving informative textures and restoring natural details since they treat the feature maps at different layers equally and result in lossing some detail parts in the reconstructed image.</p><p>To address these problems, we present a novel approach termed as holistic attention network (HAN) that is capable of exploring the correlations among hierarchical layers, channels of each layer, and all positions of each channel. Therefore, HAN is able to stimulate the representational power of CNNs. Specifically, we propose a layer attention module (LAM) and a channel-spatial attention module (CSAM) in the HAN for more powerful feature expression and correlation learning. These two sub-attention modules are inspired by channel attention <ref type="bibr" target="#b39">[40]</ref> which weighs the internal features of each layer to make the network pay more attention to information-rich feature channels. However, we notice that channel attention cannot weight the features from multi-scale layers. Especially the long-term information from the shallow layers are easily weakened. Although the shallow features can be recycled via skip connections, they are treated equally with deep features across layers after long skip connection, hence hindering the representational ability of CNNs. To solve this problem, we consider exploring the interrelationship among features at hierarchical levels, and propose a layer attention module (LAM). On the other hand, channel attention neglects that the importance of different positions in each feature map varies significantly. Therefore, we also propose a channel-spatial attention module (CSAM) to collaboratively improve the discrimination ability of the proposed SR network.</p><p>Our contributions in this paper are summarized as follows:</p><p>• We propose a novel super-resolution algorithm named Holistic Attention Network (HAN), which enhances the representational ability of feature representations for super-resolution. • We introduce a layer attention module (LAM) to learn the weights for hierarchical features by considering correlations of multi-scale layers. Meanwhile, a channel-spatial attention module (CSAM) is presented to learn the channel and spatial interdependencies of features in each layer. • The proposed two attention modules collaboratively improve the SR results by modeling informative features among hierarchical layers, channels, and positions. Extensive experiments demonstrate that our algorithm performs favorably against the state-of-the-art SISR approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Numerous algorithms and models have been proposed to solve the problem of image SR, which can be roughly divided into two categories. One is the traditional algorithm <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b10">11]</ref>, the other one is the deep learning model based on neural network <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>. Due to the limitation of space, we only introduce the SR algorithms based on deep CNN.</p><p>Deep CNN for super-resolution. Dong et al. <ref type="bibr" target="#b2">[3]</ref> proposed a CNN architecture named SRCNN, which was the pioneering work to apply deep learning to single image super-resolution. Since SRCNN successfully applied deep learning network to SR task, various efficient and deeper architectures have been proposed for SR. Wang et al. <ref type="bibr" target="#b32">[33]</ref>combined the domain knowledge of sparse coding with a deep CNN and trained a cascade network to recover images progressively. To alleviate the phenomenon of gradient explosion and reduce the complexity of the model, DRCN <ref type="bibr" target="#b15">[16]</ref> and DRRN <ref type="bibr" target="#b29">[30]</ref> were proposed by using a recursive convolutional network. Lai et al. <ref type="bibr" target="#b18">[19]</ref> proposed a LapSR network which employs a pyramidal framework to progressively generate ×8 images by three sub-networks. Lim et al. <ref type="bibr" target="#b21">[22]</ref> modified the ResNet <ref type="bibr" target="#b6">[7]</ref> by removing batch normalization (BN) layers, which greatly improves the SR effect.</p><p>In addition to above MSE minimizing based methods, perceptual constraints are proposed to achieve better visual quality <ref type="bibr" target="#b27">[28]</ref>. SRGAN <ref type="bibr" target="#b19">[20]</ref> uses a generative adversarial networks (GAN) to predict high-resolution outputs by introducing a multi-task loss including a MSE loss, a perceptual loss <ref type="bibr" target="#b13">[14]</ref>, and an adversarial loss <ref type="bibr" target="#b4">[5]</ref>. Zhang et al. <ref type="bibr" target="#b41">[42]</ref> further transferred textures from reference images according to the textural similarity to enhance textures. However, the aforementioned models either result in the loss of detailed textures in intermediate features due to the very deep depth, or produce some unpleasing artifacts or inauthentic textures. In contrast, we propose a holistic attention network consists of a layer attention and a channel-spatial attention to investigate the interaction of different layers, channels, and positions.</p><p>Attention mechanism. Attention mechanisms direct the operational focus of deep neural networks to areas where there is more information. In short, they help the network ignore irrelevant information and focus on important information <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. Recently, attention mechanism has been successfully applied into deep CNN based image enhancement methods. Zhang et al. <ref type="bibr" target="#b39">[40]</ref> proposed a residual channel attention network (RCAN) in which residual channel attention blocks (RCAB) allow the network to focus on the more informative channels. Woo et al. <ref type="bibr" target="#b33">[34]</ref> proposed channel attention (CA) and spatial attention (SA) modules to exploit both inter-channel and inter-spatial relationship of feature maps. Kim et al. <ref type="bibr" target="#b16">[17]</ref> introduced a residual attention module for SR which is composed of residual blocks and spatial channel attention for learning the interchannel and intra-channel correlations. More recently, Dai et al. <ref type="bibr" target="#b1">[2]</ref> presented a second-order channel attention (SOCA) module to adaptively refine features using second-order feature statistics.</p><p>However, these attention based methods only consider the channel and spatial correlations while ignore the interdependencies between multi-scale layers. To   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Holistic Attention Network (HAN) for SR</head><p>In this section, we first present the overview of HAN network for SISR. Then we give the detailed configurations of the proposed layer attention module (LAM) and channel-spatial attention module (CSAM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Network Architecture</head><p>As shown in <ref type="figure" target="#fig_1">Figure 1</ref>, our proposed HAN consists of four parts: feature extraction, layer attention module, channel-spatial attention module, and the final reconstruction block.</p><p>Features extraction. Given a LR input I LR , a convolutional layer is used to extract the shallow feature F 0 of the LR input</p><formula xml:id="formula_0">F 0 = Conv(I LR ).</formula><p>(1)</p><p>Then we use the backbone of the RCAN <ref type="bibr" target="#b39">[40]</ref> to extract the intermediate features</p><formula xml:id="formula_1">F i of the LR input F i = H RBi (F i−1 ), i = 1, 2, ..., N,<label>(2)</label></formula><p>where H RBi represents the i-th residual group (RG) in the RCAN, N is the number of the residual groups. Therefore, except F N is the final output of RCAN network backbone, all other feature maps are intermediate outputs.</p><p>Holistic attention. After extracting hierarchical features F i by a set of residual groups, we further conduct a holistic feature weighting, which includes: i ) layer attention of hierarchical features, and ii ) channel-spatial attention of the last layer of RCAN.</p><p>The proposed layer attention makes full use of features from all the preceding layers and can be represented as</p><formula xml:id="formula_2">F L = H LA (concatenate(F 1 , F 2 , ..., F N )),<label>(3)</label></formula><p>where H LA represents the LAM which learns the feature correlation matrix of all the features from RGs' output and then weights the fused intermediate features F i capitalized on the correlation matrix (see Section 3.2). As a results, LAM enables the high contribution feature layers to be enhanced and the redundant ones to be suppressed. In addition, channel-spatial attention aims to modulate features for adaptively capturing more important information of inter-channel and intra-channel for the final reconstruction, which can be written as</p><formula xml:id="formula_3">F CS = H CSA (F N ),<label>(4)</label></formula><p>where H CSA represents the CSAM to produce channel-spatial attention for discriminately abtaining feature information, F CS denotes the filtered features after channel-spatial attention (details can be found in Section 3.3). Although we can filter all the intermediate features of F i using CSAM, we only modulate the last feature layer of F N as a trade-off between accuracy and speed. Image reconstruction. After obtaining features from both LAM and CSAM, we integrate the layer attention and channel-spatial attention units by elementwise summation. Then, we employ the sub-pixel convolution <ref type="bibr" target="#b28">[29]</ref> as the last upsampling module, which converts the scale sampling with a given magnification factor by pixel translation. We perform the sub-pixel convolution operation to aggregate low-resolution feature maps and simultaneously impose projection to high dimensional space to reconstruct the HR image. We formulate the process as follows</p><formula xml:id="formula_4">I SR = U ↑ (F 0 + F L + F CS ),<label>(5)</label></formula><p>where U ↑ represents the operation of sub-pixel convolution, and I SR is the reconstructed SR result. The long skip connection is introduced in HAN to stabilize the training of the proposed deep network, i.e., the sub-pixel upsampling block takes F 0 + F L + F CS as input.</p><p>Loss function. Since we employ the RCAN network as the backbone of the proposed method, only L 1 distance is selected as our loss function as in <ref type="bibr" target="#b39">[40]</ref> for a fair comparison</p><formula xml:id="formula_5">L(Θ) = 1 m m i=1 H HAN (I i LR ) − I i HR 1 = 1 m m i=1 I i SR − I i HR 1 ,<label>(6)</label></formula><p>where H HAN , Θ, and m denote the function of the proposed HAN, the learned parameter of the HAN, and the number of training pairs, respectively. Note that  we do not use other sophisticated loss functions such as adversarial loss <ref type="bibr" target="#b4">[5]</ref> and perceptual loss <ref type="bibr" target="#b13">[14]</ref>. We show that simply using the naive image intensity loss L(Θ) can already achieve competitive results as demonstrated in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Layer Attention Module</head><p>Although dense connections <ref type="bibr" target="#b9">[10]</ref> and skip connections <ref type="bibr" target="#b6">[7]</ref> allow shallow information to be bypassed to deep layers, these operations do not exploit interdependencies between the different layers. In contrast, we treat the feature maps from each layer as a response to a specific class, and the responses from different layers are related to each other. By obtaining the dependencies between features of different depths, the network can allocate different attention weights to features of different depths and automatically improve the representation ability of extracted features. Therefore, we propose an innovative LAM that learns the relationship between features of different depths, which automatically improve the feature representation ability.</p><p>The structure of the proposed layer attention is shown in <ref type="figure" target="#fig_3">Figure 2</ref>. The input of the module is the extracted intermediate feature groups F Gs, with the dimension of N ×H×W ×C, from N residual groups. Then, we reshape the feature groups F Gs into a 2D matrix with the dimension of N × HW C, and apply matrix multiplication with the corresponding transpose to calculate the correlation W la = w N i,j=1 between different layers</p><formula xml:id="formula_6">w j,i = δ(ϕ(F G) i · (ϕ(F G)) T j ), i, j = 1, 2, ..., N,<label>(7)</label></formula><p>where δ(·) and ϕ(·) denote the softmax and reshape operations, x i,j represents the correlation index between i-th and j-th feature groups. Finally, we multiply the reshaped feature groups F Gs by the predicted correlation matrix with a scale factor α, and add the input features F Gs</p><formula xml:id="formula_7">F Lj = α N i=1 w i,j F G i + F G j ,<label>(8)</label></formula><p>where α is initialized to 0 and is automatically assigned by the network in the following epochs. As a result, the weighted sum of features allow the main parts of network to focus on more informative layers of the intermediate LR features. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Channel-Spatial Attention</head><p>The existing spatial attention mechanisms <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b16">17]</ref> mainly focuse on the scale dimension of the feature, with little uptake of channel dimension information, while the recent channel attention mechanisms <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b1">2]</ref> ignore the scale information. To solve this problem, we propose a novel channel-spatial attention mechanism (CSAM) that contains responses from all dimensions of the feature maps. Note that although we can perform the CSAM for all the feature groups F Gs extracted from RCAN, we only modulate the last feature group of F N for a trade-off between accuracy and speed as shown in <ref type="figure" target="#fig_1">Figure 1</ref>.</p><p>The architecture of the proposed CSAM is shown in <ref type="figure" target="#fig_4">Figure 3</ref>. Given the last layer feature maps F N ∈ R H×W ×C , we feed F N to a 3D convolution layer <ref type="bibr" target="#b12">[13]</ref> to generate attention map by capturing joint channel and spatial features. We operate the 3D convolution via convolving 3D kernels with the cube constructed from multiple neighboring channels of F N . Specifically, we perform 3D convolutions with kernel size of 3 × 3 × 3 with step size of 1 (i.e., three groups of consecutive channels are convolved with a set of 3D kernels respectively), resulting in three groups of channel-spatial attention maps W csa . By doing so, our CSAM can extract powerful representations to describe inter-channel and intra-channel information in continuous channels.</p><p>In addition, we perform element-wise multiplication with the attention map W csa and the input feature F N . Finally, multiply the weighted result by a scale factor β, and then add the input feature F N to obtain the weighted features</p><formula xml:id="formula_8">F CS = βσ(W csa ) F N + F N ,<label>(9)</label></formula><p>where σ(·) is the sigmoid function, is the element-wise product, the scale factor β is initialized as 0 and progressively improved in the follow iterations. As a results, F CS is the weighted sum of all channel-spatial position features as well as the original features. Compared with conventional spatial attention and channel attention, our CSAM adaptively learns the inter-channel and intra-channel feature responses by explicitly modelling channel-wise and spatial feature interdependencies. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we first analyze the contributions of the proposed two attention modules. We then compare our HAN with state-of-the-art algorithms on five benchmark datasets. The implementation code will be made available to the public. Results on more images can be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Settings</head><p>Datasets. We selecte DIV2K <ref type="bibr" target="#b31">[32]</ref> as the training set as like in <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b21">22]</ref>. For the testing set, we choose five standard datasets: Set5 <ref type="bibr" target="#b0">[1]</ref>, Set14 <ref type="bibr" target="#b35">[36]</ref>, B100 <ref type="bibr" target="#b22">[23]</ref>, Urban100 <ref type="bibr" target="#b10">[11]</ref>, and Manga109 <ref type="bibr" target="#b23">[24]</ref>. Degraded data was obtained by bilinear interpolation and Blur-downscale Degradation model. Following <ref type="bibr" target="#b39">[40]</ref>, the reconstruct RGB results by the proposed HAN are first converted to YCbCr space, and then we only consider the luminance channel to calculate PSNR and SSIM in our experiments. Implementation Details. We implement the proposed network using Py-Torch platform and use the pre-trained RCAN (×2), (×3), (×4), (×8) model to initialize the corresponding holistic attention networks, respectively. In our network, patch size is set as 64 × 64. We use ADAM <ref type="bibr" target="#b17">[18]</ref> optimizer with a batch size 16 for training. The learning rate is set as 10 −5 . Default values of β 1 and β 2 are used, which are 0.9 and 0.999, respectively, and we set = 10 −8 . We do not use any regularization operations such as batch normalization and group normalization in our network. In addition to random rotation and translation, we do not apply other data augmentation methods in the training. The input of the LAM is selected as the outputs of all residual groups of RCAN, we use N = 10 residual groups in out network. For all the results reported in the paper, we train the network for 250 epochs, which takes about two days on an Nvidia GTX 1080Ti GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study about the Proposed LAM and CSAM</head><p>The proposed LAM and CSAM ensure that the proposed SR method generate the feature correlations between hierarchical layers, channels, and locations. One may wonder whether the LAM and CSAM help SISR. To verify the performance of these two attention mechanisms, we compare the method without using LAM and CSAM in <ref type="table">Table 1</ref>, where we conduct experiments on the Manga109 dataset with the magnification factor of ×4. <ref type="table">Table 1</ref> shows the quantitative evaluations. Compared with the baseline method which is identical to the proposed network except for the absence of these two modules LAM and CSAM. CSAM achieves better results by up to 0.06 dB in terms of PSNR, while LAM promotes 0.16 dB on the test dataset. In addition, the improvement of using both LAM and CSAM is significant as the proposed algorithm improves 0.2 dB, which demonstrates the effectiveness of the proposed layer attention and channel-spatial attention blocks. <ref type="figure">Figure 4</ref> further shows that using the LAM and CSAM is able to generate the results with clearer structures and details. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study about the Number of Residual Group</head><p>We conduct an ablation study about feeding different numbers of RGs to the proposed LAM. Specifically, we apply severally three, six, and ten RGs to the LAM, and we evaluate our model on five standard datasets. As shown in <ref type="table">Table  2</ref>, we compare our three models with RCAN, although using fewer RGs, our algorithm still generates higher PSNR values than the baseline of RCAN. This ablation study demonstrates the effectiveness of the proposed LAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study about the Number of CSAM</head><p>In the paper, the channel-spatial attention module (CSAM) can extract powerful representations to describe inter-channel and intra-channel information in continuous channels. We conduct an ablation study about using different numbers of CSAM. We use one, three, five, and ten CSAMs in RGs. As shown in Table5, with the increase of CSAM, the values of PSNR are increasing on the testing datasets. This ablation study demonstrates the effectiveness of the proposed CSAM. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Results with Bicubic (BI) Degradation Model</head><p>We compare the proposed algorithm with 11 state-of-the-art methods: SRCNN <ref type="bibr" target="#b2">[3]</ref>, FSRCNN <ref type="bibr" target="#b3">[4]</ref>, VDSR <ref type="bibr" target="#b14">[15]</ref>, LapSRN <ref type="bibr" target="#b18">[19]</ref>, MemNet <ref type="bibr" target="#b30">[31]</ref>, SRMDNF <ref type="bibr" target="#b37">[38]</ref>, D-DBPN <ref type="bibr" target="#b5">[6]</ref>, RDN <ref type="bibr" target="#b40">[41]</ref>, EDSR <ref type="bibr" target="#b21">[22]</ref>, SRFBN <ref type="bibr" target="#b20">[21]</ref> and SAN <ref type="bibr" target="#b1">[2]</ref>. We provide more comparisons in supplementary material. Following <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b39">40]</ref>, we also propose self-ensemble model and donate it as HAN+.</p><p>Quantitative results. <ref type="table" target="#tab_3">Table 3</ref> shows the comparison of 2×, 3×, 4×, and 8× SR quantitative results. Compared to existing methods, our HAN+ performs best on all the scales of reconstructed test datasets. Without using self-ensemble, our network HAN still obtains great gain compared with the recent SR methods.</p><p>In particular, our model is much better than SAN which also uses the same backbone network of RCAN and has more computationally intensive attention module. Specifically, when we compare the reconstruction results at ×8 scale on the Set5 dataset, the proposed HAN advances 0.11 dB in terms of PSNR than the competitive SAN.</p><p>To further evaluate the proposed HAN, we conduct experiments on the large test sets of B100, Urban100, and Manga109. Our algorithm still performs favorably against the state-of-the-art methods. For example, the super-resolved  Visual results. We also show visual comparisons of various methods on the Urban100 dataset for 4× SR in <ref type="figure">Figure 4</ref>. As shown, most compared SR networks cannot recover the grids of buildings accurately and suffer from unpleasant blurring artifacts. In contrast, the proposed HAN obtains clearer details and reconstructs sharper high-frequency textures.</p><p>Take the first and fourth images in <ref type="figure">Figure 4</ref> as example, VDSR and EDSR fail to generate the clear structures. The results generated by the recent work of RCAN, SRFBN, and SAN still contain noticeable artifacts caused by spatial aliasing. In contrast, our approach effectively suppresses such artifacts through the proposed two attention modules. As shown, our method accurately reconstructs the grid patterns on windows in the first row and the parallel straight lines on the building in the fourth image.</p><p>For 8× SR, we also show the super-resolved results by different SR methods in <ref type="figure">Figure 5</ref>. As show, it is challenging to predict HR images from bicubicupsampled input by VDSR and EDSR. Even the state-of-the-art methods of RCAN and SRFBN cannot super-resolve the fine structures well. In contrast, our HAN reconstructs high-quality HR images for 8× results by using cross-scale layer attention and channel-spatial attention modules on the limited information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Results with Blur-downscale Degradation (BD) Model</head><p>Quantitative results. Following the protocols of <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b40">41]</ref>, we further compare the SR results on images with blur-downscale degradation model. We compare the proposed method with nine state-of-the-art super-resolution methods: SPMSR <ref type="bibr" target="#b24">[25]</ref>, SRCNN <ref type="bibr" target="#b2">[3]</ref>, FSRCNN <ref type="bibr" target="#b3">[4]</ref>, VDSR <ref type="bibr" target="#b14">[15]</ref>, IRCNN <ref type="bibr" target="#b36">[37]</ref>, SRMD <ref type="bibr" target="#b38">[39]</ref>, RDN <ref type="bibr" target="#b40">[41]</ref>, RCAN <ref type="bibr" target="#b39">[40]</ref>,SRFBN <ref type="bibr" target="#b20">[21]</ref> and SAN <ref type="bibr" target="#b1">[2]</ref>. Quantitative results on the 3× SR are reported in <ref type="table" target="#tab_5">Table 4</ref>. As shown, both the proposed HAN and HAN+ perform favorably against existing methods. In particular, our HAN+ yields the best quantitative results and HAN obtains the second best scores for all the datasets, 0.06-0.2 dB PSNR better than the attention-based methods of RCAN and SAN and 0.2-0.8 dB better than the recently proposed SRFBN.</p><p>Visual quality. In <ref type="figure">Figure 6</ref>, we show visual results on images from the Urban 100 dataset with blur-downscale degradation model by a scale factor of 3. Both the full images and the cropped regions are shown for comparison. We find that our proposed HAN is able to recover structured details that were missing in the LR image by properly exploiting the layer, channel, and spatial attention in the feature space.</p><p>As shown, VDSR and EDSR suffer from unpleasant blurring artifacts and some results even are out of shape. RCAN alleviate it to a certain extent, but still misses some details and structures. SRFBN and SAN also fail to recover these structured details. In contrast, our proposed HAN effectively suppresses artifacts and exploits the scene details and the internal natural image statistics to super-resolve the high-frequency contents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we propose a holistic attention network for single image superresolution, which adaptively learns the global dependencies among different depths, channels, and positions using the self-attention mechanism. Specifically, the layer attention module captures the long-distance dependencies among hierarchical layers. Meanwhile, the channel-spatial attention module incorporates the channel and contextual information in each layer. These two attention modules are collaboratively applied to multi-level features and then more informative features can be captured. Extensive experimental results on benchmark datasets demonstrate that the proposed model performs favorably against the state-ofthe-art SR algorithms in terms of accuracy and visual quality.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Network architecture of the proposed holistic attention network(HAN). Given a low-resolution image, the first convolutional layer of the HAN extracts a set of shallow feature maps. Then a series of residual groups further extract deeper feature representations of the low-resolution input. We propose a layer attention module (LAM) to learn the correlations of each output from RGs and a channel-spatial attention module (CSAM) to investigate the interdependencies between channels and pixels. Finally, an upsampling block produces the high-resolution image solve this problem, we propose a layer attention module (LAM) to exploit the nonlinear feature interactions among hierarchical layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Architecture of the proposed layer attention module</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Architecture of the proposed channel-spatial attention module</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>SSIM 24.18/0.678 25.63/0.763 27.66/0.849 27.12/0.832 27.95/0.857 27.43/0.843 27.99/0.857 28.05/0.859 PSNR/SSIM 22.97/0.636 24.59/0.741 24.00/0.695 24.00/0.698 24.26/ 0.711 24.13/0.706 24.20/0.709 24.87/ 0.710</figDesc><table><row><cell>HR</cell><cell>Bicubic</cell><cell>VDSR[15]</cell><cell>EDSR[22]</cell><cell>RDN[41]</cell><cell>RCAN[40] SRFBN[21]</cell><cell>SAN[2]</cell><cell>HAN(our)</cell></row><row><cell>PSNR/</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>PSNR/SSIM 25.71/0.680 26.62/0.725 27.96/0.795 27.53/0.782 28.63/0.805 27.74/0.789 28.40/0.800 28.67/0.805 PSNR/SSIM 21.32/0.686 23.07/0.783 26.33/0.895 25.62/0.880 26.46/0.897 26.57/0.897 26.87/0.900 26.98/0.900 Fig. 4. Visual comparison for 4× SR with BI degradation model on the Urban100 datasets. The best results are highlighted. Our method obtains better visual quality and recovers more image details compared with other state-of-the-art SR methods</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Effectiveness of the proposed LAM and CSAM for image super-resolution Ablation study about using different numbers of RGs</figDesc><table><row><cell></cell><cell>baseline</cell><cell cols="2">w/o CSAM</cell><cell>w/o LAM</cell><cell>Ours</cell></row><row><cell>PSNR/SSIM</cell><cell>31.22/0.9173</cell><cell cols="2">31.38/0.9175</cell><cell>31.28/0.9174</cell><cell>31.42/0.9177</cell></row><row><cell></cell><cell>Set5</cell><cell>Set14</cell><cell>B100</cell><cell>Urban100</cell><cell>Manga100</cell></row><row><cell>RCAN</cell><cell>32.63</cell><cell>28.87</cell><cell>27.77</cell><cell>26.82</cell><cell>31.22</cell></row><row><cell>HAN 3RGs</cell><cell>32.63</cell><cell>28.89</cell><cell>27.79</cell><cell>26.82</cell><cell>31.40</cell></row><row><cell>HAN 6RGs</cell><cell>32.64</cell><cell>28.90</cell><cell>27.79</cell><cell>26.84</cell><cell>31.42</cell></row><row><cell>HAN 10RGs</cell><cell>32.64</cell><cell>28.90</cell><cell>27.80</cell><cell>26.85</cell><cell>31.42</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Quantitative results with BI degradation model. The best and second best results are highlighted in bold and underlined</figDesc><table><row><cell>Methods</cell><cell>Scale</cell><cell cols="10">Set5 PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM Set14 B100 Urban100 Manga109</cell></row><row><cell>Bicubic</cell><cell>×2</cell><cell>33.66</cell><cell>0.9299</cell><cell>30.24</cell><cell>0.8688</cell><cell>29.56</cell><cell>0.8431</cell><cell>26.88</cell><cell>0.8403</cell><cell>30.80</cell><cell>0.9339</cell></row><row><cell>SRCNN [3]</cell><cell>×2</cell><cell>36.66</cell><cell>0.9542</cell><cell>32.45</cell><cell>0.9067</cell><cell>31.36</cell><cell>0.8879</cell><cell>29.50</cell><cell>0.8946</cell><cell>35.60</cell><cell>0.9663</cell></row><row><cell>FSRCNN [4]</cell><cell>×2</cell><cell>37.05</cell><cell>0.9560</cell><cell>32.66</cell><cell>0.9090</cell><cell>31.53</cell><cell>0.8920</cell><cell>29.88</cell><cell>0.9020</cell><cell>36.67</cell><cell>0.9710</cell></row><row><cell>VDSR [15]</cell><cell>×2</cell><cell>37.53</cell><cell>0.9590</cell><cell>33.05</cell><cell>0.9130</cell><cell>31.90</cell><cell>0.8960</cell><cell>30.77</cell><cell>0.9140</cell><cell>37.22</cell><cell>0.9750</cell></row><row><cell>LapSRN [19]</cell><cell>×2</cell><cell>37.52</cell><cell>0.9591</cell><cell>33.08</cell><cell>0.9130</cell><cell>31.08</cell><cell>0.8950</cell><cell>30.41</cell><cell>0.9101</cell><cell>37.27</cell><cell>0.9740</cell></row><row><cell>MemNet [31]</cell><cell>×2</cell><cell>37.78</cell><cell>0.9597</cell><cell>33.28</cell><cell>0.9142</cell><cell>32.08</cell><cell>0.8978</cell><cell>31.31</cell><cell>0.9195</cell><cell>37.72</cell><cell>0.9740</cell></row><row><cell>EDSR [22]</cell><cell>×2</cell><cell>38.11</cell><cell>0.9602</cell><cell>33.92</cell><cell>0.9195</cell><cell>32.32</cell><cell>0.9013</cell><cell>32.93</cell><cell>0.9351</cell><cell>39.10</cell><cell>0.9773</cell></row><row><cell>SRMDNF [38]</cell><cell>×2</cell><cell>37.79</cell><cell>0.9601</cell><cell>33.32</cell><cell>0.9159</cell><cell>32.05</cell><cell>0.8985</cell><cell>31.33</cell><cell>0.9204</cell><cell>38.07</cell><cell>0.9761</cell></row><row><cell>D-DBPN [6]</cell><cell>×2</cell><cell>38.09</cell><cell>0.9600</cell><cell>33.85</cell><cell>0.9190</cell><cell>32.27</cell><cell>0.9000</cell><cell>32.55</cell><cell>0.9324</cell><cell>38.89</cell><cell>0.9775</cell></row><row><cell>RDN [41]</cell><cell>×2</cell><cell>38.24</cell><cell>0.9614</cell><cell>34.01</cell><cell>0.9212</cell><cell>32.34</cell><cell>0.9017</cell><cell>32.89</cell><cell>0.9353</cell><cell>39.18</cell><cell>0.9780</cell></row><row><cell>RCAN [40]</cell><cell>×2</cell><cell>38.27</cell><cell>0.9614</cell><cell>34.12</cell><cell>0.9216</cell><cell>32.41</cell><cell>0.9027</cell><cell>33.34</cell><cell>0.9384</cell><cell>39.44</cell><cell>0.9786</cell></row><row><cell>SRFBN [21]</cell><cell>×2</cell><cell>38.11</cell><cell>0.9609</cell><cell>33.82</cell><cell>0.9196</cell><cell>32.29</cell><cell>0.9010</cell><cell>32.62</cell><cell>0.9328</cell><cell>39.08</cell><cell>0.9779</cell></row><row><cell>SAN [2]</cell><cell>×2</cell><cell>38.31</cell><cell>0.9620</cell><cell>34.07</cell><cell>0.9213</cell><cell>32.42</cell><cell>0.9028</cell><cell>33.10</cell><cell>0.9370</cell><cell>39.32</cell><cell>0.9792</cell></row><row><cell>HAN(ours)</cell><cell>×2</cell><cell>38.27</cell><cell>0.9614</cell><cell>34.16</cell><cell>0.9217</cell><cell>32.41</cell><cell>0.9027</cell><cell>33.35</cell><cell>0.9385</cell><cell>39.46</cell><cell>0.9785</cell></row><row><cell>HAN+(ours)</cell><cell>×2</cell><cell>38.33</cell><cell>0.9617</cell><cell>34.24</cell><cell>0.9224</cell><cell>32.45</cell><cell>0.9030</cell><cell>33.53</cell><cell>0.9398</cell><cell>39.62</cell><cell>0.9787</cell></row><row><cell>Bicubic</cell><cell>×3</cell><cell>30.39</cell><cell>0.8682</cell><cell>27.55</cell><cell>0.7742</cell><cell>27.21</cell><cell>0.7385</cell><cell>24.46</cell><cell>0.7349</cell><cell>26.95</cell><cell>0.8556</cell></row><row><cell>SRCNN [3]</cell><cell>×3</cell><cell>32.75</cell><cell>0.9090</cell><cell>29.30</cell><cell>0.8215</cell><cell>28.41</cell><cell>0.7863</cell><cell>26.24</cell><cell>0.7989</cell><cell>30.48</cell><cell>0.9117</cell></row><row><cell>FSRCNN [4]</cell><cell>×3</cell><cell>33.18</cell><cell>0.9140</cell><cell>29.37</cell><cell>0.8240</cell><cell>28.53</cell><cell>0.7910</cell><cell>26.43</cell><cell>0.8080</cell><cell>31.10</cell><cell>0.9210</cell></row><row><cell>VDSR [15]</cell><cell>×3</cell><cell>33.67</cell><cell>0.9210</cell><cell>29.78</cell><cell>0.8320</cell><cell>28.83</cell><cell>0.7990</cell><cell>27.14</cell><cell>0.8290</cell><cell>32.01</cell><cell>0.9340</cell></row><row><cell>LapSRN [19]</cell><cell>×3</cell><cell>33.82</cell><cell>0.9227</cell><cell>29.87</cell><cell>0.8320</cell><cell>28.82</cell><cell>0.7980</cell><cell>27.07</cell><cell>0.8280</cell><cell>32.21</cell><cell>0.9350</cell></row><row><cell>MemNet [31]</cell><cell>×3</cell><cell>34.09</cell><cell>0.9248</cell><cell>30.00</cell><cell>0.8350</cell><cell>28.96</cell><cell>0.8001</cell><cell>27.56</cell><cell>0.8376</cell><cell>32.51</cell><cell>0.9369</cell></row><row><cell>EDSR [22]</cell><cell>×3</cell><cell>34.65</cell><cell>0.9280</cell><cell>30.52</cell><cell>0.8462</cell><cell>29.25</cell><cell>0.8093</cell><cell>28.80</cell><cell>0.8653</cell><cell>34.17</cell><cell>0.9476</cell></row><row><cell>SRMDNF [38]</cell><cell>×3</cell><cell>34.12</cell><cell>0.9254</cell><cell>30.04</cell><cell>0.8382</cell><cell>28.97</cell><cell>0.8025</cell><cell>27.57</cell><cell>0.8398</cell><cell>33.00</cell><cell>0.9403</cell></row><row><cell>RDN [41]</cell><cell>×3</cell><cell>34.71</cell><cell>0.9296</cell><cell>30.57</cell><cell>0.8468</cell><cell>29.26</cell><cell>0.8093</cell><cell>28.80</cell><cell>0.8653</cell><cell>34.13</cell><cell>0.9484</cell></row><row><cell>RCAN [40]</cell><cell>×3</cell><cell>34.74</cell><cell>0.9299</cell><cell>30.65</cell><cell>0.8482</cell><cell>29.32</cell><cell>0.8111</cell><cell>29.09</cell><cell>0.8702</cell><cell>34.44</cell><cell>0.9499</cell></row><row><cell>SRFBN [21]</cell><cell>×3</cell><cell>34.70</cell><cell>0.9292</cell><cell>30.51</cell><cell>0.8461</cell><cell>29.24</cell><cell>0.8084</cell><cell>28.73</cell><cell>0.8641</cell><cell>34.18</cell><cell>0.9481</cell></row><row><cell>SAN [2]</cell><cell>×3</cell><cell>34.75</cell><cell>0.9300</cell><cell>30.59</cell><cell>0.8476</cell><cell>29.33</cell><cell>0.8112</cell><cell>28.93</cell><cell>0.8671</cell><cell>34.30</cell><cell>0.9494</cell></row><row><cell>HAN(ours)</cell><cell>×3</cell><cell>34.75</cell><cell>0.9299</cell><cell>30.67</cell><cell>0.8483</cell><cell>29.32</cell><cell>0.8110</cell><cell>29.10</cell><cell>0.8705</cell><cell>34.48</cell><cell>0.9500</cell></row><row><cell>HAN+(ours)</cell><cell>×3</cell><cell>34.85</cell><cell>0.9305</cell><cell>30.77</cell><cell>0.8495</cell><cell>29.39</cell><cell>0.8120</cell><cell>29.30</cell><cell>0.8735</cell><cell>34.80</cell><cell>0.9514</cell></row><row><cell>Bicubic</cell><cell>×4</cell><cell>28.42</cell><cell>0.8104</cell><cell>26.00</cell><cell>0.7027</cell><cell>25.96</cell><cell>0.6675</cell><cell>23.14</cell><cell>0.6577</cell><cell>24.89</cell><cell>0.7866</cell></row><row><cell>SRCNN [3]</cell><cell>×4</cell><cell>30.48</cell><cell>0.8628</cell><cell>27.50</cell><cell>0.7513</cell><cell>26.90</cell><cell>0.7101</cell><cell>24.52</cell><cell>0.7221</cell><cell>27.58</cell><cell>0.8555</cell></row><row><cell>FSRCNN [4]</cell><cell>×4</cell><cell>30.72</cell><cell>0.8660</cell><cell>27.61</cell><cell>0.7550</cell><cell>26.98</cell><cell>0.7150</cell><cell>24.62</cell><cell>0.7280</cell><cell>27.90</cell><cell>0.8610</cell></row><row><cell>VDSR [15]</cell><cell>×4</cell><cell>31.35</cell><cell>0.8830</cell><cell>28.02</cell><cell>0.7680</cell><cell>27.29</cell><cell>0.0726</cell><cell>25.18</cell><cell>0.7540</cell><cell>28.83</cell><cell>0.8870</cell></row><row><cell>LapSRN [19]</cell><cell>×4</cell><cell>31.54</cell><cell>0.8850</cell><cell>28.19</cell><cell>0.7720</cell><cell>27.32</cell><cell>0.7270</cell><cell>25.21</cell><cell>0.7560</cell><cell>29.09</cell><cell>0.8900</cell></row><row><cell>MemNet [31]</cell><cell>×4</cell><cell>31.74</cell><cell>0.8893</cell><cell>28.26</cell><cell>0.7723</cell><cell>27.40</cell><cell>0.7281</cell><cell>25.50</cell><cell>0.7630</cell><cell>29.42</cell><cell>0.8942</cell></row><row><cell>EDSR [22]</cell><cell>×4</cell><cell>32.46</cell><cell>0.8968</cell><cell>28.80</cell><cell>0.7876</cell><cell>27.71</cell><cell>0.7420</cell><cell>26.64</cell><cell>0.8033</cell><cell>31.02</cell><cell>0.9148</cell></row><row><cell>SRMDNF [38]</cell><cell>×4</cell><cell>31.96</cell><cell>0.8925</cell><cell>28.35</cell><cell>0.7787</cell><cell>27.49</cell><cell>0.7337</cell><cell>25.68</cell><cell>0.7731</cell><cell>30.09</cell><cell>0.9024</cell></row><row><cell>D-DBPN [6]</cell><cell>×4</cell><cell>32.47</cell><cell>0.8980</cell><cell>28.82</cell><cell>0.7860</cell><cell>27.72</cell><cell>0.7400</cell><cell>26.38</cell><cell>0.7946</cell><cell>30.91</cell><cell>0.9137</cell></row><row><cell>RDN [41]</cell><cell>×4</cell><cell>32.47</cell><cell>0.8990</cell><cell>28.81</cell><cell>0.7871</cell><cell>27.72</cell><cell>0.7419</cell><cell>26.61</cell><cell>0.8028</cell><cell>31.00</cell><cell>0.9151</cell></row><row><cell>RCAN [40]</cell><cell>×4</cell><cell>32.63</cell><cell>0.9002</cell><cell>28.87</cell><cell>0.7889</cell><cell>27.77</cell><cell>0.7436</cell><cell>26.82</cell><cell>0.8087</cell><cell>31.22</cell><cell>0.9173</cell></row><row><cell>SRFBN [21]</cell><cell>×4</cell><cell>32.47</cell><cell>0.8983</cell><cell>28.81</cell><cell>0.7868</cell><cell>27.72</cell><cell>0.7409</cell><cell>26.60</cell><cell>0.8015</cell><cell>31.15</cell><cell>0.9160</cell></row><row><cell>SAN [2]</cell><cell>×4</cell><cell>32.64</cell><cell>0.9003</cell><cell>28.92</cell><cell>0.7888</cell><cell>27.78</cell><cell>0.7436</cell><cell>26.79</cell><cell>0.8068</cell><cell>31.18</cell><cell>0.9169</cell></row><row><cell>HAN(ours)</cell><cell>×4</cell><cell>32.64</cell><cell>0.9002</cell><cell>28.90</cell><cell>0.7890</cell><cell>27.80</cell><cell>0.7442</cell><cell>26.85</cell><cell>0.8094</cell><cell>31.42</cell><cell>0.9177</cell></row><row><cell>HAN+(ours)</cell><cell>×4</cell><cell>32.75</cell><cell>0.9016</cell><cell>28.99</cell><cell>0.7907</cell><cell>27.85</cell><cell>0.7454</cell><cell>27.02</cell><cell>0.8131</cell><cell>31.73</cell><cell>0.9207</cell></row><row><cell>Bicubic</cell><cell>×8</cell><cell>24.40</cell><cell>0.6580</cell><cell>23.10</cell><cell>0.5660</cell><cell>23.67</cell><cell>0.5480</cell><cell>20.74</cell><cell>0.5160</cell><cell>21.47</cell><cell>0.6500</cell></row><row><cell>SRCNN [3]</cell><cell>×8</cell><cell>25.33</cell><cell>0.6900</cell><cell>23.76</cell><cell>0.5910</cell><cell>24.13</cell><cell>0.5660</cell><cell>21.29</cell><cell>0.5440</cell><cell>22.46</cell><cell>0.6950</cell></row><row><cell>FSRCNN [4]</cell><cell>×8</cell><cell>20.13</cell><cell>0.5520</cell><cell>19.75</cell><cell>0.4820</cell><cell>24.21</cell><cell>0.5680</cell><cell>21.32</cell><cell>0.5380</cell><cell>22.39</cell><cell>0.6730</cell></row><row><cell>SCN [33]</cell><cell>×8</cell><cell>25.59</cell><cell>0.7071</cell><cell>24.02</cell><cell>0.6028</cell><cell>24.30</cell><cell>0.5698</cell><cell>21.52</cell><cell>0.5571</cell><cell>22.68</cell><cell>0.6963</cell></row><row><cell>VDSR [15]</cell><cell>×8</cell><cell>25.93</cell><cell>0.7240</cell><cell>24.26</cell><cell>0.6140</cell><cell>24.49</cell><cell>0.5830</cell><cell>21.70</cell><cell>0.5710</cell><cell>23.16</cell><cell>0.7250</cell></row><row><cell>LapSRN [19]</cell><cell>×8</cell><cell>26.15</cell><cell>0.7380</cell><cell>24.35</cell><cell>0.6200</cell><cell>24.54</cell><cell>0.5860</cell><cell>21.81</cell><cell>0.5810</cell><cell>23.39</cell><cell>0.7350</cell></row><row><cell>MemNet [31]</cell><cell>×8</cell><cell>26.16</cell><cell>0.7414</cell><cell>24.38</cell><cell>0.6199</cell><cell>24.58</cell><cell>0.5842</cell><cell>21.89</cell><cell>0.5825</cell><cell>23.56</cell><cell>0.7387</cell></row><row><cell>MSLapSRN[19]</cell><cell>×8</cell><cell>26.34</cell><cell>0.7558</cell><cell>24.57</cell><cell>0.6273</cell><cell>24.65</cell><cell>0.5895</cell><cell>22.06</cell><cell>0.5963</cell><cell>23.90</cell><cell>0.7564</cell></row><row><cell>EDSR [22]</cell><cell>×8</cell><cell>26.96</cell><cell>0.7762</cell><cell>24.91</cell><cell>0.6420</cell><cell>24.81</cell><cell>0.5985</cell><cell>22.51</cell><cell>0.6221</cell><cell>24.69</cell><cell>0.7841</cell></row><row><cell>D-DBPN [6]</cell><cell>×8</cell><cell>27.21</cell><cell>0.7840</cell><cell>25.13</cell><cell>0.6480</cell><cell>24.88</cell><cell>0.6010</cell><cell>22.73</cell><cell>0.6312</cell><cell>25.14</cell><cell>0.7987</cell></row><row><cell>RCAN [40]</cell><cell>×8</cell><cell>27.31</cell><cell>0.7878</cell><cell>25.23</cell><cell>0.6511</cell><cell>24.98</cell><cell>0.6058</cell><cell>23.00</cell><cell>0.6452</cell><cell>25.24</cell><cell>0.8029</cell></row><row><cell>SAN [2]</cell><cell>×8</cell><cell>27.22</cell><cell>0.7829</cell><cell>25.14</cell><cell>0.6476</cell><cell>24.88</cell><cell>0.6011</cell><cell>22.70</cell><cell>0.6314</cell><cell>24.85</cell><cell>0.7906</cell></row><row><cell>HAN(ours)</cell><cell>×8</cell><cell>27.33</cell><cell>0.7884</cell><cell>25.24</cell><cell>0.6510</cell><cell>24.98</cell><cell>0.6059</cell><cell>22.98</cell><cell>0.6437</cell><cell>25.20</cell><cell>0.8011</cell></row><row><cell>HAN+(ours)</cell><cell>×8</cell><cell>27.47</cell><cell>0.7920</cell><cell>25.39</cell><cell>0.6552</cell><cell>25.04</cell><cell>0.6075</cell><cell>23.20</cell><cell>0.6518</cell><cell>25.54</cell><cell>0.8080</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>SSIM 27.70/ 0.774 30.10/0.854 30.64/0.878 36.39/0.951 30.75/.879 34.31/0.930 36.44/ 0.955 36.62/0.956</figDesc><table><row><cell>HR</cell><cell>Bicubic</cell><cell>VDSR [15] EDSR [22] RCAN [40] SRFBN [21]</cell><cell>SAN [2]</cell><cell>HAN(our) HAN+(our)</cell></row><row><cell cols="5">PSNR/PSNR/SSIM 22.17/0.674 23.39/ 0.747 24.19/0.785 27.18/0.882 24.20/0.788 26.56/0.873 27.40/0.889 27.67/0.893</cell></row><row><cell cols="5">PSNR/SSIM 19.93/0.425 20.66/ 0.508 20.89/0.531 22.34/ 0.675 20.92/ 0.534 22.07/0.656 22.35/.677 22.49/0.681</cell></row><row><cell cols="5">PSNR/SSIM 20.85/0.590 21.92/0.671 22.17/ 0.692 24.26/ 0.814 23.98/ 0.802 24.20/ 0.805 24.28/0.819 24.65/0.828</cell></row><row><cell cols="5">Fig. 6. Visual comparison for 3× SR with BD model on the Urban100 dataset. The</cell></row><row><cell cols="3">best results are highlighted</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Quantitative results with BD degradation model. The best and second best results are highlighted in bold and underlined</figDesc><table><row><cell>Method</cell><cell>Scale</cell><cell cols="10">Set5 PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM Set14 B100 Urban100 Manga109</cell></row><row><cell>Bicubic</cell><cell>×3</cell><cell>28.78</cell><cell>0.8308</cell><cell>26.38</cell><cell>0.7271</cell><cell>26.33</cell><cell>0.6918</cell><cell>23.52</cell><cell>0.6862</cell><cell>25.46</cell><cell>0.8149</cell></row><row><cell>SPMSR [25]</cell><cell>×3</cell><cell>32.21</cell><cell>0.9001</cell><cell>28.89</cell><cell>0.8105</cell><cell>28.13</cell><cell>0.7740</cell><cell>25.84</cell><cell>0.7856</cell><cell>29.64</cell><cell>0.9003</cell></row><row><cell>SRCNN [3]</cell><cell>×3</cell><cell>32.05</cell><cell>0.8944</cell><cell>28.80</cell><cell>0.8074</cell><cell>28.13</cell><cell>0.7736</cell><cell>25.70</cell><cell>0.7770</cell><cell>29.47</cell><cell>0.8924</cell></row><row><cell>FSRCNN [4]</cell><cell>×3</cell><cell>26.23</cell><cell>0.8124</cell><cell>24.44</cell><cell>0.7106</cell><cell>24.86</cell><cell>0.6832</cell><cell>22.04</cell><cell>0.6745</cell><cell>23.04</cell><cell>0.7927</cell></row><row><cell>VDSR [15]</cell><cell>×3</cell><cell>33.25</cell><cell>0.9150</cell><cell>29.46</cell><cell>0.8244</cell><cell>28.57</cell><cell>0.7893</cell><cell>26.61</cell><cell>0.8136</cell><cell>31.06</cell><cell>0.9234</cell></row><row><cell>IRCNN [37]</cell><cell>×3</cell><cell>33.38</cell><cell>0.9182</cell><cell>29.63</cell><cell>0.8281</cell><cell>28.65</cell><cell>0.7922</cell><cell>26.77</cell><cell>0.8154</cell><cell>31.15</cell><cell>0.9245</cell></row><row><cell>SRMDNF [38]</cell><cell>×3</cell><cell>34.01</cell><cell>0.9242</cell><cell>30.11</cell><cell>0.8364</cell><cell>28.98</cell><cell>0.8009</cell><cell>27.50</cell><cell>0.8370</cell><cell>32.97</cell><cell>0.9391</cell></row><row><cell>RDN [41]</cell><cell>×3</cell><cell>34.58</cell><cell>0.9280</cell><cell>30.53</cell><cell>0.8447</cell><cell>29.23</cell><cell>0.8079</cell><cell>28.46</cell><cell>0.8582</cell><cell>33.97</cell><cell>0.9465</cell></row><row><cell>RCAN [40]</cell><cell>×3</cell><cell>34.70</cell><cell>0.9288</cell><cell>30.63</cell><cell>0.8462</cell><cell>29.32</cell><cell>0.8093</cell><cell>28.81</cell><cell>0.8647</cell><cell>34.38</cell><cell>0.9483</cell></row><row><cell>SRFBN [21]</cell><cell>×3</cell><cell>34.66</cell><cell>0.9283</cell><cell>30.48</cell><cell>0.8439</cell><cell>29.21</cell><cell>0.8069</cell><cell>28.48</cell><cell>0.8581</cell><cell>34.07</cell><cell>0.9466</cell></row><row><cell>SAN [2]</cell><cell>×3</cell><cell>34.75</cell><cell>0.9290</cell><cell>30.68</cell><cell>0.8466</cell><cell>29.33</cell><cell>0.8101</cell><cell>28.83</cell><cell>0.8646</cell><cell>34.46</cell><cell>0.9487</cell></row><row><cell>HAN(ours)</cell><cell>×3</cell><cell>34.76</cell><cell>0.9294</cell><cell>30.70</cell><cell>0.8475</cell><cell>29.34</cell><cell>0.8106</cell><cell>28.99</cell><cell>0.8676</cell><cell>34.56</cell><cell>0.9494</cell></row><row><cell>HAN+(ours)</cell><cell>×3</cell><cell>34.85</cell><cell>0.9300</cell><cell>30.79</cell><cell>0.8487</cell><cell>29.41</cell><cell>0.8116</cell><cell>29.21</cell><cell>0.8710</cell><cell>34.87</cell><cell>0.9509</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Ablation study about using different numbers of CSAMs results by the proposed HAN is 0.06 dB and 0.35 dB higher than the very recent work of SAN for the 4× and 8× scales, respectively.</figDesc><table><row><cell></cell><cell>Set5</cell><cell>Set14</cell><cell>B100</cell><cell>Urban100</cell><cell>Manga100</cell></row><row><cell>HAN(1 CSAM)</cell><cell>32.64</cell><cell>28.90</cell><cell>27.80</cell><cell>26.85</cell><cell>31.42</cell></row><row><cell>HAN(3 CSAM)</cell><cell>32.67</cell><cell>28.91</cell><cell>27.80</cell><cell>26.89</cell><cell>31.46</cell></row><row><cell>HAN(5 CSAM)</cell><cell>32.69</cell><cell>28.91</cell><cell>27.80</cell><cell>26.89</cell><cell>31.43</cell></row><row><cell>HAN(10 CSAM)</cell><cell>32.67</cell><cell>28.91</cell><cell>27.80</cell><cell>26.89</cell><cell>31.43</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Low-complexity single-image super-resolution based on nonnegative neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Alberi-Morel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Second-order attention network for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Accelerating the super-resolution convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Generative adversarial nets</title>
		<imprint>
			<publisher>NIPS</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deep back-projection networks for superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ukita</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Channel-wise and spatial feature modulation network for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Single image super-resolution from transformed self-exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Robust single-image super-resolution based on adaptive edge-preserving smoothing regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Que</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2650" to="2663" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kwon Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mu Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deeply-recursive convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kwon Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mu Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Ram: Residual attention module for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12043</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deep laplacian pyramid networks for fast and accurate super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Photo-realistic single image superresolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Feedback network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mu Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sketch-based manga retrieval using manga109 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aramaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="21811" to="21838" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A statistical prediction model based on sparse representations for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Peleg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2569" to="2582" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Face video deblurring using 3d facial priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9388" to="9397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep non-blind deconvolution via generalized low-rank approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="297" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Enhancenet: Single image super-resolution through automated texture synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Image super-resolution via deep recursive residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Memnet: A persistent memory network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">Ntire 2017 challenge on single image super-resolution: Methods and results</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Deep networks for image superresolution with sparse prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>So Kweon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Image super-resolution as sparse representation of raw image patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On single image scale-up using sparserepresentations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on curves and surfaces</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Learning deep cnn denoiser prior for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Learning a single convolutional super-resolution network for multiple degradations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An edge-guided image interpolation algorithm via directional filtering and data fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2226" to="2238" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Image super-resolution by neural texture transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
