<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING 1 Investigating Typed Syntactic Dependencies for Targeted Sentiment Classification Using Graph Attention Neural Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuefeng</forename><surname>Bai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengbo</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
						</author>
						<title level="a" type="main">IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING 1 Investigating Typed Syntactic Dependencies for Targeted Sentiment Classification Using Graph Attention Neural Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Targeted sentiment analysis</term>
					<term>graph neural networks</term>
					<term>dependency tree</term>
					<term>attention mechanism</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Targeted sentiment classification predicts the sentiment polarity on given target mentions in input texts. Dominant methods employ neural networks for encoding the input sentence and extracting relations between target mentions and their contexts. Recently, graph neural network has been investigated for integrating dependency syntax for the task, achieving the state-of-the-art results. However, existing methods do not consider dependency label information, which can be intuitively useful. To solve the problem, we investigate a novel relational graph attention network that integrates typed syntactic dependency information. Results on standard benchmarks show that our method can effectively leverage label information for improving targeted sentiment classification performances. Our final model significantly outperforms state-of-the-art syntaxbased approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>context information. The assumption is that deep syntactic and semantic features can be represented by neural encoding.</p><p>With the advance of structured neural encoders <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b17">[18]</ref>- <ref type="bibr" target="#b19">[20]</ref>, syntactic structures predicted by external parsers have shown their usefulness for the task. Intuitively, syntactic structures such as dependency trees can help better encode the correlation between a target mention and the relevant sentiment keywords. Recent methods consider dependency trees as adjacency matrices, using graph neural networks such as graph convolutional networks (GCN <ref type="bibr" target="#b18">[19]</ref>) and graph attention network (GAT <ref type="bibr" target="#b19">[20]</ref>) to encode the input sentence according to such matrices <ref type="bibr" target="#b20">[21]</ref>- <ref type="bibr" target="#b22">[23]</ref>. As shown in <ref type="figure" target="#fig_0">Figure 1(a)</ref>, such dependency tree structures can help bring target mentions closer to its relevant contexts, thereby facilitating feature representation. In this example, the relevant sentiment word "good" is distant from the target mention "Chinese dumplings" in the surface string, but close in the dependency tree (i.e., "Chinese dumplings" nsubj ←− "taste" xcomp −→ "good"). While being more effective compared with the stateof-the-art approaches that do not use syntactic structures, these methods do not consider dependency labels, which can potentially be useful for sentiment disambiguation. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>(b) and <ref type="figure" target="#fig_0">Figure 1</ref>(c), while "Arsenal" has similar dependency arc relations with the word "defeated" in both sentence, the sentiment polarities are different. Apparently differentiating the label types can help differentiate such cases. On the other hand, considering arc label information can make the syntactic structure more sparse and thus increase arXiv:2002.09685v3 [cs.CL] 17 Dec 2020 the difficulty in learning. Thus it remains a open research question how to effectively make use of such fine-grained syntax features for better targeted sentiment classification.</p><p>We investigate a graph attention network to integrate such typed dependency features. In particular, the proposed model incorporates label features into the attention mechanism, using a novel extended attention function to guide information propagation from a target mention's syntactic context to target mention itself. We name our model relational graph attention network (RGAT). With the help of these label features, our model can better capture the relationship between words, thus addressing the problems shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Moreover, the dependency labels can serve as additional features, enriching the representation of words.</p><p>Experiments over four benchmarks show that using GAT to encode the input gives better results compared to a Transformer encoder, which coincides with recent observation that syntax is useful for targeted sentiment classifications <ref type="bibr" target="#b20">[21]</ref>- <ref type="bibr" target="#b22">[23]</ref>. Further, adding typed dependency features gives consistently better results compared with a baseline without such information, thereby proving our research hypothesis. Our model gives significantly better results than state-of-theart syntax-based work on the standard Laptop, Restaurant, Twitter and MAMS datasets. To our knowledge, we obtain the best reported results on all datasets without using external resources. Our code is released at https://github.com/muyeby/ RGAT-ABSA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>With regard to structures, existing work on targeted sentiment classification can be classified into two main categories, namely those methods that do not rely on external syntax information and those using syntax structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Conventional Methods</head><p>Most work along the first line splits a given input sentence into two sections, including the target mention and its context. Features are extracted from each section and combining the features for making prediction. For example, Vo and Zhang <ref type="bibr" target="#b2">[3]</ref> use word2vec embeddings and pooling mechanisms for extracting features from target mention and its left and right context, respectively, before concatenating all the features for classification. Zhang et al. <ref type="bibr" target="#b3">[4]</ref> use gated recurrent neural network for extracting features from target mentions and its context, before further defining a gate to integrate such features. Tang et al. <ref type="bibr" target="#b10">[11]</ref> exploit two long-short memory network (LSTM <ref type="bibr" target="#b23">[24]</ref>) for feature encoding, and combine the last hidden state of two networks for classification.</p><p>There are also attempts exploiting convolutional neural networks (CNN) for targeted sentiment classification. For instance, Huang and Carley <ref type="bibr" target="#b12">[13]</ref> use parameterized filters and parameterized gates to incorporate aspect information into CNN, and apply the resulting CNN to encode the sentence. Xue and Li <ref type="bibr" target="#b24">[25]</ref> further employ a gated CNN layer to extract aspect-specific features from the hidden states originated from a bi-directional RNN layer.</p><p>The attention mechanism <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref> is also shown to be useful for the task. Ma et al. <ref type="bibr" target="#b13">[14]</ref> use a bidirectional attention network, which learns attention weights from the target mention to its context vectors, to model the target-context relationship. Li et al. <ref type="bibr" target="#b27">[28]</ref> further improve the attentionbased models with position information. Tang et al. <ref type="bibr" target="#b14">[15]</ref> introduce a self-supervised attention learning approach, which automatically mines useful supervision information to refine attention mechanisms.</p><p>In addition to attention, Memory networks have also been applied to this task. Tang et al. <ref type="bibr" target="#b15">[16]</ref> develop a deep memory network (MemNet), which uses pre-trained word vectors as a memory and exploits attention mechanism to update the memory. Chen et al. <ref type="bibr" target="#b16">[17]</ref> improve MemNet by taking the hidden states generated by LSTM as memory and adopting gated recurrent units (GRU) to update the representation of target mentions. Wang et al. <ref type="bibr" target="#b9">[10]</ref> deploy a targeted sensitive memory network for better information integration.</p><p>Recently, contextualized language models such as BERT <ref type="bibr" target="#b28">[29]</ref>, GPT <ref type="bibr" target="#b29">[30]</ref> and ALBERT <ref type="bibr" target="#b30">[31]</ref> ELECTRA <ref type="bibr" target="#b31">[32]</ref> have shown their usefulness for a wide range of natural language inference (NLI) works. With regard to the task of targeted sentiment classification, Song et al. <ref type="bibr" target="#b32">[33]</ref> use BERT to encode a target mention and it context, before applying attention to draw semantic interaction between targets and context words. Gao et al. <ref type="bibr" target="#b33">[34]</ref> further introduce several variants to apply BERT for targeted sentiment classification, showing that incorporating target mention is helpful for BERT based models. Sun et al. <ref type="bibr" target="#b34">[35]</ref> propose to construct an auxiliary sentence from the aspect and convert aspect-based sentiment classification into a sentence-pair classification task. Xu et al. <ref type="bibr" target="#b35">[36]</ref> re-train a BERT model on a customer reviews dataset and use the resulting model for targeted sentiment classification. Li et al. <ref type="bibr" target="#b36">[37]</ref> explore BERT for end-to-end targeted sentiment classification, jointly learning to predict the target mentions and its sentiment polarity.</p><p>It has been shown that BERT contains implicit syntactic information, which can be useful for downstream tasks <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>. In contrast to these efforts, however, we make explicit use of syntactic information by encoding discrete structures. Our method is orthogonal to the implicit use of knowledge in BERT, and can be combined with contextualized embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Syntax-based Methods</head><p>Among work that uses syntax structures, early work <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref> rely on manually-defined syntactic rules and use nonneural models. Subsequently, neural network models are explored for this task. For example, Dong et al. <ref type="bibr" target="#b1">[2]</ref> use an adaptive recursive neural network (AdaRNN) to encode a syntax tree, transformed by placing the target mention onto the root node. Nguyen and Shirai <ref type="bibr" target="#b41">[42]</ref> further extend AdaRNN into a phrase-level recursive neural network (Phrase AdaRNN), which takes both dependency and constituent trees as input. He et al. <ref type="bibr" target="#b42">[43]</ref> use the distance on a dependency tree to guide the attention mechanism, thus helping model to focus on more important context words.</p><p>Recent work uses graph neural networks to encode the syntactic structure, obtaining better results. In particular, Sun et al. <ref type="bibr" target="#b20">[21]</ref> explore a GCN for encoding syntactical features to help the information exchange between target mention and context. Similar to that, Zhang et al. <ref type="bibr" target="#b22">[23]</ref> introduce a aspectspecific GCN to incorporate the syntactical information and long-range word dependencies into the classification model. Huang and Carley <ref type="bibr" target="#b21">[22]</ref> apply a GAT to model a dependency tree and integrate the GAT layer into LSTM to model the cross-layer dependency.</p><p>Our work is in this line. We take GAT as our base model and investigate a relational extension. Different from existing syntax-based work, we 1) exploit dependency relation information, which is proven to be useful by our work yet not investigated by <ref type="bibr" target="#b20">[21]</ref>- <ref type="bibr" target="#b22">[23]</ref>; 2) use an independent encoder for structure modeling instead of exploiting syntactic structures to refine contextual representations <ref type="bibr" target="#b20">[21]</ref>- <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b43">[44]</ref>. Our double-encoder structure has two main advantages: a) the structure encoder is detachable and can be relatively easily applied to a new sequence encoder model (especially when the sentence length is not equal to the number of nodes in graph); b) it can reduce error propagation from automatic parsed dependency trees, as loss do not back-propagate from the tree representation to the sequence encoder.</p><p>Another similar attempt to this end is Shaw et al. <ref type="bibr" target="#b44">[45]</ref>, who extends a self-attention network (SAN) by integrating relative position information for neural machine translation. However, the relative position information is a simpler type of relation and thus is less informative. In contrast to their work, we investigate relational GAT for targeted sentiment classification, with significantly improved results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. APPROACH</head><p>For our model, each training instance consists of three components: a target mention, a sentence and a dependency tree of the sentence. Formally, we denote these components as a triplet: T , S, G , where T = {w i , w i+1 , ..., w i+m−1 } denotes a target mention word sequence, and S = {w 1 , w 2 , ...w i , ..., w i+m , ...w n } denotes a sentence. The lengths of T and S are m and n, respectively. G = (V, A, R) represents a syntactic graph (for example, a dependency tree) over S, where V includes all nodes (or words) {w 1 , w 2 , ..., w n }, A is an adjacent matrix A ∈ R n×n with A ij = 1 if there is a dependency arc relation between word w i and w j , and A ij = 0 otherwise, and R is a label matrix, where R ij records the corresponding label of A ij if A ij = 1, and R ij = None otherwise. The goal of targeted sentiment classification is to predict the sentiment polarity y ∈ {1, −1, 0} of the sentence S over the target mention T , where 1, −1 and 0 denote positive, negative and neutral, respectively.</p><p>The overall architecture of our model is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. It is mainly composed of three components: the Contextual Encoder, the Relational Graph Attention (RGAT) Encoder and the Classifier. The Contextual Encoder can be viewed as a conventional model which applies Contextual encoder layers (e.g., BiLSTM, CNN, BERT) for feature learning and uses a simple pooling function for feature aggregation. The RGAT encoder is a syntax based encoder which incorporates syntax information into the progress of sentence modeling, thus generates syntax-aware word embeddings. The Feature Fusion mechanism is designed to dynamically combine the contextual and syntactic representations (denoted as h syn. and h con. ). The final representation h f is then fed into a Classifier for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Contextual Encoder</head><p>We consider two types of models for contextual modeling: the first is a BiLSTM model, which is widely used for targeted sentiment classification and other tasks <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b45">[46]</ref>. The other is BERT <ref type="bibr" target="#b28">[29]</ref>, which is pre-trained on large-scale raw texts and has more parameters than BiLSTM. BiLSTM We use BiLSTM to model the bidirectional context information. Following previous work <ref type="bibr" target="#b20">[21]</ref>, we use GloVe embeddings v i ∈ R de , POS-tag embeddings t i ∈ R dt as well as position embeddings p i ∈ R dp as inputs features, where d e , d t and d p denote the dimensions of word, POStag and position embeddings, respectively. Therefore, the representation of a word w i is denoted as</p><formula xml:id="formula_0">x i = [v i ; t i ; p i ], which is the concatenation of v i , t i and p i .</formula><p>Given an word embedding sequence</p><formula xml:id="formula_1">x = {x 1 , x 2 , ..., x n }, a forward − −−− → LSTM generates a set of hidden states − → h = { − → h 1 , − → h 2 , ..., − → h n }, and a backward ← −−− − LSTM generates a set of hidden states ← − h = { ← − h 1 , ← − h 2 , ..., ← − h n }.</formula><p>Finally, the output hidden states h = {h 1 , h 2 , ..., h n } are obtained by concatenating the corresponding forward and backward hidden states:</p><formula xml:id="formula_2">− → h = − −−− → LSTM([x 1 , x 2 , ..., x n ]) ← − h = ← −−− − LSTM([x 1 , x 2 , ..., x n ]) h = [ − → h ; ← − h ]<label>(1)</label></formula><p>BERT is a pre-trained masked language model, which is based on a Transformer <ref type="bibr" target="#b26">[27]</ref>. Previous work have shown that BERT can significantly boost the classification accuracy <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b36">[37]</ref>. For comparability, we also adopt BERT to generate contextual embeddings.</p><p>To facilitate fine-tuning of the BERT model, we follow [33] to refactor the sequence as "[CLS]" + sentence + "[SEP]" + target mention + "[SEP]" as input to BERT. Formally, denoting the resulting sequence as 3</p><formula xml:id="formula_3">x = {x 0 , x 1 , ..., x n , x n+1 , x n+2 , ..., x n+1+m , x n+2+m },<label>(2)</label></formula><p>where x 0 and x n+1 are vector representations of "[CLS]" and "[SEP]" respectively. The BERT model generates a new sequence with the same length as x, denoted as 4 :</p><formula xml:id="formula_4">h = {h 0 , h 1 , ..., h n , h n+1 , h n+2 , ..., h n+1+m , h n+2+m }, (3)</formula><p>where h 0 is called a "BERT pooling" vector, h 1 , h 2 , ..., h n are output contextual representations of the input word sequence. and h n+2 , ..., h n+1+m corresponds to the output embedding of target mention. In this work, we use h 1 , h 2 , ..., h n for pooling and feature fusion. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Relational Graph Attention Network</head><p>A relational graph attention network (RGAT) aims to perform information exchange among words according to the syntactic dependency paths. Compared with standard GAT networks, the RGAT method can additionally make use of the labeled relations (i.e., typed syntactic dependencies), thus generating more informative representations. In this section, we start by introducing the baseline GAT model which operates on an unlabeled graph G = (V, A), and then present RGAT, which extends the GAT with the ability to model a labeled graph G = (V, A, R).</p><p>1) Vanilla Graph Attention Network: We take the graph attention network as a baseline. <ref type="bibr" target="#b5">6</ref> The graph attention network <ref type="bibr" target="#b19">[20]</ref> is a variant of graph neural networks, which leverages masked self-attention layers to encode graph structures. Compared with other types of graph neural networks (e.g. graph convolutional networks <ref type="bibr" target="#b18">[19]</ref>, graph recurrent networks <ref type="bibr" target="#b47">[48]</ref>), GAT can be better interpreted thanks to the attention mechanism. Input and output A GAT takes a set of word (or node) embeddings {x 1 , x 2 , ..., x n } as initial hidden states</p><formula xml:id="formula_5">{h 0 1 , h 0 2 , ..., h 0 n }, iteratively producing more abstract features {h l 1 , h l 2 , ..., h l n } with increasing l, l ∈ [1, L].</formula><p>The lth GAT layer takes predecessor word features {h l−1 1 , h l−1 2 , ..., h l−1 n } and an adjacent matrix A as input, and produces a new set of word features {h l 1 , h l 2 , ..., h l n } as its output. Feature aggregation Given a word w i with its neighbor word index j ∈ N (i) 7 , a GAT updates the word's representation at layer l by calculating a weighted sum of the neighbor states.</p><p>Briefly, the aggregation process of a multi-head-attentionbased GAT can be described as:</p><formula xml:id="formula_6">h l i = Z || z=1 σ j∈N (i) α lz ij W lz V h l−1 j ,<label>(4)</label></formula><p>where || denotes vector concatenation, W lz V ∈ R d Z ×d is a parameter matrix of the zth head at layer l, d denotes the dimension of word feature vectors, Z is the number of attention heads, and σ represents the sigmoid activation function. The weight α lz ij models to what extent h l i depends on h l−1 j :</p><formula xml:id="formula_7">α lz ij = exp(f (h l−1 i , h l−1 j )) j ∈N (i) exp (f (h l−1 i , h l−1 j )) .<label>(5)</label></formula><p>If j / ∈ N (i), α lz ij = 0. f is an attention function. We use the scaled dot-product attention function <ref type="bibr" target="#b26">[27]</ref>  <ref type="bibr" target="#b7">8</ref> :</p><formula xml:id="formula_8">f (h l−1 i , h l−1 j ) = (W lz Q h l−1 i ) T (W lz K h l−1 j ) d/Z ,<label>(6)</label></formula><p>where W lz Q , W lz K ∈ R d Z ×d are parameter matrices of the zth head at layer l.</p><p>Apart from the aforementioned attention layer, we further add a point-wise convolution transformation (PCT) layer following the attention layer, which gives each node more information capacity. The convolution kernel size is 1, and convolution is applied to every single token belonging to the input. Given an output hidden states h l = {h l 1 , h l 2 , ..., h l n } of the lth attention layer, the PCT layer is defined as:</p><formula xml:id="formula_9">P CT (h l ) = δ(h l W P1 + b P1 ) W P2 + b P2 ,<label>(7)</label></formula><p>where δ refers to the ReLU activation function, denotes the convolution operation, W P1 , W P2 , b P1 , b P2 are weights and bias of two convolutional kernels.</p><p>2) Relational Graph Attention Network: The vanilla GAT mentioned above uses an adjacent matrix as structural information, thus omitting dependency label features. RGAT incorporates relational features into attention calculation and aggregation process to obtain more informative representations. As shown in <ref type="figure" target="#fig_2">Figure 3</ref> Relations as input Denoting the relation between word w i and w j as R ij , we transform R ij into a vector r ij ∈ R dr , where d r is the dimension of relation embeddings. During the training process, the relation embeddings are jointly optimized with the model. Relation-aware attention Inspired by previous work on SAN <ref type="bibr" target="#b44">[45]</ref>, we consider relation features when calculating attentions weights between nodes. In practice, the RGAT method calculates an unnormalized node-aware attention e N together with an unnormalized relation-aware attention e R simultaneously. In particular, the node-aware attention e N of the lth layer is the same as Equation <ref type="bibr" target="#b5">6</ref>:</p><formula xml:id="formula_10">e N ij = f (h l−1 i , h l−1 j ), j ∈ N (i) −inf, otherwise<label>(8)</label></formula><p>while the relation-aware attention weight is given by:</p><formula xml:id="formula_11">e R ij = f (h l−1 i , r ij ), j ∈ N (i) −inf, otherwise<label>(9)</label></formula><p>where r ij denotes the vector representation of relation R ij . It should be noted that the relation embedding r ij is shared among multiple layers and attention heads. The above two types of attention scores are combined and normalized as:</p><formula xml:id="formula_12">α ij = exp(e N ij + e R ij ) j ∈N (i) exp (e N ij + e R ij )</formula><p>.</p><p>In this way, the resulting attention scores take both node features and relation features into consideration. Relation-aware feature aggregation Relations can also be important in the feature aggregation process, as these additional fine-grained information can be used to enrich the feature of each word h l i . To this end, we use the hidden feature vector of neighbor words h l−1 j together with their corresponding relation vector r ij as inputs to update the representation of h l−1 i :</p><formula xml:id="formula_14">h l i = Z || k=1 σ j∈N (i)α lz ij (W lz V h l−1 j + W l V r r ij ) ,<label>(11)</label></formula><p>where W l V r ∈ R d Z ×dr is a parameter matrix. In order to learn deep features, we apply a stacked relational graph attention network with multiple layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Pooling and Feature Fusion</head><p>With a contextual encoder (Section III-A) and a RGAT encoder (Section III-B), we obtain the contextual and syntax-aware target mention vectors, denoted as {h i , h i+1 , ..., h i+m−1 } and {ĥ i ,ĥ i+1 , ...,ĥ i+m−1 }, respectively. We then apply a pooling function over these vectors to obtain two global representation h con and h syn :</p><formula xml:id="formula_15">h con = pool h i , h i+1 , ..., h i+m−1 , h syn = pool ĥ i ,ĥ i+1 , ...,ĥ i+m−1 .<label>(12)</label></formula><p>In our implementation, pool is an average pooling function <ref type="bibr" target="#b8">9</ref> .</p><p>In order to learn a composite representation which contains both contextual and syntax features, a fine-grained feature fusion mechanism is introduced to control the fusion ratio. We implement the feature fusion mechanism based on the gating mechanism <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b48">[49]</ref>. Formally, the syntax representation h syn is fused into the contextual representation h con by:</p><formula xml:id="formula_16">h f = g • h syn + (1 − g) • h con ,<label>(13)</label></formula><p>where • is element-wise product operation, and g is a gate computed by:</p><formula xml:id="formula_17">g = σ(W g [h syn ; h con ] + b g ).<label>(14)</label></formula><p>In above equation, [h syn ; h con ] is the concatenation of h syn and h con , W g and b g are model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. The Classifier</head><p>The classifier is a fully connected network, which takes the fused representation h f as input and computes the probability of each sentiment class c:</p><formula xml:id="formula_18">P (y = c) = exp(W h f + b) c c ∈C exp(W h f + b) c ,<label>(15)</label></formula><p>where W and b are tunable model parameters, and C is the set of sentiment classes. Given a set of training instances D = {d 1 , d 2 , ..., d N }, the training objective is a cross-entropy loss with L 2 regularization:</p><formula xml:id="formula_19">= − N i=1 c∈C I(y = c)log(P (y = c)) + λ Θ 2 ,<label>(16)</label></formula><p>where I is an indicator function, N is the number of training examples, λ is a regularization hyperparameter and Θ denotes all the set of parameters in the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>We conduct experiments on 4 benchmark datasets, including the Restaurant reviews (Restaurant) and Laptop reviews (Laptop) datasets of SemEval 2014 <ref type="bibr" target="#b49">[50]</ref>, the ACL14 Twitter dataset <ref type="bibr" target="#b1">[2]</ref> and the MAMS dataset <ref type="bibr" target="#b50">[51]</ref>. These datasets are labeled with three sentiment polarities: positive, neutral and negative. The number of samples in each category are summarized in <ref type="table" target="#tab_1">Table I</ref>.  <ref type="table" target="#tab_1">Train Test Train Test Train Test   Restaurant  2164  727  807  196  637  196  Laptop  976  337  851  128  455  167  Twitter  1507  172  1528  169  3016  336  MAMS  3380  400  2764  329</ref> 5042 607</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Settings</head><p>We adopt similar experimental settings as previous work <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>. Two types of contextual encoders are considered: 1) The BiLSTM-based encoder; 2) the BERTbased encoder. With regard to the BiLSTM-based encoder, 300-dimensional GloVe vectors as adopted for word representation. The values are fixed during the training process. The dimensions of POS-tag, position and relation embeddings are set as 30. The dropout rate on input word embeddings is 0.7, and the L 2 regularization term λ = 10 −5 . The Adamax <ref type="bibr" target="#b51">[52]</ref> optimizer with a learning rate of 10 −3 is adopted to train our models. For the BERT-based encoder, we adopt a pre-trained BERT 10 for fine-tuning. The dropout rate on BERT embeddings is 0.1, and regularization term λ = 10 −5 . The Adam <ref type="bibr" target="#b51">[52]</ref> optimizer with a learning rate 2 * 10 −5 is adopted for model training. For RGAT encoder, we use 5 attention heads in each layer. The input/output dimension of each layer is listed in <ref type="table" target="#tab_1">Table II</ref>. We use the Deep Biaffine Parser <ref type="bibr" target="#b52">[53]</ref>  <ref type="bibr" target="#b10">11</ref> to obtain dependency trees.</p><p>We consider two metrics for model evaluation: Accuracy and Macro-Averaged F1. The latter is more appropriate for datasets with unbalanced classes. Pairwise t-test is conducted on both Accuracy and Macro-Averaged F1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Baselines</head><p>We compare our model with the state-of-the-art systems without and with syntactic knowledge. The syntax-free baselines include:</p><p>-SVM applys a support vector machine (SVM, <ref type="bibr">[</ref>   <ref type="bibr" target="#b42">[43]</ref> incorporates syntactic distance into the attention mechanism to model the interaction between target mention and context. -CDT <ref type="bibr" target="#b20">[21]</ref> and ASGCN <ref type="bibr" target="#b22">[23]</ref> integrate dependency trees with GCN for aspect representation learning. Compared with CDT, ASGCN additionally apply attention mechanism to obtain final representation. -TD-GAT <ref type="bibr" target="#b21">[22]</ref> and TD-GAT-BERT <ref type="bibr" target="#b21">[22]</ref> apply GAT to capture the syntax structure and improves it with LSTM to model relation across layers. For fair comparison, we leave out baselines which rely on external resources such as auxiliary sentences <ref type="bibr" target="#b34">[35]</ref> and other domain/language review corpus <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b58">[59]</ref>. <ref type="table" target="#tab_1">Table III</ref> shows the results of different models. We first compare RGAT with the baseline (Transformer), which replace RGAT with a Transformer network. RGAT significantly (p &lt; 0.01) improves the performance, with an accuracy improvement of 2.85 percent on average. Similarly, the average F1 score increases by 2.84 percent. This indicates that dependency trees can provide useful information for the targeted aspect sentiment classification task, which is consistent with observations by recent work <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>. It is reasonable that Transformer gives weaker results than the state-of-the-art models, as the Transformer model adopts a simple pooling function rather than attention or CNN for target-context interaction modeling. Compared with systems that do not rely on syntactic feature (SVM, IAN, TNet, MGAN, AOA, AEN, CapsNet), the RGAT model gives significantly (p &lt; 0.01) better results. This observation is consistent with the results of our baseline. In addition, compared with recent work which takes dependency trees but without relation labels as input (AdaRNN, PhraseRNN, SynAttn, ASGCN, CDT, TD-GAT), RGAT also gives better results across all datasets. In particular, RGAT outperforms the state-of-the-art CDT model by 1.25 and 1.97 points on Restaurant with regard to accuracy and F1, respectively. Furthermore, the pre-trained BERT model can significantly boost the performance of each approach. With the help of BERT, the proposed model achieves better results than all the baselines, giving accuracy of 86.68, 80.94, 76.28 and 84.52 on Restaurant, Laptop, Twitter and MAMS, respectively. To our knowledge, we obtain the best reported results on all the datasets without using external resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Main Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. ANALYSIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Ablation Study</head><p>We present an ablation study on the effects of structure information and relation information. The experiments are conducted on Restaurant, Twitter and MAMS as these three datasets have more training instances than Laptops. We consider three ablation baselines for comparison, including 1) Transformer: we replace the relational graph attention layers with self-attention layers. This model can be viewed as our model without dependency tree as supervision; and 2) GAT: the relation graph attention layers are substituted by graph attention layers, serving as our model without relation features; 3) GAT-Ratt: we remove the relation-aware feature aggregation module of RGAT to test the effectiveness of the relation-aware attention independently. Effectiveness of structural information As shown in <ref type="table" target="#tab_1">Table IV</ref>, the GAT model gives consistently better accuracies and F1 scores over the Transformer model, with 1.3 and 1.2 percent average improvement of accuracy and F1 score, respectively. In addition, the performance of GAT-BERT is also better than that of Transformer-BERT. Such results indicates that explicit syntax knowledge is helpful for targeted sentiment classification. A possible reason for why improvement over BERT is relatively small is that structural information is contained in the BERT representations to some extent because of contextual language modeling <ref type="bibr" target="#b59">[60]</ref>. models. Such results indicate that the relation-aware attention mechanism is useful for targeted sentiment analysis. Effectiveness of RGAT for typed dependencies It can be observed that RGAT gives better results than GAT-Ratt and GAT on both datasets. In particular, RGAT outperforms GAT-Ratt on Twitter by 0.76 percent and 1.1 percent with regard to accuracy and F1, respectively. This observation confirms our intuition that the fine-grained relation information is helpful. Combined with relation-aware attention and relation-aware feature aggregation, the accuracy of GAT increases by 1.1 percent and 1.6 percent on Restaurant and Twitter, respectively. The improvement on F1 score reaches 0.9 percent and 2.6 percent, respectively. Similar trends can also be observed when comparing RGAT-BERT with GAT-BERT. Such results demonstrate that dependency labels can bring significant improvement to the baseline model, regardless of different input embeddings and contextual models.</p><p>We present the learning curves of GAT, RGAT, GAT-BERT and RGAT-BERT in <ref type="figure" target="#fig_3">Figure 4</ref>. It can be seen that RGAT gives consistently higher accuracy and lower loss compared with GAT. Similarly, the results of RGAT-BERT are better than GAT-BERT. Such results prove that RGAT(-BERT) is more powerful than GAT(-BERT) from another perspective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Impacts of Parsing Accuracy</head><p>We conduct experiments to study the effects of parsing accuracy on the classification performance. Specifically, we consider the following settings: 1) Transformer, which serves as a baseline model without dependency tree; 2) Random Tree, which uses a random tree as input for training <ref type="bibr" target="#b11">12</ref> ; 3) Random Label, which uses the gold tree structure (dependency heads) but randomly permutes the dependency labels; 4) Stanford Parser, for which dependency trees are parsed by Stanford Transition-based Parser <ref type="bibr" target="#b60">[61]</ref>; 5) Biaffine Parser, which applies a Deep Biaffine Parser <ref type="bibr" target="#b52">[53]</ref> to obtain dependency trees. 6) BERT Biaffine Parser, which adopts the BERT based Biaffine Parser to obtain a dependency tree. The performances of the <ref type="bibr" target="#b11">12</ref> We select 10 random seeds and report the averaged results. Stanford Parser and Deep Biaffine Parser on the Penn Treebank are given in <ref type="table" target="#tab_6">Table V</ref>. <ref type="table" target="#tab_1">Table VI</ref> gives the results of different systems on Restaurant and Twitter. It can be seen that a random tree has negative influences on the classification performance. The Random Label model gives better results than Transformer, showing that the tree structure is still useful for targeted sentiment classification, despite label noise. In addition, the trees parsed by pre-trained models (Stanford, Biaffine and BERT Biaffine parser) have more positive impact on model performance than random trees. This is likely because pre-trained parsers can give more accurate predictions and the parsed results are more consistent.</p><p>Comparing the results of different parsers, it can be seen that both Biaffine Parser and BERT Biaffine Parser give better results than the Stanford Parser, and the performance of the Biaffine Parser and the BERT Biaffine Parser are comparable. Such results indicate that the classification performance is positively correlated with the parsing accuracy.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Effects of different dependency labels</head><p>We select the most frequent dependency labels and study their contribution to the model performance. In particular, the following categories of dependency labels are considered: 1) Clausal Argument Relations, including {nsubj, dobj, ccomp, xcomp}; 2) Nominal Modifier Relations, including {nmod, amod, advmod, det, case}; 3) Other Notable Relations, including {conj, cc, punc}. <ref type="figure" target="#fig_4">Figure 5</ref> gives the results of the RGAT model on the two benchmarks. We report the decrement of classification accuracy when removing different dependency labels independently. The accuracy of RGAT decreases most rapidly when removing nsubj and amod relations, indicating that nsubj and amod carry most important information for classification. This is intuitive as the nsubj label represents the nominal subject relation, which is especially informative when one side of this relation is a target mention. Similarly, the amod label denotes the adjective modifier relation, and is always related to the adjective which conveys the sentiment polarity. In addition, the dobj, nmod and advmod labels are also important factors to the model performance. The accuracy decreases by a range from 0.23 to 0.32 when these labels are removed. Last but not least, although having a high frequency, the det, case and punc dependency labels are the most irrelevant to the classification accuracy, with accuracy decline of about 0.07, 0.04 and 0.05, respectively. This indicates that the label frequency is not strongly correlated to the classification accuracy, and the relation label matters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Node attention VS Edge attention</head><p>In order to compare the importance of node attention and edge attention, we assign two trainable weighted factors to node attention and edge attention, respectively. In particular, we modified Equation 10 as:</p><formula xml:id="formula_20">α ij = exp(β 1 e N ij + β 2 e R ij ) j ∈N (i) exp (β 1 e N ij + β 2 e R ij ) ,<label>(17)</label></formula><p>where β 1 , β 2 ∈ R 1 are model parameters.</p><p>We conduct evaluation on the Restaurant and Twitter datasets, and results are shown in <ref type="table" target="#tab_1">Table VII</ref>. RGAT-weighted factors achieves comparable performance to RGAT, and RGAT-BERT-weighted factors gives slightly lower results compared with RGAT-BERT.</p><p>The values of β 1 and β 2 are shown in <ref type="table" target="#tab_1">Table VIII</ref>. Interestingly, both values are close to 1, and this phenomenon is more obvious when using the BERT model. Such results can explain why the performance of RGAT-weighted factors is comparable to RGAT, and further indicate that node attention is of equal importance to relation attention. <ref type="figure" target="#fig_5">Figure 6</ref> shows the accuracy curves for RGAT and RGAT-BERT on Restaurant. Different numbers of layers ranging from 1 to 8 are considered. For RGAT, the initial accuracy is low and then increases along with the depth, reaching the best score of 83.55 with 6 layers. This is intuitive as the target-related sentiment words can be many-hops away from the target mention, which means that multiple layers of node communication is necessary for passing information from relevant context to the target mention. In contrast to RGAT, RGAT-BERT reaches the best accuracy (86.68) faster with 2 layers, which can be because that implicate syntax contained in BERT allows faster information propagation.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Impacts of Model Depth</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Case Study</head><p>We provide 4 examples to further analyze the proposed model. The attention scores over dependency edges are given for comparison. <ref type="bibr" target="#b12">13</ref> Consider the case shown in <ref type="figure" target="#fig_6">Figure 7</ref>(a) and <ref type="figure" target="#fig_6">Figure 7</ref>(b). GAT gives a right sentiment in case 1 and a incorrect sentiment in case 2, while RGAT obtains the correct sentiment in both cases, which indicates that RGAT can differentiate similar structures, thanks to the use of dependency labels. In <ref type="figure" target="#fig_6">Figure 7</ref>(c), GAT predicts a "positive" sentiment for target mention "scallops", and the attention weight on the edge "as"−→"scallops" is high, which demonstrates that GAT is negatively influenced by "well", which does not have positive meaning in this case. In contrast, RGAT gives the correct result and the attention weight on the edge "as" cc −→"scallops" is close to zero. <ref type="figure" target="#fig_6">Figure 7(d)</ref> shows another case where the decision of GAT is influenced by an irrelevant word "bad". GAT's attention weight on the edge "get" cc −→"menu" is 0.8 while that of RGAT is 0.5. The last two examples indicate that <ref type="bibr" target="#b12">13</ref> For brevity, we omit the attention weight on self-loop edges. the dependency labels have a positive influence on modeling the relationship between two words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. DISCUSSION</head><p>Our approach is more complicated than previous syntax-free work, due to the modeling of typed dependencies. However, the total number of parameters in RGAT does not increase significantly. The main reason is that we use a small hidden size (100) in RGAT encoder. <ref type="table" target="#tab_1">Table IX</ref> shows the number of parameters of our method and two syntax-free systems (BiLSTM <ref type="bibr" target="#b10">[11]</ref>, BERT-SPC <ref type="bibr" target="#b32">[33]</ref>).</p><p>In addition, our model requires more calculations than previous work. However, the encoding process of sentence and dependency tree are independent, so that the forward calculation and backward calculation of RGAT encoder and contextual encoder can be parallelized theoretically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>We investigated the usage of typed dependency structures for targeted sentiment classification, by extending a graph attention network encoder with relation features. Extensive experiments on four standard benchmarks show that label information is useful for sentiment classification, and our relational GAT model can effectively encode such features. Our final model gives results that are better than the existing best results in the literature. We further study the impact of parsing performance, dependency labels and network depth on model performance, finding that different dependency arc labels do have different effects on targeted sentiment signal propagation, thereby motivating the use of arc label information. In addition, contextualized embeddings are complementary to structured dependencies for improving the results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>(a) An example sentence with a dependency tree, (b,c) Two sentences which have similar dependency trees. The target mentions are underlined.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>An overview of the proposed network for targeted sentiment classification 2 . It consists of a contextual encoder (orange dotted frame), a RGAT encoder (blue dotted frame) and a classifier. The BiLSTM module can be replaced by BERT. Different from BiLSTM, the BERT based model does not take GloVe, POS-tag or position embeddings as additional input features, as such features are inherently learned by BERT itself 5 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>, the RGAT calculates two attention The attention distribution of RGAT model is a mixture of node-aware attention and relation-aware attention distribution. denotes the attention mixing operation (see Equation 10). distributions, named as node-aware attention and relationaware attention, taking their combination as final attention weights for feature aggregation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Effectiveness of relation-aware attention GAT-Ratt consistently shows better performance over GAT. Specifically, GAT-Ratt outperforms GAT by 1.0 percent accuracy and 1.5 percent F1. Similar improvements are also observed for BERT based The learning curve of GAT, RGAT, GAT-BERT and RGAT-BERT on MAMS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>The decrement of accuracy (compared with the baseline) on Restaurant and Twitter when removing different dependency labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>The effects of model depth on classification accuracy on Restaurant.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Several samples, the numbers denote the attention weights given by RGAT (red) and GAT (blue). The target mentions are underlined.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I STATISTICS</head><label>I</label><figDesc>OF DATASETS.</figDesc><table><row><cell>Dataset</cell><cell>Positive</cell><cell>Negative</cell><cell>Neutral</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II INPUT</head><label>II</label><figDesc>/OUTPUT DIMENSION OF MODEL SUB-MODULES.</figDesc><table><row><cell>Name</cell><cell>GloVe</cell><cell>BERT</cell></row><row><cell>Embedding layer</cell><cell>1/300</cell><cell>1/768</cell></row><row><cell cols="3">Contextual encoder layer 360/100 768/768</cell></row><row><cell>RGAT encoder layer</cell><cell cols="2">100/100 100/100</cell></row><row><cell>Feature fusion layer</cell><cell>200/50</cell><cell>868/868</cell></row><row><cell>Classifier layer</cell><cell>50/3</cell><cell>868/3</cell></row><row><cell cols="3">-CapsNet [51] utilizes capsule networks [58] to model</cell></row><row><cell cols="3">complicated relationships between target mentions and</cell></row><row><cell>contexts.</cell><cell></cell><cell></cell></row><row><cell cols="3">-BERT-PT [36] uses a post-training approach on a</cell></row><row><cell cols="3">pre-trained BERT model to improve the performance</cell></row><row><cell cols="3">for review reading comprehension and targeted aspect</cell></row><row><cell>sentiment classification.</cell><cell></cell><cell></cell></row><row><cell cols="3">-BERT-SPC [33] feeds "[CLS]" + sentence + "[SEP]" +</cell></row><row><cell cols="3">target mention + "[SEP]" into a pre-trained BERT model,</cell></row><row><cell cols="3">and then uses pooled embedding for classification.</cell></row><row><cell cols="3">-CapsNet-BERT [51] builds capsule networks on the top</cell></row><row><cell cols="3">of BERT layers to predict sentiment polarities.</cell></row><row><cell cols="2">The syntax-based baselines are:</cell><cell></cell></row><row><cell cols="3">-AdaRNN [2] learns the sentence representation toward</cell></row><row><cell cols="3">target via RNN semantic composition over a dependency</cell></row><row><cell>tree.</cell><cell></cell><cell></cell></row><row><cell cols="3">-PhraseRNN [42] extends AdaRNN by adding two</cell></row><row><cell cols="3">phrase composition functions. The PhraseRNN takes a</cell></row><row><cell cols="3">dependency tree as well as a constituent tree as input.</cell></row><row><cell>-SynAttn</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III PERFORMANCE</head><label>III</label><figDesc>COMPARISON OF DIFFERENT MODELS ON THE BENCHMARK DATASETS. THE BEST PERFORMANCE ARE BOLD-TYPED, † DENOTES THAT THE MODEL REQUIRES DEPENDENCY TREE AS INPUT. RESULTS UNDERLINED INDICATE THAT THE PROPOSED METHOD IS SIGNIFICANTLY BETTER THAN STATE-OF-THE-ART MODEL AT SIGNIFICANCE LEVEL P&lt;0.01.</figDesc><table><row><cell cols="2">Category Model</cell><cell cols="2">Restaurant</cell><cell cols="2">Laptop</cell><cell cols="2">Twitter</cell><cell cols="2">MAMS</cell></row><row><cell></cell><cell></cell><cell cols="8">Accuracy Macro-F1 Accuracy Macro-F1 Accuracy Macro-F1 Accuracy Macro-F1</cell></row><row><cell></cell><cell>SVM</cell><cell>80.16</cell><cell>-</cell><cell>70.49</cell><cell>-</cell><cell>63.40</cell><cell>63.30</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>IAN</cell><cell>78.60</cell><cell>-</cell><cell>72.10</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>76.60</cell><cell>-</cell></row><row><cell></cell><cell>TNet</cell><cell>80.69</cell><cell>71.27</cell><cell>76.54</cell><cell>71.75</cell><cell>74.97</cell><cell>73.60</cell><cell>-</cell><cell>-</cell></row><row><cell>w/o Syn.</cell><cell>MGAN AOA</cell><cell>81.25 81.20</cell><cell>71.94 -</cell><cell>75.39 74.5</cell><cell>72.47 -</cell><cell>72.54 -</cell><cell>70.81 -</cell><cell>-77.26</cell><cell>--</cell></row><row><cell></cell><cell>AEN</cell><cell>80.98</cell><cell>72.14</cell><cell>73.51</cell><cell>69.04</cell><cell>72.83</cell><cell>69.81</cell><cell>66.72</cell><cell>-</cell></row><row><cell></cell><cell>CapsNet</cell><cell>80.79</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>79.78</cell><cell>-</cell></row><row><cell></cell><cell>BERT-PT</cell><cell>84.95</cell><cell>76.96</cell><cell>78.07</cell><cell>75.08</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>BERT-SPC</cell><cell>84.46</cell><cell>76.98</cell><cell>78.99</cell><cell>75.03</cell><cell>73.55</cell><cell>72.14</cell><cell>82.82</cell><cell>81.90</cell></row><row><cell></cell><cell>AEN-BERT</cell><cell>83.12</cell><cell>73.76</cell><cell>79.93</cell><cell>76.31</cell><cell>74.71</cell><cell>73.13</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>CapsNet-BERT</cell><cell>85.93</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>83.39</cell><cell>-</cell></row><row><cell></cell><cell>AdaRNN</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>66.30</cell><cell>65.90</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>PhraseRNN</cell><cell>66.20</cell><cell>59.32</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>SynAttn</cell><cell>80.45</cell><cell>71.26</cell><cell>72.57</cell><cell>69.13</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>ASGCN</cell><cell>80.77</cell><cell>72.02</cell><cell>75.55</cell><cell>71.05</cell><cell>72.15</cell><cell>70.40</cell><cell>-</cell><cell>-</cell></row><row><cell>w Syn.</cell><cell>CDT</cell><cell>82.30</cell><cell>74.02</cell><cell>77.19</cell><cell>72.99</cell><cell>74.66</cell><cell>73.66</cell><cell>80.70</cell><cell>79.79</cell></row><row><cell></cell><cell>TD-GAT</cell><cell>81.20</cell><cell>-</cell><cell>74.00</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>TD-GAT-BERT</cell><cell>83.00</cell><cell>-</cell><cell>80.10</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Transformer</cell><cell>80.78</cell><cell>72.10</cell><cell>74.09</cell><cell>69.42</cell><cell>72.78</cell><cell>70.23</cell><cell>79.63</cell><cell>78.92</cell></row><row><cell>Ours</cell><cell>RGAT</cell><cell>83.55</cell><cell>75.99</cell><cell>78.02</cell><cell>74.00</cell><cell>75.36</cell><cell>74.15</cell><cell>81.75</cell><cell>80.87</cell></row><row><cell></cell><cell>Ours-RGAT-BERT</cell><cell>86.68</cell><cell>80.92</cell><cell>80.94</cell><cell>78.20</cell><cell>76.28</cell><cell>75.25</cell><cell>84.52</cell><cell>83.74</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV ABLATION</head><label>IV</label><figDesc>STUDY ON THE RESTAURANT AND LAPTOP DATASETS. THE BEST PERFORMANCE ARE BOLD-TYPED. RATT REFERS TO RELATIONAL ATTENTION.</figDesc><table><row><cell>Model</cell><cell cols="2">Restaurant</cell><cell cols="2">Twitter</cell></row><row><cell></cell><cell cols="4">Accuracy Macro-F1 Accuracy Macro-F1</cell></row><row><cell>Transformer</cell><cell>80.78</cell><cell>72.10</cell><cell>72.78</cell><cell>70.23</cell></row><row><cell>GAT</cell><cell>82.41</cell><cell>75.10</cell><cell>73.75</cell><cell>71.58</cell></row><row><cell>GAT-Ratt</cell><cell>82.68</cell><cell>75.39</cell><cell>74.60</cell><cell>73.05</cell></row><row><cell>RGAT</cell><cell>83.55</cell><cell>75.99</cell><cell>75.36</cell><cell>74.15</cell></row><row><cell>Transformer-BERT</cell><cell>84.89</cell><cell>77.90</cell><cell>73.43</cell><cell>72.08</cell></row><row><cell>GAT-BERT</cell><cell>85.70</cell><cell>78.80</cell><cell>74.67</cell><cell>73.50</cell></row><row><cell>GAT-Ratt-BERT</cell><cell>86.13</cell><cell>79.61</cell><cell>75.35</cell><cell>74.27</cell></row><row><cell>RGAT-BERT</cell><cell>86.68</cell><cell>80.92</cell><cell>76.28</cell><cell>75.25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V PERFORMANCE</head><label>V</label><figDesc>OF DIFFERENT PARSERS. UAS REFERS TO UNLABELED ATTACHMENT SCORE AND LAS REFERS TO LABELED ATTACHMENT SCORE</figDesc><table><row><cell>Model</cell><cell></cell><cell>UAS</cell><cell>LAS</cell><cell></cell></row><row><cell cols="2">Stanford Parser</cell><cell cols="2">94.10 91.49</cell><cell></cell></row><row><cell cols="2">Biaffine Parser</cell><cell cols="2">95.90 94.25</cell><cell></cell></row><row><cell cols="4">BERT Biaffine Parser 96.64 95.11</cell><cell></cell></row><row><cell></cell><cell cols="2">TABLE VI</cell><cell></cell><cell></cell></row><row><cell cols="5">THE ACCURACY AND F1 WHEN USING DIFFERENT DEPENDENCY TREES AS</cell></row><row><cell></cell><cell></cell><cell>INPUT.</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="2">Restaurant</cell><cell cols="2">Twitter</cell></row><row><cell></cell><cell cols="4">Accuracy Macro-F1 Accuracy Macro-F1</cell></row><row><cell>Transformer</cell><cell>80.78</cell><cell>72.10</cell><cell>72.78</cell><cell>70.23</cell></row><row><cell>Random Tree</cell><cell>78.97</cell><cell>70.46</cell><cell>71.20</cell><cell>68.75</cell></row><row><cell>Random Labels</cell><cell>82.03</cell><cell>74.67</cell><cell>73.63</cell><cell>71.42</cell></row><row><cell>Stanford Parser</cell><cell>83.37</cell><cell>75.82</cell><cell>75.13</cell><cell>74.12</cell></row><row><cell>Biaffine Parser</cell><cell>83.55</cell><cell>75.99</cell><cell>75.36</cell><cell>74.15</cell></row><row><cell>BERT Biaffine Parser</cell><cell>83.64</cell><cell>75.91</cell><cell>75.28</cell><cell>74.10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VII MODEL</head><label>VII</label><figDesc>PERFORMANCE ON BENCHMARK DATASETS.</figDesc><table><row><cell></cell><cell></cell><cell>Model</cell><cell cols="2">Restaurant</cell><cell></cell><cell>Twitter</cell><cell>MAMS</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">Accuracy Macro-F1 Accuracy Macro-F1 Accuracy Macro-F1</cell></row><row><cell></cell><cell></cell><cell>Transformer</cell><cell>80.78</cell><cell>72.10</cell><cell>72.78</cell><cell>70.23</cell><cell>79.63</cell><cell>78.92</cell></row><row><cell></cell><cell></cell><cell>RGAT-weighted factors</cell><cell>83.01</cell><cell>76.35</cell><cell>75.51</cell><cell>73.84</cell><cell>81.52</cell><cell>80.68</cell></row><row><cell></cell><cell></cell><cell>RGAT</cell><cell>83.55</cell><cell>75.99</cell><cell>75.36</cell><cell>74.15</cell><cell>81.75</cell><cell>80.87</cell></row><row><cell></cell><cell></cell><cell>Transformer-BERT</cell><cell>84.89</cell><cell>77.90</cell><cell>73.43</cell><cell>72.08</cell><cell>82.23</cell><cell>81.47</cell></row><row><cell></cell><cell></cell><cell>RGAT-BERT-weighted factors</cell><cell>86.23</cell><cell>79.83</cell><cell>75.84</cell><cell>75.13</cell><cell>84.30</cell><cell>83.50</cell></row><row><cell></cell><cell></cell><cell>RGAT-BERT</cell><cell>86.68</cell><cell>80.92</cell><cell>76.28</cell><cell>75.25</cell><cell>84.52</cell><cell>83.74</cell></row><row><cell></cell><cell>0.6</cell><cell></cell><cell cols="2">Restaurant Twitter</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Accuracy</cell><cell>0.3 0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.0</cell><cell cols="2">nsubj dobj ccomp xcomp nmod amodadvmod det case conj cc punc</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VIII THE</head><label>VIII</label><figDesc>VALUES OF TRAINABLE PARAMETER β 1 AND β 2 . WE REPORT MEAN ±std VALUE OF MULTIPLE RGAT LAYERS.</figDesc><table><row><cell>Dataset</cell><cell cols="2">RGAT</cell><cell cols="2">RGAT-BERT</cell></row><row><cell></cell><cell>β 1</cell><cell>β 2</cell><cell>β 1</cell><cell>β 2</cell></row><row><cell cols="2">Restaurant 0.992 ±0.19</cell><cell>1.194 ±0.17</cell><cell>1.000 ±5e −5</cell><cell>1.000 ±3e−5</cell></row><row><cell>Twitter</cell><cell>0.973 ±0.04</cell><cell>1.080 ±0.05</cell><cell>1.000 ±2e −4</cell><cell>1.000 ±2e −4</cell></row><row><cell>MAMS</cell><cell>0.925 ±0.15</cell><cell>1.093 ±0.22</cell><cell>0.993 ±5e−4</cell><cell>1.000 ±3e−4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE IX COMPARISON</head><label>IX</label><figDesc>OF MODEL PARAMETERS, 1M= 1e 6 .</figDesc><table><row><cell>Dataset</cell><cell cols="2">GloVe</cell><cell cols="2">BERT</cell></row><row><cell></cell><cell>BiLSTM</cell><cell>Ours</cell><cell>BERT-SPC</cell><cell>Ours</cell></row><row><cell>Restaurant</cell><cell>1.53M</cell><cell>1.90M</cell><cell>109.59M</cell><cell>110.00M</cell></row><row><cell>Laptop</cell><cell>1.25M</cell><cell>1.49M</cell><cell>109.58M</cell><cell>110.00M</cell></row><row><cell>Twitter</cell><cell>4.13M</cell><cell>4.50M</cell><cell>110.37M</cell><cell>113.97M</cell></row><row><cell>MAMS</cell><cell>2.71M</cell><cell>2.96M</cell><cell>109.59M</cell><cell>110.00M</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">In order to prevent the input graph from being too sparse, we remove the edge directions and add a self-loop edge for each word.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">For simplicity, we assume that the BERT tokenizer<ref type="bibr" target="#b46">[47]</ref> does not change the original tokenize results.<ref type="bibr" target="#b3">4</ref> Please refer to<ref type="bibr" target="#b28">[29]</ref> for detailed BERT.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">We also tried to use such features, without observing significant improvements.<ref type="bibr" target="#b5">6</ref> We do not take the TD-GAT model of Huang and Carley<ref type="bibr" target="#b21">[22]</ref> as our baseline, as TD-GAT contains a LSTM module which makes model more complicated and harder to analyze. Besides, our preliminary experiments showed that our GAT baseline gives comparable results with TD-GAT.<ref type="bibr" target="#b6">7</ref> N (i) is induced from the adjacent matrix A.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">We also tried other attention functions, where no significant improvement founded.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">We also tried other functions such as max pooling and random pooling, without observing significant improvements.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">We use pre-trained BERT-base-uncased model from https://github.com/ huggingface/transformers.<ref type="bibr" target="#b10">11</ref> https://github.com/yzhangcs/parser</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Target-dependent twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adaptive recursive neural network for target-dependent twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Target-dependent twitter sentiment classification with rich automatic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-T</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Gated neural networks for targeted sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-T</forename><surname>Vo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Target-sensitive memory networks for aspect sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mazumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Thumbs up? sentiment classification using machine learning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vaithyanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sentence level sentiment analysis in the presence of conjuncts using linguistic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Meena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">V</forename><surname>Prabhakar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Information Retrieval</title>
		<editor>G. Amati, C. Carpineto, and G. Romano</editor>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="573" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-level structured models for document-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yessenalina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Attention-based LSTM for aspect-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Target-sensitive memory networks for aspect sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mazumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Effective LSTMs for targetdependent sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Transformation networks for target-oriented sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Parameterized convolutional neural networks for aspect level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Carley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Interactive attention networks for aspect-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Progressive self-supervised attention learning for aspect-level sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Aspect level sentiment classification with deep memory network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recurrent attention network on memory for aspect sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Aspect-level sentiment analysis via convolution over dependency tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mensah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Syntax-aware aspect level sentiment classification with graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Carley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Aspect-based sentiment classification with aspect-specific graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Aspect based sentiment analysis with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Attention is all you need,&quot; in NIPS</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hierarchical attention based position-aware network for aspect-level sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conll</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">BERT: Pretraining of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Electra: Pretraining text encoders as discriminators rather than generators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Attentional encoder network for targeted sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<idno>abs/1902.09314</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Target-dependent sentiment classification with bert</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="154" to="290" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Utilizing BERT for aspect-based sentiment analysis via constructing auxiliary sentence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">BERT post-training for review reading comprehension and aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Exploiting BERT for end-to-end aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Workshop on Noisy User-generated Text</title>
		<meeting>the 5th Workshop on Noisy User-generated Text</meeting>
		<imprint>
			<publisher>W-NUT</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Assessing bert&apos;s syntactic abilities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">What does bert look at? an analysis of bert&apos;s attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<pubPlace>BlackBoxNLP@ACL</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Opinion word expansion and target extraction through double propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="9" to="27" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Opinion target extraction using partially-supervised word alignment model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Phrasernn: Phrase recursive neural network for aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shirai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Effective attention modeling for aspect-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dahlmeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Relational graph attention network for aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-main.295" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="page" from="3229" to="3238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Self-attention with relative position representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Lstm neural networks for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<editor>INTERSPEECH</editor>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Riesa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rudnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1609.08144</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A graph-to-sequence model for AMR-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gildea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">SemEval-2014 task 4: Aspect based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manandhar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>in SemEval</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A challenge dataset and effective models for aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D19-1654" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11" />
			<biblScope unit="page" from="6280" to="6285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Deep biaffine attention for neural dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>international conference on learning representations</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="273" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Interactive attention networks for aspect-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Multi-grained attention network for aspect-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Aspect level sentiment classification with attention-over-attention neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Carley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Social Computing, Behavioral-Cultural Modeling and Prediction and Behavior Representation in Modeling and Simulation</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="197" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Dynamic routing between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton ; I. Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2017/file/2cad8fa47bbef282badbb8de5374b894-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="3856" to="3866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A multi-task learning model for chinese-oriented aspect polarity classification and aspect term extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">419</biblScope>
			<biblScope unit="page" from="344" to="356" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sagot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Seddah</surname></persName>
		</author>
		<title level="m">What does BERT learn about the structure of language?&quot; in ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
