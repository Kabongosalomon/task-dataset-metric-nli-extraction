<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Polysemous Visual-Semantic Embedding for Cross-Modal Retrieval *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
							<email>yalesong@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Cognition</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Soleymani</surname></persName>
							<email>soleymani@ict.usc.edu</email>
							<affiliation key="aff1">
								<orgName type="department">USC Institute for Creative Technologies</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Polysemous Visual-Semantic Embedding for Cross-Modal Retrieval *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Visual-semantic embedding aims to find a shared latent space where related visual and textual instances are close to each other. Most current methods learn injective embedding functions that map an instance to a single point in the shared space. Unfortunately, injective embedding cannot effectively handle polysemous instances with multiple possible meanings; at best, it would find an average representation of different meanings. This hinders its use in real-world scenarios where individual instances and their cross-modal associations are often ambiguous. In this work, we introduce Polysemous Instance Embedding Networks (PIE-Nets) that compute multiple and diverse representations of an instance by combining global context with locally-guided features via multi-head self-attention and residual learning. To learn visual-semantic embedding, we tie-up two PIE-Nets and optimize them jointly in the multiple instance learning framework. Most existing work on cross-modal retrieval focuses on image-text data. Here, we also tackle a more challenging case of video-text retrieval. To facilitate further research in video-text retrieval, we release a new dataset of 50K video-sentence pairs collected from social media, dubbed MRW (my reaction when). We demonstrate our approach on both image-text and video-text retrieval scenarios using MS-COCO, TGIF, and our new MRW dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Visual-semantic embedding <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">26]</ref> aims to find a joint mapping of instances from visual and textual domains to a shared embedding space so that related instances from source domains are mapped to nearby places in the target space. This has a variety of downstream applications in computer vision including tagging <ref type="bibr" target="#b11">[12]</ref>, retrieval <ref type="bibr" target="#b13">[14]</ref>, captioning <ref type="bibr" target="#b25">[26]</ref>, visual question answering <ref type="bibr" target="#b23">[24]</ref>.</p><p>Formally, the goal of visual-semantic embedding is to learn two mapping functions f : X → Z and g : Y → Z jointly, where X and Y are visual and textual domains, re- * Code and data: https://yalesong.github.io/pvse "My ex texts me at 2am" … Awkward Surprised Agitated … Ambiguous Instance … Surprised Suspicious Sneaky … Ambiguous Instance Partial Association <ref type="figure">Figure 1</ref>. Cross-modal retrieval in the real-world could be challenging with ambiguous instances (each instance can have multiple meanings/concepts) and their partial associations (not all individual meanings/concepts may match). Addressing these two challenges is the focus of this work. spectively, and Z is a shared embedding space. The functions are often designed to be injective so that there is a oneto-one mapping from an instance x (or y) to a single point z ∈ R d in the embedding space. They are often optimized to satisfy the following constraint:</p><p>d(f (x i ), g(y i )) &lt; d(f (x i ), g(y j )), ∀i = j</p><p>where d(·, ·) is a certain distance measure, such as Euclidean and cosine distance. This simple and intuitive setup, which we refer to as injective instance embedding, is currently the most popular approach in the literature <ref type="bibr" target="#b52">[53]</ref>. Unfortunately, injective embedding can suffer when there is ambiguity in individual instances. Consider an ambiguous instance with multiple meanings/senses, e.g., polysemy words and images containing multiple objects. Even though each of the meanings/senses can map to different points in the embedding space, injective embedding is always forced to find a single point, which could be an (inaccurate) weighted geometric mean of all the desirable points. The issue gets intensified for videos and sentences because the ambiguity in individual images and words can aggregate and get compounded, severely limiting its use in real-world applications such as text-to-video retrieval.</p><p>Another case where injective embedding could be problematic is partial cross-domain association, a characteristic commonly observed in the real-world datasets. For instance, a text sentence may describe only certain regions of an image while ignoring other parts <ref type="bibr" target="#b55">[56]</ref>, and a video may contain extra frames not described by its associated sentence <ref type="bibr" target="#b30">[31]</ref>. These associations are implicit/hidden, making it unclear which part(s) of the image/video the text description refers to. This is especially problematic for injective embedding because information about any ignored parts will be lost in the mapped point and, once mapped, there is no way to recover from the information loss.</p><p>In this work, we address the above issues by <ref type="bibr" target="#b0">(1)</ref> formulating instance embedding as a one-to-many mapping task and <ref type="bibr" target="#b1">(2)</ref> optimizing the mapping functions to be robust to ambiguous instances and partial cross-modal associations.</p><p>To address the issues with ambiguous instances, we propose a novel one-to-many instance embedding model, Polysemous Instance Embedding Network (PIE-Net), which extracts K embeddings of each instance by combining global and local information of its input. Specifically, we obtain K locally-guided representations by attending to different parts of an input instance (e.g., regions, frames, words) using a multi-head self-attention module <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b49">50]</ref>. We then combine each of such local representation with global representation via residual learning <ref type="bibr" target="#b19">[20]</ref> to avoid learning redundant information. Furthermore, to prevent the K embeddings from collapsing into the mode (or the mean) of all the desirable embeddings, we regularize the K locally-guided representations to be diverse. To our knowledge, we are the first to apply multi-head self-attention with residual learning for the application of instance embedding.</p><p>To address the partial association issue, we tie-up two PIE-Nets and train our model in the multiple-instance learning (MIL) framework <ref type="bibr" target="#b6">[7]</ref>. We call this approach Polysemous Visual-Semantic Embedding (PVSE). Our intuition is: when two instances are only partially associated, the learning constraint of Equation (1) will unnecessarily penalize embedding mismatches because it expects two instances to be perfectly associated. Capitalizing on our oneto-many instance embedding, our MIL objective relaxes the constraint of Equation <ref type="bibr" target="#b0">(1)</ref> so that only one of K ×K embedding pairs is well-aligned, making our model more robust to partial cross-domain association. We illustrate this intuition in <ref type="figure">Figure 2</ref>. This relaxation, however, could cause a discrepancy between two embedding distributions because (K × K − 1) embedding pairs are left unconstrained. We thus regularize the learned embedding space by minimizing the discrepancy using the Maximum Mean Discrepancy (MMD) <ref type="bibr" target="#b15">[16]</ref>, a popular technique for determining whether two sets of data are from the same probability distribution.</p><p>We demonstrate our approach on two cross-modal retrieval scenarios: image-text and video-text. For image-text <ref type="bibr">Figure 2</ref>. We represent each instance with k embeddings, each representing different parts of the instance, e.g., regions of an image, frames of a video, or words of a sentence. Conventional approaches measure the visual-semantic distance by considering all k embeddings, and thus would suffer when not all concepts are related. We instead assume there is a partial match and measure the distance between only the most related combination (squares). retrieval, we evaluate on the MS-COCO dataset <ref type="bibr" target="#b31">[32]</ref>; for video-text retrieval, we evaluate on the TGIF dataset <ref type="bibr" target="#b30">[31]</ref> as well as our new MRW (my reaction when) dataset, which we collected to promote further research in cross-modal video-text retrieval under ambiguity and partial association. The dataset contains 50K video-sentence pairs collected from social media, where the videos depict physical or emotional reactions to certain situations described in text. We compare our method with well-established baselines and carefully conduct an ablation study to justify various design choices. We report strong performance on all three datasets, and achieve the state-of-the-art result on image-to-text retrieval task on the MS-COCO dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Here we briefly review some of the most relevant work on instance embedding for cross-modal retrieval.</p><p>Correlation maximization: Most existing methods are based on one-to-one mapping of instances into a shared embedding space. One popular approach is maximizing correlation between related instances in the embedding space. Rasiwasia et al. <ref type="bibr" target="#b40">[41]</ref> use canonical correlation analysis (CCA) to maximize correlation between images and text, while Gong et al. <ref type="bibr" target="#b13">[14]</ref> extend CCA to a triplet scenario, e.g., images, tags, and their semantic concepts. Most recent methods incorporate deep neural networks to learn their embedding models in an end-to-end fashion. Andrew et al. <ref type="bibr" target="#b1">[2]</ref> propose deep CCA (DCCA), and Yan et al. <ref type="bibr" target="#b56">[57]</ref> apply it to image-to-sentence and sentence-to-image retrieval.</p><p>Triplet ranking: Another popular approach is based on triplet ranking <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b57">58]</ref>, which encourages the distance between positive pairs (e.g., ground-truth pairs) to be closer than negative pairs (e.g., randomly selected pairs). Frome et al. <ref type="bibr" target="#b11">[12]</ref> propose a deep visual-semantic embedding (DeViSE) model, using a hinge loss to implement triplet ranking. Faghri et al. <ref type="bibr" target="#b9">[10]</ref> extend this with the idea of hard negative mining, which focuses on maximum violating negative pairs, and report improved convergence rates.</p><p>Learning with auxiliary tasks: Several methods learn the embeddings in conjunction by solving auxiliary tasks, e.g., signal reconstruction <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b48">49]</ref>, semantic concept categorization <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b22">23]</ref>, and minimizing the divergence between embedding distributions induced by different modalities <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b58">59]</ref>. Adversarial training <ref type="bibr" target="#b14">[15]</ref> is also used by many: Wang et al. <ref type="bibr" target="#b51">[52]</ref> encourage the embeddings from different modalities to be indistinguishable using a domain discriminator, while Gu et al. <ref type="bibr" target="#b16">[17]</ref> learn the embeddings with imageto-text and text-to-image synthesis tasks in the adversarial learning framework.</p><p>Attention-based embedding: All the above approaches are based on one-to-one mapping and thus could suffer from polysemous instances. To alleviate this, recent methods incorporate cross-attention mechanisms to selectively attend to local parts of an instance given the context of a conditioning instance from another modality <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b29">30]</ref>, e.g., attend to different image regions given different text queries. Intuitively, this can resolve the issues with ambiguous instances and their partial associations because the same instance can be mapped to different points depending on the presence of the conditioning instance. However, such approach comes with computational overhead at inference time because each query instance needs to be encoded as many times as the number of references instances in the database; this severely limits its use in real-world applications. Different from previous approaches, our method is based on multi-head selfattention <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b49">50]</ref> which does not require a conditioning instance when encoding, and therefore each instance is encoded only once, significantly reducing computational overhead at inference time.</p><p>Beyond injective embedding: Similar to our motivation, some attempts have been made to go beyond the injective mapping. One approach is to design the embedding function to be stochastic and map an instance to a certain probability distribution (e.g., Gaussian) instead of a single point <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref>. However, learning distributions is typically difficult/expensive and often lead to approximate solutions such as Monte Carlo sampling.</p><p>The work most similar to ours is by Ren et al. <ref type="bibr" target="#b43">[44]</ref>, where they compute multiple representations of an image by extracting local features using the region proposal method <ref type="bibr" target="#b12">[13]</ref>; text instances are still represented by a single embedding vector. Different from theirs, our method computes multiple and diverse representations from both modalities, where each representation is a combination of global context and locally-guided features, instead of just a local feature. Song et al. <ref type="bibr" target="#b47">[48]</ref>, a prequel to this work, also compute multiple representations of each instance using multihead self-attention. We extend their approach by combin-ing global and locally-guided features via residual learning. We also extend the preliminary version of the MRW dataset with an increased number of sample pairs. Lastly, we report more comprehensive experimental results, adding results on the MS-COCO <ref type="bibr" target="#b31">[32]</ref> dataset for image-text cross-retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>Our Polysemous Visual-Semantic Embedding (PVSE) model, shown in <ref type="figure" target="#fig_1">Figure 3</ref>, is composed of modality-specific feature extractors followed by two sub-networks with an identical architecture; we call the sub-network Polysemous Instance Embedding Network (PIE-Net). The two PIE-Nets are independent of each other and do not share the weights.</p><p>The PIE-Net takes as input a global context vector and multiple local feature vectors (Section 3.1), computes locally-guided features using the local feature transformer (Section 3.2), and outputs K embeddings by combining the global context vector with locally-guided features (Section 3.3). We train the PVSE model in the Multiple Instance Learning (MIL) <ref type="bibr" target="#b6">[7]</ref> framework. We explain how we make our model robust to ambiguous instances and partial crossmodal associations via our loss functions (Section 3.4) and finish with implementation details (Section 3.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Modality-Specific Feature Encoder</head><p>Image encoder: We use the ResNet-152 <ref type="bibr" target="#b19">[20]</ref> pretrained on ImageNet <ref type="bibr" target="#b45">[46]</ref> to encode an image x. We take the feature map before the final average pooling layer as local features Ψ(x) ∈ R 7×7×2048 . We then apply average pooling to Ψ(x) and feed the output to one fully-connected layer to obtain global features φ(x) ∈ R H .</p><p>Video encoder: We use the ResNet-152 to encode each of T frames from a video x, taking the 2048-dim output from the final average pooling layer, and use them as local features Ψ(x) ∈ R T ×2048 . We then feed Ψ(x) into a bidirectional GRU (bi-GRU) <ref type="bibr" target="#b5">[6]</ref> with H hidden units, and take the final hidden states as global features φ(x) ∈ R H .</p><p>Sentence encoder: We encode each of L words from a sentence x using the GloVe <ref type="bibr" target="#b39">[40]</ref> pretrained on the Common-Crawl dataset, producing L 300-dim vectors, and use them as local features Ψ(x) ∈ R L×300 . We then feed them into a bi-GRU with H hidden units, and take the final hidden states as global features φ(x) ∈ R H .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Local Feature Transformer</head><p>The local feature transformer takes local features Ψ(x) and transforms them into K locally-guided representations Υ(x). Our intuition is that different combinations of local information could yield diverse and refined representations of an instance. We implement this intuition by employing a multi-head self-attention module to obtain K attention maps, prepare K combinations of local features by attend-  ing to different parts of an instance, and apply non-linear transformations to obtain K locally-guided representations.</p><p>We use a two-layer perceptron to implement the multihead self-attention module. <ref type="bibr" target="#b0">1</ref> Given local features Ψ(x) ∈ R B×D 2 , it computes K attention maps α ∈ R K×B :</p><formula xml:id="formula_1">α = softmax (w 2 tanh (w 1 Ψ(x) ))<label>(2)</label></formula><p>where w 2 ∈ R K×A , w 1 ∈ R A×D ; we set A = D/2 per empirical evidence. The softmax is applied row-wise so that each of the K attention coefficients sum up to one. Finally, we multiply the attention map with local features and further apply a non-linear transformation to obtain K locally-guided representations Υ(x) ∈ R K×H :</p><formula xml:id="formula_2">Υ(x) = σ ((αΨ(x))w 3 + b 3 )<label>(3)</label></formula><p>where w 3 ∈ R D×H and b 3 ∈ R H . We use the sigmoid as our activation function σ(·).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Feature Fusion With Residual Learning</head><p>The fusion block combines global features φ(x) and locally-guided features Υ(x) to obtain the final K embedding output. We note that there is an inherent information overlap between the two features (both are derived from the same instance). To prevent Υ(x) from becoming redundant with φ(x) and encourage it to learn only locally-specific information, we cast the feature fusion as a residual learning task. Specifically, we consider φ(x) as input to the residual block and Υ(x) as residuals with its own parameters to optimize (w 1 , w 2 , w 3 , b 3 ). As shown in <ref type="bibr" target="#b19">[20]</ref>, this residual mapping makes it easier to optimize the parameters associated with Υ(x), helping us find meaningful locally-specific information; in the extreme case, if global features φ(x) were the optimal, the residuals will be pushed to zero and the approach will fall back to the standard injective embedding.</p><p>We compute K embedding vectors z ∈ R K×H as:</p><formula xml:id="formula_3">z = LayerNorm (Φ(x) + Υ(x)) (4) where Φ(x) ∈ R K×H is K repetitions of φ(x).</formula><p>Following <ref type="bibr" target="#b49">[50]</ref>, we apply the layer normalization <ref type="bibr" target="#b2">[3]</ref> to the output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Optimization and Inference</head><formula xml:id="formula_4">Given a dataset D = {(x i , y i )} N i=1</formula><p>with N instance pairs (x are either images or videos, y are sentences), we optimize our PVSE model to minimize a learning objective:</p><formula xml:id="formula_5">L = L mil + λ 1 L div + λ 2 L mmd<label>(5)</label></formula><p>where λ 1 and λ 2 are scalar weights that balance the influence of the loss terms. We describe each loss term below. MIL Loss: We train our model in the Multiple Instance Learning (MIL) framework <ref type="bibr" target="#b6">[7]</ref>, designing a learning constraint for the cross-modal retrieval scenario:</p><formula xml:id="formula_6">min p,q d(z x i,p , z y i,q ) &lt; d(z x i,p , z y j,q ), ∀i = j, ∀p, q<label>(6)</label></formula><p>where z x and z y are the PIE-Net embeddings of x and y, respectively, and p, q = 1, · · · , K. We use the cosine distance as our distance metric, d(a, b) = (a · b)/( a b ).</p><p>Making an analogy to the MIL for binary classification <ref type="bibr" target="#b0">[1]</ref>, the left side of the constraint is the "positive" bag where at least one of K × K embedding pairs is assumed to be positive (match), while the right side is the "negative" bag containing only negative (mismatch) pairs. Optimizing under this constraint allows our model to be robust to partial cross-modal association because it can ignore mismatching embedding pairs of partially associated instances.</p><p>We implement the above constraint by designing our MIL loss function L mil to be:</p><formula xml:id="formula_7">1 N 2 N i,j max 0, ρ − min p,q d(z x i,p , z y j,q ) + min p,q d(z x i,p , z y i,q )</formula><p>where ρ is a margin parameter. Notice that we have the min operator for d(z x i,p , z y j,q ), similar to <ref type="bibr" target="#b43">[44]</ref>; this can be seen as a form of hard negative mining, which we found to be effective and accelerate the convergence.</p><p>Diversity Loss: To ensure that our PIE-Net produces diverse representations of an instance, we design a diversity loss L div that penalizes the redundancy among K locallyguided features. To measure the redundancy, we compute a Gram matrix of Υ(x) (and of Υ(y)) that encodes the correlations between all combinations of locally-guided features, i.e., G i,j = h Υ(x) ih Υ(x) jh . We normalize each Υ(x) i prior to the computation so that they are on an l 2 ball.</p><p>The diagonal entries in G are always one (they are on a unit ball); the off-diagonals are zero iff two locally-guided features are orthogonal to each other. Therefore, the sum of off-diagonal entries in G indicates the redundancy among K locally-guided features. Based on this, we define our diversity loss as:</p><formula xml:id="formula_8">L div = 1 K 2 ( G x − I 2 + G y − I 2 )<label>(7)</label></formula><p>where G x and G y are the gram matrices of Υ(x) and Υ(y), respectively, and I ∈ R K×K is an identity matrix. Note that we do not compute the diversity loss on the final embedding representations z x and z y because they already have global information baked in, making the orthogonality constraint invalid. This also ensures that the loss gets back-propagated through appropriate parts in the computational graph, and does not affect the global feature encoders, i.e., the FC layer for the image encoder, and the bi-GRUs for the video and sentence encoders.</p><p>Domain Discrepancy Loss: Optimizing our model under the MIL loss has one drawback: two distributions induced by z x and z y , which we denote by Z x and Z y , respectively, may diverge quickly because we only consider the minimum distance pair, min p,q d(z x p , z y q ), in loss computation and let the other (K × K − 1) pairs left to be unconstrained. It is therefore necessary to regularize the discrepancy between the two distributions.</p><p>One popular way to measure the discrepancy between two probability distributions is the Maximum Mean Discrepancy (MMD) <ref type="bibr" target="#b15">[16]</ref>. The MMD between two distributions P and Q over a function space F is</p><formula xml:id="formula_9">MMD(P, Q) = sup f ∈F (E X∼P [f (X)] − E Y ∼Q [f (Y )]) (8)</formula><p>When F is a reproducing kernel Hilbert space (RKHS) with a kernel κ : X × X → R that measures the similarity between two samples, Gretton et al. <ref type="bibr" target="#b15">[16]</ref> showed that the supremum is achieved at</p><formula xml:id="formula_10">f (x) = E X ∼P [κ(x, X )] − E X ∼Q [κ(x, X )]</formula><p>. Substituting this to Equation <ref type="formula">(8)</ref> and squaring the result, and approximating the expectation over our empirical distributions Z x and Z y , we have our domain discrepancy loss L mmd defined as</p><formula xml:id="formula_11">κ(z x i,p , z x j,q ) − 2 κ(z x i,p , z y j,q ) + κ(z y i,p , z y j,q ) K 2 N 2</formula><p>where the summation in each term is taken over all pairs of embeddings (i, j, p, q) ∈ [1, · · · , K 2 N 2 ]. We use a radial basis function (RBF) kernel as our kernel function.</p><p>Inference: At test time, we assume a database of M instances (e.g., videos) and their KM embedding vectors. Given a query instance (e.g., a sentence), we compute K embedding vectors and find the best matching instance in the database by comparing the cosine distances between all K 2 M combinations of embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Implementation Details</head><p>We subsample frames at 8 FPS and store them in a binary storage format. <ref type="bibr" target="#b2">3</ref> We set the maximum length of video to be 8 frames; for videos longer than 8 frames we select random subsequences during training, while during inference we sample 8 frames evenly spread across each video. We do not limit the sentence length as it has a minimal effect on the GPU memory footprint. We cross-validate the optimal hyper-parameter settings, varying K ∈ [1 :</p><formula xml:id="formula_12">8], H ∈ [512, 1024, 2048], ρ ∈ [0.1 : 1.0], λ 1 , λ 2 ∈ [0.1, 0.01, 0.001].</formula><p>We use the AMSGRAD optimizer <ref type="bibr" target="#b41">[42]</ref> with an initial learning rate of 2e-4 and reduce it by half when the loss stagnates. We train our model end-to-end, except for the pretrained CNN weights, for 50 epochs with a batch of 128 samples. We then finetune the whole model (including the CNN weights) for another 50 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">MRW Dataset</head><p>To promote future research in video-text cross-modal retrieval, especially with ambiguous instances and their partial cross-domain association, we release a new dataset of 50K video-sentence pairs collected from social media; we call our dataset MRW (my reaction when). <ref type="table" target="#tab_0">Table 1</ref> provides descriptive statistics of several videosentence datasets. Most existing datasets are designed for video captioning <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b30">31]</ref>, with sentences providing textual descriptions of visual content in videos (video → text relationship). Our dataset is unique in that it provides videos that display physical or emotional reactions to the given sentences (text → video relationship); these are called reaction GIFs. According to a subreddit r/reactiongif 4 :</p><p>A reaction GIF is a physical or emotional response that is captured in an animated GIF which you can link in response to someone or something on the Internet. The reaction must not be in response to something that happens within the GIF, or it is considered a "scene".  This definition clearly differentiates ours from existing datasets: There is an inherently weaker association of concepts between video and text; see <ref type="figure" target="#fig_2">Figure 4</ref>. This introduces several additional challenges to cross-modal retrieval, part of which are the focus of this work, i.e., dealing with ambiguous instances and partial cross-domain association. We provide detailed data analyses and compare it with existing video captioning datasets in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We evaluate our approach on image-text and videotext cross-modal retrieval scenarios. For image-text crossretrieval, we evaluate on the MS-COCO dataset <ref type="bibr" target="#b31">[32]</ref>; for video-text we use the TGIF <ref type="bibr" target="#b30">[31]</ref> and our MRW datasets.</p><p>For MS-COCO we use the data split of <ref type="bibr" target="#b27">[28]</ref>, which provides 113,287 training, 5K validation and 5K test samples; each image comes with 5 captions. We report results on both 1K unique test images (averaged over 5 folds) and the full 5K test images. For TGIF we use the original data split <ref type="bibr" target="#b30">[31]</ref>  Following the convention in cross-modal retrieval, we report results using Recall@k (R@k) at k = 1, 5, 10, which measures the the fraction of queries for which the correct item is retrieved among the top k results. We also report the median rank (Med R) of the closest ground truth result in the list, as well as the normalized median rank (nMR) that divides the median rank by the number of total items. For cross-validation, we select the best model that achieves the highest rsum = R@1 + R@5 + R@10 in both directions (visual-to-text and text-to-visual) on a validation set.</p><p>While we report quantitative results in the main paper, our supplementary material contains qualitative results with visualizations of multi-head self-attention maps. <ref type="table">Table 2</ref> shows the results on MS-COCO. To facilitate comprehensive comparisons, we provide previously reported results on this dataset. <ref type="bibr" target="#b4">5</ref> Our approach outperforms most of the baselines, and achieves the new state-of-the-art on the image-to-text task on the 5K test set. We note that both GXN <ref type="bibr" target="#b16">[17]</ref> and SCO <ref type="bibr" target="#b22">[23]</ref> are trained with multiple objectives; in addition to solving the ranking task, GXN performs image-text cross-modal synthesis as part of training, while SCO performs classification of semantic concepts and their orders as part of training. Compared to the two methods, our model is trained with a single objective (ranking) and thus could be considered as a simpler model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Image-Text Retrieval Results</head><p>The most direct comparison to ours would be with VSE++ <ref type="bibr" target="#b9">[10]</ref>. Both our model and VSE++ share the same image and sentence encoders. When we let our PIE-Net to produce single embeddings for input instances (K=1), the only difference becomes that VSE++ directly uses our global features as their embedding representations, while we use the output from our PIE-Nets. The performance gap between ours (K=1) and VSE++ shows the effectiveness of our PIE-Net, which combines global context with locallyguided features produced by our local feature transformer. <ref type="table">Table 3</ref> and <ref type="table">Table 4</ref> show the results on TGIF and MRW datasets. Because there is no previously reported results on these datasets for the cross-model retrieval scenario, we run the baseline models and report their results. We can see that our method show strong performance compared to all the baselines. We provide implementation details of the baseline models in the supplementary material.  We notice is that the overall performance is much lower than the results from MS-COCO. This shows how challenging video-text retrieval is (and video understanding in a broader context), and calls for further research in this task. We can also see that there is a large performance gap between the two datasets. This suggests the two datasets have significantly different characteristics: the TGIF contains sentences describing visual content in videos, while our MRW dataset contains videos showing one of possible reactions to certain situations described in sentences. This makes the association between video and text modalities much weaker for the MRW than for the TGIF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Video-Text Retrieval Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation Results</head><p>The number of embeddings K: Tables 2, 3, 4 show that computing multiple embeddings per instance improves performance compared to just a single embedding (see the last two rows in each table). To better understand the effect of K, we vary it from 1 to 8, and also compare with K = 0, a baseline where we bypass our Local Feature Transformer and simply use the global feature as the final embedding representation. <ref type="figure" target="#fig_3">Figure 5</ref> shows the performance on all three datasets based on the rsum metric (R@1  + R@5 + R@10 for image/video-to-text and back). The results are from the models before fine-tuning the ResNet-152 weights. We can see that there is a significant improvement from K = 0 to K = 1; this shows the effectiveness of our Local Feature Transformer. We can make an interesting observation by comparing the optimal K settings across different datasets: K = 3 for COCO and TGIF, and K = 5 for MRW. While this cannot be used as strong evidence, we believe this shows the level of ambiguity is higher on MRW than the other two datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Video-to-Text</head><p>Text-to-Video R@1 R@5 R@10 Med R (nMR) R@1 R@5 R@10 Med R (nMR) VSE++ <ref type="bibr">[</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global vs. locally-guided features:</head><p>We analyze the importance of global and locally-guided features, as well as different strategies to combine them. <ref type="figure" target="#fig_4">Figure 6</ref> shows results on several ablative settings: No Global is when we use locally-guided features alone (discard global features); No Residual is when we simply concatenate global and locally-guided features, instead of combining them via residual learning. We report results on both MS-COCO and MRW because the two datasets exhibit the biggest difference in the level of ambiguity.</p><p>We notice that the performance drops significantly on both datasets when we discard global features. Together with K = 0 results in <ref type="figure" target="#fig_3">Figure 5</ref> (discard locally-guided features), this shows the importance of balancing global and local information in the final embedding. We also see that simply concatenating the two features (no residual learning) hurts the performance, and the drop is more significant on the MRW dataset. This suggests our residual learning setup is especially crucial for highly ambiguous data.</p><p>MIL objective: <ref type="figure" target="#fig_4">Figure 6</ref> also shows the result of No MIL, which is when we concatenate the K embeddings and optimize the standard triplet ranking objective <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b9">10]</ref>, i.e., the "Conventional" setup in <ref type="figure">Figure 2</ref>. While the differences are relatively smaller than with the other ablative settings, there are statistically significant differences between the two results on both datasets (p = 0.046 on MS-COCO and p = 0.015 on MRW). We also see that the difference between No MIL and Ours on MRW is more pronounced than on MS-COCO. This suggests thed MIL objective is especially effective for highly ambiguous data.</p><p>Sensitivity analysis on different loss weights: <ref type="figure" target="#fig_5">Figure 7</ref> shows the sensitivity of our approach when we vary the relative loss weights, i.e., λ 1 and λ 2 in Equation <ref type="bibr" target="#b4">(5)</ref>. Note that the weights are relative, not absolute, e.g., instead of directly multiplying λ 1 = 1.0 to L div , we first scale it to λ 1 × (L mil /L div ) and then multiply it to L div . The results show that both loss terms are important in our model. We can see, in particular, that L mmd plays an important role in our model. Without it, the two embedding spaces induced by different modalities may diverge quickly due to the MIL objective, which may result in a poor convergence rate. Overall, our results suggests that the model is not much sensitive to the two relative weight terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Ambiguous instances and their partial associations pose significant challenges to cross-modal retrieval. Unlike the traditional approaches that use injective embedding to compute a single representation per instance, we propose a Polysemous Instance Embedding Network (PIE-Net) that computes multiple and diverse representations per instance. To obtain visual-semantic embedding that is robust to partial cross-modal association, we tie-up two PIE-Nets, one per modality, and jointly train them using the Multiple Instance Learning objective. We demonstrate our approach on the image-text and video-text cross-modal retrieval scenarios and report strong results compared to several baselines.</p><p>Part of our contribution is also in the newly collected MRW dataset. Unlike existing video-sentence datasets that contain sentences describing visual content in videos, ours contain videos illustrating one of possible reactions to certain situations described in sentences, which makes videosentence association somewhat ambiguous. This poses new challenges to cross-modal retrieval; we hope there will be further progress on this challenging new dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. MRW Dataset</head><p>Our dataset consists of 50,107 video-sentence pairs collected from popular social media websites including reddit, Imgur, and Tumblr. We crawled the data using the GIPHY API 6 with query terms mrw, mfw, hifw, reaction, and reactiongif; we crawled the data from August 2016 to March 2019. <ref type="table">Table 5</ref> shows the descriptive statistics of our dataset. We are continuously crawling the data, and plan to release updated versions in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Previous Work on Animated GIF</head><p>Note that most of the videos in our dataset have the animated GIF format. Technically speaking, animated GIFs and videos have different formats; the former is lossless, palette-based, and has no audio. In this paper, however, we use the two terms interchangeably because the distinction is unnecessary in our method. Below, to provide the context for our work, we briefly review previous work that focused on animated GIF.</p><p>There is increasing interest in conducting research around animated GIFs. Bakhshi et al. <ref type="bibr" target="#b3">[4]</ref> studied what makes animated GIFs engaging on social networks and identified a number of factors that contribute to it: the animation, lack of sound, immediacy of consumption, low bandwidth and minimal time demands, the storytelling capabilities and utility for expressing emotions. Previous work in the computer vision and multimedia communities used animated GIFs for various tasks in video understanding. Jou et al. <ref type="bibr" target="#b24">[25]</ref> propose a method to predict viewer perceived emotions for animated GIFs. Gygli et al. <ref type="bibr" target="#b18">[19]</ref> propose the Video2GIF dataset for video highlighting, and further extended it to emotion recognition <ref type="bibr" target="#b17">[18]</ref>. Chen et al. <ref type="bibr" target="#b4">[5]</ref> propose the GIFGIF+ dataset for emotion recognition. Zhou et al. <ref type="bibr" target="#b59">[60]</ref> propose the Image2GIF dataset for video prediction, along with a method to generate cinemagraphs from a single image by predicting future frames.</p><p>Recent work use animated GIFs to tackle the vision &amp; language problems. Li et al. <ref type="bibr" target="#b30">[31]</ref> propose the TGIF dataset for video captioning; Jang et al. <ref type="bibr" target="#b23">[24]</ref> propose the TGIF-QA dataset for video visual question answering. Similar to the TGIF dataset <ref type="bibr" target="#b30">[31]</ref>, our dataset includes video-sentence pairs. However, our sentences are created by real users from Internet communities rather than study participants, thus posing real-world challenges. More importantly, our dataset has implicit concept association between videos and sentences (videos contain physical or emotional reactions to sentences), while the TGIF dataset has explicit concept association (sentences describe visual content in videos).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Analysis of Facial Expressions</head><p>Facial expression plays an important role in our dataset: 6,380 samples contain the hashtag MFW (my face when), indicating that those GIFs contain emotional reactions manifested by facial expressions. To better understand the landscape of our dataset, we analyze the types of facial expressions contained in our dataset by leverage automatic tools.</p><p>First, we count the number of faces appearing in the animated GIFs. To do this, we applied the dlib CNN face detector <ref type="bibr" target="#b26">[27]</ref> on five frames sampled from each animated GIF with an equal interval. The results show that there are, on average, 0.73 faces in a given frame of an animated GIF. Also, 34,052 animated GIFs contain at least one face. This means that 72% of our videos contain faces, which is quite significant. This suggests that employing techniques tailored specifically for face understanding could potentially improve performance on our dataset.</p><p>Next, we use the Affectiva Affdex <ref type="bibr" target="#b36">[37]</ref> to analyze facial expressions depicted in the animated GIFs, detecting the intensity of expressions from two frames per second in each animated GIF. We looked at six expressions of basic emotions <ref type="bibr" target="#b8">[9]</ref>, namely, joy, fear, sadness, disgust, surprise and anger. We analyzed only the frames that contain a face with its bounding box region larger than 15% of the image. <ref type="figure">Figure 8</ref> shows the results. Overall, joy with average intensity of 9.1% and disgust (7.4%) are the most common facial expressions in our dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Comparison to the TGIF Dataset</head><p>Image and video captioning often involves describing objects and actions depicted explicitly in visual content <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b30">31]</ref>. For reaction GIFs, however, visual-textual association is not always explicit. For example, as is the case in our dataset, objects and actions depicted in visual content might be a physical or emotional reaction to the scenario posed in the sentence.</p><p>In this section, we qualitatively compare our dataset with the TGIF dataset <ref type="bibr" target="#b30">[31]</ref>, which contains 120K video-sentence pairs for video captioning. We chose the dataset because both datasets contain animated GIFs collected from social media, and thus contain similar visual content.</p><p>We first compare words appearing in both datasets. <ref type="figure">Figure 9</ref> shows word clouds of nouns and verbs extracted from our MRW dataset and the TGIF dataset <ref type="bibr" target="#b30">[31]</ref>. Sentences in the TGIF dataset are constructed by crowdworkers to describe the visual content explicitly displayed in animated GIFs. Therefore, its nouns and verbs mainly describe physical objects, people and actions that can be visualized, e.g., cat, shirt, stand, dance. In contrast, MRW sentences are constructed by the Internet users, typically from subcommunities in social networks that focus on reaction GIFs. As can be seen from <ref type="figure">Figure 9</ref>, verbs and nouns in our MRW dataset additionally include abstract terms that cannot nec-  <ref type="table">Table 5</ref>. Descriptive statistics of the MRW dataset. <ref type="figure">Figure 8</ref>. Histograms of the intensity of facial expressions. The horizontal axis represents the intensity of the detected expression, while the vertical axis is the sample count in frames with faces. We clip the y-axis at 1000 for visualization. Overall, joy, with average intensity of 9.1% and disgust (7.4%) are the most common facial expressions in our dataset. essarily be visualized, e.g., time, day, realize, think. This shows that our dataset contains ambiguous terms and their associations, which pose significant challenges to crossmodal retrieval.</p><p>Next, we compare whether video-sentence associations are explicit/implicit in both datasets. To this end, we conducted a user study in which we asked six participants to verify the association between sentences and animated GIFs. We randomly sampled 100 animated GIFs from the test sets of both our dataset and TGIF dataset <ref type="bibr" target="#b30">[31]</ref>. We paired each animated GIF with both its associated sentence and a randomly selected sentence from the corresponding dataset, resulting in 200 GIF-sentence pairs per dataset.</p><p>The results show that, in case of our dataset (MRW), 80.4% of the associated pairs are positively marked as being relevant, suggesting humans are able to distinguish the true vs. fake pairs despite implicit concept association. On the other hand, 50.7% of the randomly assigned sentences are also marked as matching sentences. The high false positive rate shows the ambiguous nature of GIF-sentence association in our dataset.</p><p>In contrast, for the TGIF dataset with clear explicit association, 95.2% of the positive pairs are correctly marked as relevant and only 2.6% of the irrelevant pairs are marked as being relevant. This human baseline demonstrates the challenging nature of GIF-sentence association in our dataset, due to their implicit rather than explicit association. <ref type="figure">Figure 9</ref>. Distributions of nouns and verbs in our MRW and the TGIF <ref type="bibr" target="#b30">[31]</ref> datasets. Compared to the TGIF dataset, words in our dataset depict more abstract concepts (e.g., post, time, day, start, realize, think, try), suggesting the ambiguous nature in our dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Application: Animated GIF Search</head><p>Animated GIFs are becoming increasingly popular <ref type="bibr" target="#b3">[4]</ref>; more people use them to tell stories, summarize events, express emotion, and enhance (or even replace) text-based communication. To reflect this trend, several social networks and messaging apps have recently incorporated GIFrelated features into their systems, e.g., Facebook users can create posts and leave comments using GIFs, Instagram and Snapchat users can put "GIF stickers" into their personal videos, and Slack users can send messages using GIFs. This rapid increase in popularity and real-world demand necessitates more advanced and specialized systems for animated GIF search.</p><p>Current solutions to animated GIF search rely entirely on concept tags associated with animated GIFs and matching them with user queries. The tags are typically provided by users or produced by editors at companies like GIPHY. In the former case, noise becomes an issue; in the latter, it is expensive and would not scale well.</p><p>One of the motivations behind collecting our MRW dataset is to build a text-based animated GIF search engine, targeted for real-world scenarios mentioned above. Existing video captioning datasets, such as TGIF <ref type="bibr" target="#b30">[31]</ref>, are inappropriate for our purpose because of the explicit nature of visual-textual association, i.e., sentences simply describe what is being shown in videos. Rather, we need a dataset that captures various types of nuances used in social media, e.g., humor, irony, satire, sarcasm, incongruity, etc. Because our dataset provides video-text pairs with implicit visual-textual association, we believe that it has the potential to provide training data for building text-based animated GIF search engines targeted for social media.</p><p>To demonstrate the potential, we provide qualitative results on text-to-video retrieval using our dataset, shown in <ref type="figure">Figure 12</ref>. Each set of results show a query text and the top five retrieved videos, along with their ranks and cosine similarity scores. We would like the readers to take a close look at each set of results and decide which of the five retrieved videos depict the most likely visual response to the query sentence. The answers are provided below. For better viewing experience, we provide an HTML page with animated GIFs instead of static images. We strongly encourage the readers to check the HTML page to better appreciate the results. (Answers: 3, 5, 2, 4, 1, 5, 4)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Baseline Implementation Details</head><p>In the experiment section, we provided baseline results for MS-COCO, TGIF, and MRW datasets. For MS-COCO, we provided previously reported results. For TGIF and MRW, on the other hand, we reported our own results because there has not been previous results on the datasets. Due to the space limit, we omitted implementation details of the baseline approaches; here we provide implementation details of the four baseline approaches: DeViSE <ref type="bibr" target="#b11">[12]</ref>, VSE++ <ref type="bibr" target="#b9">[10]</ref>, Order Embedding <ref type="bibr" target="#b50">[51]</ref>, and Corr-AE <ref type="bibr" target="#b10">[11]</ref>.</p><p>For fair comparison, all four baselines share the same video and sentence encoders as described in Section 3.1 of the main paper. The only difference is in the loss function</p><formula xml:id="formula_13">L DeV iSE = 1 N N i,j,k=1 max (0, ρ − d(φ(x i ), φ(y i )) + d(φ(x j ), φ(y k )) , ∀(i = j ∨ i = k) ∧ j = k (9) L V SE++ = 1 N N i=1 q={j,k} max q max (0, ρ − d(φ(x i ), φ(y i )) + d(φ(x j ), φ(y k )) , ∀(i = j ∨ i = k) ∧ j = k<label>(10)</label></formula><formula xml:id="formula_14">L CorrAE = L DeV iSE + 1 N N i=1 ci={xi,yi} φ(x i ) −φ(x i |c i ) 2 2 + φ(y i ) −φ(x i |c i ) 2 2<label>(11)</label></formula><p>we train the models with. Following the notation used in the main paper, we denote the output of the video and sentence encoders by φ(x) and φ(y), respectively. We employ the following loss functions for the baselines: DeViSE <ref type="bibr" target="#b11">[12]</ref>: We implement the conventional hinge loss in the triplet ranking setup; see Equation <ref type="bibr" target="#b8">(9)</ref>. It penalizes the cases when the distance between positive pairs (i.e., the ground truth) is further away than negative pairs (e.g., randomly sampled) with a margin parameter ρ (we measure the cosine distance). VSE++ <ref type="bibr" target="#b9">[10]</ref>: We implement the hard negative mining version of the conventional hinge loss triplet ranking loss; see Equation <ref type="bibr" target="#b9">(10)</ref>. We have experimented with the original version and found that it fails to find a suitable solution to the objective, producing retrieval results that are almost identical to random guess. We suspect that the high noise present in both TGIF and MRW datasets makes the max function too strict as a constraint. We therefore replace the max q function with a "filter" function that includes only highly-violating cases while ignoring others.</p><p>Intuitively, we implement the filter function to be an outlier detection function based on z-scores, where any z-score greater than 3 or less than -3 is considered to be an outlier. Specifically, we compute the z-scores for all of possible (i, j, k) combinations inside Equation <ref type="bibr" target="#b9">(10)</ref> and discard instances if their absolute z-score is below 3.0. This way, we are considering multiple hard negatives instead of just one. We have empirically found this modification to be crucial to achieve reasonable performances on the TGIF and MRW datasets.</p><p>Order Embedding <ref type="bibr" target="#b50">[51]</ref>: We used the original implementation provided by the authors of <ref type="bibr" target="#b50">[51]</ref>.</p><p>Corr-AE <ref type="bibr" target="#b10">[11]</ref>: We implement the correspondence crossmodal autoencoder proposed by Feng et al. <ref type="bibr" target="#b10">[11]</ref> (see <ref type="bibr">Figure 4 in [11]</ref>). Given the encoder output φ(x) and φ(y), we build two autoencoders, one per modality, so that each autoencoder can reconstruct both φ(x) and φ(y). The autoencoders have four fully-connected layers with [512, 256, 256, 512] hidden units, respectively. Each of the fully con-nected layers is followed by a ReLU activation and a layer normalization <ref type="bibr" target="#b2">[3]</ref>.</p><p>Formally, a video autoencoder takes as input φ(x) and outputs [φ(x|x);φ(y|x)], and a sentence autoencoder takes as input φ(y) and outputs [φ(x|y);φ(y|y)]. We then train the model by optimizing the loss form shown in Equation <ref type="bibr" target="#b10">(11)</ref>. We note that this loss is different from the original formulation of Corr-AE <ref type="bibr" target="#b10">[11]</ref>, where the first term in Equation <ref type="formula" target="#formula_0">(11)</ref> is replaced by a Euclidean loss, i.e.,</p><formula xml:id="formula_15">L 2 = 1 N N i=1 φ(x i ) − φ(y i ) 2 2 .</formula><p>We found that using L 2 instead of L DeV iSE makes the learning much harder, producing results that is almost identical to random guess.</p><p>C. Visualization of Multi-Head Self-Attention C.1. Image-to-Text Retrieval Results on MS-COCO <ref type="figure">Figure 10</ref> shows examples of visual-textual attention maps on the MS-COCO dataset; the task is image-to-text retrieval. The first column shows query images with groundtruth sentences. Each of the other three columns shows visual (spatial) attention maps and their top-ranked text retrieval results, as well as their ranks and cosine similarity scores (green: correct, red: incorrect). We color-code words in the retrieved sentences according to their textual attention intensity values, normalized between [0, 1].</p><p>A glimpse at the results in each row shows that the three attention maps attend to different regions of the query image. Looking closely, we notice that salient regions are typically attended by multiple attention maps. For example, all three attention maps in <ref type="figure">Figure 10</ref> highlight: (a) the photographer, (b) the bench, (c) the fruit stand, (e) the pink flowers, (f) the stop sign, (h) the woman, (j) the fire hydrant. However, this is not always the case: In <ref type="figure">Figure 10</ref> (i), none of the attention maps highlights the most salient object, the black dog, and each attention map highlights different regions in the image. Even though all three attention maps do not "attend to" the dog, their top-ranked text retrieval results are still highly relevant to the query image; all three retrieved sentences have the word dog in them. This is possible because our PIE-Net computes embedding vectors by combining global context with locally-guided features. In this example, the global context provides information about the black dog, while each of the three locally-guided features contains region-specific information, specifically, (first map): the book shelf, (second map): the floor, (third map): the brown cushion.</p><p>The most interesting observation is that there are subtle variations in the retrieved sentences depending on where the visual attention is focused on. For example, in <ref type="figure">Figure 10 (a)</ref>, the first result focuses on the photographer as a whole, the second focuses on the tiny camera (the visual attention is more narrowly focused on the photographer), and the third focuses on the pizza on the table (notice the visual attention on the table). In <ref type="figure">Figure 10 (d)</ref>, the first result focuses on the ship, the second focuses on the building, and the third on an (imaginary) bird that could have been flying over the buildings. In <ref type="figure">Figure 10</ref> (g), the first result focuses on the boat and the muddy water (notice visual attention on the muddy water region at the lower left corner), while the second focuses on the table of people (notice visual attention on the table region). In <ref type="figure">Figure 10</ref> (j), the first results focuses on the fire hydrant and the yellow wall that is right behind the hydrant, while the second focuses on the hydrant as well as the building with two windows (notice now the visual attention is more widely spread out than the first result). We encourage the readers to look closely at <ref type="figure">Figure 10</ref> to appreciate the subtle variations in the retrieved sentences depending on their corresponding visual attention. <ref type="figure">Figure 11</ref> shows examples of visual-textual attention maps on the TGIF dataset; the task is video-to-text retrieval. In each set of results, we show: (top) a query video and its ground-truth sentence, (bottom three rows): three visual (temporal) attention maps and their top-ranked text retrieval results, as well as their ranks and cosine similarity scores (green: correct, red: incorrect). We color-code words in the retrieval results according to their textual attention intensity values, normalized between [0, 1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Video-to-Text Retrieval Results on TGIF</head><p>Similar to the results on MS-COCO, here we see that visual and textual attention maps tend to highlight salient video frames and words, respectively. Looking closely, we notice that the retrieved results tend to capture the concepts highlighted by their corresponding visual attention. For example, in <ref type="figure">Figure 11 (a)</ref>, the top ranked result contain "lady dressed in black" and "drinking a glass of wine", and the visual attention highlights both the early part of the video, where a woman is drinking from a bottle of whisky, and the latter part, where her black dress is shown. For the second ranked result, the visual attention no longer highlights the latter part, and the retrieved text focuses solely on drinking action (no mention of her black dress). In <ref type="figure">Figure 11 (b)</ref>, the top ranked result focuses on scoring a goal, while the second rank result also focus on the guy being hit in the face with the ball. Notice the difference of visual attention maps between the first and the second case.</p><p>C.3. Text-to-Video Retrieval Results on MRW <ref type="figure">Figure 12</ref> shows examples of text-to-video retrieval results on the MRW dataset. In each row, we show a query sentence and top five retrieved videos along with their ranks and cosine similarity scores. Unlike the previous two figures, here we do not directly show the ground-truth matches (but rather ask the readers to find them; we provide the answers above). The purpose of this is to emphasize the ambiguous and implicit nature of visual-textual association present in our dataset.</p><p>Most of the top five retrieved videos seem to be a good match to the query sentence. For example, <ref type="figure">Figure 12</ref> (a) shows five videos that all contain a human face, each expressing subtly different emotions. <ref type="figure">Figure 12</ref> (b) shows five videos that all contain an animal (squirrel, cat, etc), and most videos contain food. All five retrieved videos in <ref type="figure">Figure 12</ref> show some form of awkward (dancing) moves.</p><p>We believe that the relatively poor retrieval performance reported in our main paper is partly explained by our qualitative results: visual-textual associations are highly ambiguous and there could be multiple correct matches. This calls for a different metric that measures the perceptual similarity between queries and retrieved results, rather than exact match. There has been some progress on perceptual metrics in the image synthesis literature (e.g., Inception Score <ref type="bibr" target="#b46">[47]</ref>). We are not aware of a suitable perceptual metric for crossmodal retrieval, and this could be a promising direction for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A man sits in a diner photographing his meal</head><p>Photographer taking a picture of a meal in a small restaurant A man is taking a picture with a tiny camera A pizza with lots of greens and meat is sitting on the  <ref type="figure">Figure 10</ref>. Image-to-text retrieval results on MS-COCO. For each query image we show three visual attention maps and their topranked text retrieval results, along with their ranks and cosine similarity scores (green: correct, red: incorrect). Words in each sentence is color-coded with textual attention intensity, using the color map shown at the top.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>The architecture of Polysemous Visual-Semantic Embedding (PVSE) for video-sentence data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Our dataset contains videos depicting reactions to the situations described in the corresponding sentences. Here we show the four most common reaction types: (a) physical, (b) emotional, (c) animal, (d) lexical.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Performance (rsum) with different numbers of embeddings, K = [0 : 8]. The results at K = 0 is when we take out the PIE-Net and use the global feature as the embedding output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Performance (rsum) on MS-COCO and MRW with different ablative settings. The error bars are obtained from multiple runs over K = [1 : 8].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Performance (rsum) on MS-COCO with different loss weights for L div and L mmd . The error bars are obtained from multiple runs of K = [2 : 4] and λ (·) = [0.0, 0.01, 0.1, 1.0].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Descriptive statistics of our dataset compared to existing video-sentence datasets.</figDesc><table><row><cell></cell><cell cols="2">#clips #sentences</cell><cell>vocab</cell><cell>text source</cell></row><row><cell cols="2">LSMDC16 [45] 128,085</cell><cell cols="2">128,085 22,898</cell><cell>DVS</cell></row><row><cell>MSR-VTT [55]</cell><cell>10,000</cell><cell cols="2">200,000 29,316</cell><cell>AMT</cell></row><row><cell>TGIF [31]</cell><cell>100,000</cell><cell cols="2">125,781 11,806</cell><cell>AMT</cell></row><row><cell>DiDeMo [21]</cell><cell>26,982</cell><cell>40,543</cell><cell>7,785</cell><cell>AMT</cell></row><row><cell>MRW</cell><cell>50,107</cell><cell cols="3">50,107 34,835 In-the-wild</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>with 80K training, 10,708 validation and 34,101 test samples; since most test videos come with 3 captions, we report results on 11,360 unique test videos. For MRW, we use a data split of 44,107 training, 1K validation and 5K test samples; all the videos come with one caption.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>table Rank A woman in her underwear riding on top of a paddle boat A woman in her underwear riding on top of a paddle boat A woman is riding a raft as an audience watches on the dock A woman on a paddle board with people in the background</head><label>Rank</label><figDesc></figDesc><table><row><cell>Low</cell><cell></cell><cell></cell><cell></cell><cell>High</cell></row><row><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>: 1, score: 0.46</cell><cell>Rank: 3, score: 0.43</cell><cell>Rank: 4, score: 0.41</cell></row><row><cell>(b)</cell><cell>A wooden and metal bench near a over</cell><cell>A park at night is shown, with an empty bench centered</cell><cell>A park and walkway lined with benches and bushes</cell><cell>Benches are next to a bush in a lighted park</cell></row><row><cell></cell><cell>grown bush</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Rank: 1, score: 0.55</cell><cell>Rank: 2, score: 0.57</cell><cell>Rank: 4, score: 0.57</cell></row><row><cell></cell><cell></cell><cell>An outdoor fruit stand</cell><cell>A fruit stand on the</cell><cell>Many fruits in baskets</cell></row><row><cell>(c)</cell><cell>A farmers market fulled of fresh fruits</cell><cell>with various types of fruits for sale</cell><cell>side of the street with vehicles going by</cell><cell>with buildings in the background</cell></row><row><cell></cell><cell>and vegetables</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Rank: 1, score: 0.46</cell><cell>Rank: 2, score: 0.54</cell><cell>Rank: 10, score: 0.49</cell></row><row><cell></cell><cell></cell><cell>A ship in the water</cell><cell>Kiaks in water with</cell><cell>A bunch of buildings</cell></row><row><cell>(d)</cell><cell>A ship in the water sailing past the city in</cell><cell>sailing past the city in the background</cell><cell>buildings in the background</cell><cell>in a city and a bird flying over buildings</cell></row><row><cell></cell><cell>the background</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Rank: 1, score: 0.64</cell><cell>Rank: 6, score: 0.50</cell><cell>Rank: 8, score: 0.45</cell></row><row><cell>(e)</cell><cell>A wooden desk outdoors with pink</cell><cell>A wooden desk outdoors with pink flowers in front of it</cell><cell>Three chairs next to a wooden table and flowers</cell><cell>A walled garden has a bench and a fountain</cell></row><row><cell></cell><cell>flowers in front of it</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Rank: 1, score: 0.50</cell><cell>Rank: 4, score: 0.48</cell><cell>Rank: 5, score: 0.46</cell></row><row><cell>(f)</cell><cell>Long line of cards on a busy street at night</cell><cell>The cars has stopped at the red stop sign</cell><cell>A red stop sign sitting on the side of a road at night</cell><cell>A number of cars on a street with traffic lights</cell></row><row><cell></cell><cell></cell><cell>Rank: 1, score: 0.53</cell><cell>Rank: 3, score: 0.50</cell><cell>Rank: 4, score: 0.49</cell></row><row><cell></cell><cell></cell><cell>A big blue boat</cell><cell>A blue boat docked</cell><cell>A boat parked in a</cell></row><row><cell>(g)</cell><cell>A boat is in a muddy body of water</cell><cell>docked in muddy water</cell><cell>next to a table full of people</cell><cell>harbor next to smaller buildings</cell></row><row><cell></cell><cell></cell><cell>Rank: 1, score: 0.59</cell><cell>Rank: 3, score: 0.51</cell><cell>Rank: 4, score: 0.50</cell></row><row><cell>(h)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Rank: 1, score: 0.55</cell><cell>Rank: 2, score: 0.52</cell><cell>Rank: 3, score: 0.51</cell></row><row><cell>(i)</cell><cell>A dog is laying in a chair in front of a</cell><cell>A large black dog laying next to a book shelf filled with books</cell><cell>A very cute looking black dog laying on the floor</cell><cell>A black dog is resting on a brown cushion</cell></row><row><cell></cell><cell>book shelf</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Rank: 1, score: 0.63</cell><cell>Rank: 4, score: 0.51</cell><cell>Rank: 5, score: 0.50</cell></row><row><cell>(j)</cell><cell>A fire hydrant sitting on the side of a road</cell><cell>A red fire hydrant is next to a yellow wall</cell><cell>A fire hydrant in front of a building with two windows</cell><cell>A yellow fire hydrant by a wall and a sign</cell></row><row><cell></cell><cell>near a building</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Rank: 1, score: 0.54</cell><cell>Rank: 3, score: 0.50</cell><cell>Rank: 4, score: 0.48</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We have experimented with a more sophisticated version of the multihead self-attention<ref type="bibr" target="#b49">[50]</ref>, but it did not improve performance further.<ref type="bibr" target="#b1">2</ref> B is 49 (= 7 × 7) for images, T for videos, and L for sentences; D is 2048 for images and videos, and 300 for sentences</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/TwentyBN/GulpIO 4 https://www.reddit.com/r/reactiongifs</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">We omit results from cross-attention models<ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b29">30]</ref> that require a pair of instances (e.g., image and text) when encoding each instance.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://developers.giphy.com</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A man attempts to jump across a stream and falls in Rank: 5 Score: 0.58</p><p>This guy runs into the wall and falls to the ground <ref type="figure">Figure 11</ref>. Video-to-text retrieval results on TGIF. For each query video we show three visual attention maps and their top-ranked text retrieval results, along with their ranks and cosine similarity scores (green: correct, red: incorrect). Words in each sentence is color-coded with textual attention intensity.  <ref type="figure">Figure 12</ref>. Text-to-video retrieval results on MRW. For each query sentence we show top five retrieved videos, along with their visual (temporal) attention maps, rank, and cosine similarity scores. For better viewing, we provide an HTML file with animated GIFs instead of static images. Quiz: We encourage the readers to find the best matching video in each set of results (see the text for answers).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multiple instance classification: Review, taxonomy and comparative study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaume</forename><surname>Amores</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep canonical correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galen</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raman</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">ton. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast, Cheap, and Good: Why Animated GIFs Engage Us</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeideh</forename><surname>Bakhshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyndon</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paloma</forename><surname>De Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jofish</forename><surname>Kaye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">GIFGIF+: Collecting Emotional Animated GIFs with Clustered Multi-Task Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ognjen</forename><surname>Rudovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosalind</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACII</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Solving the multiple instance problem with axis-parallel rectangles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomás</forename><surname>Lathrop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lozano-Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Linking image and text with 2-way nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviv</forename><surname>Eisenschtat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An argument for basic emotions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Ekman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition &amp; emotion</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="169" to="200" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Vse++: improved visual-semantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fartash</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cross-modal retrieval with correspondence autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangxiang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruifan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Devise: A deep visualsemantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A multi-view embedding space for modeling internet images, tags, and their semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifa</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A kernel method for the two-sample-problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malte</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Look, imagine and match: Improving textual-visual cross-modal retrieval with generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiuxiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Analyzing and predicting GIF interestingness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Soleymani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Video2GIF: Automatic generation of animated GIFs from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangliang</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Localizing moments in video with natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5803" to="5812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Instance-aware image and sentence matching with selective multimodal lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning semantic concepts and order for image and sentence matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">TGIF-QA: Toward spatio-temporal reasoning in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunseok</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Predicting viewer perceived emotions in animated GIFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Jou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhabrata</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dlib-ml: A machine learning toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davis</forename><forename type="middle">E</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1755" to="1758" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.2539</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Associating neural word embeddings with deep image representations using fisher vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Lev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gil</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Gang Hua, Houdong Hu, and Xiaodong He. Stacked cross attention for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuang-Huei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">TGIF: A new dataset and benchmark on animated GIF description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuncheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangliang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Jaimes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Leveraging visual question answering for image-caption ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cicero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Learning a recurrent residual fusion network for multimodal matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanming</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Erwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael S</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lew</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multimodal convolutional neural networks for matching image and sentence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Affdex sdk: A cross-platform real-time multi-face expression recognition toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Mcduff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mahmoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Mavadati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">May</forename><surname>Amr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Turcot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rana</forename><surname>El Kaliouby</surname></persName>
		</author>
		<editor>CHI EA</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Gaussian visual-linguistic embedding for zero-shot recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmoy</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Modeling uncertainty with hedged instance embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyan</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gallagher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00319</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A new approach to cross-modal multimedia retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Rasiwasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">Costa</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Coviello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Doyle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Gert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Lanckriet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">On the convergence of adam and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sashank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satyen</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Joint image-text representation by gaussian visual-semantic embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Multiple instance visual-semantic embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atousa</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niket</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<title level="m">Aaron Courville, and Bernt Schiele. Movie description. IJCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Cross-modal retrieval with implicit concept association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Soleymani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.04318</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning robust visual-semantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Kang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Order-embeddings of images and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vendrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Adversarial cross-modal retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bokun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Hanjalic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng Tao</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiye</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiyue</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06215</idno>
		<title level="m">A comprehensive survey on cross-modal retrieval</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning deep structure-preserving image-text embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Msr-vtt: A large video description dataset for bridging video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deep correlation for matching images and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Advise: Symbolism and external knowledge for decoding advertisements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keren</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Kovashka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deep cross-modal projection learning for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Image2GIF: Generating cinemagraphs using recurrent deep q-networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yipin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In WACV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
