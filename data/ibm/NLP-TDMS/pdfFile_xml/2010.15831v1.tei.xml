<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RelationNet++: Bridging Visual Representations for Object Detection via Transformer Decoder</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Chi</surname></persName>
							<email>chicheng15@mails.ucas.ac.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyun</forename><surname>Wei</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
							<email>hanhu@microsoft.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<address>
									<region>CAS</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">RelationNet++: Bridging Visual Representations for Object Detection via Transformer Decoder</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing object detection frameworks are usually built on a single format of object/part representation, i.e., anchor/proposal rectangle boxes in RetinaNet and Faster R-CNN, center points in FCOS and RepPoints, and corner points in Corner-Net. While these different representations usually drive the frameworks to perform well in different aspects, e.g., better classification or finer localization, it is in general difficult to combine these representations in a single framework to make good use of each strength, due to the heterogeneous or non-grid feature extraction by different representations. This paper presents an attention-based decoder module similar as that in Transformer [31] to bridge other representations into a typical object detector built on a single representation format, in an end-to-end fashion. The other representations act as a set of key instances to strengthen the main query representation features in the vanilla detectors. Novel techniques are proposed towards efficient computation of the decoder module, including a key sampling approach and a shared location embedding approach. The proposed module is named bridging visual representations (BVR). It can perform in-place and we demonstrate its broad effectiveness in bridging other representations into prevalent object detection frameworks, including RetinaNet, Faster R-CNN, FCOS and ATSS, where about 1.5 ∼ 3.0 AP improvements are achieved. In particular, we improve a state-of-the-art framework with a strong backbone by about 2.0 AP, reaching 52.7 AP on COCO test-dev. The resulting network is named RelationNet++. The code will be available at https://github.com/microsoft/RelationNet2. * The work is done when Cheng Chi is an intern at Microsoft</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Object detection is a vital problem in computer vision that many visual applications build on. While there have been numerous approaches towards solving this problem, they usually leverage a single visual representation format. For example, most object detection frameworks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b17">18]</ref> utilize the rectangle box to represent object hypotheses in all intermediate stages. Recently, there have also been some frameworks adopting points to represent an object hypothesis, e.g., center point in Center-Net <ref type="bibr" target="#b37">[38]</ref> and FCOS <ref type="bibr" target="#b28">[29]</ref>, point set in RepPoints <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b2">3]</ref> and PSN <ref type="bibr" target="#b33">[34]</ref>. In contrast to representing whole objects, some keypoint-based methods, e.g., CornerNet <ref type="bibr" target="#b14">[15]</ref>, leverage part representations of corner points to compose an object. In general, different representation methods usually steer the detectors to perform well in different aspects. For example, the bounding box representation is better aligned with annotation formats for object detection. The center representation avoids the need for an anchoring design and is usually friendly to small objects. The corner representation is usually more accurate for finer localization.</p><p>It is natural to raise a question: could we combine these representations into a single framework to make good use of each strength? Noticing that different representations and their feature extractions are usually heterogeneous, combination is difficult. To address this issue, we present an attention based decoder module similar as that in Transformer <ref type="bibr" target="#b30">[31]</ref>, which can effectively model dependency between heterogeneous features. The main representations in an object detector are set as the query input, and other visual representations act as the auxiliary keys to enhance the query features by certain interactions, where both appearance and geometry relationships are considered.</p><p>In general, all feature map points can act as corner/center key instances, which are usually too many for practical attention computation. In addition, the pairwise geometry term is computation and memory consuming. To address these issues, two novel techniques are proposed, including a key sampling approach and a shared location embedding approach for efficient computation of the geometry term. The proposed module is named bridging visual representations (BVR). <ref type="figure" target="#fig_0">Figure 1a</ref> illustrates the application of this module to bridge center and corner representations into an anchor-based object detector. The center and corner representations act as key instances to enhance the anchor box features, and the enhanced features are then used for category classification and bounding box regression to produce the detection results. The module can work in-place. Compared with the original object detector, the main change is that the input features for classification and regression are replaced by the enhanced features, and thus the strengthened detector largely maintains its convenience in use.</p><p>The proposed BVR module is general. It is applied to various prevalent object detection frameworks, including RetinaNet, Faster R-CNN, FCOS and ATSS. Extensive experiments on the COCO dataset <ref type="bibr" target="#b18">[19]</ref> show that the BVR module substantially improves these various detectors by 1.5 ∼ 3.0 AP. In particular, we improve a strong ATSS detector by about 2.0 AP with small overhead, reaching 52.7 AP on COCO test-dev. The resulting network is named RelationNet++, which strengthens the relation modeling in <ref type="bibr" target="#b11">[12]</ref> from bbox-to-bbox to across heterogeneous object/part representations.</p><p>The main contributions of this work are summarized as:</p><p>• A general module, named BVR, to bridge various heterogeneous visual representations and combine the strengths of each. The proposed module can be applied in-place and does not break the overall inference process by the main representations. • Novel techniques to make the proposed bridging module efficient, including a key sampling approach and a shared location embedding approach. • Broad effectiveness of the proposed module for four prevalent object detectors: RetinaNet, Faster R-CNN, FCOS and ATSS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">A Representation View for Object Detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Object / Part Representations</head><p>Object detection aims to find all objects in a scene with their location described by rectangle bounding boxes. To discriminate object bounding boxes from background and to categorize objects, intermediate geometric object/part candidates with associated features are required. We refer to the joint geometric description and feature extraction as the representation, where typical representations used in object detection are illustrated in <ref type="figure" target="#fig_0">Figure 1b</ref> and summarized below.  bounding box can be described by a 4-d vector, either as center-size (x c , y c , w, h) or as opposing corners (x tl , y tl , x br , y br ). Besides the final output, this representation is also commonly used as initial and intermediate object representations, such as anchors <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b17">18]</ref> and proposals <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b10">11]</ref>. For bounding box representations, features are usually extracted by pooling operators within the bounding box area on an image feature map. Common pooling operators include RoIPool <ref type="bibr" target="#b7">[8]</ref>, RoIAlign <ref type="bibr" target="#b10">[11]</ref>, and Deformable RoIPool <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b39">40]</ref>. There are also simplified feature extraction methods, e.g., the box center features are usually employed in the anchor box representation <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b17">18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Object bounding box representation</head><p>Object center representation The 4-d vector space of a bounding box representation is at a scale of O(H 2 × W 2 ) for an image with resolution H × W , which is too large to fully process. To reduce the representation space, some recent frameworks <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b31">32]</ref> use the center point as a simplified representation. Geometrically, a center point is described by a 2-d vector (x c , y c ), in which the hypothesis space is of the scale O(H × W ), which is much more tractable. For a center point representation, the image feature on the center point is usually employed as the object feature.</p><p>Corner representation A bounding box can be determined by two points, e.g., a top-left corner and a bottom-right corner. Some approaches <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b25">26]</ref> first detect these individual points and then compose bounding boxes from them. We refer to these representation methods as corner representation. The image feature at the corner location can be employed as the part feature.</p><p>Summary and comparison Different representation approaches usually have strengths in different aspects. For example, object based representations (bounding box and center) are better in category classification while worse in object localization than part based representations (corners). Object based representations are also more friendly for end-to-end learning because they do not require a post-processing step to compose objects from corners as in part-based representation methods.</p><p>Comparing different object-based representations, while the bounding box representation enables more sophisticated feature extraction and multiple-stage processing, the center representation is attractive due to the simplified system design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Object Detection Frameworks in a Representation View</head><p>Object detection methods can be seen as  <ref type="figure" target="#fig_1">Figure 2</ref> shows the representation flows of several typical object detection frameworks, as detailed below. RetinaNet <ref type="bibr" target="#b17">[18]</ref> is a one-stage object detector, which also employs bounding boxes as its intermediate representation. Due to its one-stage nature, it usually requires denser anchor hypotheses, i.e., 9 anchor boxes at each feature map position. The final bounding box outputs are also obtained by applying a localization refinement head network.</p><p>FCOS <ref type="bibr" target="#b28">[29]</ref> is also a one-stage object detector but uses object center points as its intermediate object representation. It directly regresses the four sides from the center points to form the final bounding box outputs. There are concurrent works, such as <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b34">35]</ref>. Although center points can be seen as a degenerated geometric representation from bounding boxes, these center point based methods show competitive or even better performance on benchmarks.</p><p>CornerNet </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Bridging Visual Representations</head><p>For the typical frameworks in Section 2.2, mainly one kind of representation approach is employed. While they have strengths in some aspects, they may also fall short in other ways. However, it is in general difficult to combine them in a single framework, due to the heterogeneous or non-grid feature extraction by different representations. In this section, we will first present a general method to bridge different representations. Then we demonstrate its applications to various frameworks, including RetinaNet <ref type="bibr" target="#b17">[18]</ref>, Faster R-CNN <ref type="bibr" target="#b23">[24]</ref>, FCOS <ref type="bibr" target="#b28">[29]</ref> and ATSS <ref type="bibr" target="#b36">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">A General Attention Module to Bridge Visual Representations</head><p>Without loss of generality, for an object detector, the representation it leverages is referred to as the master representation, and the general module aims to bridge other representations to enhance this master representation. Such other representations are referred to as auxiliary ones.</p><p>Inspired by the success of the decoder module for neural machine translation where an attention block is employed to bridge information between different languages, e.g., Transformer <ref type="bibr" target="#b30">[31]</ref>, we adapt this mechanism to bridge different visual representations. Specifically, the master representation acts as the query input, and the auxiliary representations act as the key input. The attention module outputs strengthened features for the master representation (queries), which have bridged the information from auxiliary representations (keys). We use a general attention formulation as:</p><formula xml:id="formula_0">f q i = f q i + j S f q i , f k j , g q i , g k j · T v (f k j ),<label>(1)</label></formula><p>where f q i , f q i , g q i are the input feature, output feature, and geometric vector for a query instance i; f k j , g k j are the input feature and geometric vector for a key instance j; T v (·) is a linear value transformation function; S(·) is a similarity function between i and j, instantiated as <ref type="bibr" target="#b11">[12]</ref>:</p><formula xml:id="formula_1">S f q i , f k j , g q i , g k j = softmax j S A (f q i , f k j ) + S G (g q i , g k j ) ,<label>(2)</label></formula><p>where S A (f q i , f k j ) denotes the appearance similarity computed by a scaled dot product between query and key features <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b11">12]</ref>, and S G (g q i , g k j ) denotes a geometric term computed by applying a small network on the relative locations between i and j, i.e., cosine/sine location embedding <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b11">12]</ref> plus a 2-layer MLP. In the case of different dimensionality between the query geometric vector and key geometric vector (4-d bounding box vs. 2-d point), we first extract a 2-d point from the bounding box, i.e., center or corner, such that the two representations are homogeneous in geometry description for subtraction operations. The same as in <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b11">12]</ref>, multi-head attention is employed, which performs substantially better than using single-head attention. We use an attention head number of 8 by default.</p><p>The above module is named bridging visual representations (BVR), which takes query and key representations of any dimension as input and generates strengthened features for the query considering both their appearance and geometric relationships. The module can be easily plugged into prevalent detectors as described in Section 3.2 and 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">BVR for RetinaNet</head><p>We take RetinaNet as an example to showcase how we apply the BVR module to an existing object detector. As mentioned in Section 2.2, RetinaNet adopts anchor bounding boxes as its master representation, where 9 bounding boxes are anchored at each feature map location. Totally, there are 9 × H × W bounding box instances for a feature map of H × W resolution. BVR takes the  C ×9×H ×W feature map (C is the feature map channel) as query input, and generates strengthened query features of the same dimension.</p><p>We use two kinds of key (auxiliary) representations to strengthen the query (master) features. One is the object center, and the other is the corners. As shown in <ref type="figure" target="#fig_6">Figure 3a</ref>, the center/corner points are predicted by applying a small point head network on the output feature map of the backbone. Then a small set of key points are selected from all predictions, and are fed into the attention modules to strengthen the classification and regression feature, respectively. In the following, we provide details of these modules and the crucial designs.</p><p>Auxiliary (key) representation learning The point head network consists of two shared 3 × 3 conv layers, followed by two independent sub-networks (a 3 × 3 conv layer and a sigmoid layer) to predict the scores and sub-pixel offsets for center and corner prediction, respectively <ref type="bibr" target="#b14">[15]</ref>. The score indicates the probability of a center/corner point locating at the feature map bin. The sub-pixel offset ∆x, ∆y denotes the displacement between its precise location and the top-left (integer coordinate) of each feature bin, which accounts for the resolution loss by down-sampling of feature maps.</p><p>In learning, for the object detection frameworks with an FPN structure, we assign all ground-truth center/corner points to all feature levels. We find it performs better than the common practice where objects are assigned to a particular level <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b34">35]</ref>, probably because it speeds up the learning of center/corner representations due to more positive samples employed in each level. The focal loss <ref type="bibr" target="#b17">[18]</ref> and smooth L1 loss are employed for the center/corner score and sub-pixel offset learning, with loss weights of 0.05 and 0.2, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Key selection</head><p>We use corner points to demonstrate the processing of auxiliary representation selection since the principle is same for center point representation. We treat each feature map position as an object corner candidate. If all candidates are employed in the key set, the computation cost of BVR module is unaffordable. In addition, too many background candidates may suppress real corners. To address these issues, we propose a top-k (k = 50 by default) key selection strategy. Concretely, a 3 × 3 MaxPool operator with stride 1 is performed on the corner score map, and the top-k corner candidates are selected according to their corner-ness scores. For an FPN backbone, we select the top-k keys from all pyramidal levels, and the key set is shared by all levels. This shared key set outperforms that of independent key set for different levels, as shown in <ref type="table" target="#tab_1">Table 1</ref>.  <ref type="table" target="#tab_3">Table 3</ref>, the default setting (d 0 = 512, d 1 = 512, G = 8, K = 50) is time-consuming and space-consuming.</p><p>Noting that the range of relative locations are limited, i.e., [−H + 1, H − 1] × [−W + 1, W − 1], we apply the cosine/sine embedding and the 2-layer MLP network on a fixed 2-d relative location map to produce a G-channel geometric map, and then compute the geometric terms for a key/query pair by bilinear sampling on this geometric map. To further reduce the computation, we use a 2-d relative location map with the unit length U larger than 1, e.g., U = 4, where each location bin indicates a length of U in the original image. In our implementation, we adopt U = 1 2 S (S indicates the stride of the pyramid level) and a location map of 400 × 400 resolution, which accounts for a [−100S, 100S) × [−100S, 100S) range on the original image for a pyramid level of stride S. <ref type="figure" target="#fig_6">Figure 3b</ref> gives an illustration. The computation and memory complexities are reduced to  O(time) = (d 0 +d 0 d 1 +d 1 G)·400 2 +GKHW and O(memory) = (2+d 0 +d 1 +G)·400 2 +GKHW , respectively, which are significantly smaller than direct computation, as shown in <ref type="table" target="#tab_3">Table 3</ref>.</p><p>Separate BVR modules for classification and regression Object center representations can provide rich context for object categorization, while the corner representations can facilitate localization. Therefore, we apply separate BVR modules to enhance classification and regression features respectively, as shown in Figure3a. Such separate design is beneficial, as demonstrated in <ref type="table" target="#tab_5">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">BVR for Other Frameworks</head><p>The BVR module is general, and can be applied to other object detection frameworks.</p><p>ATSS <ref type="bibr" target="#b36">[37]</ref> applies several techniques from anchor-free detectors to improve the anchor-based detectors, e.g. RetinaNet. The BVR used for RetinaNet can be directly applied.</p><p>FCOS <ref type="bibr" target="#b28">[29]</ref> is an anchor-free detector which utilizes center point as object representation. Since there is no corner information in this representation, we always use the center point location and the corresponding feature to represent the query instance in our BVR module. Other settings are maintained the same as those for RetinaNet.</p><p>Faster R-CNN <ref type="bibr" target="#b23">[24]</ref> is a two-stage detector which employs bounding boxes as the inter-mediate object representations in all stages. We adopt BVR to enhance the features of bounding box proposals, the diagram is shown in <ref type="figure" target="#fig_9">Figure 4a</ref>. In each of the proposals, RoIAlign feature is used to predict center and corner representations. <ref type="figure" target="#fig_9">Figure 4b</ref> shows the network structure of point (center/corner) head, which is similar with mask head in mask R-CNN <ref type="bibr" target="#b10">[11]</ref>. The selection of keys is same with the process in RetinaNet, which is stated in Section 3.2. We use the features interpolated from the point head as the key features, center and corner features are also employed to enhance classification and regression, respectively. Since the number of the querys is much smaller than that in RetinaNet, we directly compute the geometry term other than using the shared geometric map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Relation to Other Attention Mechanisms in Object Detection</head><p>Non-Local Networks (NL) <ref type="bibr" target="#b32">[33]</ref> and RelationNet <ref type="bibr" target="#b11">[12]</ref> are two pioneer works utilizing attention modules to enhance detection performance. However, they are both designed to enhance instances of a single representation format: non-local networks <ref type="bibr" target="#b32">[33]</ref> use self-attention to enhance a pixel feature by fusing in other pixels' features; RelationNet <ref type="bibr" target="#b11">[12]</ref> enhance a bounding box feature by fusing in other bounding box features.</p><p>In contrast, BVR aims to bridge representations in different forms to combine the strengths of each. In addition to this conceptual difference, there are also new techniques in the modeling aspect. For example, techniques are proposed to enable homogeneous difference/similarity computation between heterogeneous representations, i.e., 4-d bounding box vs 2-d corner/center points. Also, there are new techniques proposed to effectively model relationship between different kinds of representations as well as to speed-up computation, such as key representation selection, and the shared relative location embedding approach. The proposed BVR is actually complementary to these pioneer works, as shown in <ref type="table" target="#tab_7">Table 7</ref> and 8.</p><p>Learning Region Features (LRF) <ref type="bibr" target="#b9">[10]</ref> and DeTr <ref type="bibr" target="#b0">[1]</ref> use an attention module to compute the features of object proposals <ref type="bibr" target="#b9">[10]</ref> or querys <ref type="bibr" target="#b0">[1]</ref> from image features. BVR shares similar formulation as them, but has a different aim to bridge different forms of object representations.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We first ablate each component of the proposed BVR module using a RetinaNet base detector in Section 4.1. Then we show benefits of BVR applied to four representative detectors, including two-stage (i.e., faster R-CNN), one-stage (i.e., RetinaNet and ATSS) and anchor-free (i.e., FCOS) detectors. Finally, we compare our approach with the state-of-the-art methods.</p><p>Our experiments are all implemented on the MMDetection v1.1.0 codebase <ref type="bibr" target="#b1">[2]</ref>. All experiments are performed on MS COCO dataset <ref type="bibr" target="#b18">[19]</ref>. A union of 80k train images and a 35k subset of val images are used for training. Most ablation experiments are studied on a subset of 5k unused val images (denoted as minival). Unless otherwise stated, all the training and inference details keep the same as the default settings in MMDetection, i.e., initializing the backbone using the ImageNet <ref type="bibr" target="#b24">[25]</ref> pretrained model, resizing the input images to keep their shorter side being 800 and their longer side less than or equal to 1333, optimizing the whole network via the SGD algorithm with 0.9 momentum, 0.0001 weight decay, setting the initial learning rate as 0.02 with the 0.1 decrease at epoch 8 and 11. In the large model experiments in <ref type="table" target="#tab_1">Table 10</ref> and 12, we train 20 epochs and decrease the learning rate at epoch 16 and 19. Multi-scale training is also adopted in large model experiments, for each mini-batch, the shorter side is randomly selected from a range of [400, 1200].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Method Analysis using RetinaNet</head><p>Our ablation study is built on a RetinaNet detector using ResNet-50, which achieves 35.6 AP on COCO minival (1× settings). Components in the BVR module are ablated using this base detector.</p><p>Key selection As shown in <ref type="table" target="#tab_1">Table 1</ref>, compared with independent keys across feature levels, sharing keys can bring +1.6 and +1.5 AP gains for 20 and 50 keys, respectively. Using 50 keys achieves the best accuracy, probably because that too few keys cannot sufficiently cover the representative keypoints, while too large number of keys include many low-quality candidates.</p><p>On the whole, the BVR enhanced RetinaNet significantly outperforms the original RetinaNet by 2.9 AP, demonstrating the great benefit of bridging other representations. <ref type="table" target="#tab_2">Table 2</ref> shows the benefits of using sub-pixel representations for centers and corners. While sub-pixel representation benefits both classification and regression, it is more critical for the localization task. <ref type="table" target="#tab_3">Table 3</ref>, compared with direct computation of position embedding <ref type="bibr" target="#b11">[12]</ref>, the proposed shared location embedding approach saves 42× memory cost (+134M vs +5690M) and saves 102× FLOPs (+2G vs +204G) in the geometry term computation, while achieves slightly better performance (38.5 AP vs 38.3 AP).    Ablation study of the unit length and the size of the shared location map in <ref type="table" target="#tab_4">Table 4</ref> indicates stable performance. We adopt a unit length of <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr">64]</ref> and map size of 400 × 400 by default. <ref type="table" target="#tab_5">Table 5</ref> ablates the effect of using separate BVR modules for classification and regression, indicating the center representation is a more suitable auxiliary for classification and the corner representation is a more suitable auxiliary for regression. <ref type="table" target="#tab_6">Table 6</ref> ablates the effect of appearance and geometry terms. Using the two terms together outperforms that using the appearance term alone by 1.1 AP and outperforms that using the geometry term alone by 0.9 AP. In general, the geometry term benefits more at larger IoU criteria, and less at lower IoU criteria.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sub-pixel corner/center</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shared relative location embedding As shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Separate BVR modules for classification and regression</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of appearance and geometry terms</head><p>Compare with multi-task learning Only including an auxiliary point head without using it can boost the RetinaNet baseline by 0.8 AP (from 35.6 to 36.4). Noting the BVR brings a 2.9 AP improvement (from 35.6 to 38.5) under the same settings, the major improvements are not due to multi-task learning. <ref type="table" target="#tab_9">Table 9</ref> shows the flops analysis. The input images are resized to 800 × 1333. The proposed BVR module introduces about 3% more parameters (39M vs 38M) and about 10% more computations (266G vs 239G) than the original RetinaNet. We also conduct RetinaNet with heavier head network to have similar parameters and computations as our approach. By adding one more layer, the accuracy slightly drops to 35.2, probably due to the increasing difficulty in optimization. We introduce a GN layer after every head conv layer to alleviate it, and one additional conv layer improves the accuracy by 0.3 AP. These results indicate that the improvements by BVR are mostly not due to more parameters and computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Complexity analysis</head><p>The real inference speed of different models using a V100 GPU (fp32 mode is used) are shown in <ref type="table" target="#tab_1">Table 11</ref>. By using a ResNet-50 backbone, the BVR module usually takes less than 10% overhead. By using a larger ResNeXt-101-DCN backbone, the BVR module usually takes less than 3% overhead.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">BVR is Complementary to Other Attention Mechanisms</head><p>The BVR module acts differently compared to the pioneer works of the non-local module <ref type="bibr" target="#b32">[33]</ref> and the relation module <ref type="bibr" target="#b11">[12]</ref> which also model dependencies between representations. While the BVR module models relationships between different kinds of representations, the latter modules model relationships within the same kinds of representations (pixels <ref type="bibr" target="#b32">[33]</ref> and proposal boxes <ref type="bibr" target="#b11">[12]</ref>). To compare with the object relation module (ORM) <ref type="bibr" target="#b11">[12]</ref>, we first apply BVR to enhance RoIAlign features with corner/center representations, the process of which is same as <ref type="figure" target="#fig_9">Figure 4a</ref>. Then the enhanced features are utilized to perform object relation between proposals. Different from <ref type="bibr" target="#b11">[12]</ref>, keys are sampled to make the module more efficient. <ref type="table" target="#tab_8">Table 8</ref> shows that the BVR module and the relation module are mostly complementary. On the basis of faster R-CNN baseline, ORM can obtain +1.0 AP improvement, while our BVR improves AP by 1.9. Applying our BVR on the basis of the ORM continually improves AP by 2.0. <ref type="table" target="#tab_7">Table 7</ref> and 8 show that the BVR modules is mostly complementary with non-local and object relation module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Generally Applicable to Representative Detectors</head><p>We apply the proposed BVR to four representative frameworks, i.e., RetinaNet <ref type="bibr" target="#b17">[18]</ref>, Faster R-CNN <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b16">17]</ref>, FCOS <ref type="bibr" target="#b28">[29]</ref> and ATSS <ref type="bibr" target="#b36">[37]</ref>, as shown in <ref type="table" target="#tab_1">Table 10</ref>. The ResNeXt-64x4d-101-DCN backbone, multi-scale and longer training (20 epochs) are adopted to test whether our approach effects on strong baselines. The BVR module improve these strong detectors by 1.5 ∼ 2.0 AP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with State-of-the-Arts</head><p>We build our detector by applying the BVR module on a strong detector of ATSS, which achieves 50.7 AP on COCO test-dev using multi-scale testing based on the ResNeXt-64x4d-101-DCN backbone. Our approach improves it by 2.0 AP, reaching 52.7 AP. <ref type="table" target="#tab_1">Table 12</ref> shows the comparison with state-of-the-arts methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we present a new module, BVR, which bridge various other visual representations by an attention mechanism like that in Transformer <ref type="bibr" target="#b30">[31]</ref> to enhance the main representations in a detector. The BVR module can be applied plug-in for an existing detector, and proves broad effectiveness for prevalent object detection frameworks, i.e. RetinaNet, faster R-CNN, FCOS and ATSS, where about 1.5 ∼ 3.0 AP improvements are achieved. We reach 52.7 AP on COCO test-dev by improving a strong ATSS detector. The resulting network is named RelationNet++, which advances the relation modeling in <ref type="bibr" target="#b11">[12]</ref> from bbox-to-bbox to across heterogeneous object/part representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>This work aims for better object detection algorithms. Any object oriented visual applications may benefit from this work, as object detection is usually an indispensable component for them. There may be unpredictable failures, similar as most other detectors. The consequences of failures by this algorithm are determined on the down-stream applications, and please do not use it for scenarios where failures will lead to serious consequences. The method is data driven, and the performance may be affected by the biases in the data. So please also be careful about the data collection process when using it.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) An illustration of bridging various representations, specifically leveraging corner/center representations to enhance the anchor box features. (b) Object/part representations used in object detection (geometric description and feature extraction). The red dashed box denotes ground-truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Representation flows for several typical detection frameworks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>evolving intermediate object/part representations until the final bounding box outputs. The representation flows largely shape different object detectors. Several major categorization of object detectors are based on such representation flow, such as top-down (object-based representation) vs bottom-up (part-based representation), anchor-based (bounding box based) vs anchor-free (center point based), and single-stage (one-time representation flow) vs multiple-stage (multiple-time representation flow).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Faster R-CNN [ 24 ]</head><label>24</label><figDesc>employs bounding boxes as its intermediate object representations in all stages. At the beginning, multiple anchor boxes at each feature map position are hypothesized to coarsely cover the 4-d bounding box space in an image, i.e., 3 anchor boxes with different aspect ratios. The image feature vector at the center point is extracted to represent each anchor box, which is then used for foreground/background classification and localization refinement. After anchor box selection and localization refinement, the object representation is evolved to a set of proposal boxes, where the object features are usually extracted by an RoIAlign operator within each box area. The final bounding box outputs are obtained by localization refinement, through a small network on the proposal features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>[ 15 ]</head><label>15</label><figDesc>is built on the intermediate part representation of corners, in contrast to the above frameworks where object representations are employed. The predicted corners (top-left and bottomright) are grouped according to their embedding similarity, to compose the final bounding box outputs. The detectors based on corner representation usually reveal better object localization than those based on an object-level representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Attention-based feature enhancement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Applying BVR into an object detector and an illustration of the attention computation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Shared relative location embedding The computation and memory complexities for direct computation of the geometry term are O(time) = (d 0 + d 0 d 1 + d 1 G)KHW and O(memory) = (2 + d 0 + d 1 + G)KHW , respectively, where d 0 , d 1 , G, K are the cosine/sine embedding dimension, inner dimension of the MLP network, head number of the multi-head attention module and the number of selected key instances, respectively. As shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Point head for center (corner) prediction in faster R-CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 4 :</head><label>4</label><figDesc>Design of applying BVR to faster R-CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Ablation on key selection approaches</figDesc><table><row><cell>#keys</cell><cell>share</cell><cell>AP</cell><cell>AP50</cell><cell>AP75</cell></row><row><cell>-</cell><cell>-</cell><cell>35.6</cell><cell>55.5</cell><cell>39.0</cell></row><row><cell>20</cell><cell></cell><cell>36.1</cell><cell>54.9</cell><cell>39.6</cell></row><row><cell>50</cell><cell></cell><cell>37.0</cell><cell>55.8</cell><cell>40.6</cell></row><row><cell>20</cell><cell></cell><cell>37.7</cell><cell>56.5</cell><cell>41.4</cell></row><row><cell>50</cell><cell></cell><cell>38.5</cell><cell>57.0</cell><cell>42.3</cell></row><row><cell>100</cell><cell></cell><cell>38.3</cell><cell>56.9</cell><cell>42.0</cell></row><row><cell>200</cell><cell></cell><cell>38.2</cell><cell>56.7</cell><cell>41.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ablation of sub-pixel corner/centers</figDesc><table><row><cell cols="3">CLS (ct.) REG (cn.) AP AP50 AP75 AP90</cell></row><row><cell>-</cell><cell>-</cell><cell>35.6 55.5 39.0 9.3</cell></row><row><cell>integer</cell><cell cols="2">integer 37.0 55.6 40.8 11.0</cell></row><row><cell cols="3">integer sub-pixel 38.0 56.1 41.7 12.5</cell></row><row><cell cols="3">sub-pixel integer 37.2 56.7 41.2 10.4</cell></row><row><cell cols="3">sub-pixel sub-pixel 38.5 57.0 42.3 12.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Effect of shared relative location embedding</figDesc><table><row><cell>geometry</cell><cell>memory FLOPs AP AP50 AP75</cell></row><row><cell>baseline</cell><cell>2933M 239G 35.6 55.5 39.0</cell></row><row><cell cols="2">appearance only 3345M 264G 37.4 56.7 40.4</cell></row><row><cell>non-shared</cell><cell>9035M 468G 38.3 57.2 41.7 (+5690M) (+204G)</cell></row><row><cell>shared</cell><cell>3479M 266G 38.5 57.0 42.3 (+134M) (+2G)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell cols="3">Comparison of different unit length</cell></row><row><cell cols="3">and size of the shared location map</cell></row><row><cell>unit length</cell><cell>size</cell><cell>AP AP50 AP75</cell></row><row><cell cols="3">[2, 4, 8, 16, 32] 400 × 400 38.2 56.7 41.8</cell></row><row><cell cols="3">[4, 8, 16, 32, 64] 400 × 400 38.5 57.0 42.3</cell></row><row><cell cols="3">[8, 16, 32, 64, 128] 400 × 400 38.4 56.8 42.2</cell></row><row><cell cols="3">[4, 8, 16, 32, 64] 800 × 800 38.3 56.9 42.1</cell></row><row><cell cols="3">[4, 8, 16, 32, 64] 200 × 200 38.1 56.7 41.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Effect of different representations ('ct.': center, 'cn.': corner) for classification and regression</figDesc><table><row><cell cols="4">CLS REG AP AP50 AP75 AP90</cell></row><row><cell cols="3">none none 35.6 55.5 39.0</cell><cell>9.3</cell></row><row><cell>none</cell><cell>ct.</cell><cell cols="2">36.4 54.7 38.9 10.1</cell></row><row><cell cols="4">none cn. 37.5 54.6 40.3 12.2</cell></row><row><cell>ct.</cell><cell cols="3">none 37.3 56.6 39.9 10.5</cell></row><row><cell cols="3">cn. none 36.2 55.1 38.4</cell><cell>9.8</cell></row><row><cell>ct.</cell><cell cols="3">cn. 38.5 57.0 42.3 12.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Ablation of appearance and geometry terms</figDesc><table><row><cell>appearance geometry AP AP50 AP75 AP90</cell></row><row><cell>35.6 55.5 39.0 9.3</cell></row><row><cell>37.4 56.7 41.3 10.7</cell></row><row><cell>37.6 55.8 41.5 12.0</cell></row><row><cell>38.5 57.0 42.3 12.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell cols="4">: Compatibility with the non-local module</cell></row><row><cell>(NL) [33]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>method</cell><cell>AP</cell><cell cols="2">AP50 AP75</cell></row><row><cell>RetinaNet</cell><cell>35.6</cell><cell>55.5</cell><cell>39.0</cell></row><row><cell>RetinaNet + NL</cell><cell>37.0</cell><cell>57.0</cell><cell>39.3</cell></row><row><cell>RetinaNet + BVR</cell><cell>38.5</cell><cell>57.0</cell><cell>42.3</cell></row><row><cell cols="2">RetinaNet + NL + BVR 39.4</cell><cell>58.2</cell><cell>42.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell cols="2">: Compatibility with the object relation</cell></row><row><cell cols="2">module (ORM) [12]. ResNet-50-FPN is used</cell></row><row><cell>method</cell><cell>AP AP50 AP75</cell></row><row><cell>faster R-CNN</cell><cell>37.4 58.1 40.4</cell></row><row><cell>faster R-CNN + ORM</cell><cell>38.4 59.0 41.3</cell></row><row><cell>faster R-CNN + BVR</cell><cell>39.3 59.5 43.1</cell></row><row><cell cols="2">faster R-CNN + ORM + BVR 40.4 60.6 44.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Complexity analysis</figDesc><table><row><cell>method</cell><cell>#conv #ch. FLOP param AP</cell></row><row><cell>RetinaNet</cell><cell>4 256 239G 38M 35.6</cell></row><row><cell>RetinaNet (deep)</cell><cell>5 256 265G 39M 35.2</cell></row><row><cell>RetinaNet (wide)</cell><cell>4 288 267G 39M 35.6</cell></row><row><cell>RetinaNet+BVR</cell><cell>4 256 266G 39M 38.5</cell></row><row><cell>RetinaNet+GN</cell><cell>4 256 239G 38M 36.5</cell></row><row><cell cols="2">RetinaNet (deep)+GN 5 256 265G 39M 36.8</cell></row><row><cell cols="2">RetinaNet (wide)+GN 4 288 267G 39M 36.5</cell></row><row><cell cols="2">RetinaNet+GN+BVR 4 256 266G 39M 39.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc></figDesc><table><row><cell cols="3">BVR for four representative detectors</cell></row><row><cell cols="3">using a ResNeXt-64x4d-101-DCN backbone</cell></row><row><cell>method</cell><cell>AP</cell><cell>AP50 AP75</cell></row><row><cell>RetinaNet</cell><cell>42.9</cell><cell>63.4 46.9</cell></row><row><cell>RetinaNet + BVR</cell><cell cols="2">44.7 (+1.8) 64.9 49.0</cell></row><row><cell>faster R-CNN</cell><cell>45.0</cell><cell>66.2 48.8</cell></row><row><cell cols="3">faster R-CNN + BVR 46.5 (+1.5) 67.4 50.5</cell></row><row><cell>FCOS</cell><cell>46.1</cell><cell>65.0 49.6</cell></row><row><cell>FCOS + BVR</cell><cell cols="2">47.6 (+1.5) 66.2 51.4</cell></row><row><cell>ATSS</cell><cell>48.3</cell><cell>67.1 52.6</cell></row><row><cell>ATSS + BVR</cell><cell cols="2">50.3 (+2.0) 69.0 55.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>Time cost of the BVR module.</figDesc><table><row><cell>method</cell><cell>backbone</cell><cell>FPS</cell><cell>FPS (+BVR)</cell></row><row><cell cols="3">Faster R-CNN ResNet-50/ResNeXt-101-DCN 21.3/7.5</cell><cell>19.5/7.3</cell></row><row><cell>RetinaNet</cell><cell cols="2">ResNet-50/ResNeXt-101-DCN 18.9/7.0</cell><cell>17.4/6.8</cell></row><row><cell>FCOS</cell><cell cols="2">ResNet-50/ResNeXt-101-DCN 22.7/7.4</cell><cell>20.7/7.2</cell></row><row><cell>ATSS</cell><cell cols="2">ResNet-50/ResNeXt-101-DCN 19.6/7.1</cell><cell>17.9/6.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 12 :</head><label>12</label><figDesc>Results on MS COCO test-dev set, ' * ' denotes the multi-scale testing</figDesc><table><row><cell>method</cell><cell>backbone</cell><cell>AP</cell><cell cols="5">AP50 AP75 APS APM APL</cell></row><row><cell>DCN v2* [40]</cell><cell>ResNet-101-DCN</cell><cell>46.0</cell><cell>67.9</cell><cell>50.8</cell><cell>27.8</cell><cell>49.1</cell><cell>59.5</cell></row><row><cell>SNIPER* [27]</cell><cell>ResNet-101</cell><cell>46.5</cell><cell>67.5</cell><cell>52.2</cell><cell>30.0</cell><cell>49.4</cell><cell>58.4</cell></row><row><cell>RepPoints* [35]</cell><cell>ResNet-101-DCN</cell><cell>46.5</cell><cell>67.4</cell><cell>50.9</cell><cell>30.3</cell><cell>49.7</cell><cell>57.1</cell></row><row><cell>MAL* [13]</cell><cell>ResNeXt-101</cell><cell>47.0</cell><cell>66.1</cell><cell>51.2</cell><cell>30.2</cell><cell>50.1</cell><cell>58.9</cell></row><row><cell>CentripetalNet* [6]</cell><cell>Hourglass-104</cell><cell>48.0</cell><cell>65.1</cell><cell>51.8</cell><cell>29.0</cell><cell>50.4</cell><cell>59.9</cell></row><row><cell>ATSS* [37]</cell><cell cols="2">ResNeXt-64x4d-101-DCN 50.7</cell><cell>68.9</cell><cell>56.3</cell><cell>33.2</cell><cell>52.9</cell><cell>62.4</cell></row><row><cell>TSD* [28]</cell><cell>SENet154-DCN</cell><cell>51.2</cell><cell>71.9</cell><cell>56.0</cell><cell>33.8</cell><cell>54.8</cell><cell>64.2</cell></row><row><cell>RelationNet++ (our)</cell><cell cols="2">ResNeXt-64x4d-101-DCN 50.3</cell><cell>69.0</cell><cell>55.0</cell><cell>32.8</cell><cell>55.0</cell><cell>65.8</cell></row><row><cell cols="3">RelationNet++ (our)* ResNeXt-64x4d-101-DCN 52.7</cell><cell>70.4</cell><cell>58.3</cell><cell>35.8</cell><cell>55.3</cell><cell>64.7</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">MMDetection: Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08508</idno>
		<title level="m">Reppoints v2: Verification meets regression for object detection</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Centripetalnet: Pursuing high-quality keypoint pairs for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Centernet: Object detection with keypoint triplets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning region features for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Relation networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multiple anchor learning for visual object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03797</idno>
		<title level="m">Foveabox: Beyond anchor-based object detector</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Cornernet-lite: Efficient keypoint based object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08900</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Grid R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Houghnet: Integrating near and long-range evidence for bottom-up object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Samet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hicsonmez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Akbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Sniper: Efficient multi-scale training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Revisiting the sibling head in object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">FCOS: fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Denet: Scalable real-time object detection with directed sparse sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tychsen-Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Petersson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Region proposal by guided anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Point-set anchors for object detection, instance segmentation and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Reppoints: Point set representation for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dense reppoints: Representing visual objects with dense point sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<title level="m">Objects as points</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Bottom-up object detection by grouping extreme and center points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
