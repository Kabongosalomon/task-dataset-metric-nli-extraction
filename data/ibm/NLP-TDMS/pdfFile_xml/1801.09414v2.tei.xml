<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CosFace: Large Margin Cosine Loss for Deep Face Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
							<email>hawelwang@tencent.com</email>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitong</forename><surname>Wang</surname></persName>
							<email>yitongwang@tencent.com</email>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Ji</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dihong</forename><surname>Gong</surname></persName>
							<email>gongdihong@gmail.comwliu@ee.columbia.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingchao</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CosFace: Large Margin Cosine Loss for Deep Face Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Face recognition has made extraordinary progress owing to the advancement of deep convolutional neural networks (CNNs). The central task of face recognition, including face verification and identification, involves face feature discrimination. However, the traditional softmax loss of deep CNNs usually lacks the power of discrimination. To address this problem, recently several loss functions such as center loss, large margin softmax loss, and angular softmax loss have been proposed. All these improved losses share the same idea: maximizing inter-class variance and minimizing intra-class variance. In this paper, we propose a novel loss function, namely large margin cosine loss (LMCL), to realize this idea from a different perspective. More specifically, we reformulate the softmax loss as a cosine loss by L 2 normalizing both features and weight vectors to remove radial variations, based on which a cosine margin term is introduced to further maximize the decision margin in the angular space. As a result, minimum intra-class variance and maximum inter-class variance are achieved by virtue of normalization and cosine decision margin maximization. We refer to our model trained with LMCL as CosFace.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recently progress on the development of deep convolutional neural networks (CNNs) <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b44">44]</ref> has significantly advanced the state-of-the-art performance on <ref type="bibr">Figure 1</ref>. An overview of the proposed CosFace framework. In the training phase, the discriminative face features are learned with a large margin between different classes. In the testing phase, the testing data is fed into CosFace to extract face features which are later used to compute the cosine similarity score to perform face verification and identification. a wide variety of computer vision tasks, which makes deep CNN a dominant machine learning approach for computer vision. Face recognition, as one of the most common computer vision tasks, has been extensively studied for decades <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b1">2]</ref>. Early studies build shallow models with low-level face features, while modern face recognition techniques are greatly advanced driven by deep CNNs. Face recognition usually includes two sub-tasks: face verification and face identification. Both of these two tasks involve three stages: face detection, feature extraction, and classification. A deep CNN is able to extract clean highlevel features, making itself possible to achieve superior performance with a relatively simple classification architecture: usually, a multilayer perceptron networks followed by a softmax loss <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b32">32]</ref>. However, recent studies <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b22">23]</ref> found that the traditional softmax loss is insufficient to acquire the discriminating power for classification.</p><p>To encourage better discriminating performance, many research studies have been carried out <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b22">23]</ref>. All these studies share the same idea for maximum discrimination capability: maximizing inter-class variance and minimizing intra-class variance. For example, <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b39">39]</ref> propose to adopt multi-loss learning in order to increase the feature discriminating power. While these methods improve classification performance over the traditional softmax loss, they usually come with some extra limitations. For <ref type="bibr" target="#b42">[42]</ref>, it only explicitly minimizes the intra-class variance while ignoring the inter-class variances, which may result in suboptimal solutions. <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b39">39]</ref> require thoroughly scheming the mining of pair or triplet samples, which is an extremely time-consuming procedure. Very recently, <ref type="bibr" target="#b22">[23]</ref> proposed to address this problem from a different perspective. More specifically, <ref type="bibr" target="#b22">[23]</ref> (A-softmax) projects the original Euclidean space of features to an angular space, and introduces an angular margin for larger inter-class variance.</p><p>Compared to the Euclidean margin suggested by <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b9">10]</ref>, the angular margin is preferred because the cosine of the angle has intrinsic consistency with softmax. The formulation of cosine matches the similarity measurement that is frequently applied to face recognition. From this perspective, it is more reasonable to directly introduce cosine margin between different classes to improve the cosine-related discriminative information.</p><p>In this paper, we reformulate the softmax loss as a cosine loss by L 2 normalizing both features and weight vectors to remove radial variations, based on which a cosine margin term m is introduced to further maximize the decision margin in the angular space. Specifically, we propose a novel algorithm, dubbed Large Margin Cosine Loss (LMCL), which takes the normalized features as input to learn highly discriminative features by maximizing the inter-class cosine margin. Formally, we define a hyper-parameter m such that the decision boundary is given by cos(θ 1 ) − m = cos(θ 2 ), where θ i is the angle between the feature and weight of class i.</p><p>For comparison, the decision boundary of the A-Softmax is defined over the angular space by cos(mθ 1 ) = cos(θ 2 ), which has a difficulty in optimization due to the nonmonotonicity of the cosine function. To overcome such a difficulty, one has to employ an extra trick with an ad-hoc piecewise function for A-Softmax. More importantly, the decision margin of A-softmax depends on θ, which leads to different margins for different classes. As a result, in the decision space, some inter-class features have a larger margin while others have a smaller margin, which reduces the discriminating power. Unlike A-Softmax, our approach defines the decision margin in the cosine space, thus avoiding the aforementioned shortcomings.</p><p>Based on the LMCL, we build a sophisticated deep model called CosFace, as shown in <ref type="figure">Figure 1</ref>. In the training phase, LMCL guides the ConvNet to learn features with a large cosine margin. In the testing phase, the face features are extracted from the ConvNet to perform either face verification or face identification. We summarize the contributions of this work as follows:</p><p>(1) We embrace the idea of maximizing inter-class variance and minimizing intra-class variance and propose a novel loss function, called LMCL, to learn highly discriminative deep features for face recognition.</p><p>(2) We provide reasonable theoretical analysis based on the hyperspherical feature distribution encouraged by LMCL.</p><p>(3) The proposed approach advances the state-of-the-art performance over most of the benchmarks on popular face databases including LFW <ref type="bibr" target="#b12">[13]</ref>, YTF <ref type="bibr" target="#b43">[43]</ref> and Megaface <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b24">25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Deep Face Recognition. Recently, face recognition has achieved significant progress thanks to the great success of deep CNN models <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b8">9]</ref>. In DeepFace <ref type="bibr" target="#b35">[35]</ref> and DeepID <ref type="bibr" target="#b32">[32]</ref>, face recognition is treated as a multiclass classification problem and deep CNN models are first introduced to learn features on large multi-identities datasets. DeepID2 <ref type="bibr" target="#b30">[30]</ref> employs identification and verification signals to achieve better feature embedding. Recent works DeepID2+ <ref type="bibr" target="#b33">[33]</ref> and DeepID3 <ref type="bibr" target="#b31">[31]</ref> further explore the advanced network structures to boost recognition performance. FaceNet <ref type="bibr" target="#b29">[29]</ref> uses triplet loss to learn an Euclidean space embedding and a deep CNN is then trained on nearly 200 million face images, leading to the state-ofthe-art performance. Other approaches <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b10">11]</ref> also prove the effectiveness of deep CNNs on face recognition.</p><p>Loss Functions. Loss function plays an important role in deep feature learning. Contrastive loss <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7]</ref> and triplet loss <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b39">39]</ref> are usually used to increase the Euclidean margin for better feature embedding. Wen et al. <ref type="bibr" target="#b42">[42]</ref> proposed a center loss to learn centers for deep features of each identity and used the centers to reduce intra-class variance. Liu et al. <ref type="bibr" target="#b23">[24]</ref> proposed a large margin softmax (L-Softmax) by adding angular constraints to each identity to improve feature discrimination. Angular softmax (A-Softmax) <ref type="bibr" target="#b22">[23]</ref> improves L-Softmax by normalizing the weights, which achieves better performance on a series of open-set face recognition benchmarks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b16">17]</ref>. Other loss functions <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b2">3]</ref> based on contrastive loss or center loss also demonstrate the performance on enhancing discrimination.</p><p>Normalization Approaches. Normalization has been studied in recent deep face recognition studies. <ref type="bibr" target="#b38">[38]</ref> normalizes the weights which replace the inner product with cosine similarity within the softmax loss. <ref type="bibr" target="#b28">[28]</ref> applies the L 2 constraint on features to embed faces in the normalized space. Note that normalization on feature vectors or weight vectors achieves much lower intra-class angular variability by concentrating more on the angle during training. Hence the angles between identities can be well optimized. The von Mises-Fisher (vMF) based methods <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b7">8]</ref> and A-Softmax <ref type="bibr" target="#b22">[23]</ref> also adopt normalization in feature learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Approach</head><p>In this section, we firstly introduce the proposed LMCL in detail (Sec. 3.1). And a comparison with other loss functions is given to show the superiority of the LMCL (Sec. 3.2). The feature normalization technique adopted by the LMCL is further described to clarify its effectiveness (Sec. 3.3). Lastly, we present a theoretical analysis for the proposed LMCL (Sec. 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Large Margin Cosine Loss</head><p>We start by rethinking the softmax loss from a cosine perspective. The softmax loss separates features from different classes by maximizing the posterior probability of the ground-truth class. Given an input feature vector x i with its corresponding label y i , the softmax loss can be formulated as:</p><formula xml:id="formula_0">L s = 1 N N i=1 − log p i = 1 N N i=1 − log e fy i C j=1 e fj ,<label>(1)</label></formula><p>where p i denotes the posterior probability of x i being correctly classified. N is the number of training samples and C is the number of classes. f j is usually denoted as activation of a fully-connected layer with weight vector W j and bias B j . We fix the bias B j = 0 for simplicity, and as a result f j is given by:</p><formula xml:id="formula_1">f j = W T j x = W j x cos θ j ,<label>(2)</label></formula><p>where θ j is the angle between W j and x. This formula suggests that both norm and angle of vectors contribute to the posterior probability.</p><p>To develop effective feature learning, the norm of W should be necessarily invariable. To this end, We fix W j = 1 by L 2 normalization. In the testing stage, the face recognition score of a testing face pair is usually calculated according to cosine similarity between the two feature vectors. This suggests that the norm of feature vector x is not contributing to the scoring function. Thus, in the training stage, we fix x = s. Consequently, the posterior probability merely relies on cosine of angle. The modified loss can be formulated as  Because we remove variations in radial directions by fixing x = s, the resulting model learns features that are separable in the angular space. We refer to this loss as the Normalized version of Softmax Loss (NSL) in this paper.</p><p>However, features learned by the NSL are not sufficiently discriminative because the NSL only emphasizes correct classification. To address this issue, we introduce the cosine margin to the classification boundary, which is naturally incorporated into the cosine formulation of Softmax.</p><p>Considering a scenario of binary-classes for example, let θ i denote the angle between the learned feature vector and the weight vector of Class C i (i = 1, 2). The NSL forces cos(θ 1 ) &gt; cos(θ 2 ) for C 1 , and similarly for C 2 , so that features from different classes are correctly classified. To develop a large margin classifier, we further require cos(θ 1 ) − m &gt; cos(θ 2 ) and cos(θ 2 ) − m &gt; cos(θ 1 ), where m ≥ 0 is a fixed parameter introduced to control the magnitude of the cosine margin. Since cos(θ i ) − m is lower than cos(θ i ), the constraint is more stringent for classification. The above analysis can be well generalized to the scenario of multi-classes. Therefore, the altered loss reinforces the discrimination of learned features by encouraging an extra margin in the cosine space.</p><p>Formally, we define the Large Margin Cosine Loss (LMCL) as:</p><formula xml:id="formula_2">L lmc = 1 N i − log e s(cos(θy i ,i)−m) e s(cos(θy i ,i)−m) + j =yi e s cos(θj,i) ,<label>(4)</label></formula><formula xml:id="formula_3">subject to W = W * W * , x = x * x * , cos(θ j , i) = W j T x i ,<label>(5)</label></formula><p>where N is the numer of training samples, x i is the i-th feature vector corresponding to the ground-truth class of y i , the W j is the weight vector of the j-th class, and θ j is the angle between W j and x i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Comparison on Different Loss Functions</head><p>In this subsection, we compare the decision margin of our method (LMCL) to: Softmax, NSL, and A-Softmax, as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. For simplicity of analysis, we consider the binary-classes scenarios with classes C 1 and C 2 . Let W 1 and W 2 denote weight vectors for C 1 and C 2 , respectively.</p><p>Softmax loss defines a decision boundary by:</p><formula xml:id="formula_4">W 1 cos(θ 1 ) = W 2 cos(θ 2 ).</formula><p>Thus, its boundary depends on both magnitudes of weight vectors and cosine of angles, which results in an overlapping decision area (margin &lt; 0) in the cosine space. This is illustrated in the first subplot of <ref type="figure" target="#fig_1">Figure 2</ref>. As noted before, in the testing stage it is a common strategy to only consider cosine similarity between testing feature vectors of faces. Consequently, the trained classifier with the Softmax loss is unable to perfectly classify testing samples in the cosine space. NSL normalizes weight vectors W 1 and W 2 such that they have constant magnitude 1, which results in a decision boundary given by:</p><formula xml:id="formula_5">cos(θ 1 ) = cos(θ 2 ).</formula><p>The decision boundary of NSL is illustrated in the second subplot of <ref type="figure" target="#fig_1">Figure 2</ref>. We can see that by removing radial variations, the NSL is able to perfectly classify testing samples in the cosine space, with margin = 0. However, it is not quite robust to noise because there is no decision margin: any small perturbation around the decision boundary can change the decision.</p><p>A-Softmax improves the softmax loss by introducing an extra margin, such that its decision boundary is given by:</p><formula xml:id="formula_6">C 1 : cos(mθ 1 ) ≥ cos(θ 2 ), C 2 : cos(mθ 2 ) ≥ cos(θ 1 ).</formula><p>Thus, for C 1 it requires θ 1 ≤ θ2 m , and similarly for C 2 . The third subplot of <ref type="figure" target="#fig_1">Figure 2</ref> depicts this decision area, where gray area denotes decision margin. However, the margin of A-Softmax is not consistent over all θ values: the margin becomes smaller as θ reduces, and vanishes completely when θ = 0. This results in two potential issues. First, for difficult classes C 1 and C 2 which are visually similar and thus have a smaller angle between W 1 and W 2 , the margin is consequently smaller. Second, technically speaking one has to employ an extra trick with an ad-hoc piecewise function to overcome the nonmonotonicity difficulty of the cosine function.</p><p>LMCL (our proposed) defines a decision margin in cosine space rather than the angle space (like A-Softmax) by:</p><formula xml:id="formula_7">C 1 : cos(θ 1 ) ≥ cos(θ 2 ) + m, C 2 : cos(θ 2 ) ≥ cos(θ 1 ) + m.</formula><p>Therefore, cos(θ 1 ) is maximized while cos(θ 2 ) being minimized for C 1 (similarly for C 2 ) to perform the large-margin classification. The last subplot in <ref type="figure" target="#fig_1">Figure 2</ref> illustrates the decision boundary of LMCL in the cosine space, where we can see a clear margin( √ 2m) in the produced distribution of the cosine of angle. This suggests that the LMCL is more robust than the NSL, because a small perturbation around the decision boundary (dashed line) less likely leads to an incorrect decision. The cosine margin is applied consistently to all samples, regardless of the angles of their weight vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Normalization on Features</head><p>In the proposed LMCL, a normalization scheme is involved on purpose to derive the formulation of the cosine loss and remove variations in radial directions. Unlike <ref type="bibr" target="#b22">[23]</ref> that only normalizes the weight vectors, our approach simultaneously normalizes both weight vectors and feature vectors. As a result, the feature vectors distribute on a hypersphere, where the scaling parameter s controls the magnitude of radius. In this subsection, we discuss why feature normalization is necessary and how feature normalization encourages better feature learning in the proposed LMCL approach.</p><p>The necessity of feature normalization is presented in two respects: First, the original softmax loss without feature normalization implicitly learns both the Euclidean norm (L 2 -norm) of feature vectors and the cosine value of the angle. The L 2 -norm is adaptively learned for minimizing the overall loss, resulting in the relatively weak cosine constraint. Particularly, the adaptive L 2 -norm of easy samples becomes much larger than hard samples to remedy the inferior performance of cosine metric. On the contrary, our approach requires the entire set of feature vectors to have the same L 2 -norm such that the learning only depends on cosine values to develop the discriminative power. Feature vectors from the same classes are clustered together and those from different classes are pulled apart on the surface of the hypersphere. Additionally, we consider the situation when the model initially starts to minimize the LMCL. Given a feature vector x, let cos(θ i ) and cos(θ j ) denote cosine scores of the two classes, respectively. Without normalization on features, the LMCL forces x (cos(θ i ) − m) &gt; x cos(θ j ). Note that cos(θ i ) and cos(θ j ) can be initially comparable with each other. Thus, as long as (cos(θ i ) − m) is smaller than cos(θ j ), x is required to decrease for minimizing the loss, which degenerates the optimization. Therefore, feature normalization is critical under the supervision of LMCL, especially when the networks are trained from scratch. Likewise, it is more favorable to fix the scaling parameter s instead of adaptively learning.</p><p>Furthermore, the scaling parameter s should be set to a properly large value to yield better-performing features with lower training loss. For NSL, the loss continuously goes down with higher s, while too small s leads to an insufficient convergence even no convergence. For LMCL, we also need adequately large s to ensure a sufficient hyperspace for feature learning with an expected large margin.</p><p>In the following, we show the parameter s should have a lower bound to obtain expected classification performance. Given the normalized learned feature vector x and unit weight vector W , we denote the total number of classes as C. Suppose that the learned feature vectors separately lie on the surface of the hypersphere and center around the corresponding weight vector. Let P W denote the expected minimum posterior probability of class center (i.e., W ), the lower bound of s is given by 1 :</p><formula xml:id="formula_8">s ≥ C − 1 C log (C − 1)P W 1 − P W .<label>(6)</label></formula><p>Based on this bound, we can infer that s should be enlarged consistently if we expect an optimal P w for classification with a certain number of classes. Besides, by keeping a fixed P w , the desired s should be larger to deal with more classes since the growing number of classes increase the difficulty for classification in the relatively compact space. A hypersphere with large radius s is therefore required for embedding features with small intra-class distance and large inter-class distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Theoretical Analysis for LMCL</head><p>The preceding subsections essentially discuss the LMCL from the classification point of view. In terms of learning the discriminative features on the hypersphere, the cosine margin servers as momentous part to strengthen the discriminating power of features. Detailed analysis about the quantitative feasible choice of the cosine margin (i.e., the bound of hyper-parameter m) is necessary. The optimal choice of m potentially leads to more promising learning of highly discriminative face features. In the following, we delve into the decision boundary and angular margin in the feature space to derive the theoretical bound for hyper-parameter m.</p><p>First, considering the binary-classes case with classes C 1 and C 2 as before, suppose that the normalized feature vector x is given. Let W i denote the normalized weight vector, and θ i denote the angle between x and W i . For NSL, the decision boundary defines as cos θ 1 − cos θ 2 = 0, which is equivalent to the angular bisector of W 1 and W 2 as shown in the left of <ref type="figure" target="#fig_2">Figure 3</ref>. This addresses that the model supervised by NSL partitions the underlying feature space to two close regions, where the features near the boundary are extremely ambiguous (i.e., belonging to either class is acceptable). In contrast, LMCL drives the decision boundary formulated by cos θ 1 − cos θ 2 = m for C 1 , in which θ 1 should be much smaller than θ 2 (similarly for C 2 ). Consequently, the inter-class variance is enlarged while the intraclass variance shrinks.</p><p>Back to <ref type="figure" target="#fig_2">Figure 3</ref>, one can observe that the maximum angular margin is subject to the angle between W 1 and W 2 . Accordingly, the cosine margin should have the limited variable scope when W 1 and W 2 are given. Specifically, suppose a scenario that all the feature vectors belonging to class i exactly overlap with the corresponding weight vector W i of class i. In other words, every feature vector is identical to the weight vector for class i, and apparently the feature space is in an extreme situation, where all the feature vectors lie at their class center. In that case, the margin of decision boundaries has been maximized (i.e., the strict upper bound of the cosine margin).</p><p>To extend in general, we suppose that all the features are well-separated and we have a total number of C classes. The theoretical variable scope of m is supposed to be:</p><formula xml:id="formula_9">0 ≤ m ≤ (1 − max(W T i W j ))</formula><p>, where i, j ≤ n, i = j. The softmax loss tries to maximize the angle between any of the two weight vectors from two different classes in order to perform perfect classification. Hence, it is clear that the optimal solution for the softmax loss should uniformly distribute the weight vectors on a unit hypersphere. Based on this assumption, the variable scope of the introduced cosine margin m can be inferred as follows <ref type="bibr" target="#b1">2</ref> :</p><formula xml:id="formula_10">0 ≤ m ≤ 1 − cos 2π C , (K = 2) 0 ≤ m ≤ C C − 1 , (C ≤ K + 1) 0 ≤ m C C − 1 , (C &gt; K + 1)<label>(7)</label></formula><p>where C is the number of training classes and K is the dimension of learned features. The inequalities indicate that as the number of classes increases, the upper bound of the cosine margin between classes are decreased correspondingly. Especially, if the number of classes is much larger than the feature dimension, the upper bound of the cosine margin will get even smaller. A reasonable choice of larger m ∈ [0, C C−1 ) should effectively boost the learning of highly discriminative features. Nevertheless, parameter m usually could not reach the theoretical upper bound in practice due to the vanishing of the feature space. That is, all the feature vectors are centered together according to the weight vector of the corresponding class. In fact, the model fails to converge when m is too large, because the cosine constraint (i.e., cos θ 1 −m &gt; cos θ 2 or cos θ 2 −m &gt; cos θ 1 for two classes) becomes stricter and is hard to be satisfied. Besides, the cosine constraint with overlarge m forces the training process to be more sensitive to noisy data. The ever-increasing m starts to degrade the overall performance at some point because of failing to converge.</p><p>We perform a toy experiment for better visualizing on features and validating our approach. We select face images from 8 distinct identities containing enough samples to clearly show the feature points on the plot. Several models are trained using the original softmax loss and the proposed LMCL with different settings of m. We extract 2-D features of face images for simplicity. As discussed above, m should be no larger than 1 − cos π 4 (about 0.29), so we set up three choices of m for comparison, which are m = 0, m = 0.1, and m = 0.2. As shown in <ref type="figure" target="#fig_3">Figure 4</ref>, the first row and second row present the feature distributions in Euclidean space and angular space, respectively. We can observe that the original softmax loss produces ambiguity in decision boundaries while the proposed LMCL performs much better. As m increases, the angular margin between different classes has been amplified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Preprocessing. Firstly, face area and landmarks are detected by MTCNN <ref type="bibr" target="#b15">[16]</ref> for the entire set of training and testing images. Then, the 5 facial points (two eyes, nose and two mouth corners) are adopted to perform similarity transformation. After that we obtain the cropped faces which are then resized to be 112 × 96. Following <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b22">23]</ref>, each pixel (in [0, 255]) in RGB images is normalized by subtracting 127.5 then dividing by 128.</p><p>Training. For a direct and fair comparison to the existing results that use small training datasets (less than 0.5M images and 20K subjects) <ref type="bibr" target="#b16">[17]</ref>, we train our models on a small training dataset, which is the publicly available CASIA-WebFace <ref type="bibr" target="#b46">[46]</ref> dataset containing 0.49M face images from 10,575 subjects. We also use a large training dataset to evaluate the performance of our approach for benchmark comparison with the state-of-the-art results (using large training dataset) on the benchmark face dataset. The large training dataset that we use in this study is composed of several public datasets and a private face dataset, containing about 5M images from more than 90K identities. The training faces are horizontally flipped for data augmentation. In our experiments we remove face images belong to identities that appear in the testing datasets.</p><p>For the fair comparison, the CNN architecture used in our work is similar to <ref type="bibr" target="#b22">[23]</ref>, which has 64 convolutional layers and is based on residual units <ref type="bibr" target="#b8">[9]</ref>. The scaling parameter s in Equation <ref type="formula" target="#formula_2">(4)</ref> is set to 64 empirically. We use Caffe <ref type="bibr" target="#b13">[14]</ref> to implement the modifications of the loss layer and run the models. The CNN models are trained with SGD algorithm, with the batch size of 64 on 8 GPUs. The weight decay is set to 0.0005. For the case of training on the small dataset, the learning rate is initially 0.1 and divided by 10 at the 16K, 24K, 28k iterations, and we finish the training process at 30k iterations. While the training on the large dataset terminates at 240k iterations, with the initial learning rate 0.05 dropped at 80K, 140K, 200K iterations.</p><p>Testing. At testing stage, features of original image and the flipped image are concatenated together to compose the final face representation. The cosine distance of features is computed as the similarity score. Finally, face verification and identification are conducted by thresholding and ranking the scores. We test our models on several popular public face datasets, including LFW <ref type="bibr" target="#b12">[13]</ref>, YTF <ref type="bibr" target="#b43">[43]</ref>, and MegaFace <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b24">25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Exploratory Experiments</head><p>Effect of m. The margin parameter m plays a key role in LMCL. In this part we conduct an experiment to investigate the effect of m. By varying m from 0 to 0.45 (If m is larger than 0.45, the model will fail to converge), we use the small training data (CASIA-WebFace <ref type="bibr" target="#b46">[46]</ref>) to train our CosFace model and evaluate its performance on the LFW <ref type="bibr" target="#b12">[13]</ref> and YTF <ref type="bibr" target="#b43">[43]</ref> datasets, as illustrated in <ref type="figure" target="#fig_4">Figure 5</ref>. We can see that the model without the margin (in this case m=0) leads to the worst performance. As m being increased, the accuracies are improved consistently on both datasets, and get saturated at m = 0.35. This demonstrates the effectiveness of the margin m. By increasing the margin m, the discriminative power of the learned features can be significantly improved. In this study, m is set to fixed 0.35 in the subsequent experiments.</p><p>Effect of Feature Normalization. To investigate the effect of the feature normalization scheme in our approach, we train our CosFace models on the CASIA-WebFace with  <ref type="table">Table 1</ref>. Comparison of our models with and without feature normalization on Megaface Challenge 1 (MF1). "Rank 1" refers to rank-1 face identification accuracy and "Veri." refers to face verification TAR (True Accepted Rate) under 10 −6 FAR (False Accepted Rate). and without the feature normalization scheme by fixing m to 0.35, and compare their performance on LFW <ref type="bibr" target="#b12">[13]</ref>, YTF <ref type="bibr" target="#b43">[43]</ref>, and the Megaface Challenge 1(MF1) <ref type="bibr" target="#b16">[17]</ref>. Note that the model trained without normalization is initialized by softmax loss and then supervised by the proposed LMCL. The comparative results are reported in <ref type="table">Table 1</ref>. It is very clear that the model using the feature normalization scheme consistently outperforms the model without the feature normalization scheme across the three datasets. As discussed above, feature normalization removes radical variance, and the learned features can be more discriminative in angular space. This experiment verifies this point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with state-of-the-art loss functions</head><p>In this part, we compare the performance of the proposed LMCL with the state-of-the-art loss functions. Following the experimental setting in <ref type="bibr" target="#b22">[23]</ref>, we train a model with the guidance of the proposed LMCL on the CAISA-WebFace <ref type="bibr" target="#b46">[46]</ref> using the same 64-layer CNN architecture described in <ref type="bibr" target="#b22">[23]</ref>. The experimental comparison on LFW, YTF and MF1 are reported in <ref type="table">Table 2</ref>. For fair comparison, we are strictly following the model structure (a 64-layers ResNet-Like CNNs) and the detailed experimental settings of SphereFace <ref type="bibr" target="#b22">[23]</ref>. As can be seen in <ref type="table">Table 2</ref>, LMCL consistently achieves competitive results compared to the other losses across the three datasets. Especially, our method not only surpasses the performance of A-Softmax with feature normalization (named as A-Softmax-NormFea in <ref type="table">Table 2</ref>), but also significantly outperforms the other loss functions on YTF and MF1, which demonstrates the effectiveness of LMCL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Overall Benchmark Comparison</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Evaluation on LFW and YTF</head><p>LFW <ref type="bibr" target="#b12">[13]</ref> is a standard face verification testing dataset in unconstrained conditions. It includes 13,233 face images from 5749 identities collected from the website. We evaluate our model strictly following the standard protocol of unrestricted with labeled outside data <ref type="bibr" target="#b12">[13]</ref>, and report the result on the 6,000 pair testing images. YTF <ref type="bibr" target="#b43">[43]</ref>     <ref type="table">Table 5</ref>. Face identification and verification evaluation on MF2. "Rank 1" refers to rank-1 face identification accuracy and "Veri." refers to face verification TAR under 10 −6 FAR . unrestricted with labeled outside data protocol and report the result on 5,000 video pairs. As shown in <ref type="table" target="#tab_2">Table 3</ref>, the proposed CosFace achieves state-of-the-art results of 99.73% on LFW and 97.6% on YTF. FaceNet achieves the runner-up performance on LFW with the large scale of the image dataset, which has approximately 200 million face images. In terms of YTF, our model reaches the first place over all other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Evaluation on MegaFace</head><p>MegaFace <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b24">25]</ref> is a very challenging testing benchmark recently released for large-scale face identification and verification, which contains a gallery set and a probe set. The gallery set in Megaface is composed of more than 1 million face images. The probe set has two existing databases: Facescrub <ref type="bibr" target="#b25">[26]</ref> and FGNET <ref type="bibr" target="#b0">[1]</ref>. In this study, we use the Facescrub dataset (containing 106,863 face images of 530 celebrities) as the probe set to evaluate the performance of our approach on both Megaface Challenge 1 and Challenge 2.</p><p>MegaFace Challenge 1 (MF1). On the MegaFace Challenge 1 <ref type="bibr" target="#b16">[17]</ref>, The gallery set incorporates more than 1 million images from 690K individuals collected from Flickr photos <ref type="bibr" target="#b36">[36]</ref>. <ref type="table" target="#tab_3">Table 4</ref> summarizes the results of our models trained on two protocols of MegaFace where the training dataset is regarded as small if it has less than 0.5 million images, large otherwise. The CosFace approach shows its superiority for both the identification and verification tasks on both the protocols.</p><p>MegaFace Challenge 2 (MF2). In terms of MegaFace Challenge 2 <ref type="bibr" target="#b24">[25]</ref>, all the algorithms need to use the training data provided by MegaFace. The training data for Megaface Challenge 2 contains 4.7 million faces and 672K identities, which corresponds to the large protocol. The gallery set has 1 million images that are different from the challenge 1 gallery set. Not surprisingly, Our method wins the first place of challenge 2 in table 5, setting a new state-of-the-art with a large margin (1.39% on rank-1 identification accuracy and 5.46% on verification performance).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed an innovative approach named LMCL to guide deep CNNs to learn highly discriminative face features. We provided a well-formed geometrical and theoretical interpretation to verify the effectiveness of the proposed LMCL. Our approach consistently achieves the state-of-the-art results on several face benchmarks. We wish that our substantial explorations on learning discriminative features via LMCL will benefit the face recognition community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Supplementary Material</head><p>This supplementary document provides mathematical details for the derivation of the lower bound of the scaling parameter s (Equation 6 in the main paper), and the variable scope of the cosine margin m (Equation 7 in the main paper).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposition of the Scaling Parameter s</head><p>Given the normalized learned features x and unit weight vectors W , we denote the total number of classes as C where C &gt; 1. Suppose that the learned features separately lie on the surface of a hypersphere and center around the corresponding weight vector. Let P w denote the expected minimum posterior probability of the class center (i.e., W ). The lower bound of s is formulated as follows:</p><formula xml:id="formula_11">s ≥ C − 1 C ln (C − 1)P W 1 − P W</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof:</head><p>Let W i denote the i-th unit weight vector. ∀i, we have:</p><p>e s e s + j,j =i e s(W T i Wj ) ≥ P W ,</p><formula xml:id="formula_12">1 + e −s j,j =i e s(W T i Wj ) ≤ 1 P W ,<label>(8)</label></formula><formula xml:id="formula_13">C i=1<label>(9)</label></formula><p>(1 + e −s j,j =i e s(W T i Wj ) ) ≤ C P W ,</p><p>1 + e −s C i,j,i =j e s(W T i Wj ) ≤ 1 P W .</p><p>Because f (x) = e s·x is a convex function, according to Jensen's inequality, we obtain: (12) Besides, it is known that</p><formula xml:id="formula_16">i,j,i =j W T i W j = ( i W i ) 2 − ( i W 2 i ) ≥ −C.<label>(13)</label></formula><p>Thus, we have:</p><formula xml:id="formula_17">1 + (C − 1)e − sC C−1 ≤ 1 P W .<label>(14)</label></formula><p>Further simplification yields:</p><formula xml:id="formula_18">s ≥ C − 1 C ln (C − 1)P W 1 − P W .<label>(15)</label></formula><p>The equality holds if and only if every W T i W j is equal (i = j), and i W i = 0. Because at most K + 1 unit vectors are able to satisfy this condition in the K-dimension hyper-space, the equality holds only when C ≤ K + 1, where K is the dimension of the learned features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposition of the Cosine Margin m</head><p>Suppose that the weight vectors are uniformly distributed on a unit hypersphere. The variable scope of the introduced cosine margin m is formulated as follows :</p><formula xml:id="formula_19">0 ≤ m ≤ 1 − cos 2π C , (K = 2) 0 ≤ m ≤ C C − 1 , (K &gt; 2, C ≤ K + 1) 0 ≤ m C C − 1 , (K &gt; 2, C &gt; K + 1)</formula><p>where C is the total number of training classes and K is the dimension of the learned features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof:</head><p>For K = 2, the weight vectors uniformly spread on a unit circle. Hence, max(W T i W j ) = cos 2π C . It follows 0 ≤ m ≤ (1 − max(W T i W j )) = 1 − cos 2π C . For K &gt; 2, the inequality below holds:</p><formula xml:id="formula_20">C(C − 1) max(W T i W j ) ≥ i,j,i =j W T i W j (16) = ( i W i ) 2 − ( i W 2 i ) ≥ −C.</formula><p>Therefore, max(W T i W j ) ≥ −1 C−1 , and we have 0 ≤ m ≤ (1 − max(W T i W j )) ≤ C C−1 . Similarly, the equality holds if and only if every W T i W j is equal (i = j), and i W i = 0. As discussed above, this is satisfied only if C ≤ K + 1. On this condition, the distance between the vertexes of two arbitrary W should be the same. In other words, they form a regular simplex such as an equilateral triangle if C = 3, or a regular tetrahedron if C = 4.</p><p>For the case of C &gt; K + 1, the equality cannot be satisfied. In fact, it is unable to formulate the strict upper bound. Hence, we obtain 0 ≤ m C C−1 . Because the number of classes can be much larger than the feature dimension, the equality cannot hold in practice.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>e s cos(θy i ,i) j e s cos(θj,i) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The comparison of decision margins for different loss functions the binary-classes scenarios. Dashed line represents decision boundary, and gray areas are decision margins.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>A geometrical interpretation of LMCL from feature perspective. Different color areas represent feature space from distinct classes. LMCL has a relatively compact feature region compared with NSL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>A toy experiment of different loss functions on 8 identities with 2D features. The first row maps the 2D features onto the Euclidean space, while the second row projects the 2D features onto the angular space. The gap becomes evident as the margin term m increases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Accuracy (%) of CosFace with different margin parameters m on LFW<ref type="bibr" target="#b12">[13]</ref> and YTF<ref type="bibr" target="#b43">[43]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1 C(C − 1 )</head><label>11</label><figDesc>i,j,i =j e s(W T i Wj ) ≥ e s C(C−1) i,j,i =j W T i Wj .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Face verification (%) on the LFW and YTF datasets. "#Models" indicates the number of models that have been used in the method for evaluation.</figDesc><table><row><cell>Method</cell><cell>Protocol</cell><cell>MF1 Rank1</cell><cell>MF1 Veri.</cell></row><row><cell>SIAT MMLAB[42]</cell><cell>Small</cell><cell>65.23</cell><cell>76.72</cell></row><row><cell>DeepSense -Small</cell><cell>Small</cell><cell>70.98</cell><cell>82.85</cell></row><row><cell>SphereFace -Small[23]</cell><cell>Small</cell><cell>75.76</cell><cell>90.04</cell></row><row><cell>Beijing FaceAll V2</cell><cell>Small</cell><cell>76.66</cell><cell>77.60</cell></row><row><cell>GRCCV</cell><cell>Small</cell><cell>77.67</cell><cell>74.88</cell></row><row><cell>FUDAN-CS SDS[41]</cell><cell>Small</cell><cell>77.98</cell><cell>79.19</cell></row><row><cell>CosFace(Single-patch)</cell><cell>Small</cell><cell>77.11</cell><cell>89.88</cell></row><row><cell>CosFace(3-patch ensemble)</cell><cell>Small</cell><cell>79.54</cell><cell>92.22</cell></row><row><cell>Beijing FaceAll Norm 1600</cell><cell>Large</cell><cell>64.80</cell><cell>67.11</cell></row><row><cell>Google -FaceNet v8[29]</cell><cell>Large</cell><cell>70.49</cell><cell>86.47</cell></row><row><cell>NTechLAB -facenx large</cell><cell>Large</cell><cell>73.30</cell><cell>85.08</cell></row><row><cell>SIATMMLAB TencentVision</cell><cell>Large</cell><cell>74.20</cell><cell>87.27</cell></row><row><cell>DeepSense V2</cell><cell>Large</cell><cell>81.29</cell><cell>95.99</cell></row><row><cell>YouTu Lab</cell><cell>Large</cell><cell>83.29</cell><cell>91.34</cell></row><row><cell>Vocord -deepVo V3</cell><cell>Large</cell><cell>91.76</cell><cell>94.96</cell></row><row><cell>CosFace(Single-patch)</cell><cell>Large</cell><cell>82.72</cell><cell>96.65</cell></row><row><cell>CosFace(3-patch ensemble)</cell><cell>Large</cell><cell>84.26</cell><cell>97.96</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Face identification and verification evaluation on MF1. "Rank 1" refers to rank-1 face identification accuracy and "Veri." refers to face verification TAR under 10 −6 FAR.</figDesc><table><row><cell>Method</cell><cell>Protocol</cell><cell>MF2 Rank1</cell><cell>MF2 Veri.</cell></row><row><cell>3DiVi</cell><cell>Large</cell><cell>57.04</cell><cell>66.45</cell></row><row><cell>Team 2009</cell><cell>Large</cell><cell>58.93</cell><cell>71.12</cell></row><row><cell>NEC</cell><cell>Large</cell><cell>62.12</cell><cell>66.84</cell></row><row><cell>GRCCV</cell><cell>Large</cell><cell>75.77</cell><cell>74.84</cell></row><row><cell>SphereFace</cell><cell>Large</cell><cell>71.17</cell><cell>84.22</cell></row><row><cell>CosFace (Single-patch)</cell><cell>Large</cell><cell>74.11</cell><cell>86.77</cell></row><row><cell>CosFace(3-patch ensemble)</cell><cell>Large</cell><cell>77.06</cell><cell>90.30</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Proof is attached in the supplemental material.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Proof is attached in the supplemental material.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="http://www.fgnet.rsunit.com/.8" />
		<title level="m">FG-NET Aging Database</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Eigenfaces vs. fisherfaces: Recognition using class specific linear projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Hespanha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="711" to="720" />
			<date type="published" when="1997-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.03144</idno>
		<title level="m">Island Loss for Learning Discriminative Features in Facial Expression Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Beyond triplet loss: a deep quadruplet network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01719</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Marginal loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Hasnat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bohne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Milgram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gentric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04264</idno>
		<title level="m">von Mises-Fisher Mixture Model-based Deep learning: Application to Face Verification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep metric learning using triplet network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ailon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Similarity-Based Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">When face recognition meets with deep learning: an evaluation of convolutional neural networks for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Christmas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision Workshops (ICCVW)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01507</idno>
		<title level="m">Squeeze-and-Excitation Networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Technical Report 07-49</title>
		<meeting><address><addrLine>Amherst</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM on Multimedia Conference (ACM MM)</title>
		<meeting>the 2016 ACM on Multimedia Conference (ACM MM)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The megaface benchmark: 1 million faces for recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Nonparametric discriminant analysis for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="755" to="761" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Nonparametric subspace analysis for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Targeting ultimate accuracy: Face recognition via deep embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.07310</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Spatio-temporal embedding for statistical face recognition from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">SphereFace: Deep Hypersphere Embedding for Face Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Large-Margin Softmax Loss for Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Level playing field for million scale face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A data-driven approach to cleaning large face datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Winkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<title level="m">IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="343" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09507</idno>
		<title level="m">L2-constrained Softmax Loss for Discriminative Face Verification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep learning face representation by joint identification-verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.00873</idno>
		<title level="m">DeepID3: Face recognition with very deep neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep learning face representation from predicting 10,000 classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deeply learned face representations are sparse, selective, and robust</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">YFCC100M: The new data in multimedia research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Poland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Face recognition using eigenfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">NormFace: L 2 Hypersphere Embedding for Face Verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Multimedia Conference</title>
		<meeting>the 2017 ACM on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM MM</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning fine-grained image similarity with deep ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A unified framework for subspace face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1222" to="1228" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multi-task Deep Neural Network for Joint Face Recognition and Facial Attribute Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on International Conference on Multimedia Retrieval</title>
		<meeting>the 2017 ACM on International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="499" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Face recognition in unconstrained videos with matched background similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Maoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05431</idno>
		<title level="m">Aggregated residual transformations for deep neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Face recognition via archetype hull ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Learning face representation from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7923</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Range Loss for Deep Face Recognition with Long-tail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.09662</idno>
		<title level="m">Directional Statistics-based Deep Metric Learning for Image Classification and Retrieval</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
