<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Safeguarded Dynamic Label Regression for Generalized Noisy Supervision</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20151">AUGUST 2015 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">Safeguarded Dynamic Label Regression for Generalized Noisy Supervision</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20151">AUGUST 2015 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Bayesian learning</term>
					<term>noisy labels</term>
					<term>Gibbs sampling</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning with noisy labels, which aims to reduce expensive labors on accurate annotations, has become imperative in the Big Data era. Previous noise transition based method has achieved promising results and presented a theoretical guarantee on performance in the case of class-conditional noise. However, this type of approaches critically depend on an accurate preestimation of the noise transition, which is usually impractical. Subsequent improvement adapts the pre-estimation along with the training progress via a Softmax layer. However, the parameters in the Softmax layer are highly tweaked for the fragile performance due to the ill-posed stochastic approximation. To address these issues, we propose a Latent Class-Conditional Noise model (LCCN) that naturally embeds the noise transition under a Bayesian framework. By projecting the noise transition into a Dirichlet-distributed space, the learning is constrained on a simplex based on the whole dataset, instead of some ad-hoc parametric space. We then deduce a dynamic label regression method for LCCN to iteratively infer the latent labels, to stochastically train the classifier and to model the noise. Our approach safeguards the bounded update of the noise transition, which avoids previous arbitrarily tuning via a batch of samples. We further generalize LCCN for open-set noisy labels and the semi-supervised setting. We perform extensive experiments with the controllable noise data sets, CIFAR-10 and CIFAR-100, and the agnostic noise data sets, Clothing1M and WebVision17. The experimental results have demonstrated that the proposed model outperforms several state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-Learning with noisy labels, which aims to reduce expensive labors on accurate annotations, has become imperative in the Big Data era. Previous noise transition based method has achieved promising results and presented a theoretical guarantee on performance in the case of class-conditional noise. However, this type of approaches critically depend on an accurate preestimation of the noise transition, which is usually impractical. Subsequent improvement adapts the pre-estimation along with the training progress via a Softmax layer. However, the parameters in the Softmax layer are highly tweaked for the fragile performance due to the ill-posed stochastic approximation. To address these issues, we propose a Latent Class-Conditional Noise model (LCCN) that naturally embeds the noise transition under a Bayesian framework. By projecting the noise transition into a Dirichlet-distributed space, the learning is constrained on a simplex based on the whole dataset, instead of some ad-hoc parametric space. We then deduce a dynamic label regression method for LCCN to iteratively infer the latent labels, to stochastically train the classifier and to model the noise. Our approach safeguards the bounded update of the noise transition, which avoids previous arbitrarily tuning via a batch of samples. We further generalize LCCN for open-set noisy labels and the semi-supervised setting. We perform extensive experiments with the controllable noise data sets, CIFAR-10 and CIFAR-100, and the agnostic noise data sets, Clothing1M and WebVision17. The experimental results have demonstrated that the proposed model outperforms several state-of-the-art methods.</p><p>Index Terms-Bayesian learning, noisy labels, Gibbs sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>L ARGE scale datasets with editorial labels have driven the success of deep neural networks (DNNs) in computer vision <ref type="bibr" target="#b0">[1]</ref>, natural language processing <ref type="bibr" target="#b1">[2]</ref>, and speech recognition <ref type="bibr" target="#b2">[3]</ref>. However, for many real-world applications, it is usually expensive to collect accurately annotated data in large volume. Instead, samples with noisy supervision, as an alternative to alleviate the annotation burden, can be acquired inexhaustibly on the social websites and have shown potential to many applications in the deep learning area <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref>.</p><p>It is challenging to train DNNs in the presence of noisy supervision since it can easily memorize the clean data as well as the noisy data <ref type="bibr" target="#b7">[8]</ref>. To overcome the above issue, several methods have been explored from the perspective of model regularization and sample re-weighting, respectively. Arpit et al. <ref type="bibr" target="#b7">[8]</ref> applied the dropout regularization in DNNs to limit its speed of memorizing noise, which prevents the classifier from noise pollution. Ren et al. <ref type="bibr" target="#b8">[9]</ref> explored dynamically weighting noisy labels with the corresponding predictions to weaken the noise effect. However, the model regularization and sample re-weighting based methods usually require either a careful hyperparameter setting <ref type="bibr" target="#b7">[8]</ref>, or auxiliary samples <ref type="bibr" target="#b9">[10]</ref> or elaborate curricula <ref type="bibr" target="#b10">[11]</ref>.</p><p>The study of this paper falls into the third popular perspective, learning with noise transition, which places a noise transition on top of the classifier. Early study <ref type="bibr" target="#b11">[12]</ref> presents a two-step solution, that is, first pre-estimate the noise transition and then fix it to train the classifier. However, it suffers from the inaccurate pre-estimation via an ideal but impractical anchor set. Subsequent improvement <ref type="bibr" target="#b12">[13]</ref> uses the stochastic approximation to adapt the noise transition in the form of a Softmax layer along with the training progress. Although it shows promise, the optimization of the Softmax layer depends on highly tweaking and the model parameters easily fall into undesired local minimums. Essentially, such instability is due to the inconsideration of the global dependency in the stochastic approximation, yielding a "local" mini-batch of samples can unbounded update the "global" noise transition in the back-propagation.</p><p>To solve this issue, we propose a Latent Class-Conditional Noise model (LCCN) that embeds the noise transition into a Dirichlet-distributed space. Compared to the previous Softmax layer <ref type="bibr" target="#b12">[13]</ref>, LCCN constrains the learning of the noise transition as a global variable depending on the whole dataset. Namely, a "local" mini-batch of samples can only partially affect the estimation of the "global" noise transition. Besides, a new dynamic label regression method is derived to stochastically optimize LCCN. Although it iteratively infers the latent labels and applies them for the classifier training and the noise modeling, only a small amount of extra computational cost is introduced. We theoretically demonstrate our method safeguards the bounded update of the noise transition via a minibatch of samples. <ref type="figure" target="#fig_1">Fig. 1</ref> provides a simple illustration of our safeguarded dynamic label regression for LCCN. As can be seen, images are first inputted to the classifier to have the prediction of latent labels. Noisy labels are also forwarded to Bayesian noise modeling to compute the conditional transition of latent labels. Then, the latent labels are sampled based on their product and used to supervise the classifier training and refine the noise modeling. In a nutshell, our main contributions can be summarized into the following three points.</p><p>• We propose a Latent Class-Conditional Noise model that embeds the noise transition into a Dirichlet space to emphasize its global dependency, and then deduce a scalable dynamic label regression method for its optimization. • The theoretical analysis on the convergence of the dynamic label regression, the generalization gap as well as the complexity is provided. Importantly, we prove that our optimization of the noise transition via a batch of samples  <ref type="figure" target="#fig_1">Fig. 1</ref>. Safeguarded dynamic label regression for LCCN. The images and noisy labels are respectively inputted to the classifier and the safeguarded Bayesian noise modeling to compute the prediction and the conditional transition. Then, the latent labels are sampled based on their product and then used for the classifier training and the safeguarded Bayesian noise modeling. is bounded to avoid previous non-trivial tweaking. • A more general variant of LCCN is further extended in order to handle the open-set noisy labels setting and the semi-supervised learning setting for the practical needs. <ref type="bibr">•</ref> We conduct a range of experiments in the popular CIFAR-10, CIFAR-100 datasets and large real-world noisy datasets, Clothing1M and WebVision17. Comprehensive results have demonstrated the superior performance of our model compared with existing state-of-the-art methods. The rest part of this paper is organized as follows. Section II briefly reviews the related research of learning with noisy labels in deep learning. Then, we introduce our Latent Class-Conditional model and the dynamic label regression method in Section III, where the corresponding theoretical analysis and the further extension of LCCN is also included. We validate the efficiency of our method over a range of experiments in Section IV. Section V concludes the whole paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Recently, several approaches combined with deep learning have been developed for learning with noisy labels. In this section, we review these works according to noise transition, sample re-weighting and model regularization.</p><p>1) Learning with Noise Transition: This branch of research models a noise transition on top of the classifier to minimize the influence of label noise. Sukhbaatar et al. <ref type="bibr" target="#b13">[14]</ref> introduced a noise transition matrix on top of CNN to learn with noisy supervision. With a heuristic learning procedure, they gradually make the transition matrix absorb the noise among labels. Misra et al. <ref type="bibr" target="#b14">[15]</ref> considered the "reporting bias" phenomenon in human-centric annotations via a content-based transition, which is a special case of learning with noisy labels. Patrini et al. <ref type="bibr" target="#b11">[12]</ref> theoretically demonstrated: the backward correction with the inverse of the noise transition is unbiased to train the classifier in the presence of noisy labels; the forward noise transition make the training share the same minimizer with that on the clean data. However, the performance quite depends on the accuracy of the pre-estimated noise transition. Subsequent improvement in <ref type="bibr" target="#b12">[13]</ref> models the noise transition via a Softmax layer and tunes its parameters along with the training progress. Based on this research, Yao et al. <ref type="bibr" target="#b6">[7]</ref> introduced an auxiliary variable to augment the noise transition with more uncertainty. The structure information <ref type="bibr" target="#b15">[16]</ref> is further added to constrain the optimization. Although better performance has been achieved, these methods depend on the carefully tweaking. However, our model embeds the noise transition into a nonparametric space and naturally constrains its optimization to avoid undesired minimums via a dynamic label regression method.</p><p>2) Learning with Sample Re-weighting: This line of works weight the contribution of each training sample in parameter estimation to reduce the effect of label noise <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">17]</ref>. It can be implemented by the label or the training pair re-weighting. For example, Reed et al. <ref type="bibr" target="#b17">[18]</ref> facilitated the notion of perceptual consistency to linearly combine the label and the prediction as the new supervision, which shows the substantial robustness to label noise. Then, Li et al. <ref type="bibr" target="#b18">[19]</ref> substituted the prediction with the refined label by the graph distillation. Wang et al. <ref type="bibr" target="#b19">[20]</ref> leveraged the local intrinsic dimensionality to design an selfweighting strategy for Bootstrapping <ref type="bibr" target="#b17">[18]</ref>. Recently, several works <ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref> also explore to collaboratively learn a weight or selection for each training pair and adjust their contribution to the training of the classifier. However, these methods critically depend on the elaborate sample re-weighting strategy.</p><p>3) Learning with Model Regularization: This type of methods attempt to regularize the training procedure in the presence of noisy supervision. Zhang et al. <ref type="bibr" target="#b23">[24]</ref> have shown DNNs can easily memorize the random labels completely, characterizing the challenge to deep learning with noisy labels. Their further study <ref type="bibr" target="#b24">[25]</ref> that used the convex combinations of images and noisy labels as the data augmentation, has been demonstrated as an efficient regularization to prevent DNNs from overfitting. Arpit et al. <ref type="bibr" target="#b7">[8]</ref> investigated the memorization order of DNNs on feature patterns in noisy datasets and demonstrated dropout can efficiently limit the speed of memorization on noise in DNNs. Tanaka et al. <ref type="bibr" target="#b25">[26]</ref> explicitly introduced a regularization term to prevent the trivial case of assigning all labels to a single class in label correction. Compared with above methods, we indirectly regularize the training by Bayesian noise modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE PROPOSED FRAMEWORK A. Preliminaries</head><p>In the c-class classification setting, a collection of N noisy training pairs {(x n , y n )} N n=1 is given, where x n is the raw input data or the feature vector and y n ∈ {1, . . . , K} is the corresponding noisy label. Assume z n denotes the latent label of x n , which is unknown in practice. Then the goal in this task is to train a deep network classifier from the noisy dataset {(x n , y n )} N n=1 analogous to the one trained from the clean dataset {(x n , z n )} N n=1 , so that a promising performance can be achieved in a clean test dataset. As shown in <ref type="bibr" target="#b23">[24]</ref>, directly minimizing the following equation will make DNNs memorize both the classification pattern and noise,</p><formula xml:id="formula_0">f θ = arg min f θ ∈F − 1 N N n=1 (y n , f θ (x n )),<label>(1)</label></formula><p>where f θ is from the function class F, which is parameterized by θ via DNNs, and is the loss function between y n and the prediction f θ (x n ). Eq. (1) leads to a bad performance in the clean test dataset since it does not squeeze out the noise influence from f θ . Therefore, we follows one mainstream of approaches to handle this dilemma, which models a noise transition φ in simplex ∆ when learning with noisy labels. The objective is then mathematically expressed with the following empirical risk minimization problem</p><formula xml:id="formula_1">f θ , φ = arg min f θ ∈F ,φ∈∆ − 1 N N n=1 (y n , φ • f θ (x n )),<label>(2)</label></formula><p>Patrini et al. <ref type="bibr" target="#b11">[12]</ref> theoretically demonstrate Eq. (2) trained with the noisy data shares the same minimizer with Eq. (1) trained with the clean data, if φ is accurately estimated. Unfortunately, it is usually impractical to acquire such a φ in advance. Thus, subsequent work <ref type="bibr" target="#b12">[13]</ref> adapts the pre-estimation with a Softmax layer along with the training progress. Although this shows a promising performance, expensive tweaking is required due to the ill-posed stochastic approximation as a simple neural layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Latent Class-Conditional Noise model</head><p>In this section, we will present our Latent Class-Conditional Noise model (LCCN). Specifically, it avoids non-trivially tweaking for the fragile performance in <ref type="bibr" target="#b12">[13]</ref> by modeling φ in a Bayesian form. The graphical notation is illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref> and the generative procedure is summarized as follows,</p><p>• The latent label z n ∼ P (·|x n ), where P (·|x n ) is a Categorical distribution modeled by the deep neural network f θ and the given x n is its input feature.</p><formula xml:id="formula_2">• The transition vector of the kth class φ k ∼ Dirichlet(α),</formula><p>where α is the parameter of a Dirichlet distribution and [φ 1 , · · · , φ K ] T constitutes the noise transition matrix.</p><p>• The observed noisy label y n ∼ P (·|φ zn ), where P (·|φ zn ) is a Categorical distribution parameterized by φ zn . The general way to solve such a probabilistic model combined with deep learning is amortized variational inference <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>. However, this way for LCCN will require an approximate Categorical reparameterization <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref> and introduce an unstable Digamma function to optimize. To avoid this issue, we specifically deduce a dynamic label regression method for optimization and demonstrate its safeguarded update for φ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Dynamic Label Regression</head><p>In the following, we will give the dynamic label regression method for LCCN, which stacks an autoencoded Gibbs sampling to infer the latent labels and loss minimization for parameter learning. It naturally suits LCCN and we show its deduction via a two-step formulation. Note that, in despite of the complex deduction, only a small computational cost is extra introduced, which will be explained in the following section. Simply, the first step is computing the probability of each z conditional on the others Z ¬1 , i.e., P (z n |Z ¬n ). Then, with the samples from P (z n |Z ¬n ), the classifier training and the noise modeling can be explicitly decoupled as the following optimization problem,</p><formula xml:id="formula_3">min − 1 n N n=1 1 (z n , P (z n |x n )) min − 1 n N n=1 2 (y n , P (y n |z n )).<label>(3)</label></formula><p>1 is the ξ-clipped cross-entropy loss 2 and 2 is the likelihood loss. Alternating between the sampling of P (z n |Z ¬n ) and the optimization of Eq. (3) constructs our final algorithm to learn with noisy supervision. Specifically, when P (z|x) approach the true distribution of clean labels, the classifier training is similar to that on the clean dataset. This yields the asymptotically unbiased estimation as on the clean datasets.</p><p>Autoencoded Gibbs sampling. Firstly, according to the aforementioned generative process, we can easily deduce the posterior of z conditioned on the observed training pair {(x n , y n )} N n=1 and the Dirichlet parameter α. This is implemented by factorizing the target conditional probability based on <ref type="figure" target="#fig_0">Fig. 2</ref> and applying the Bayes theorem as follows,</p><formula xml:id="formula_4">P (Z|X, Y ; α) = φ K k=1 P (φ k ; α) N n=1 P (z n |x n , y n , φ)dφ = φ K k=1 P (φ k ; α) N n=1 P (z n |x n )P (y n |z n , φ) P (y n |x n ) dφ = S * φ K k=1 Γ( K k α k ) K k Γ(α k ) K k φ α k −1 kk N n=1 φ znyn dφ,<label>(4)</label></formula><p>where S represents N n=1 P (zn|xn) P (yn|xn) to simplify above equation. If we use the notation N (·)(·) to represent the confusion matrix of the noisy dataset, then we have</p><formula xml:id="formula_5">K k K k N kk =N and N n=1 φ znyn = K k K k φ N kk kk .</formula><p>Putting the later equation into Eq. (4) and then using the conjugation characteristic between the Dirichlet distribution and the Multinomial distribution, the following form can be further deduced,</p><formula xml:id="formula_6">P (Z|X, Y ; α) = S * φ K k=1 Γ( K k α k ) K k Γ(α k ) K k φ N kk +α k −1 kk dφ = S * K k=1 Γ( K k α k ) K k Γ(α k ) K k=1 K k Γ(α k + N kk ) Γ( K k (α k + N kk )) .<label>(5)</label></formula><p>Unfortunately, Eq. (5) is non-analytical and cannot be used to generate the samples of z directly, which can be solved by Gibbs sampling. According to the Gibbs sampling, we need to compute P (z n |Z ¬n ) first. And then based on P (z n |Z ¬n ), a sequence of observations can be sampled, which are approximately from P (z n |x n , y n , φ). The following deduction facilitates Eq. (5) and Γ(x + 1) = xΓ(x) to acquire the final conditional probability for our autoencoded Gibbs sampling.</p><formula xml:id="formula_7">P (z n |Z ¬n , X, Y ; α) = P (Z|X, Y ; α) P (Z ¬n |X, Y ; α) = P (z n |x n ) P (y n |x n ) α yn + N ¬n znyn K k (α k + N ¬n znk ) ∝ P (z n |x n ) Classifier encoder α yn + N ¬n znyn K k (α k + N ¬n znk ) Conditional transition .<label>(6)</label></formula><p>With Eq. <ref type="formula" target="#formula_7">(6)</ref>, we can sample a collection of latent labels {z n }. Such samples are then used to solve the optimization problem in Eq. (3). Iterating the procedure of Eq. (6) and Eq. <ref type="formula" target="#formula_3">(3)</ref>, we gradually approach the latent label, and at the same time train the classifier and estimate the noise transition.</p><p>Lemma 1. For a reversible, irreducible and aperiodic Markov chain with state space Ω, let λ * be the maximal absolute eigenvalue of the state transition matrix and π be the underlying stationary probability measure where π min = min Z∈Ω π(Z). Then, the -mixing time from the initial arbitrary state to the equilibrium is characterized by the following bounds,</p><formula xml:id="formula_8">λ * 1 − λ * ln 1 2 ≤ τ mix ( ) ≤ 1 1 − λ * ln 1 π min ,<label>(7)</label></formula><p>where τ mix ( ) = min{t : ||P t (Z) − π|| T V ≤ } and || · || T V is the total variation distance between two probability measures.</p><p>The above lemma indicates <ref type="bibr" target="#b30">[31]</ref> the mixing time of LCCN is at most constantly linear to the inverse of 1 − λ * . Although it is hard for Gibbs sampling to accurately quantify λ * due to the evolving state transition matrix, the recent work <ref type="bibr" target="#b31">[32]</ref> shows Gibbs sampling is efficient enough and almost proportional to the logarithm of the dataset size N . In experiments, we will show LCCN is well converged after same epochs as baselines.</p><p>In statistical learning theory, the excess risk 3 and the error bound w.r.t. the expected risk and Bayes risk, are two impor-tant quantities to measure model generalization performance. In the setting of noisy labels, such two quantities are bounded by the following generalization bound (see the Appendix A)</p><formula xml:id="formula_9">∆ F = sup f θ ∈F E [ 1 (z, f θ (x))] − E (D N ) [ 1 (z, f θ (x))] ,</formula><p>where E [·] and E (D N ) [·] respectively represents the expectation on the clean data distribution and the empirical estimation with the data whose labels are from the Gibbs sampling. Thus, by analyzing the upper bound of ∆ F , we can then understand which factors affect the generalization performance of LCCN. Specifically, we deduce the following theorem to interpret this.</p><p>Theorem 2. Assume f * θ and f † θ respectively are the underlying groundtruth labeling functions X → Y of clean test data and data from the Gibbs sampling. Define the composite function</p><formula xml:id="formula_10">class G = {x → 1 (f θ (x), f θ (x)) : f θ , f θ ∈ F }.</formula><p>Then, for any probability δ &gt; 0, with probability at least 1 − δ,</p><formula xml:id="formula_11">∆ F ≤ ∆ + R(G) + 3ρ ln( 2 δ ) 2N (8) where ∆ = sup f θ ∈F E 1 (f * θ (x) − f † θ (x), f θ (x)) , R(G)</formula><p>is the Rademacher complexity <ref type="bibr" target="#b32">[33]</ref> of G and ρ is the maximum of the ξ-clipped cross entropy loss, i.e., − ln ξ.</p><p>The above theorem indicates the generalization performance of the classifier learned by LCCN depends upon three factors, i.e., the inherent gap ∆ between the noisy training domain and the clean test domain, the function complexity R(G) and the sample number N . In particular, if LCCN can exactly infer all the latent labels of noisy data and eliminate the domain bias, we will have f * θ = f † θ and ∆ = 0. In this case, Eq. (8) will degenerate to the Rademacher bound <ref type="bibr" target="#b33">[34]</ref> after scaling the loss to [0, 1], and equal to the training on the clean data. However, it is usually hard to completely remove the domain bias, since the distribution of the corrected samples could still be different from that of the clean test data. For example, the web data may contain many outlier classes. Thus, ∆ is an important factor to the generalization performance of LCCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Safeguarded Transition Update</head><p>In this section, we will show that our method safeguards the bounded update of the noise transition by a batch of samples, avoiding the arbitrarily tuning via a Softmax layer in <ref type="bibr" target="#b12">[13]</ref>. Theorem 3. Suppose α i is a positive smoothing scalar, N i is the current sample number of the ith category (i=1,. . . ,K), M i is the sum of the sample numbers newly allocated into (positive) and removed from (negative) the ith category after a batch of training samples, and M i is its absolute sum of such two cases. Then, for the transition vector φ i of the ith category, its variation via a training batch is characterized by the following equation,</p><formula xml:id="formula_12">φ new i − φ old i ≤ |r i | + r i 1 + r i<label>(9)</label></formula><p>where r i = Mi Ni+ K j=1 αj and r i = Mi Ni+ K j=1 αj . According to the definition, we have r i &gt; −1, r i ≥ 0 and r i ≥ |r i |.</p><p>Proof. The variation of φ i after a training batch is,</p><formula xml:id="formula_13">φ new i − φ old i = K j=1 φ new ij − φ old ij = K j=1 N ij + α j + M ij N i + K j =1 α j + M i − N ij + α j N i + K j =1 α j ≤ K j=1 (N i + K j =1 α j )M ij + (N ij + α j )M i (N i + K j =1 α j )(N i + K j =1 α j + M i ) = (N i + K j =1 α j ) M i + (N i + K j =1 α j ) M i (N i + K j =1 α j )(N i + K j =1 α j + M i ) = |r i | + r i 1 + r i<label>(10)</label></formula><p>Corollary 3.1. Suppose M is the batch size in the training.</p><p>If it satisfies the condition M N i , we have r i &lt; M Ni in a small scale. Then the variation of φ i after a training batch will be bounded by |ri|+ ri 1+ri ≤ 2 ri 1− ri ≈ 2 r i in a small scale. The core drawback in <ref type="bibr" target="#b12">[13]</ref> is the noise transition modeled by a Softmax layer can be arbitrarily updated via a batch of samples. This is because the gradients of the parameters estimated by a "local" batch can be arbitrarily large in the backpropagation. Then, the noise transition decided by the "global" dataset might be pushed into a bad local minimum by a batch of some extremely noisy training samples, yielding a serious harm on the classifier. The later experimental analysis in <ref type="figure">Fig. 7</ref> will confirm this point. Instead, our dynamic label regression theoretically safeguards the bounded update of the noise transition via a batch of samples. Specifically, with the bounded update, the conditional transition in Equation <ref type="formula" target="#formula_7">(6)</ref> is gradually changing towards at a true distribution when the classifier is well trained. Similarly, with more reliable sampled labels, the classifier is better trained and the noise modeling is refined. Finally, we acquire a virtuous cycle for optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Complexity Analysis</head><p>The learning procedure is summarized in Algorithm 1. Note that, we give the complete implementation including details, like pretraining and warming-up used in the experiments.</p><p>As we know, the stochastic optimization of a DNN model involves two steps, the forward and backward computations. In each mini-batch update, its time complexity is O(M Λ), where M is the mini-batch size and Λ is the parameter size. Here, in Algorithm 1, we additionally add a sampling operation via Eq. (6) whose complexity is O(M + K 2 ) (K is the class size). Note that, the first term in the RHS of Eq. (6) has been computed in the forward procedure. Since M and K is usually significant smaller than Λ, the extra cost for the sampling is negligible compared to O(M Λ). Besides, the optimization for noise modeling in Eq. (3) can be ignored, as this only involves the normalization of a confusion matrix whose complexity is O(K 2 ). In total, since the big-O complexity of each minibatch remains the same, our method is scalable to big data. for batch j = 1 to N/M do <ref type="bibr">5:</ref> Let step=i× N/M +j and hook a batch of samples. <ref type="bibr">6:</ref> if step &lt; δ then <ref type="bibr">7:</ref> Substitute the transition in Equation <ref type="formula" target="#formula_7">(6)</ref> with φ , and then sample z n for each x n in the batch. <ref type="bibr" target="#b7">8</ref>:</p><formula xml:id="formula_14">else 9:</formula><p>Sample z n with Equation <ref type="formula" target="#formula_7">(6)</ref> for the batch. Update the confusion matrix N (·)(·) based on the existing sampling observations {(z n , y n )}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12:</head><p>Optimize Equation <ref type="formula" target="#formula_3">(3)</ref> to learn the classifier f θ and estimate the noise transition matrix φ. <ref type="bibr">13:</ref> end for 14: end for <ref type="bibr">15:</ref> Output the classifier f θ and the noise transition φ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Extensions on Outlier and Semi-supervised Learning</head><p>In this section, we will extend the original model in <ref type="figure" target="#fig_0">Fig. 2</ref> to the generalized version in <ref type="figure" target="#fig_4">Fig. 4</ref>, which shares the optimization procedure but is more useful in the real-world applications.</p><p>Extension on Outlier Learning: In practise, datasets collected from online websites or real-world scenarios, usually contains the open-set label noise <ref type="bibr" target="#b19">[20]</ref>. That means data from other distributions might be disturbed as the given class samples involving in the training. Previous class-conditional noise model <ref type="bibr">[12-14, 18, 35]</ref>, mainly focus on the closed-set label perturbation and thus can not handle the outlier classes. For example, it is impossible to estimate the transition matrix via the mentioned two-step formulation in <ref type="bibr" target="#b11">[12]</ref> since this requires the selection of the representative outlier samples. Similarly, learning the transition matrix by a noise adaptation layer <ref type="bibr" target="#b12">[13]</ref> still suffers from the instability. Therefore, it is useful to extend LCCN to deal with this open-set noisy label setting. Actually, the modification for LCCN only requires to add an outlier choice for the latent variable z, i.e., z ∈ {1, ..., K, K + 1}, where K + 1 indexes the collapsed outlier classes and then change the noise transition from R KxK to R (K+1)xK . The model modification is shown in <ref type="figure" target="#fig_4">Fig. 4</ref> and the the network modification is illustrated in <ref type="figure" target="#fig_3">Fig. 3</ref>. Importantly, the above modifications do not alter the aforementioned deduction.</p><p>Extension on Semi-supervised Learning: It is common to improve the model performance by augmenting the large scale noisy dataset with a small set of clean samples. Many works <ref type="bibr">[10-12, 35, 36]</ref> have leveraged such a semi-supervised setting to calibrate the classifier and achieve a better result. In our model, it is naturally compatible with this case, where we can directly utilize the clean labels instead of labels from sampling when they are available. As illustrated in <ref type="figure" target="#fig_3">Fig. 3</ref>,  this configuration is specifically marked in red in parallel to sampling. The corresponding model modification is indicated in <ref type="figure" target="#fig_4">Fig. 4</ref>. In a broad sense, clean labels can be as accurate as a given category or as weak as a coarse hint that tells outlier or not. In the former case, a standard cross entropy loss can be applied to the classifier; while in the latter case, a collapsed cross entropy loss that defines on outliers vs. non-outliers could be applied. Since this is tightly related to the work on domain adaptation <ref type="bibr" target="#b36">[37]</ref>, we leave the latter case in the future and only validate the former semi-supervised setting in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>The experiments involve both the simulated noisy datasets and the real-world noisy datasets. We verify the performance of our model by comparing with state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets and Baselines</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Datasets:</head><p>We conduct the toy experiments on CIFAR-10, CIFAR-100 and the real-world experiments on Clothing1M and WebVision. CIFAR-10 and CIFAR-100 <ref type="bibr" target="#b37">[38]</ref> respectively consist of 60,000 32x32 color images from 10 and 100 classes. Both of them contain 50,000 training samples and 10,000 test samples. For the toy experiments without outliers, we inject the asymmetric noise to disturb their labels to form the noisy datasets. Concretely, on CIFAR-10, we set a probability r to disturb the label to its similar class, i.e., truck → automobile, bird → airplane, deer → horse, cat → dog. For CIFAR-100, a similar r is set but the label flip only happens in each super-class. The label is randomly disturbed into the next class circularly within the super-classes. For the toy experiments that consider the open-set noisy labels, we randomly select 10,000 samples from the original datasets and shuffle the order of the pixel values as the outliers. In the semi-supervised learning, we utilize the clean labels of the first 5,000 clean samples and the first 500 outlier samples for the training.</p><p>Clothing1M <ref type="bibr" target="#b34">[35]</ref> dataset has 1 million noisy clothes samples collected from the shopping websites. The authors in <ref type="bibr" target="#b34">[35]</ref> predefined 14 categories and assigned the clothes images with the labels extracted from the surrounding text provided by sellers, which thus might be very noisy. According to <ref type="bibr" target="#b34">[35]</ref>, only about 61.54% labels are reliable. Besides, this dataset contains 50k, 14k and 10k clean samples respectively for auxiliary training, validation and test. WebVision 4 <ref type="bibr" target="#b38">[39]</ref> is a more challenging noisy dataset, which contains more than 2.4 million images. It is crawled from the Internet by using the 1,000 concepts of ILSVRC <ref type="bibr" target="#b39">[40]</ref> as queries. In addition, a clean validation set which contains 50,000 annotated images, are provided to boost and validate the proposed models in diverse applications. We use the validation set of ImageNet <ref type="bibr" target="#b39">[40]</ref> as its test set.</p><p>2) Baselines: For the toy experiments, we compare LCCN with the classifier that is directly trained on the dataset (termed as CE), the method Bootstrapping proposed in <ref type="bibr" target="#b17">[18]</ref>, the transition based method Forward <ref type="bibr" target="#b11">[12]</ref> and the method that fine-tunes the transition S-adaptation <ref type="bibr" target="#b12">[13]</ref>. Note that, we choose the hard mode for Bootstrapping, since it is empirically better than the soft mode. In the outlier corrupted datasets, we denote the extension of our model that considers the outlier as LCCN*. In the semi-supervised learning setting, we denote our model as LCCN+, meaning the clean samples are used in the training. For the experiments on real-world datasets, we also report the result of Joint Optimization <ref type="bibr" target="#b25">[26]</ref> that leverages the auxiliary noisy label distribution and the state-of-the-art result Forward+ <ref type="bibr" target="#b11">[12]</ref> that finetunes on clean samples. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>For CIFAR-10 and CIFAR-100, the PreAct ResNet-32 [41] is adopted as the classifier. The image data is augmented by horizontal random flip and 32×32 random crops after padding with 4 pixels. Then, the per-image standardization is used to normalize pixel values. For the optimizer, we utilize SGD with a momentum of 0.9 and a weight decay of 0.0005. The batch size is set to 128. The training runs totally 120 epochs and is divided into three phases in 40 and 80 epochs. In these three phases, we respectively set the learning rate as 0.5, 0.1 and 0.01. Note that, the reason that we adopt a large learning rate (others may set the learning rate smaller than 0.001), is that the small learning rate will lead to overfitting on the noisy dataset as claimed in <ref type="bibr" target="#b7">[8]</ref>. Following the benchmark in <ref type="bibr" target="#b11">[12]</ref>, we use CE to initialize the classifier in other baselines and LCCN. For S-adaptation, the following transition is computed to warm-up the transition parameters in the first 80 epochs.</p><formula xml:id="formula_15">φ ij = Σ t 1 yt=j p(z t = i|x t ) Σ t p(z t = i|x t )<label>(11)</label></formula><p>Similarly on CIFAR-10, we use above transition to warm up the sampling procedure in LCCN for the first 20,000 steps. However, on CIFAR-100, we set φ ij = 1[i = j] in the warming-up since Equation (11) will induce the high sampling variance and need long time to converge. For Clothing1M and WebVision, the ResNet-50 is leveraged as the classifier. We resize the short side of their images to 224 and do the random crop of 224×224. The training images are augmented with the random flip, whiteness and saturation. For the optimizer, we deploy SGD with a momentum of 0.9 with a weight decay of 10 −3 . The batch size for Clothing1M is set to 32 and we fix the learning rate as 0.001 to run 5 epochs. For the warming-up transition, we both validate the one <ref type="bibr" target="#b34">[35]</ref> from manual annotation and the one estimated by Equation <ref type="formula" target="#formula_0">(11)</ref> for 40,000 steps. Note that, on the large real-world datasets, due to the strong capacity of ResNet-50, it is easy for LCCN to occur the sampling collapsed problem, i.e., the sampled latent label is identical to the noisy label. Thus, we norm Equation <ref type="formula" target="#formula_7">(6)</ref> with a power annealed coefficient max{exp (− step max step * 0.8), 0.5} to introduce the sufficient perturbation in avoid of this issue. On WebVision, the batch size is set to 128, and the learning rate is initialized with 0.1 is divided by 10 every 30 epochs until 90 epochs. We use the diagonal transition for 10,000 steps of warming-up and then update the confusion matrix to the end, since it contains 1,000 categories. The similar powerannealed strategy for sampling is leveraged. Finally, to fairly compare LCCN+ and Forward+ in semi-supervised learning, we use the similar fine-tuning in <ref type="bibr" target="#b11">[12]</ref> to run the experiments.</p><p>C. Results on CIFAR10 and CIFAR-100 1) Classification experiments: <ref type="table" target="#tab_1">Table I</ref> summarizes the performance of LCCN and baselines on the noisy datasets without outilers. Compared with the baselines, LCCN achieves the best performance at most of noise levels. In particular, even in the large noise rates, our model still acquires the competitive classification accuracy. For example, when r=0.7 on CIFAR-10 and r=0.4 on CIFAR-100, LCCN reaches 79.48% and 65.52%, outperforming the best results of baselines by about 7% and 13% respectively. This demonstrates that our model is significantly better than baselines. Regarding r=0.5 on CIFAR-100, the way to disturb the labels <ref type="bibr" target="#b11">[12]</ref> leads that there is one undesired minimum, since two classes are mixed into one class by equal quota after injecting noise. In this case, it is hard to say which model can achieve the best result. We include it here for the complete comparison as in other works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. <ref type="figure">Fig. 5</ref>. The colormap of the confusion matrix on CIFAR-10 with r=0.5. We utilize the log-scale for each element in the confusion matrix for the fine-grained visualization. The left three maps are respectively learned by LCCN at the beginning, 30,000 step and the end, and the right one is the groundtruth.   <ref type="table" target="#tab_1">Table I</ref>, all the methods have a slight performance drop. Nevertheless, LCCN achieved the best performance in such an setting at r=0.3, 0.5, 0.7 on CIFAR-10 and r=0.1, 0.2, 0.3 and 0.4 on CIFAR-100. The baselines without the outlier detection mechanism usually have a significant degeneration. Specifically, on CIFAR-10, all the other baselines are even not better than CE, i.e., directly training. Instead, LCCN* that considers the outlier achieves a further improvement based on LCCN. Besides, as expected, by adding clean data, LCCN+ performs better than LCCN*, since the classifier is calibrated by the information of the clean data domain. In the scenario of the extreme noise, as marked by the grey color in <ref type="table" target="#tab_1">Table II</ref>, this is quite useful and even necessary to guarantee an acceptable performance. In total, according to the quantitative analysis of <ref type="table" target="#tab_1">Table I and Table II</ref>, we demonstrate the superiority of LCCN compared to baselines on toy datasets.</p><p>2) On convergence visualization: In <ref type="figure" target="#fig_5">Fig. 6</ref>, we trace the training of LCCN on CIFAR-10 to visualize its convergence. As can been in the left panel of <ref type="figure" target="#fig_5">Fig. 6</ref>, LCCN has a stable convergence on loss after the given epochs. Besides, we find the loss converges to irregular scales in different noise rates. Concretely, in most cases, i.e., r=0.1, 0.3, 0.5, the final training loss increases as r increases, while the loss shows attenuation as r &gt;0.5. It is because in the low-level noise, the model can easily correct the labels via the sampling in LCCN, yielding a small loss. While in the case of the extreme noise, it is more challenging to prevent the model from fitting on noise, which incurs difficulty for optimization and thus achieves a big loss. Furthermore, according to the right penal of <ref type="figure" target="#fig_5">Fig. 6</ref>, the test accuracy also approximately converges and persists to the end of the training without performance drop. Actually, it is not a common phenomenon for previous methods to own this merit, since all baselines tends to overfitting on noise more or less in the final few epochs. This demonstrates the advantages of LCCN in the robust training with the noisy datasets.</p><p>3) Safeguarded transition update: To show LCCN safeguards the noise transition update compared to S-adaptation, we compute the statistics about their update of noise transition on CIFAR-10 at r=0.5, and illustrate the histogram of changes in <ref type="figure">Fig. 7</ref>. Firstly, from the left panel of <ref type="figure">Fig. 7</ref>, we can see that there is a significant performance drop in the training of S-adaptation. The clue to this phenomenon can be found by inspecting the update of noise transition. As shown in the right panel of <ref type="figure">Fig. 7</ref>, the change magnitude of φ in S-adaptation is higher than that of LCCN. One is in a large scale ranging from 0 to 16, while the other one is in a very small scale ranging from 0 to 0.02. This leads to S-adaptation suffering from a high risk of over-tuning to undesired local minimums in the <ref type="figure">Fig. 9</ref>. Some representative samples in the training set that are considered as the outliers by LCCN*. We intuitively summarize these photos into four categories based on their contents, multiple different objects (RED), implicit categories (GREEN), uncertain types (BLACK) and confusing appearance (BLUE), which are respectively marked by the color of the surrounded boxes. Outliers are relative to LCCN and may contain hard examples of the pre-defined 14 categories. presence of noise. Instead, according to the histogram, LCCN updates φ in a safeguarded small scale when approaching to the minimum. In summary, this quantitative analysis confirms the claim of our Theorem 3 in the perspective of experiments. 4) The latent label and noise analysis: <ref type="figure">Fig. 5</ref> and <ref type="figure">Fig. 8</ref> respectively depict the colormap of the confusion matrix and the label correction ratio when training LCCN on CIFAR-10 with r=0.5. First, as can be seen in <ref type="figure">Fig. 5</ref>, the initial confusion matrix does not approach the true matrix and there are many incorrect entries. However, with the training progressing, the matrix is gradually corrected and at the end of training, it is approximately similar to the provided groundtruth. Besides, As shown in <ref type="figure">Fig. 8</ref>, the ratio of the image with the correct label increases along with the training progress. This reflects LCCN successfully models the class-conditional noise and gradually infer the latent labels. Specially, by visualizing the mis-corrected examples in the training process, we can find that the classifier at first make mistakes in even some simple samples, while finally has the wrong classification in only the hard examples. These two figures visualize how the dynamic label regression optimizes LCCN to infer the latent label and model the noise.   in this dataset, even though they use the annotated noise transition matrix <ref type="bibr" target="#b34">[35]</ref>. And S-adaptation only improves Forward by 0.5%. Joint Optimization that trains the classifier with label correction <ref type="bibr" target="#b25">[26]</ref> achieves better results than the other baselines. Nevertheless, this method requires the provided noisy label distribution to prevent degeneration and is not scalable to the large number of classes <ref type="bibr" target="#b25">[26]</ref>. Instead, LCCN that contains both the label correction and Bayesian noise modeling, gets the competitive performance 71.63%. With the warming-up of the auxiliary noise transition <ref type="bibr" target="#b34">[35]</ref>, it further achieves the best 73.07%. Even although there is no auxiliary information available, our extension LCCN* still outperforms the current stateof-the-art result. This demonstrates the potential of LCCN in handling the real-world noisy dataset. In addition, the results of LCCN+ indicates our model also has the advantages in the semi-supervised learning setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Results on Clothing1M and WebVision</head><p>In <ref type="table" target="#tab_1">Table IV</ref>, we present the noise transition matrix learned by LCCN*, where only significant transition probabilities are marked. From this noise transition, we can find the training samples in some classes are very noisy. For example, knitwear and sweater are two classes which transit most of labels to other classes. This is because such two classes usually occur with other categories like jacket or shawl in the common dress collocation, which may incur the label transition according to the visual appearance. Besides, in <ref type="table" target="#tab_1">Table IV</ref>, we can observe the outlier transition to find which class contains a lot of outliers. Furthermore, to better understand the outliers, we give some represenative samples in <ref type="figure">Fig. 9</ref>. According to the image contents, we intuitively summarize the outlier into four sub-classes, multiple different objects, implicit categories, uncertain types and confusing appearance. As can be seen, it is usually improper to asign an unique label to these outliers, since some may contain multiple kinds of clothes. And in some challenging cases, e.g., the images in the blue box in <ref type="figure">Fig. 9</ref> , the hard example is also considered as the outliers by LCCN*, even if the label is correct. Actually, this can be seen as the imperfect sample for training or the potential drawback of our model that requires more explore in the future research.  <ref type="table" target="#tab_6">Table V</ref> gives the performance of LCCN and baselines on a more challenging noisy dataset WebVision. Both Top-1 and Top-5 accuracies are reported in the experiment. According to the results either in the perspective of Top-1 accuracy or Top-5 accuracy, LCCN achieves the best performance. Similarly, LCCN* and LCCN+ respectively have the further refinement based on LCCN. Nevertheless, as can be seen, the results of all methods do not present the significant gap. One possible explanation is that this task couples two challenging sub-tasks, perfectly decoupling the clean samples from the noisy dataset in the 1000 classes and well fitting the clean samples in the 1000 classes. From the current limiting performance of image recognition in ImageNet <ref type="bibr" target="#b39">[40]</ref>, we know either sub-task mentioned above needs a long way to go for a satisfying result in such a large-scale challenging scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION AND FUTURE WORK</head><p>In this paper, we present a Latent Class-Conditional Noise model to learn with the noisy supervision. Besides, a dynamic label regression method is deployed for LCCN to iteratively infer the latent labels and jointly train the classifier and model the noise. The theoretical analysis on the model convergence and the essential gap between training and test are also provided. Most importantly, we demonstrate that our method safeguards the bounded update of the noise transition to avoid previous arbitrarily tuning via a mini-batch of samples. Finally, we generalize our model to the open-set noisy labels setting and the semi-supervised learning setting. However, although we have shown the advantages of LCCN in a range of experiments with the generalized noisy supervision, other specific settings that considers more complex noise, e.g., image content based noise, could be explored. Besides, it is important to explore more effective models on the large scale noisy dataset. To the end, more works based on LCCN can be extended to train with noisy datasets.</p><p>The second term ∆ disc in the right-hand side of Eq. (14) is the popular discrepancy distance. It has been demonstrated by the following Rademacher bound <ref type="bibr" target="#b33">[34]</ref> for any probability δ &gt; 0,</p><formula xml:id="formula_16">∆ disc ≤ R(G) + 3ρ ln( 2 δ ) 2N ,</formula><p>where G is defined by the composite functional class {x → 1 (f θ (x), f θ (x)) : f θ , f θ ∈ F} and N is the sample number. Then, combined with the given Rademacher bound, we finally proof the Theorem 2. i.e., for any probability δ, the generalization bound of the models for learning with noisy labels is</p><formula xml:id="formula_17">∆ F ≤ ∆ + R(G) + 3ρ ln( 2 δ ) 2N</formula><p>.</p><p>This bound theoretically points out three important factors to affect our model generalization performance, i.e., domain gap, the function complexity and the support sample number.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>xFig. 2 .</head><label>2</label><figDesc>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " f 2 y z i m w b R / D g j z p 6 t Z 3 6 0 f H R q N I = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c W 7 A e 0 o W y 2 k 3 b t Z h N 2 N 2 I J / Q V e P C j i 1 Z / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 H 1 F p H s t 7 M 0 n Q j + h Q 8 p A z a q z U e O q X K 2 7 V n Y O s E i 8 n F c h R 7 5 e / e o O Y p R F K w w T V u u u 5 i f E z q g x n A q e l X q o x o W x M h 9 i 1 V N I I t Z / N D 5 2 S M 6 s M S B g r W 9 K Q u f p 7 I q O R 1 p M o s J 0 R N S O 9 7 M 3 E / 7 x u a s J r P + M y S Q 1 K t l g U p o K Y m M y + J g O u k B k x s Y Q y x e 2 t h I 2 o o s z Y b E o 2 B G / 5 5 V X S u q h 6 b t V r X F Z q N 3 k c R T i B U z g H D 6 6 g B n d Q h y Y w Q H i G V 3 h z H p w X 5 9 3 5 W L Q W n H z m G P 7 A + f w B 5 j m M / A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " f 2 y z i m w b R / D g j z p 6 t Z 3 6 0 f H R q N I = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c W 7 A e 0 o W y 2 k 3 b t Z h N 2 N 2 I J / Q V e P C j i 1 Z / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 H 1 F p H s t 7 M 0 n Q j + h Q 8 p A z a q z U e O q X K 2 7 V n Y O s E i 8 n F c h R 7 5 e / e o O Y p R F K w w T V u u u 5 i f E z q g x n A q e l X q o x o W x M h 9 i 1 V N I I t Z / N D 5 2 S M 6 s M S B g r W 9 K Q u f p 7 I q O R 1 p M o s J 0 R N S O 9 7 M 3 E / 7 x u a s J r P + M y S Q 1 K t l g U p o K Y m M y + J g O u k B k x s Y Q y x e 2 t h I 2 o o s z Y b E o 2 B G / 5 5 V X S u q h 6 b t V r X F Z q N 3 k c R T i B U z g H D 6 6 g B n d Q h y Y w Q H i G V 3 h z H p w X 5 9 3 5 W L Q W n H z m G P 7 A + f w B 5 j m M / A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " f 2 y z i m w b R / D g j z p 6 t Z 3 6 0 f H R q N I = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c W 7 A e 0 o W y 2 k 3 b t Z h N 2 N 2 I J / Q V e P C j i 1 Z / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 H 1 F p H s t 7 M 0 n Q j + h Q 8 p A z a q z U e O q X K 2 7 V n Y O s E i 8 n F c h R 7 5 e / e o O Y p R F K w w T V u u u 5 i f E z q g x n A q e l X q o x o W x M h 9 i 1 V N I I t Z / N D 5 2 S M 6 s M S B g r W 9 K Q u f p 7 I q O R 1 p M o s J 0 R N S O 9 7 M 3 E / 7 x u a s J r P + M y S Q 1 K t l g U p o K Y m M y + J g O u k B k x s Y Q y x e 2 t h I 2 o o s z Y b E o 2 B G / 5 5 V X S u q h 6 b t V r X F Z q N 3 k c R T i B U z g H D 6 6 g B n d Q h y Y w Q H i G V 3 h z H p w X 5 9 3 5 W L Q W n H z m G P 7 A + f w B 5 j m M / A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " f 2 y z i m w b R / D g j z p 6 t Z 3 6 0 f H R q N I = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c W 7 A e 0 o W y 2 k 3 b t Z h N 2 N 2 I J / Q V e P C j i 1 Z / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 H 1 F p H s t 7 M 0 n Q j + h Q 8 p A z a q z U e O q X K 2 7 V n Y O s E i 8 n F c h R 7 5 e / e o O Y p R F K w w T V u u u 5 i f E z q g x n A q e l X q o x o W x M h 9 i 1 V N I I t Z / N D 5 2 S M 6 s M S B g r W 9 K Q u f p 7 I q O R 1 p M o s J 0 R N S O 9 7 M 3 E / 7 x u a s J r P + M y S Q 1 K t l g U p o K Y m M y + J g O u k B k x s Y Q y x e 2 t h I 2 o o s z Y b E o 2 B G / 5 5 V X S u q h 6 b t V r X F Z q N 3 k c R T i B U z g H D 6 6 g B n d Q h y Y w Q H i G V 3 h z H p w X 5 9 3 5 W L Q W n H z m G P 7 A + f w B 5 j m M / A = = &lt; / l a t e x i t &gt; y &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l 2 9 W x o U b 9 D E b v m h L G 7 j H t Z 0 O U 2 4 = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k I e i x 6 8 d i C / Y A 2 l M 1 2 0 q 7 d b M L u R g i h v 8 C L B 0 W 8 + p O 8 + W / c t j l o 6 4 O B x 3 s z z M w L E s G 1 c d 1 v Z 2 1 9 Y 3 N r u 7 R T 3 t 3 b P z i s H B 2 3 d Z w q h i 0 W i 1 h 1 A 6 p R c I k t w 4 3 A b q K Q R o H A T j C 5 m / m d J 1 S a x / L B Z A n 6 E R 1 J H n J G j Z W a 2 a B S d W v u H G S V e A W p Q o H G o P L V H 8 Y s j V A a J q j W P c 9 N j J 9 T Z T g T O C 3 3 U 4 0 J Z R M 6 w p 6 l k k a o / X x + 6 J S c W 2 V I w l j Z k o b M 1 d 8 T O Y 2 0 z q L A d k b U j P W y N x P / 8 3 q p C W / 8 n M s k N S j Z Y l G Y C m J i M v u a D L l C Z k R m C W W K 2 1 s J G 1 N F m b H Z l G 0 I 3 v L L q 6 R 9 W f P c m t e 8 q t Z v i z h K c A p n c A E e X E M d 7 q E B L W C A 8 A y v 8 O Y 8 O i / O u / O x a F 1 z i p k T + A P n 8 w f n v Y z 9 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l 2 9 W x o U b 9 D E b v m h L G 7 j H t Z 0 O U 2 4 = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k I e i x 6 8 d i C / Y A 2 l M 1 2 0 q 7 d b M L u R g i h v 8 C L B 0 W 8 + p O 8 + W / c t j l o 6 4 O B x 3 s z z M w L E s G 1 c d 1 v Z 2 1 9 Y 3 N r u 7 R T 3 t 3 b P z i s H B 2 3 d Z w q h i 0 W i 1 h 1 A 6 p R c I k t w 4 3 A b q K Q R o H A T j C 5 m / m d J 1 S a x / L B Z A n 6 E R 1 J H n J G j Z W a 2 a B S d W v u H G S V e A W p Q o H G o P L V H 8 Y s j V A a J q j W P c 9 N j J 9 T Z T g T O C 3 3 U 4 0 J Z R M 6 w p 6 l k k a o / X x + 6 J S c W 2 V I w l j Z k o b M 1 d 8 T O Y 2 0 z q L A d k b U j P W y N x P / 8 3 q p C W / 8 n M s k N S j Z Y l G Y C m J i M v u a D L l C Z k R m C W W K 2 1 s J G 1 N F m b H Z l G 0 I 3 v L L q 6 R 9 W f P c m t e 8 q t Z v i z h K c A p n c A E e X E M d 7 q E B L W C A 8 A y v 8 O Y 8 O i / O u / O x a F 1 z i p k T + A P n 8 w f n v Y z 9 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l 2 9 W x o U b 9 D E b v m h L G 7 j H t Z 0 O U 2 4 = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k I e i x 6 8 d i C / Y A 2 l M 1 2 0 q 7 d b M L u R g i h v 8 C L B 0 W 8 + p O 8 + W / c t j l o 6 4 O B x 3 s z z M w L E s G 1 c d 1 v Z 2 1 9 Y 3 N r u 7 R T 3 t 3 b P z i s H B 2 3 d Z w q h i 0 W i 1 h 1 A 6 p R c I k t w 4 3 A b q K Q R o H A T j C 5 m / m d J 1 S a x / L B Z A n 6 E R 1 J H n J G j Z W a 2 a B S d W v u H G S V e A W p Q o H G o P L V H 8 Y s j V A a J q j W P c 9 N j J 9 T Z T g T O C 3 3 U 4 0 J Z R M 6 w p 6 l k k a o / X x + 6 J S c W 2 V I w l j Z k o b M 1 d 8 T O Y 2 0 z q L A d k b U j P W y N x P / 8 3 q p C W / 8 n M s k N S j Z Y l G Y C m J i M v u a D L l C Z k R m C W W K 2 1 s J G 1 N F m b H Z l G 0 I 3 v L L q 6 R 9 W f P c m t e 8 q t Z v i z h K c A p n c A E e X E M d 7 q E B L W C A 8 A y v 8 O Y 8 O i / O u / O x a F 1 z i p k T + A P n 8 w f n v Y z 9 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l 2 9 W x o U b 9 D E b v m h L G 7 j H t Z 0 O U 2 4 = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k I e i x 6 8 d i C / Y A 2 l M 1 2 0 q 7 d b M L u R g i h v 8 C L B 0 W 8 + p O 8 + W / c t j l o 6 4 O B x 3 s z z M w L E s G 1 c d 1 v Z 2 1 9 Y 3 N r u 7 R T 3 t 3b P z i s H B 2 3 d Z w q h i 0 W i 1 h 1 A 6 p R c I k t w 4 3 A b q K Q R o H A T j C 5 m / m d J 1 S a x / L B Z A n 6 E R 1 J H n J G j Z W a 2 a B S d W v u H G S V e A W p Q o H G o P L V H 8 Y s j V A a J q j W P c 9 N j J 9 T Z T g T O C 3 3 U 4 0 J Z R M 6 w p 6 l k k a o / Xx + 6 J S c W 2 V I w l j Z k o b M 1 d 8 T O Y 2 0 z q L A d k b U j P W y N x P / 8 3 q p C W / 8 n M s k N S j Z Y l G Y C m J i M v u a D L l C Z k R m C W W K 2 1 s J G 1 N F m b H Z l G 0 I 3 v L L q 6 R 9 W f P c mt e 8 q t Z v i z h K c A p n c A E e X E M d 7 q E B L W C A 8 A y v 8 O Y 8 O i / O u / O x a F 1 z i p k T +A P n 8 w f n v Y z 9 &lt; / l a t e x i t &gt; z &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H D z X c h l s P l m u E y Z Z / 9 z F J + i V C 6 I = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c W 7 A e 0 o W y 2 k 3 b t Z h N 2 N 0 I N / Q V e P C j i 1 Z / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 H 1 F p H s t 7 M 0 n Q j + h Q 8 p A z a q z U e O q X K 2 7 V n Y O s E i 8 n F c h R 7 5 e / e o O Y p R F K w w T V u u u 5 i f E z q g x n A q e l X q o x o W x M h 9 i 1 V N I I t Z / N D 5 2 S M 6 s M S B g r W 9 K Q u f p 7 I q O R 1 p M o s J 0 R N S O 9 7 M 3 E / 7 x u a s J r P + M y S Q 1 K t l g U p o K Y m M y + J g O u k B k x s Y Q y x e 2 t h I 2 o o s z Y b E o 2 B G / 5 5 V X S u q h 6 b t V r X F Z q N 3 k c R T i B U z g H D 6 6 g B n d Q h y Y w Q H i G V 3 h z H p w X 5 9 3 5 W L Q W n H z m G P 7 A + f w B 6 U G M / g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H D z X c h l s P l m u E y Z Z / 9 z F J + i V C 6 I = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c W 7 A e 0 o W y 2 k 3 b t Z h N 2 N 0 I N / Q V e P C j i 1 Z / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 H 1 F p H s t 7 M 0 n Q j + h Q 8 p A z a q z U e O q X K 2 7 V n Y O s E i 8 n F c h R 7 5 e / e o O Y p R F K w w T V u u u 5 i f E z q g x n A q e l X q o x o W x M h 9 i 1 V N I I t Z / N D 5 2 S M 6 s M S B g r W 9 K Q u f p 7 I q O R 1 p M o s J 0 R N S O 9 7 M 3 E / 7 x u a s J r P + M y S Q 1 K t l g U p o K Y m M y + J g O u k B k x s Y Q y x e 2 t h I 2 o o s z Y b E o 2 B G / 5 5 V X S u q h 6 b t V r X F Z q N 3 k c R T i B U z g H D 6 6 g B n d Q h y Y w Q H i G V 3 h z H p w X 5 9 3 5 W L Q W n H z m G P 7 A + f w B 6 U G M / g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H D z X c h l s P l m u E y Z Z / 9 z F J + i V C 6 I = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c W 7 A e 0 o W y 2 k 3 b t Z h N 2 N 0 I N / Q V e P C j i 1 Z / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 H 1 F p H s t 7 M 0 n Q j + h Q 8 p A z a q z U e O q X K 2 7 V n Y O s E i 8 n F c h R 7 5 e / e o O Y p R F K w w T V u u u 5 i f E z q g x n A q e l X q o x o W x M h 9 i 1 V N I I t Z / N D 5 2 S M 6 s M S B g r W 9 K Q u f p 7 I q O R 1 p M o s J 0 R N S O 9 7 M 3 E / 7 x u a s J r P + M y S Q 1 K t l g U p o K Y m M y + J g O u k B k x s Y Q y x e 2 t h I 2 o o s z Y b E o 2 B G / 5 5 V X S u q h 6 b t V r X F Z q N 3 k c R T i B U z g H D 6 6 g B n d Q h y Y w Q H i G V 3 h z H p w X 5 9 3 5 W L Q W n H z m G P 7 A + f w B 6 U G M / g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H D z X c h l s P l m u E y Z Z / 9 z F J + i V C 6 I = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c W 7 A e 0 o W y 2 k 3 b t Z h N 2 N 0 I N / Q V e P C j i 1 Z / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 H 1 F p H s t 7 M 0 n Q j + h Q 8 p A z a q z U e O q X K 2 7 V n Y O s E i 8 n F c h R 7 5 e / e o O Y p R F K w w T V u u u 5 i f E z q g x n A q e l X q o x o W x M h 9 i 1 V N I I t Z / N D 5 2 S M 6 s M S B g r W 9 K Q u f p 7 I q O R 1 p M o s J 0 R N S O 9 7 M 3 E / 7 x u a s J r P + M y S Q 1 K t l g U p o K Y m M y + J g O u k B k x s Y Q y x e 2 t h I 2 o o s z Y b E o 2 B G / 5 5 V X S u q h 6 b t V r X F Z q N 3 k c R T i B U z g H D 6 6 g B n d Q h y Y w Q H i G V 3 h z H p w X 5 9 3 5 W L Q W n H z m G P 7 A + f w B 6 U G M / g = = &lt; / l a t e x i t &gt; N &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F s F o h X Q j c 3 O A / B B A i 7 J X P 9 T M C N 0 = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i y d p w X 5 A G 8 p m O 2 n X b j Z h d y O U 0 F / g x Y M i X v 1 J 3 v w 3 b t s c t P X B w O O 9 G W b m B Y n g 2 r j u t 1 N Y W 9 / Y 3 C p u l 3 Z 2 9 / Y P y o d H L R 2 n i m G T x S J W n Y B q F F x i 0 3 A j s J M o p F E g s B 2 M b 2 d + + w m V 5 r F 8 M J M E / Y g O J Q 8 5 o 8 Z K j f t + u e J W 3 T n I K v F y U o E c 9 X 7 5 q z e I W R q h N E x Q r b u e m x g / o 8 p w J n B a 6 q U a E 8 r G d I h d S y W N U P v Z / N A p O b P K g I S x s i U N m a u / J z I a a T 2 J A t s Z U T P S y 9 5 M / M / r p i a 8 9 j M u k 9 S g Z I t F Y S q I i c n s a z L g C p k R E 0 s o U 9 z e S t i I K s q M z a Z k Q / C W X 1 4 l r Y u q 5 1 a 9 x m W l d p P H U Y Q T O I V z 8 O A K a n A H d W g C A 4 R n e I U 3 5 9 F 5 c d 6 d j 0 V r w c l n j u E P n M 8 f p p G M 0 g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F s F o h X Q j c 3 O A / B B A i 7 J X P 9 T M C N 0 = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i y d p w X 5 A G 8 p m O 2 n X b j Z h d y O U 0 F / g x Y M i X v 1 J 3 v w 3 b t s c t P X B w O O 9 G W b m B Y n g 2 r j u t 1 N Y W 9 / Y 3 C p u l 3 Z 2 9 / Y P y o d H L R 2 n i m G T x S J W n Y B q F F x i 0 3 A j s J M o p F E g s B 2 M b 2 d + + w m V 5 r F 8 M J M E / Y g O J Q 8 5 o 8 Z K j f t + u e J W 3 T n I K v F y U o E c 9 X 7 5 q z e I W R q h N E x Q r b u e m x g / o 8 p w J n B a 6 q U a E 8 r G d I h d S y W N U P v Z / N A p O b P K g I S x s i U N m a u / J z I a a T 2 J A t s Z U T P S y 9 5 M / M / r p i a 8 9 j M u k 9 S g Z I t F Y S q I i c n s a z L g C p k R E 0 s o U 9 z e S t i I K s q M z a Z k Q / C W X 1 4 l r Y u q 5 1 a 9 x m W l d p P H U Y Q T O I V z 8 O A K a n A H d W g C A 4 R n e I U 3 5 9 F 5 c d 6 d j 0 V r w c l n j u E P n M 8 f p p G M 0 g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F s F o h X Q j c 3 O A / B B A i 7 J X P 9 T M C N 0 = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i y d p w X 5 A G 8 p m O 2 n X b j Z h d y O U 0 F / g x Y M i X v 1 J 3 v w 3 b t s c t P X B w O O 9 G W b m B Y n g 2 r j u t 1 N Y W 9 / Y 3 C p u l 3 Z 2 9 / Y P y o d H L R 2 n i m G T x S J W n Y B q F F x i 0 3 A j s J M o p F E g s B 2 M b 2 d + + w m V 5 r F 8 M J M E / Y g O J Q 8 5 o 8 Z K j f t + u e J W 3 T n I K v F y U o E c 9 X 7 5 q z e I W R q h N E x Q r b u e m x g / o 8 p w J n B a 6 q U a E 8 r G d I h d S y W N U P v Z / N A p O b P K g I S x s i U N m a u / J z I a a T 2 J A t s Z U T P S y 9 5 M / M / r p i a 8 9 j M u k 9 S g Z I t F Y S q I i c n s a z L g C p k R E 0 s o U 9 z e S t i I K s q M z a Z k Q / C W X 1 4 l r Y u q 5 1 a 9 x m W l d p P H U Y Q T O I V z 8 O A K a n A H d W g C A 4 R n e I U 3 5 9 F 5 c d 6 d j 0 V r w c l n j u E P n M 8 f p p G M 0 g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F s F o h X Q j c 3 O A / B B A i 7 J X P 9 T M C N 0 = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i y d p w X 5 A G 8 p m O 2 n X b j Z h d y O U 0 F / g x Y M i X v 1 J 3 v w 3 b t s c t P X B w O O 9 G W b m B Y n g 2 r j u t 1 N Y W 9 / Y 3 C p u l 3 Z 2 9 / Y P y o d H L R 2 n i m G T x S J W n Y B q F F x i 0 3 A j s J M o p F E g s B 2 M b 2 d + + w m V 5 r F 8 M J M E / Y g O J Q 8 5 o 8 Z K j f t + u e J W 3 T n I K v F y U o E c 9 X 7 5 q z e I W R q h N E x Q r b u e m x g / o 8 p w J n B a 6 q U a E 8 r G d I h d S y W N U P v Z / N A p O b P K g I S x s i U N m a u / J z I a a T 2 J A t s Z U T P S y 9 5 M / M / r p i a 8 9 j M u k 9 S g Z I t F Y S q I i c n s a z L g C p k R E 0 s o U 9 z e S t i I K s q M z a Z k Q / C W X 1 4 l r Y u q 5 1 a 9 x m W l d p P H U Y Q T O I V z 8 O A K a n A H d W g C A 4 R n e I U 3 5 9 F 5 c d 6 d j 0 V r w c l n j u E P n M 8 f p p G M 0 g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " v M C m C B v r Q q v M k c Z e 5 c 7 8 y K 1 9 q u 0 = " &gt; A A A B 6 3 i c b V B N S w M x E J 3 U r 1 q / q h 6 9 B I v g q e y K o M e i F 4 8 V b C 2 0 S 8 m m 2 W 5 o k l 2 S r F C W / g U v H h T x 6 h / y 5 r 8 x 2 + 5 B W x 8 M P N 6 b Y W Z e m A p u r O d 9 o 8 r a + s b m V n W 7 t r O 7 t 3 9 Q P z z q m i T T l H V o I h L d C 4 l h g i v W s d w K 1 k s 1 I z I U 7 D G c 3 B b + 4 x P T h i f q w U 5 T F k g y V j z i l N h C G q Q x H 9 Y b X t O b A 6 8 S v y Q N K N E e 1 r 8 G o 4 R m k i l L B T G m 7 3 u p D X K i L a e C z W q D z L C U 0 A k Z s 7 6 j i k h m g n x + 6 w y f O W W E o 0 S 7 U h b P 1 d 8 T O Z H G T G X o O i W x s V n 2 C v E / r 5 / Z 6 D r I u U o z y x R d L I o y g W 2 C i 8 f x i G t G r Z g 6 Q q j m 7 l Z M Y 6 I J t S 6 e m g v B X 3 5 5 l X Q v m r 7 X 9 O 8 v G 6 2 b M o 4 q n M A p n I M P V 9 C C O 2 h D B y j E 8 A y v 8 I Y k e k H v 6 G P R W k H l z D H 8 A f r 8 A R O I j j 8 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " v M C m C B v r Q q v M k c Z e 5 c 7 8 y K 1 9 q u 0 = " &gt; A A A B 6 3 i c b V B N S w M x E J 3 U r 1 q / q h 6 9 B I v g q e y K o M e i F 4 8 V b C 2 0 S 8 m m 2 W 5 o k l 2 S r F C W / g U v H h T x 6 h / y 5 r 8 x 2 + 5 B W x 8 M P N 6 b Y W Z e m A p u r O d 9 o 8 r a + s b m V n W 7 t r O 7 t 3 9 Q P z z q m i T T l H V o I h L d C 4 l h g i v W s d w K 1 k s 1 I z I U 7 D G c 3 B b + 4 x P T h i f q w U 5 T F k g y V j z i l N h C G q Q x H 9 Y b X t O b A 6 8 S v y Q N K N E e 1 r 8 G o 4 R m k i l L B T G m 7 3 u p D X K i L a e C z W q D z L C U 0 A k Z s 7 6 j i k h m g n x + 6 w y f O W W E o 0 S 7 U h b P 1 d 8 T O Z H G T G X o O i W x s V n 2 C v E / r 5 / Z 6 D r I u U o z y x R d L I o y g W 2 C i 8 f x i G t G r Z g 6 Q q j m 7 l Z M Y 6 I J t S 6 e m g v B X 3 5 5 l X Q v m r 7 X 9 O 8 v G 6 2 b M o 4 q n M A p n I M P V 9 C C O 2 h D B y j E 8 A y v 8 I Y k e k H v 6 G P R W k H l z D H 8 A f r 8 A R O I j j 8 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " v M C m C B v r Q q v M k c Z e 5 c 7 8 y K 1 9 q u 0 = " &gt; A A A B 6 3 i c b V B N S w M x E J 3 U r 1 q / q h 6 9 B I v g q e y K o M e i F 4 8 V b C 2 0 S 8 m m 2 W 5 o k l 2 S r F C W / g U v H h T x 6 h / y 5 r 8 x 2 + 5 B W x 8 M P N 6 b Y W Z e m A p u r O d 9 o 8 r a + s b m V n W 7 t r O 7 t 3 9 Q P z z q m i T T l H V o I h L d C 4 l h g i v W s d w K 1 k s 1 I z I U 7 D G c 3 B b + 4 x P T h i f q w U 5 T F k g y V j z i l N h C G q Q x H 9 Y b X t O b A 6 8 S v y Q N K N E e 1 r 8 G o 4 R m k i l L B T G m 7 3 u p D X K i L a e C z W q D z L C U 0 A k Z s 7 6 j i k h m g n x + 6 w y f O W W E o 0 S 7 U h b P 1 d 8 T O Z H G T G X o O i W x s V n 2 C v E / r 5 / Z 6 D r I u U o z y x R d L I o y g W 2 C i 8 f x i G t G r Z g 6 Q q j m 7 l Z M Y 6 I J t S 6 e m g v B X 3 5 5 l X Q v m r 7 X 9 O 8 v G 6 2 b M o 4 q n M A p n I M P V 9 C C O 2 h D B y j E 8 A y v 8 I Y k e k H v 6 G P R W k H l z D H 8 A f r 8 A R O I j j 8 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " v M C m C B v r Q q v M k c Z e 5 c 7 8 y K 1 9 q u 0 = " &gt; A A A B 6 3 i c b V B N S w M x E J 3 U r 1 q / q h 6 9 B I v g q e y K o M e i F 4 8 V b C 2 0 S 8 m m 2 W 5 o k l 2 S r F C W / g U v H h T x 6 h / y 5 r 8 x 2 + 5 B W x 8 M P N 6 b Y W Z e m A p u r O d 9 o 8 r a + s b m V n W 7 t r O 7 t 3 9 Q P z z q m i T T l H V o I h L d C 4 l h g i v W s d w K 1 k s 1 I z I U 7 D G c 3 B b + 4 x P T h i f q w U 5 T F k g y V j z i l N h C G q Q x H 9 Y b X t O b A 6 8 S v y Q N K N E e 1 r 8 G o 4 R m k i l L B T G m 7 3 u p D X K i L a e C z W q D z L C U 0 A k Z s 7 6 j i k h m g n x + 6 w y f O W W E o 0 S 7 U h b P 1 d 8 T O Z H G T G X o O i W x s V n 2 C v E / r 5 / Z 6 D r I u U o z y x R d L I o y g W 2 C i 8 f x i G t G r Z g 6 Q q j m 7 l Z M Y 6 I J t S 6 e m g v B X 3 5 5 l X Q v m r 7 X 9 O 8 v G 6 2 b M o 4 q n M A p n I M P V 9 C C O 2 h D B y j E 8 A y v 8 I Y k e k H v 6 G P R W k H l z D H 8 A f r 8 A R O I j j 8 = &lt; / l a t e x i t &gt; ↵ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " J t D q a C S Y H d U s A r J l V i G Z O Y t H m 8 o = " &gt; A A A B 7 X i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o M e g F 4 8 R z A O S J f R O J s m Y 2 Z l l Z l Y I S / 7 B i w d F v P o / 3 v w b J 8 k e N L G g o a j q p r s r S g Q 3 1 v e / v c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q G p V q y h p U C a X b E R o m u G Q N y 6 1 g 7 U Q z j C P B W t H 4 d u a 3 n p g 2 X M k H O 0 l Y G O N Q 8 g G n a J 3 U 7 K J I R t g r V / y q P w d Z J U F O K p C j 3 i t / d f u K p j G T l g o 0 p h P 4 i Q 0 z 1 J Z T w a a l b m p Y g n S M Q 9 Z x V G L M T J j N r 5 2 S M 6 f 0 y U B p V 9 K S u f p 7 I s P Y m E k c u c 4 Y 7 c g s e z P x P 6 + T 2 s F 1 m H G Z p J Z J u l g 0 S A W x i s x e J 3 2 u G b V i 4 g h S z d 2 t h I 5 Q I 7 U u o J I L I V h + e Z U 0 L 6 q B X w 3 u L y u 1 m z y O I p z A K Z x D A F d Q g z u o Q w M o P M I z v M K b p 7 w X 7 9 3 7 W L Q W v H z m G P 7 A + / w B i 4 G P G A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " J t D q a C S Y H d U s A r J l V i G Z O Y t H m 8 o = " &gt; A A A B 7 X i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o M e g F 4 8 R z A O S J f R O J s m Y 2 Z l l Z l Y I S / 7 B i w d F v P o / 3 v w b J 8 k e N L G g o a j q p r s r S g Q 3 1 v e / v c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q G p V q y h p U C a X b E R o m u G Q N y 6 1 g 7 U Q z j C P B W t H 4 d u a 3 n p g 2 X M k H O 0 l Y G O N Q 8 g G n a J 3 U 7 K J I R t g r V / y q P w d Z J U F O K p C j 3 i t / d f u K p j G T l g o 0 p h P 4 i Q 0 z 1 J Z T w a a l b m p Y g n S M Q 9 Z x V G L M T J j N r 5 2 S M 6 f 0 y U B p V 9 K S u f p 7 I s P Y m E k c u c 4 Y 7 c g s e z P x P 6 + T 2 s F 1 m H G Z p J Z J u l g 0 S A W x i s x e J 3 2 u G b V i 4 g h S z d 2 t h I 5 Q I 7 U u o J I L I V h + e Z U 0 L 6 q B X w 3 u L y u 1 m z y O I p z A K Z x D A F d Q g z u o Q w M o P M I z v M K b p 7 w X 7 9 3 7 W L Q W v H z m G P 7 A + / w B i 4 G P G A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " J t D q a C S Y H d U s A r J l V i G Z O Y t H m 8 o = " &gt; A A A B 7 X i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o M e g F 4 8 R z A O S J f R O J s m Y 2 Z l l Z l Y I S / 7 B i w d F v P o / 3 v w b J 8 k e N L G g o a j q p r s r S g Q 3 1 v e / v c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q G p V q y h p U C a X b E R o m u G Q N y 6 1 g 7 U Q z j C P B W t H 4 d u a 3 n p g 2 X M k H O 0 l Y G O N Q 8 g G n a J 3 U 7 K J I R t g r V / y q P w d Z J U F O K p C j 3 i t / d f u K p j G T l g o 0 p h P 4 i Q 0 z 1 J Z T w a a l b m p Y g n S M Q 9 Z x V G L M T J j N r 5 2 S M 6 f 0 y U B p V 9 K S u f p 7 I s P Y m E k c u c 4 Y 7 c g s e z P x P 6 + T 2 s F 1 m H G Z p J Z J u l g 0 S A W x i s x e J 3 2 u G b V i 4 g h S z d 2 t h I 5 Q I 7 U u o J I L I V h + e Z U 0 L 6 q B X w 3 u L y u 1 m z y O I p z A K Z x D A F d Q g z u o Q w M o P M I z v M K b p 7 w X 7 9 3 7 W L Q W v H z m G P 7 A + / w B i 4 G P G A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " J t D q a C S Y H d U s A r J l V i G Z O Y t H m 8 o = " &gt; A A A B 7 X i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o M e g F 4 8 R z A O S J f R O J s m Y 2 Z l l Z l Y I S / 7 B i w d F v P o / 3 v w b J 8 k e N L G g o a j q p r s r S g Q 3 1 v e / v c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q G p V q y h p U C a X b E R o m u G Q N y 6 1 g 7 U Q z j C P B W t H 4 d u a 3 n p g 2 X M k H O 0 l Y G O N Q 8 g G n a J 3 U 7 K J I R t g r V / y q P w d Z J U F O K p C j 3 i t / d f u K p j G T l g o 0 p h P 4 i Q 0 z 1 J Z T w a a l b m p Y g n S M Q 9 Z x V G L M T J j N r 5 2 S M 6 f 0 y U B p V 9 K S u f p 7 I s P Y m E k c u c 4 Y 7 c g s e z P x P 6 + T 2 s F 1 m H G Z p J Z J u l g 0 S A W x i s x e J 3 2 u G b V i 4 g h S z d 2 t h I 5 Q I 7 U u o J I L I V h + e Z U 0 L 6 q B X w 3 u L y u 1 m z y O I p z A K Z x D A F d Q g z u o Q w M o P M I z v M K b p 7 w X 7 9 3 7 W L Q W v H z m G P 7 A + / w B i 4 G P G A = = &lt; / l a t e x i t &gt; K &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N r n J F n Q i p 0 m n C Q u M Q d 5 3 f 7 6 p Y S s = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i + C l B f s B b S i b 7 a R d u 9 m E 3 Y 1 Q Q n + B F w + K e P U n e f P f u G 1 z 0 N Y H A 4 / 3 Z p i Z F y S C a + O 6 3 0 5 h b X 1 j c 6 u 4 X d r Z 3 d s / K B 8 e t X S c K o Z N F o t Y d Q K q U X C J T c O N w E 6 i k E a B w H Y w v p 3 5 7 S d U m s f y w U w S 9 C M 6 l D z k j B o r N e 7 7 5 Y p b d e c g q 8 T L S Q V y 1 P v l r 9 4 g Z m m E 0 j B B t e 5 6 b m L 8 j C r D m c B p q Z d q T C g b 0 y F 2 L Z U 0 Q u 1 n 8 0 O n 5 M w q A x L G y p Y 0 Z K 7 + n s h o p P U k C m x n R M 1 I L 3 s z 8 T + v m 5 r w 2 s + 4 T F K D k i 0 W h a k g J i a z r 8 m A K 2 R G T C y h T H F 7 K 2 E j q i g z N p u S D c F b f n m V t C 6 q n l v 1 G p e V 2 k 0 e R x F O 4 B T O w Y M r q M E d 1 K E J D B C e 4 R X e n E f n x X l 3 P h a t B S e f O Y Y / c D 5 / A K I F j M 8 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N r n J F n Q i p 0 m n C Q u M Q d 5 3 f 7 6 p Y S s = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i + C l B f s B b S i b 7 a R d u 9 m E 3 Y 1 Q Q n + B F w + K e P U n e f P f u G 1 z 0 N Y H A 4 / 3 Z p i Z F y S C a + O 6 3 0 5 h b X 1 j c 6 u 4 X d r Z 3 d s / K B 8 e t X S c K o Z N F o t Y d Q K q U X C J T c O N w E 6 i k E a B w H Y w v p 3 5 7 S d U m s f y w U w S 9 C M 6 l D z k j B o r N e 7 7 5 Y p b d e c g q 8 T L S Q V y 1 P v l r 9 4 g Z m m E 0 j B B t e 5 6 b m L 8 j C r D m c B p q Z d q T C g b 0 y F 2 L Z U 0 Q u 1 n 8 0 O n 5 M w q A x L G y p Y 0 Z K 7 + n s h o p P U k C m x n R M 1 I L 3 s z 8 T + v m 5 r w 2 s + 4 T F K D k i 0 W h a k g J i a z r 8 m A K 2 R G T C y h T H F 7 K 2 E j q i g z N p u S D c F b f n m V t C 6 q n l v 1 G p e V 2 k 0 e R x F O 4 B T O w Y M r q M E d 1 K E J D B C e 4 R X e n E f n x X l 3 P h a t B S e f O Y Y / c D 5 / A K I F j M 8 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N r n J F n Q i p 0 m n C Q u M Q d 5 3 f 7 6 p Y S s = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i + C l B f s B b S i b 7 a R d u 9 m E 3 Y 1 Q Q n + B F w + K e P U n e f P f u G 1 z 0 N Y H A 4 / 3 Z p i Z F y S C a + O 6 3 0 5 h b X 1 j c 6 u 4 X d r Z 3 d s / K B 8 e t X S c K o Z N F o t Y d Q K q U X C J T c O N w E 6 i k E a B w H Y w v p 3 5 7 S d U m s f y w U w S 9 C M 6 l D z k j B o r N e 7 7 5 Y p b d e c g q 8 T L S Q V y 1 P v l r 9 4 g Z m m E 0 j B B t e 5 6 b m L 8 j C r D m c B p q Z d q T C g b 0 y F 2 L Z U 0 Q u 1 n 8 0 O n 5 M w q A x L G y p Y 0 Z K 7 + n s h o p P U k C m x n R M 1 I L 3 s z 8 T + v m 5 r w 2 s + 4 T F K D k i 0 W h a k g J i a z r 8 m A K 2 R G T C y h T H F 7 K 2 E j q i g z N p u S D c F b f n m V t C 6 q n l v 1 G p e V 2 k 0 e R x F O 4 B T O w Y M r q M E d 1 K E J D B C e 4 R X e n E f n x X l 3 P h a t B S e f O Y Y / c D 5 / A K I F j M 8 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N r n J F n Q i p 0 m n C Q u M Q d 5 3 f 7 6 p Y S s = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i + C l B f s B b S i b 7 a R d u 9 m E 3 Y 1 Q Q n + B F w + K e P U n e f P f u G 1 z 0 N Y H A 4 / 3 Z p i Z F y S C a + O 6 3 0 5 h b X 1 j c 6 u 4 X d r Z 3 d s / K B 8 e t X S c K o Z N F o t Y d Q K q U X C J T c O N w E 6 i k E a B w H Y w v p 3 5 7 S d U m s f y w U w S 9 C M 6 l D z k j B o r N e 7 7 5 Y p b d e c g q 8 T L S Q V y 1 P v l r 9 4 g Z m m E 0 j B B t e 5 6 b m L 8 j C r D m c B p q Z d q T C g b 0 y F 2 L Z U 0 Q u 1 n 8 0 O n 5 M w q A x L G y p Y 0 Z K 7 + n s h o p P U k C m x n R M 1 I L 3 s z 8 T + v m 5 r w 2 s + 4 T F K D k i 0 W h a k g J i a z r 8 m A K 2 R G T C y h T H F 7 K 2 E j q i g z N p u S D c F b f n m V t C 6 q n l v 1 G p e V 2 k 0 e R x F O 4 B T O w Y M r q M E d 1 K E J D B C e 4 R X e n E f n x X l 3 P h a t B S e f O Y Y / c D 5 / A K I F j M 8 = &lt; / l a t e x i t &gt; Latent Class-Conditional Noise model. x and y is the observed training pair. z is the latent label. φ is the unknown noise transition. α is a Dirichlet parameter. N is the sample number and K is the class number.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1</head><label>1</label><figDesc>Dynamic Label Regression for LCCN Require: A noisy dataset D = {(x n , y n )} N n=1 , a classifier P (·|x) modeled by DNN f θ , warming-up steps δ, the running epoch number L and the batch-size M . 1: Directly pretrain the classifier f θ on the noisy dataset D. 2: Compute the warming-up noise transition matrix φ . 3: for epoch i = 1 to L do 4:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Extensions based on the original safeguarded dynamic label regression for LCCN. The images and noisy labels are respectively input to the classifier and the safeguarded Bayesian noise modeling to compute the prediction (including the outlier component) and the conditional transition. When clean labels are not available as the latent labels, they are sampled based on the product of previous two quantities. Then, the latent labels composite by both the clean parts and those inferred from sampling are used to train the classifier, and only the latent labels inferred from the sampling are used to refine the noise model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>xFig. 4 .</head><label>4</label><figDesc>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " f 2 y z i m w b R / D g j z p 6 t Z 3 6 0 f H R q N I = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c W 7 A e 0 o W y 2 k 3 b t Z h N 2 N 2 I J / Q V e P C j i 1 Z / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 H 1 F p H s t 7 M 0 n Q j + h Q 8 p A z a q z U e O q X K 2 7 V n Y O s E i 8 n F c h R 7 5 e / e o O Y p R F K w w T V u u u 5 i f E z q g x n A q e l X q o x o W x M h 9 i 1 V N I I t Z / N D 5 2 S M 6 s M S B g r W 9 K Q u f p 7 I q O R 1 p M o s J 0 R N S O 9 7 M 3 E / 7 x u a s J r P + M y S Q 1 K t l g U p o K Y m M y + J g O u k B k x s Y Q y x e 2 t h I 2 o o s z Y b E o 2 B G / 5 5 V X S u q h 6 b t V r X F Z q N 3 k c R T i B U z g H D 6 6 g B n d Q h y Y w Q H i G V 3 h z H p w X 5 9 3 5 W L Q W n H z m G P 7 A + f w B 5 j m M / A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " f 2 y z i m w b R / D g j z p 6 t Z 3 6 0 f H R q N I = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c W 7 A e 0 o W y 2 k 3 b t Z h N 2 N 2 I J / Q V e P C j i 1 Z / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 H 1 F p H s t 7 M 0 n Q j + h Q 8 p A z a q z U e O q X K 2 7 V n Y O s E i 8 n F c h R 7 5 e / e o O Y p R F K w w T V u u u 5 i f E z q g x n A q e l X q o x o W x M h 9 i 1 V N I I t Z / N D 5 2 S M 6 s M S B g r W 9 K Q u f p 7 I q O R 1 p M o s J 0 R N S O 9 7 M 3 E / 7 x u a s J r P + M y S Q 1 K t l g U p o K Y m M y + J g O u k B k x s Y Q y x e 2 t h I 2 o o s z Y b E o 2 B G / 5 5 V X S u q h 6 b t V r X F Z q N 3 k c R T i B U z g H D 6 6 g B n d Q h y Y w Q H i G V 3 h z H p w X 5 9 3 5 W L Q W n H z m G P 7 A + f w B 5 j m M / A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " f 2 y z i m w b R / D g j z p 6 t Z 3 6 0 f H R q N I = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c W 7 A e 0 o W y 2 k 3 b t Z h N 2 N 2 I J / Q V e P C j i 1 Z / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 H 1 F p H s t 7 M 0 n Q j + h Q 8 p A z a q z U e O q X K 2 7 V n Y O s E i 8 n F c h R 7 5 e / e o O Y p R F K w w T V u u u 5 i f E z q g x n A q e l X q o x o W x M h 9 i 1 V N I I t Z / N D 5 2 S M 6 s M S B g r W 9 K Q u f p 7 I q O R 1 p M o s J 0 R N S O 9 7 M 3 E / 7 x u a s J r P + M y S Q 1 K t l g U p o K Y m M y + J g O u k B k x s Y Q y x e 2 t h I 2 o o s z Y b E o 2 B G / 5 5 V X S u q h 6 b t V r X F Z q N 3 k c R T i B U z g H D 6 6 g B n d Q h y Y w Q H i G V 3 h z H p w X 5 9 3 5 W L Q W n H z m G P 7 A + f w B 5 j m M / A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " f 2 y z i m w b R / D g j z p 6 t Z 3 6 0 f H R q N I = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c W 7 A e 0 o W y 2 k 3 b t Z h N 2 N 2 I J / Q V e P C j i 1 Z / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 H 1 F p H s t 7 M 0 n Q j + h Q 8 p A z a q z U e O q X K 2 7 V n Y O s E i 8 n F c h R 7 5 e / e o O Y p R F K w w T V u u u 5 i f E z q g x n A q e l X q o x o W x M h 9 i 1 V N I I t Z / N D 5 2 S M 6 s M S B g r W 9 K Q u f p 7 I q O R 1 p M o s J 0 R N S O 9 7 M 3 E / 7 x u a s J r P + M y S Q 1 K t l g U p o K Y m M y + J g O u k B k x s Y Q y x e 2 t h I 2 o o s z Y b E o 2 B G / 5 5 V X S u q h 6 b t V r X F Z q N 3 k c R T i B U z g H D 6 6 g B n d Q h y Y w Q H i G V 3 h z H p w X 5 9 3 5 W L Q W n H z m G P 7 A + f w B 5 j m M / A = = &lt; / l a t e x i t &gt; y &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l 2 9 W x o U b 9 D E b v m h L G 7 j H t Z 0 O U 2 4 = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k I e i x 6 8 d i C / Y A 2 l M 1 2 0 q 7 d b M L u R g i h v 8 C L B 0 W 8 + p O 8 + W / c t j l o 6 4 O B x 3 s z z M w L E s G 1 c d 1 v Z 2 1 9 Y 3 N r u 7 R T 3 t 3 b P z i s H B 2 3 d Z w q h i 0 W i 1 h 1 A 6 p R c I k t w 4 3 A b q K Q R o H A T j C 5 m / m d J 1 S a x / L B Z A n 6 E R 1 J H n J G j Z W a 2 a B S d W v u H G S V e A W p Q o H G o P L V H 8 Y s j V A a J q j W P c 9 N j J 9 T Z T g T O C 3 3 U 4 0 J Z R M 6 w p 6 l k k a o / X x + 6 J S c W 2 V I w l j Z k o b M 1 d 8 T O Y 2 0 z q L A d k b U j P W y N x P / 8 3 q p C W / 8 n M s k N S j Z Y l G Y C m J i M v u a D L l C Z k R m C W W K 2 1 s J G 1 N F m b H Z l G 0 I 3 v L L q 6 R 9 W f P c m t e 8 q t Z v i z h K c A p n c A E e X E M d 7 q E B L W C A 8 A y v 8 O Y 8 O i / O u / O x a F 1 z i p k T + A P n 8 w f n v Y z 9 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l 2 9 W x o U b 9 D E b v m h L G 7 j H t Z 0 O U 2 4 = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k I e i x 6 8 d i C / Y A 2 l M 1 2 0 q 7 d b M L u R g i h v 8 C L B 0 W 8 + p O 8 + W / c t j l o 6 4 O B x 3 s z z M w L E s G 1 c d 1 v Z 2 1 9 Y 3 N r u 7 R T 3 t 3 b P z i s H B 2 3 d Z w q h i 0 W i 1 h 1 A 6 p R c I k t w 4 3 A b q K Q R o H A T j C 5 m / m d J 1 S a x / L B Z A n 6 E R 1 J H n J G j Z W a 2 a B S d W v u H G S V e A W p Q o H G o P L V H 8 Y s j V A a J q j W P c 9 N j J 9 T Z T g T O C 3 3 U 4 0 J Z R M 6 w p 6 l k k a o / X x + 6 J S c W 2 V I w l j Z k o b M 1 d 8 T O Y 2 0 z q L A d k b U j P W y N x P / 8 3 q p C W / 8 n M s k N S j Z Y l G Y C m J i M v u a D L l C Z k R m C W W K 2 1 s J G 1 N F m b H Z l G 0 I 3 v L L q 6 R 9 W f P c m t e 8 q t Z v i z h K c A p n c A E e X E M d 7 q E B L W C A 8 A y v 8 O Y 8 O i / O u / O x a F 1 z i p k T + A P n 8 w f n v Y z 9 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l 2 9 W x o U b 9 D E b v m h L G 7 j H t Z 0 O U 2 4 = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k I e i x 6 8 d i C / Y A 2 l M 1 2 0 q 7 d b M L u R g i h v 8 C L B 0 W 8 + p O 8 + W / c t j l o 6 4 O B x 3 s z z M w L E s G 1 c d 1 v Z 2 1 9 Y 3 N r u 7 R T 3 t 3 b P z i s H B 2 3 d Z w q h i 0 W i 1 h 1 A 6 p R c I k t w 4 3 A b q K Q R o H A T j C 5 m / m d J 1 S a x / L B Z A n 6 E R 1 J H n J G j Z W a 2 a B S d W v u H G S V e A W p Q o H G o P L V H 8 Y s j V A a J q j W P c 9 N j J 9 T Z T g T O C 3 3 U 4 0 J Z R M 6 w p 6 l k k a o / X x + 6 J S c W 2 V I w l j Z k o b M 1 d 8 T O Y 2 0 z q L A d k b U j P W y N x P / 8 3 q p C W / 8 n M s k N S j Z Y l G Y C m J i M v u a D L l C Z k R m C W W K 2 1 s J G 1 N F m b H Z l G 0 I 3 v L L q 6 R 9 W f P c m t e 8 q t Z v i z h K c A p n c A E e X E M d 7 q E B L W C A 8 A y v 8 O Y 8 O i / O u / O x a F 1 z i p k T + A P n 8 w f n v Y z 9 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l 2 9 W x o U b 9 D E b v m h L G 7 j H t Z 0 O U 2 4 = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k I e i x 6 8 d i C / Y A 2 l M 1 2 0 q 7 d b M L u R g i h v 8 C L B 0 W 8 + p O 8 + W / c t j l o 6 4 O B x 3 s z z M w L E s G 1 c d 1 v Z 2 1 9 Y 3 N r u 7 R T 3 t 3 b P z i s H B 2 3 d Z w q h i 0 W i 1 h 1 A 6 p R c I k t w 4 3 A b q K Q R o H A T j C 5 m / m d J 1 S a x / L B Z A n 6 E R 1 J H n J G j Z W a 2 a B S d W v u H G S V e A W p Q o H G o P L V H 8 Y s j V A a J q j W P c 9 N j J 9 T Z T g T O C 3 3 U 4 0 J Z R M 6 w p 6 l k k a o / X x + 6 J S c W 2 V I w l j Z k o b M 1 d 8 T O Y 2 0 z q L A d k b U j P W y N x P / 8 3 q p C W / 8 n M s k N S j Z Y l G Y C m J i M v u a D L l C Z k R m C W W K 2 1 s J G 1 N F m b H Z l G 0 I 3 v L L q 6 R 9 W f P c m t e 8 q t Z v i z h K c A p n c A E e X E M d 7 q E B L W C A 8 A y v 8 O Y 8 O i / O u / O x a F 1 z i p k T + A P n 8 w f n v Y z 9 &lt; / l a t e x i t &gt; z &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H D z X c h l s P l m u E y Z Z / 9 z F J + i V C 6 I = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c W 7 A e 0 o W y 2 k 3 b t Z h N 2 N 0 I N / Q V e P C j i 1 Z / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 H 1 F p H s t 7 M 0 n Q j + h Q 8 p A z a q z U e O q X K 2 7 V n Y O s E i 8 n F c h R 7 5 e / e o O Y p R F K w w T V u u u 5 i f E z q g x n A q e l X q o x o W x M h 9 i 1 V N I I t Z / N D 5 2 S M 6 s M S B g r W 9 K Q u f p 7 I q O R 1 p M o s J 0 R N S O 9 7 M 3 E / 7 x u a s J r P + M y S Q 1 K t l g U p o K Y m M y + J g O u k B k x s Y Q y x e 2 t h I 2 o o s z Y b E o 2 B G / 5 5 V X S u q h 6 b t V r X F Z q N 3 k c R T i B U z g H D 6 6 g B n d Q h y Y w Q H i G V 3 h z H p w X 5 9 3 5 W L Q W n H z m G P 7 A + f w B 6 U G M / g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H D z X c h l s P l m u E y Z Z / 9 z F J + i V C 6 I = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c W 7 A e 0 o W y 2 k 3 b t Z h N 2 N 0 I N / Q V e P C j i 1 Z / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 H 1 F p H s t 7 M 0 n Q j + h Q 8 p A z a q z U e O q X K 2 7 V n Y O s E i 8 n F c h R 7 5 e / e o O Y p R F K w w T V u u u 5 i f E z q g x n A q e l X q o x o W x M h 9 i 1 V N I I t Z / N D 5 2 S M 6 s M S B g r W 9 K Q u f p 7 I q O R 1 p M o s J 0 R N S O 9 7 M 3 E / 7 x u a s J r P + M y S Q 1 K t l g U p o K Y m M y + J g O u k B k x s Y Q y x e 2 t h I 2 o o s z Y b E o 2 B G / 5 5 V X S u q h 6 b t V r X F Z q N 3 k c R T i B U z g H D 6 6 g B n d Q h y Y w Q H i G V 3 h z H p w X 5 9 3 5 W L Q W n H z m G P 7 A + f w B 6 U G M / g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H D z X c h l s P l m u E y Z Z / 9 z F J + i V C 6 I = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c W 7 A e 0 o W y 2 k 3 b t Z h N 2 N 0 I N / Q V e P C j i 1 Z / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 H 1 F p H s t 7 M 0 n Q j + h Q 8 p A z a q z U e O q X K 2 7 V n Y O s E i 8 n F c h R 7 5 e / e o O Y p R F K w w T V u u u 5 i f E z q g x n A q e l X q o x o W x M h 9 i 1 V N I I t Z / N D 5 2 S M 6 s M S B g r W 9 K Q u f p 7 I q O R 1 p M o s J 0 R N S O 9 7 M 3 E / 7 x u a s J r P + M y S Q 1 K t l g U p o K Y m M y + J g O u k B k x s Y Q y x e 2 t h I 2 o o s z Y b E o 2 B G / 5 5 V X S u q h 6 b t V r X F Z q N 3 k c R T i B U z g H D 6 6 g B n d Q h y Y w Q H i G V 3 h z H p w X 5 9 3 5 W L Q W n H z m G P 7 A + f w B 6 U G M / g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H D z X c h l s P l m u E y Z Z / 9 z F J + i V C 6 I = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c W 7 A e 0 o W y 2 k 3 b t Z h N 2 N 0 I N / Q V e P C j i 1 Z / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 H 1 F p H s t 7 M 0 n Q j + h Q 8 p A z a q z U e O q X K 2 7 V n Y O s E i 8 n F c h R 7 5 e / e o O Y p R F K w w T V u u u 5 i f E z q g x n A q e l X q o x o W x M h 9 i 1 V N I I t Z / N D 5 2 S M 6 s M S B g r W 9 K Q u f p 7 I q O R 1 p M o s J 0 R N S O 9 7 M 3 E / 7 x u a s J r P + M y S Q 1 K t l g U p o K Y m M y + J g O u k B k x s Y Q y x e 2 t h I 2 o o s z Y b E o 2 B G / 5 5 V X S u q h 6 b t V r X F Z q N 3 k c R T i B U z g H D 6 6 g B n d Q h y Y w Q H i G V 3 h z H p w X 5 9 3 5 W L Q W n H z m G P 7 A + f w B 6 U G M / g = = &lt; / l a t e x i t &gt; N &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F s F o h X Q j c 3 O A / B B A i 7 J X P 9 T M C N 0 = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i y d p w X 5 A G 8 p m O 2 n X b j Z h d y O U 0 F / g x Y M i X v 1 J 3 v w 3 b t s c t P X B w O O 9 G W b m B Y n g 2 r j u t 1 N Y W 9 / Y 3 C p u l 3 Z 2 9 / Y P y o d H L R 2 n i m G T x S J W n Y B q F F x i 0 3 A j s J M o p F E g s B 2 M b 2 d + + w m V 5 r F 8 M J M E / Y g O J Q 8 5 o 8 Z K j f t + u e J W 3 T n I K v F y U o E c 9 X 7 5 q z e I W R q h N E x Q r b u e m x g / o 8 p w J n B a 6 q U a E 8 r G d I h d S y W N U P v Z / N A p O b P K g I S x s i U N m a u / J z I a a T 2 J A t s Z U T P S y 9 5 M / M / r p i a 8 9 j M u k 9 S g Z I t F Y S q I i c n s a z L g C p k R E 0 s o U 9 z e S t i I K s q M z a Z k Q / C W X 1 4 l r Y u q 5 1 a 9 x m W l d p P H U Y Q T O I V z 8 O A K a n A H d W g C A 4 R n e I U 3 5 9 F 5 c d 6 d j 0 V r w c l n j u E P n M 8 f p p G M 0 g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F s F o h X Q j c 3 O A / B B A i 7 J X P 9 T M C N 0 = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i y d p w X 5 A G 8 p m O 2 n X b j Z h d y O U 0 F / g x Y M i X v 1 J 3 v w 3 b t s c t P X B w O O 9 G W b m B Y n g 2 r j u t 1 N Y W 9 / Y 3 C p u l 3 Z 2 9 / Y P y o d H L R 2 n i m G T x S J W n Y B q F F x i 0 3 A j s J M o p F E g s B 2 M b 2 d + + w m V 5 r F 8 M J M E / Y g O J Q 8 5 o 8 Z K j f t + u e J W 3 T n I K v F y U o E c 9 X 7 5 q z e I W R q h N E x Q r b u e m x g / o 8 p w J n B a 6 q U a E 8 r G d I h d S y W N U P v Z / N A p O b P K g I S x s i U N m a u / J z I a a T 2 J A t s Z U T P S y 9 5 M / M / r p i a 8 9 j M u k 9 S g Z I t F Y S q I i c n s a z L g C p k R E 0 s o U 9 z e S t i I K s q M z a Z k Q / C W X 1 4 l r Y u q 5 1 a 9 x m W l d p P H U Y Q T O I V z 8 O A K a n A H d W g C A 4 R n e I U 3 5 9 F 5 c d 6 d j 0 V r w c l n j u E P n M 8 f p p G M 0 g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F s F o h X Q j c 3 O A / B B A i 7 J X P 9 T M C N 0= " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i y d p w X 5 A G 8 p m O 2 n X b j Z h d y O U 0 F / g x Y M i X v 1 J 3 v w 3 b t s c t P X B w O O 9 G W b m B Y n g 2 r j u t 1 N Y W 9 / Y 3 C p u l 3 Z 2 9 / Y P y o d H L R 2 n i m G T x S J W n Y B q F F x i 0 3 A j s J M o p F E g s B 2 M b 2 d + + w m V 5 r F 8 M J M E / Y g O J Q 8 5 o 8 Z K j f t +u e J W 3 T n I K v F y U o E c 9 X 7 5 q z e I W R q h N E x Q r b u e m x g / o 8 p w J n B a 6 q U a E 8 r G d I h d S y W N U P v Z / N A p O b P K g I S x s i U N m a u / J z I a a T 2 J A t s Z U T P S y 9 5 M / M / r p i a 8 9 j M u k 9 S g Z I t F Y S q I i c n s a z L g C p k R E 0 s o U 9 z e S t i I K s q M z a Z k Q / C W X 1 4 l r Y u q 5 1 a 9 x m W l d p P H U Y Q T O I V z 8 O A K a n A H d W g C A 4 R n e I U 3 5 9 F 5 c d 6 d j 0 V r w c l n j u E P n M 8 f p p G M 0 g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F s F o h X Q j c 3 O A / B B A i 7 J X P 9 T M C N 0= " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i y d p w X 5 A G 8 p m O 2 n X b j Z h d y O U 0 F / g x Y M i X v 1 J 3 v w 3 b t s c t P X B w O O 9 G W b m B Y n g 2 r j u t 1 N Y W 9 / Y 3 C p u l 3 Z 2 9 / Y P y o d H L R 2 n i m G T x S J W n Y B q F F x i 0 3 A j s J M o p F E g s B 2 M b 2 d + + w m V 5 r F 8 M J M E / Y g O J Q 8 5 o 8 Z K j f t +u e J W 3 T n I K v F y U o E c 9 X 7 5 q z e I W R q h N E x Q r b u e m x g / o 8 p w J n B a 6 q U a E 8 r G d I h d S y W N U P v Z / N A p O b P K g I S x s i U N m a u / J z I a a T 2 J A t s Z U T P S y 9 5 M / M / r p i a 8 9 j M u k 9 S g Z I t F Y S q I i c n s a z L g C p k R E 0 s o U 9 z e S t i I K s q M z a Z k Q / C W X 1 4 l r Y u q 5 1 a 9 x m W l d p P H U Y Q T O I V z 8 O A K a n A H d W g C A 4 R n e I U 3 5 9 F 5 c d 6 d j 0 V r w c l n j u E P n M 8 f p p G M 0 g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " v M C m C B v r Q q v M k c Z e 5 c 7 8 y K 1 9 q u 0 = " &gt; A A A B 6 3 i c b V B N S w M x E J 3 U r 1 q / q h 6 9 B I v g q e y K o M e i F 4 8 V b C 2 0 S 8 m m 2 W 5 o k l 2 S r F C W / g U v H h T x 6 h / y 5 r 8 x 2 + 5 B W x 8 M P N 6 b Y W Z e m A p u r O d 9 o 8 r a + s b m V n W 7 t r O 7 t 3 9 Q P z z q m i T T l H V o I h L d C 4 l h g i v W s d w K 1 k s 1 I z I U 7 D G c 3 B b + 4 x P T h i f q w U 5 T F k g y V j z i l N h C G q Q x H 9 Y b X t O b A 6 8 S v y Q N K N E e 1 r 8 G o 4 R m k i l L B T G m 7 3 u p D X K i L a e C z W q D z L C U 0 A k Z s 7 6 j i k h m g n x + 6 w y f O W W E o 0 S 7 U h b P 1 d 8 T O Z H G T G X o O i W x s V n 2 C v E / r 5 / Z 6 D r I u U o z y x R d L I o y g W 2 C i 8 f x i G t G r Z g 6 Q q j m 7 l Z M Y 6 I J t S 6 e m g v B X 3 5 5 l X Q v m r 7 X 9 O 8 v G 6 2 b M o 4 q n M A p n I M P V 9 C C O 2 h D B y j E 8 A y v 8 I Y k e k H v 6 G P R W k H l z D H 8 A f r 8 A R O I j j 8 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " v M C m C B v r Q q v M k c Z e 5 c 7 8 y K 1 9 q u 0 = " &gt; A A A B 6 3 i c b V B N S w M x E J 3 U r 1 q / q h 6 9 B I v g q e y K o M e i F 4 8 V b C 2 0 S 8 m m 2 W 5 o k l 2 S r F C W / g U v H h T x 6 h / y 5 r 8 x 2 + 5 B W x 8 M P N 6 b Y W Z e m A p u r O d 9 o 8 r a + s b m V n W 7 t r O 7 t 3 9 Q P z z q m i T T l H V o I h L d C 4 l h g i v W s d w K 1 k s 1 I z I U 7 D G c 3 B b + 4 x P T h i f q w U 5 T F k g y V j z i l N h C G q Q x H 9 Y b X t O b A 6 8 S v y Q N K N E e 1 r 8 G o 4 R m k i l L B T G m 7 3 u p D X K i L a e C z W q D z L C U 0 A k Z s 7 6 j i k h m g n x + 6 w y f O W W E o 0 S 7 U h b P 1 d 8 T O Z H G T G X o O i W x s V n 2 C v E / r 5 / Z 6 D r I u U o z y x R d L I o y g W 2 C i 8 f x i G t G r Z g 6 Q q j m 7 l Z M Y 6 I J t S 6 e m g v B X 3 5 5 l X Q v m r 7 X 9 O 8 v G 6 2 b M o 4 q n M A p n I M P V 9 C C O 2 h D B y j E 8 A y v 8 I Y k e k H v 6 G P R W k H l z D H 8 A f r 8 A R O I j j 8 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " v M C m C B v r Q q v M k c Z e 5 c 7 8 y K 1 9 q u 0 = " &gt; A A A B 6 3 i c b V B N S w M x E J 3 U r 1 q / q h 6 9 B I v g q e y K o M e i F 4 8 V b C 2 0 S 8 m m 2 W 5 o k l 2 S r F C W / g U v H h T x 6 h / y 5 r 8 x 2 + 5 B W x 8 M P N 6 b Y W Z e m A p u r O d 9 o 8 r a + s b m V n W 7 t r O 7 t 3 9 Q P z z q m i T T l H V o I h L d C 4 l h g i v W s d w K 1 k s 1 I z I U 7 D G c 3 B b + 4 x P T h i f q w U 5 T F k g y V j z i l N h C G q Q x H 9 Y b X t O b A 6 8 S v y Q N K N E e 1 r 8 G o 4 R m k i l L B T G m 7 3 u p D X K i L a e C z W q D z L C U 0 A k Z s 7 6 j i k h m g n x + 6 w y f O W W E o 0 S 7 U h b P 1 d 8 T O Z H G T G X o O i W x s V n 2 C v E / r 5 / Z 6 D r I u U o z y x R d L I o y g W 2 C i 8 f x i G t G r Z g 6 Q q j m 7 l Z M Y 6 I J t S 6 e m g v B X 3 5 5 l X Q v m r 7 X 9 O 8 v G 6 2 b M o 4 q n M A p n I M P V 9 C C O 2 h D B y j E 8 A y v 8 I Y k e k H v 6 G P R W k H l z D H 8 A f r 8 A R O I j j 8 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " v M C m C B v r Q q v M k c Z e 5 c 7 8 y K 1 9 q u 0 = " &gt; A A A B 6 3 i c b V B N S w M x E J 3 U r 1 q / q h 6 9 B I v g q e y K o M e i F 4 8 V b C 2 0 S 8 m m 2 W 5 o k l 2 S r F C W / g U v H h T x 6 h / y 5 r 8 x 2 + 5 B W x 8 M P N 6 b Y W Z e m A p u r O d 9 o 8 r a + s b m V n W 7 t r O 7 t 3 9 Q P z z q m i T T l H V o I h L d C 4 l h g i v W s d w K 1 k s 1 I z I U 7 D G c 3 B b + 4 x P T h i f q w U 5 T F k g y V j z i l N h C G q Q x H 9 Y b X t O b A 6 8 S v y Q N K N E e 1 r 8 G o 4 R m k i l L B T G m 7 3 u p D X K i L a e C z W q D z L C U 0 A k Z s 7 6 j i k h m g n x + 6 w y f O W W E o 0 S 7 U h b P 1 d 8 T O Z H G T G X o O i W x s V n 2 C v E / r 5 / Z 6 D r I u U o z y x R d L I o y g W 2 C i 8 f x i G t G r Z g 6 Q q j m 7 l Z M Y 6 I J t S 6 e m g v B X 3 5 5 l X Q v m r 7 X 9 O 8 v G 6 2 b M o 4 q n M A p n I M P V 9 C C O 2 h D B y j E 8 A y v 8 I Y k e k H v 6 G P R W k H l z D H 8 A f r 8 A R O I j j 8 = &lt; / l a t e x i t &gt; ↵ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " J t D q a C S Y H d U s A r J l V i G Z O Y t H m 8 o = " &gt; A A A B 7 X i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o M e g F 4 8 R z A O S J f R O J s m Y 2 Z l l Z l Y I S / 7 B i w d F v P o / 3 v w b J 8 k e N L G g o a j q p r s r S g Q 3 1 v e / v c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q G p V q y h p U C a X b E R o m u G Q N y 6 1 g 7 U Q z j C P B W t H 4 d u a 3 n p g 2 X M k H O 0 l Y G O N Q 8 g G n a J 3 U 7 K J I R t g r V / y q P w d Z J U F O K p C j 3 i t / d f u K p j G T l g o 0 p h P 4 i Q 0 z 1 J Z T w a a l b m p Y g n S M Q 9 Z x V G L M T J j N r 5 2 S M 6 f 0 y U B p V 9 K S u f p 7 I s P Y m E k c u c 4 Y 7 c g s e z P x P 6 + T 2 s F 1 m H G Z p J Z J u l g 0 S A W x i s x e J 3 2 u G b V i 4 g h S z d 2 t h I 5 Q I 7 U u o J I L I V h + e Z U 0 L 6 q B X w 3 u L y u 1 m z y O I p z A K Z x D A F d Q g z u o Q w M o P M I z v M K b p 7 w X 7 9 3 7 W L Q W v H z m G P 7 A + / w B i 4 G P G A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " J t D q a C S Y H d U s A r J l V i G Z O Y t H m 8 o = " &gt; A A A B 7 X i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o M e g F 4 8 R z A O S J f R O J s m Y 2 Z l l Z l Y I S / 7 B i w d F v P o / 3 v w b J 8 k e N L G g o a j q p r s r S g Q 3 1 v e / v c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q G p V q y h p U C a X b E R o m u G Q N y 6 1 g 7 U Q z j C P B W t H 4 d u a 3 n p g 2 X M k H O 0 l Y G O N Q 8 g G n a J 3 U 7 K J I R t g r V / y q P w d Z J U F O K p C j 3 i t / d f u K p j G T l g o 0 p h P 4 i Q 0 z 1 J Z T w a a l b m p Y g n S M Q 9 Z x V G L M T J j N r 5 2 S M 6 f 0 y U B p V 9 K S u f p 7 I s P Y m E k c u c 4 Y 7 c g s e z P x P 6 + T 2 s F 1 m H G Z p J Z J u l g 0 S A W x i s x e J 3 2 u G b V i 4 g h S z d 2 t h I 5 Q I 7 U u o J I L I V h + e Z U 0 L 6 q B X w 3 u L y u 1 m z y O I p z A K Z x D A F d Q g z u o Q w M o P M I z v M K b p 7 w X 7 9 3 7 W L Q W v H z m G P 7 A + / w B i 4 G P G A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " J t D q a C S Y H d U s A r J l V i G Z O Y t H m 8 o = " &gt; A A A B 7 X i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o M e g F 4 8 R z A O S J f R O J s m Y 2 Z l l Z l Y I S / 7 B i w d F v P o / 3 v w b J 8 k e N L G g o a j q p r s r S g Q 3 1 v e / v c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q G p V q y h p U C a X b E R o m u G Q N y 6 1 g 7 U Q z j C P B W t H 4 d u a 3 n p g 2 X M k H O 0 l Y G O N Q 8 g G n a J 3 U 7 K J I R t g r V / y q P w d Z J U F O K p C j 3 i t / d f u K p j G T l g o 0 p h P 4 i Q 0 z 1 J Z T w a a l b m p Y g n S M Q 9 Z x V G L M T J j N r 5 2 S M 6 f 0 y U B p V 9 K S u f p 7 I s P Y m E k c u c 4 Y 7 c g s e z P x P 6 + T 2 s F 1 m H G Z p J Z J u l g 0 S A W x i s x e J 3 2 u G b V i 4 g h S z d 2 t h I 5 Q I 7 U u o J I L I V h + e Z U 0 L 6 q B X w 3 u L y u 1 m z y O I p z A K Z x D A F d Q g z u o Q w M o P M I z v M K b p 7 w X 7 9 3 7 W L Q W v H z m G P 7 A + / w B i 4 G P G A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " J t D q a C S Y H d U s A r J l V i G Z O Y t H m 8 o = " &gt; A A A B 7 X i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o M e g F 4 8 R z A O S J f R O J s m Y 2 Z l l Z l Y I S / 7 B i w d F v P o / 3 v w b J 8 k e N L G g o a j q p r s r S g Q 3 1 v e / v c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q G p V q y h p U C a X b E R o m u G Q N y 6 1 g 7 U Q z j C P B W t H 4 d u a 3 n p g 2 X M k H O 0 l Y G O N Q 8 g G n a J 3 U 7 K J I R t g r V / y q P w d Z J U F O K p C j 3 i t / d f u K p j G T l g o 0 p h P 4 i Q 0 z 1 J Z T w a a l b m p Y g n S M Q 9 Z x V G L M T J j N r 5 2 S M 6 f 0 y U B p V 9 K S u f p 7 I s P Y m E k c u c 4 Y 7 c g s e z P x P 6 + T 2 s F 1 m H G Z p J Z J u l g 0 S A W x i s x e J 3 2 u G b V i 4 g h S z d 2 t h I 5 Q I 7 U u o J I L I V h + e Z U 0 L 6 q B X w 3 u L y u 1 m z y O I p z A K Z x D A F d Q g z u o Q w M o P M I z v M K b p 7 w X 7 9 3 7 W L Q W v H z m G P 7 A + / w B i 4 G P G A = = &lt; / l a t e x i t &gt; K &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N r n J F n Q i p 0 m n C Q u M Q d 5 3 f 7 6 p Y S s = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i + C l B f s B b S i b 7 a R d u 9 m E 3 Y 1 Q Q n + B F w + K e P U n e f P f u G 1 z 0 N Y H A 4 / 3 Z p i Z F y S C a + O 6 3 0 5 h b X 1 j c 6 u 4 X d r Z 3 d s / K B 8 e t X S c K o Z N F o t Y d Q K q U X C J T c O N w E 6 i k E a B w H Y w v p 3 5 7 S d U m s f y w U w S 9 C M 6 l D z k j B o r N e 7 7 5 Y p b d e c g q 8 T L S Q V y 1 P v l r 9 4 g Z m m E 0 j B B t e 5 6 b m L 8 j C r D m c B p q Z d q T C g b 0 y F 2 L Z U 0 Q u 1 n 8 0 O n 5 M w q A x L G y p Y 0 Z K 7 + n s h o p P U k C m x n R M 1 I L 3 s z 8 T + v m 5 r w 2 s + 4 T F K D k i 0 W h a k g J i a z r 8 m A K 2 R G T C y h T H F 7 K 2 E j q i g z N p u S D c F b f n m V t C 6 q n l v 1 G p e V 2 k 0 e R x F O 4 B T O w Y M r q M E d 1 K E J D B C e 4 R X e n E f n x X l 3 P h a t B S e f O Y Y / c D 5 / A K I F j M 8 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N r n J F n Q i p 0 m n C Q u M Q d 5 3 f 7 6 p Y S s = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i + C l B f s B b S i b 7 a R d u 9 m E 3 Y 1 Q Q n + B F w + K e P U n e f P f u G 1 z 0 N Y H A 4 / 3 Z p i Z F y S C a + O 6 3 0 5 h b X 1 j c 6 u 4 X d r Z 3 d s / K B 8 e t X S c K o Z N F o t Y d Q K q U X C J T c O N w E 6 i k E a B w H Y w v p 3 5 7 S d U m s f y w U w S 9 C M 6 l D z k j B o r N e 7 7 5 Y p b d e c g q 8 T L S Q V y 1 P v l r 9 4 g Z m m E 0 j B B t e 5 6 b m L 8 j C r D m c B p q Z d q T C g b 0 y F 2 L Z U 0 Q u 1 n 8 0 O n 5 M w q A x L G y p Y 0 Z K 7 + n s h o p P U k C m x n R M 1 I L 3 s z 8 T + v m 5 r w 2 s + 4 T F K D k i 0 W h a k g J i a z r 8 m A K 2 R G T C y h T H F 7 K 2 E j q i g z N p u S D c F b f n m V t C 6 q n l v 1 G p e V 2 k 0 e R x F O 4 B T O w Y M r q M E d 1 K E J D B C e 4 R X e n E f n x X l 3 P h a t B S e f O Y Y / c D 5 / A K I F j M 8 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N r n J F n Q i p 0 m n C Q u M Q d 5 3 f 7 6 p Y S s = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i + C l B f s B b S i b 7 a R d u 9 m E 3 Y 1 Q Q n + B F w + K e P U n e f P f u G 1 z 0 N Y H A 4 / 3 Z p i Z F y S C a + O 6 3 0 5 h b X 1 j c 6 u 4 X d r Z 3 d s / K B 8 e t X S c K o Z N F o t Y d Q K q U X C J T c O N w E 6 i k E a B w H Y w v p 3 5 7 S d U m s f y w U w S 9 C M 6 l D z k j B o r N e 7 7 5 Y p b d e c g q 8 T L S Q V y 1 P v l r 9 4 g Z m m E 0 j B B t e 5 6 b m L 8 j C r D m c B p q Z d q T C g b 0 y F 2 L Z U 0 Q u 1 n 8 0 O n 5 M w q A x L G y p Y 0 Z K 7 + n s h o p P U k C m x n R M 1 I L 3 s z 8 T + v m 5 r w 2 s + 4 T F K D k i 0 W h a k g J i a z r 8 m A K 2 R G T C y h T H F 7 K 2 E j q i g z N p u S D c F b f n m V t C 6 q n l v 1 G p e V 2 k 0 e R x F O 4 B T O w Y M r q M E d 1 K E J D B C e 4 R X e n E f n x X l 3 P h a t B S e f O Y Y / c D 5 / A K I F j M 8 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N r n J F n Q i p 0 m n C Q u M Q d 5 3 f 7 6 p Y S s = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i + C l B f s B b S i b 7 a R d u 9 m E 3 Y 1 Q Q n + B F w + K e P U n e f P f u G 1 z 0 N Y H A 4 / 3 Z p i Z F y S C a + O 6 3 0 5 h b X 1 j c 6 u 4 X d r Z 3 d s / K B 8 e t X S c K o Z N F o t Y d Q K q U X C J T c O N w E 6 i k E a B w H Y w v p 3 5 7 S d U m s f y w U w S 9 C M 6 l D z k j B o r N e 7 7 5 Y p b d e c g q 8 T L S Q V y 1 P v l r 9 4 g Z m m E 0 j B B t e 5 6 b m L 8 j C r D m c B p q Z d q T C g b 0 y F 2 L Z U 0 Q u 1 n 8 0 O n 5 M w q A x L G y p Y 0 Z K 7 + n s h o p P U k C m x n R M 1 I L 3 s z 8 T + v m 5 r w 2 s + 4 T F K D k i 0 W h a k g J i a z r 8 m A K 2 R G T C y h T H F 7 K 2 E j q i g z N p u S D c F b f n m V t C 6 q n l v 1 G p e V 2 k 0 e R x F O 4 B T O w Y M r q M E d 1 K E J D B C e 4 R X e n E f n x X l 3 P h a t B S e f O Y Y / c D 5 / A K I F j M 8 = &lt; / l a t e x i t &gt; x &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " f 2 y z i m w b R / D g j z p 6 t Z 3 6 0 f H R q N I = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c W 7 A e 0 o W y 2 k 3 b t Z h N 2 N 2 I J / Q V e P C j i 1 Z / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 H 1 F p H s t 7 M 0 n Q j + h Q 8 p A z a q z U e O q X K 2 7 V n Y O s E i 8 n F c h R 7 5 e / e o O Y p R F K w w T V u u u 5 i f E z q g x n A q e l X q o x o W x M h 9 i 1 V N I I t Z / N D 5 2 S M 6 s M S B g r W 9 K Q u f p 7 I q O R 1 p M o s J 0 R N S O 9 7 M 3 E / 7 x u a s J r P + M y S Q 1 K t l g U p o K Y m M y + J g O u k B k x s Y Q y x e 2 t h I 2 o o s z Y b E o 2 B G / 5 5 V X S u q h 6 b t V r X F Z q N 3 k c R T i B U z g H D 6 6 g B n d Q h y Y w Q H i G V 3 h z H p w X 5 9 3 5 W L Q W n H z m G P 7 A + f w B 5 j m M / A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " f 2 y z i m w b R / D g j z p 6 t Z 3 6 0 f H R q N I = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c W 7 A e 0 o W y 2 k 3 b t Z h N 2 N 2 I J / Q V e P C j i 1 Z / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 H 1 F p H s t 7 M 0 n Q j + h Q 8 p A z a q z U e O q X K 2 7 V n Y O s E i 8 n F c h R 7 5 e / e o O Y p R F K w w T V u u u 5 i f E z q g x n A q e l X q o x o W x M h 9 i 1 V N I I t Z / N D 5 2 S M 6 s M S B g r W 9 K Q u f p 7 I q O R 1 p M o s J 0 R N S O 9 7 M 3 E / 7 x u a s J r P + M y S Q 1 K t l g U p o K Y m M y + J g O u k B k x s Y Q y x e 2 t h I 2 o o s z Y b E o 2 B G / 5 5 V X S u q h 6 b t V r X F Z q N 3 k c R T i B U z g H D 6 6 g B n d Q h y Y w Q H i G V 3 h z H p w X 5 9 3 5 W L Q W n H z m G P 7 A + f w B 5 j m M / A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " f 2 y z i m w b R / D g j z p 6 t Z 3 6 0 f H R q N I = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c W 7 A e 0 o W y 2 k 3 b t Z h N 2 N 2 I J / Q V e P C j i 1 Z / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 H 1 F p H s t 7 M 0 n Q j + h Q 8 p A z a q z U e O q X K 2 7 V n Y O s E i 8 n F c h R 7 5 e / e o O Y p R F K w w T V u u u 5 i f E z q g x n A q e l X q o x o W x M h 9 i 1 V N I I t Z / N D 5 2 S M 6 s M S B g r W 9 K Q u f p 7 I q O R 1 p M o s J 0 R N S O 9 7 M 3 E / 7 x u a s J r P + M y S Q 1 K t l g U p o K Y m M y + J g O u k B k x s Y Q y x e 2 t h I 2 o o s z Y b E o 2 B G / 5 5 V X S u q h 6 b t V r X F Z q N 3 k c R T i B U z g H D 6 6 g B n d Q h y Y w Q H i G V 3 h z H p w X 5 9 3 5 W L Q W n H z m G P 7 A + f w B 5 j m M / A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " f 2 y z i m w b R / D g j z p 6 t Z 3 6 0 f H R q N I = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c W 7 A e 0 o W y 2 k 3 b t Z h N 2 N 2 I J / Q V e P C j i 1 Z / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 H 1 F p H s t 7 M 0 n Q j + h Q 8 p A z a q z U e O q X K 2 7 V n Y O s E i 8 n F c h R 7 5 e / e o O Y p R F K w w T V u u u 5 i f E z q g x n A q e l X q o x o W x M h 9 i 1 V N I I t Z / N D 5 2 S M 6 s M S B g r W 9 K Q u f p 7 I q O R 1 p M o s J 0 R N S O 9 7 M 3 E / 7 x u a s J r P + M y S Q 1 K t l g U p o K Y m M y + J g O u k B k x s Y Q y x e 2 t h I 2 o o s z Y b E o 2 B G / 5 5 V X S u q h 6 b t V r X F Z q N 3 k c R T i B U z g H D 6 6 g B n d Q h y Y w Q H i G V 3 h z H p w X 5 9 3 5 W L Q W n H z m G P 7 A + f w B 5 j m M / A = = &lt; / l a t e x i t &gt; y &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l 2 9 W x o U b 9 D E b v m h L G 7 j H t Z 0 O U 2 4 = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k I e i x 6 8 d i C / Y A 2 l M 1 2 0 q 7 d b M L u R g i h v 8 C L B 0 W 8 + p O 8 + W / c t j l o 6 4 O B x 3 s z z M w L E s G 1 c d 1 v Z 2 1 9 Y 3 N r u 7 R T 3 t 3b P z i s H B 2 3 d Z w q h i 0 W i 1 h 1 A 6 p R c I k t w 4 3 A b q K Q R o H A T j C 5 m / m d J 1 S a x / L B Z A n 6 E R 1 J H n J G j Z W a 2 a B S d W v u H G S V e A W p Q o H G o P L V H 8 Y s j VA a J q j W P c 9 N j J 9 T Z T g T O C 3 3 U 4 0 J Z R M 6 w p 6 l k k a o / X x + 6 J S c W 2 V I w l j Z k o b M 1 d 8 T O Y 2 0 z q L A d k b U j P W y N x P / 8 3 q p C W / 8 n M s k N S j Z Y l G Y C m J i M v u a D L l C Z k R m C W W K 2 1 s J G 1 N F m b H Z l G 0 I 3 v L L q 6 R 9 W f P c m t e 8 q t Z v i z h K c A p n c A E e X E M d 7 q E B L W C A 8 A y v 8 O Y 8 O i / O u / O x a F 1 z i p k T + A P n 8 w f n v Y z 9 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l 2 9 W x o U b 9 D E b v m h L G 7 j H t Z 0 O U 2 4 = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k I e i x 6 8 d i C / Y A 2 l M 1 2 0 q 7 d b M L u R g i h v 8 C L B 0 W 8 + p O 8 + W / c t j l o 6 4 O B x 3 s z z M w L E s G 1 c d 1 v Z 2 1 9 Y 3 N r u 7 R T 3 t 3b P z i s H B 2 3 d Z w q h i 0 W i 1 h 1 A 6 p R c I k t w 4 3 A b q K Q R o H A T j C 5 m / m d J 1 S a x / L B Z A n 6 E R 1 J H n J G j Z W a 2 a B S d W v u H G S V e A W p Q o H G o P L V H 8 Y s j VA a J q j W P c 9 N j J 9 T Z T g T O C 3 3 U 4 0 J Z R M 6 w p 6 l k k a o / X x + 6 J S c W 2 V I w l j Z k o b M 1 d 8 T O Y 2 0 z q L A d k b U j P W y N x P / 8 3 q p C W / 8 n M s k N S j Z Y l G Y C m J i M v u a D L l C Z k R m C W W K 2 1 s J G 1 N F m b H Z l G 0 I 3 v L L q 6 R 9 W f P c m t e 8 q t Z v i z h K c A p n c A E e X E M d 7 q E B L W C A 8 A y v 8 O Y 8 O i / O u / O x a F 1 z i p k T + A P n 8 w f n v Y z 9 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l 2 9 W x o U b 9 D E b v m h L G 7 j H t Z 0 O U 2 4 = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k I e i x 6 8 d i C / Y A 2 l M 1 2 0 q 7 d b M L u R g i h v 8 C L B 0 W 8 + p O 8 + W / c t j l o 6 4 O B x 3 s z z M w L E s G 1 c d 1 v Z 2 1 9 Y 3 N r u 7 R T 3 t 3b P z i s H B 2 3 d Z w q h i 0 W i 1 h 1 A 6 p R c I k t w 4 3 A b q K Q R o H A T j C 5 m / m d J 1 S a x / L B Z A n 6 E R 1 J H n J G j Z W a 2 a B S d W v u H G S V e A W p Q o H G o P L V H 8 Y s j VA a J q j W P c 9 N j J 9 T Z T g T O C 3 3 U 4 0 J Z R M 6 w p 6 l k k a o / X x + 6 J S c W 2 V I w l j Z k o b M 1 d 8 T O Y 2 0 z q L A d k b U j P W y N x P / 8 3 q p C W / 8 n M s k N S j Z Y l G Y C m J i M v u a D L l C Z k R m C W W K 2 1 s J G 1 N F m b H Z l G 0 I 3 v L L q 6 R 9 W f P c m t e 8 q t Z v i z h K c A p n c A E e X E M d 7 q E B L W C A 8 A y v 8 O Y 8 O i / O u / O x a F 1 z i p k T + A P n 8 w f n v Y z 9 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l 2 9 W x o U b 9 D E b v m h L G 7 j H t Z 0 O U 2 4 = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k I e i x 6 8 d i C / Y A 2 l M 1 2 0 q 7 d b M L u R g i h v 8 C L B 0 W 8 + p O 8 + W / c t j l o 6 4 O B x 3 s z z M w L E s G 1 c d 1 v Z 2 1 9 Y 3 N r u 7 R T 3 t 3b P z i s H B 2 3 d Z w q h i 0 W i 1 h 1 A 6 p R c I k t w 4 3 A b q K Q R o H A T j C 5 m / m d J 1 S a x / L B Z A n 6 E R 1 J H n J G j Z W a 2 a B S d W v u H G S V e A W p Q o H G o P L V H 8 Y s j VA a J q j W P c 9 N j J 9 T Z T g T O C 3 3 U 4 0 J Z R M 6 w p 6 l k k a o / X x + 6 J S c W 2 V I w l j Z k o b M 1 d 8 T O Y 2 0 z q L A d k b U j P W y N x P / 8 3 q p C W / 8 n M s k N S j Z Y l G Y C m J i M v u a D L l C Z k R m C W W K 2 1 s J G 1 N F m b H Z l G 0 I 3 v L L q 6 R 9 W f P c m t e 8 q t Z v i z h K c A p n c A E e X E M d 7 q E B L W C A 8 A y v 8 O Y 8 O i / O u / O x a F 1 z i p k T + A P n 8 w f n v Y z 9 &lt; / l a t e x i t &gt; z &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H D z X c h l s P l m u E y Z Z / 9 z F J + i V C 6 I = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c W 7 A e 0 o W y 2 k 3 b t Z h N 2 N 0 I N / Q V e P C j i 1 Z / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 H 1 F p H s t 7 M 0 n Q j + h Q 8 p A z a q z U e O q X K 2 7 V n Y O s E i 8 n F c h R 7 5 e / e o O Y p R F K w w T V u u u 5 i f E z q g x n A q e l X q o x o W x M h 9 i 1 V N I I t Z / N D 5 2 S M 6 s M S B g r W 9 K Q u f p 7 I q O R 1 p M o s J 0 R N S O 9 7 M 3 E / 7 x u a s J r P + M y S Q 1 K t l g U p o K Y m M y + J g O u k B k x s Y Q y x e 2 t h I 2 o o s z Y b E o 2 B G / 5 5 V X S u q h 6 b t V r X F Z q N 3 k c R T i B U z g H D 6 6 g B n d Q h y Y w Q H i G V 3 h z H p w X 5 9 3 5 W L Q W n H z m G P 7 A + f w B 6 U G M / g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H D z X c h l s P l m u E y Z Z / 9 z F J + i V C 6 I = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c W 7 A e 0 o W y 2 k 3 b t Z h N 2 N 0 I N / Q V e P C j i 1 Z / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 H 1 F p H s t 7 M 0 n Q j + h Q 8 p A z a q z U e O q X K 2 7 V n Y O s E i 8 n F c h R 7 5 e / e o O Y p R F K w w T V u u u 5 i f E z q g x n A q e l X q o x o W x M h 9 i 1 V N I I t Z / N D 5 2 S M 6 s M S B g r W 9 K Q u f p 7 I q O R 1 p M o s J 0 R N S O 9 7 M 3 E / 7 x u a s J r P + M y S Q 1 K t l g U p o K Y m M y + J g O u k B k x s Y Q y x e 2 t h I 2 o o s z Y b E o 2 B G / 5 5 V X S u q h 6 b t V r X F Z q N 3 k c R T i B U z g H D 6 6 g B n d Q h y Y w Q H i G V 3 h z H p w X 5 9 3 5 W L Q W n H z m G P 7 A + f w B 6 U G M / g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H D z X c h l s P l m u E y Z Z / 9 z F J + i V C 6 I = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c W 7 A e 0 o W y 2 k 3 b t Z h N 2 N 0 I N / Q V e P C j i 1 Z / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 H 1 F p H s t 7 M 0 n Q j + h Q 8 p A z a q z U e O q X K 2 7 V n Y O s E i 8 n F c h R 7 5 e / e o O Y p R F K w w T V u u u 5 i f E z q g x n A q e l X q o x o W x M h 9 i 1 V N I I t Z / N D 5 2 S M 6 s M S B g r W 9 K Q u f p 7 I q O R 1 p M o s J 0 R N S O 9 7 M 3 E / 7 x u a s J r P + M y S Q 1 K t l g U p o K Y m M y + J g O u k B k x s Y Q y x e 2 t h I 2 o o s z Y b E o 2 B G / 5 5 V X S u q h 6 b t V r X F Z q N 3 k c R T i B U z g H D 6 6 g B n d Q h y Y w Q H i G V 3 h z H p w X 5 9 3 5 W L Q W n H z m G P 7 A + f w B 6 U G M / g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H D z X c h l s P l m u E y Z Z / 9 z F J + i V C 6 I = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c W 7 A e 0 o W y 2 k 3 b t Z h N 2 N 0 I N / Q V e P C j i 1 Z / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 H 1 F p H s t 7 M 0 n Q j + h Q 8 p A z a q z U e O q X K 2 7 V n Y O s E i 8 n F c h R 7 5 e / e o O Y p R F K w w T V u u u 5 i f E z q g x n A q e l X q o x o W x M h 9 i 1 V N I I t Z / N D 5 2 S M 6 s M S B g r W 9 K Q u f p 7 I q O R 1 p M o s J 0 R N S O 9 7 M 3 E / 7 x u a s J r P + M y S Q 1 K t l g U p o K Y m M y + J g O u k B k x s Y Q y x e 2 t h I 2 o o s z Y b E o 2 B G / 5 5 V X S u q h 6 b t V r X F Z q N 3 k c R T i B U z g H D 6 6 g B n d Q h y Y w Q H i G V 3 h z H p w X 5 9 3 5 W L Q W n H z m G P 7 A + f w B 6 U G M / g = = &lt; / l a t e x i t &gt; N &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F s F o h X Q j c 3 O A / B B A i 7 J X P 9 T M C N 0 = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i y d p w X 5 A G 8 p m O 2 n X b j Z h d y O U 0 F / g x Y M i X v 1 J 3 v w 3 b t s c t P X B w O O 9 G W b m B Y n g 2 r j u t 1 N Y W 9 / Y 3 C p u l 3 Z 2 9 / Y P y o d H L R 2 n i m G T x S J W n Y B q F F x i 0 3 A j s J M o p F E g s B 2 M b 2 d + + w m V 5 r F 8 M J M E / Y g O J Q 8 5 o 8 Z K j f t + u e J W 3 T n I K v F y U o E c 9 X 7 5 q z e I W R q h N E x Q r b u e m x g / o 8 p w J n B a 6 q U a E 8 r G d I h d S y W N U P v Z / N A p O b P K g I S x s i U N m a u / J z I a a T 2 J A t s Z U T P S y 9 5 M / M / r p i a 8 9 j M u k 9 S g Z I t F Y S q I i c n s a z L g C p k R E 0 s o U 9 z e S t i I K s q M z a Z k Q / C W X 1 4 l r Y u q 5 1 a 9 x m W l d p P H U Y Q T O I V z 8 O A K a n A H d W g C A 4 R n e I U 3 5 9 F 5 c d 6 d j 0 V r w c l n j u E P n M 8 f p p G M 0 g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F s F o h X Q j c 3 O A / B B A i 7 J X P 9 T M C N 0 = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i y d p w X 5 A G 8 p m O 2 n X b j Z h d y O U 0 F / g x Y M i X v 1 J 3 v w 3 b t s c t P X B w O O 9 G W b m B Y n g 2 r j u t 1 N Y W 9 / Y 3 C p u l 3 Z 2 9 / Y P y o d H L R 2 n i m G T x S J W n Y B q F F x i 0 3 A j s J M o p F E g s B 2 M b 2 d + + w m V 5 r F 8 M J M E / Y g O J Q 8 5 o 8 Z K j f t + u e J W 3 T n I K v F y U o E c 9 X 7 5 q z e I W R q h N E x Q r b u e m x g / o 8 p w J n B a 6 q U a E 8 r G d I h d S y W N U P v Z / N A p O b P K g I S x s i U N m a u / J z I a a T 2 J A t s Z U T P S y 9 5 M / M / r p i a 8 9 j M u k 9 S g Z I t F Y S q I i c n s a z L g C p k R E 0 s o U 9 z e S t i I K s q M z a Z k Q / C W X 1 4 l r Y u q 5 1 a 9 x m W l d p P H U Y Q T O I V z 8 O A K a n A H d W g C A 4 R n e I U 3 5 9 F 5 c d 6 d j 0 V r w c l n j u E P n M 8 f p p G M 0 g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F s F o h X Q j c 3 O A / B B A i 7 J X P 9 T M C N 0 = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i y d p w X 5 A G 8 p m O 2 n X b j Z h d y O U 0 F / g x Y M i X v 1 J 3 v w 3 b t s c t P X B w O O 9 G W b m B Y n g 2 r j u t 1 N Y W 9 / Y 3 C p u l 3 Z 2 9 / Y P y o d H L R 2 n i m G T x S J W n Y B q F F x i 0 3 A j s J M o p F E g s B 2 M b 2 d + + w m V 5 r F 8 M J M E / Y g O J Q 8 5 o 8 Z K j f t + u e J W 3 T n I K v F y U o E c 9 X 7 5 q z e I W R q h N E x Q r b u e m x g / o 8 p w J n B a 6 q U a E 8 r G d I h d S y W N U P v Z / N A p O b P K g I S x s i U N m a u / J z I a a T 2 J A t s Z U T P S y 9 5 M / M / r p i a 8 9 j M u k 9 S g Z I t F Y S q I i c n s a z L g C p k R E 0 s o U 9 z e S t i I K s q M z a Z k Q / C W X 1 4 l r Y u q 5 1 a 9 x m W l d p P H U Y Q T O I V z 8 O A K a n A H d W g C A 4 R n e I U 3 5 9 F 5 c d 6 d j 0 V r w c l n j u E P n M 8 f p p G M 0 g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F s F o h X Q j c 3 O A / B B A i 7 J X P 9 T M C N 0 = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i y d p w X 5 A G 8 p m O 2 n X b j Z h d y O U 0 F / g x Y M i X v 1 J 3 v w 3 b t s c t P X B w O O 9 G W b m B Y n g 2 r j u t 1 N Y W 9 / Y 3 C p u l 3 Z 2 9 / Y P y o d H L R 2 n i m G T x S J W n Y B q F F x i 0 3 A j s J M o p F E g s B 2 M b 2 d + + w m V 5 r F 8 M J M E / Y g O J Q 8 5 o 8 Z K j f t + u e J W 3 T n I K v F y U o E c 9 X 7 5 q z e I W R q h N E x Q r b u e m x g / o 8 p w J n B a 6 q U a E 8 r G d I h d S y W N U P v Z / N A p O b P K g I S x s i U N m a u / J z I a a T 2 J A t s Z U T P S y 9 5 M / M / r p i a 8 9 j M u k 9 S g Z I t F Y S q I i c n s a z L g C p k R E 0 s o U 9 z e S t i I K s q M z a Z k Q / C W X 1 4 l r Y u q 5 1 a 9 x m W l d p P H U Y Q T O I V z 8 O A K a n A H d W g C A 4 R n e I U 3 5 9 F 5 c d 6 d j 0 V r w c l n j u E P n M 8 f p p G M 0 g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " v M C m C B v r Q q v M k c Z e 5 c 7 8 y K 1 9 q u 0 = " &gt; A A A B 6 3 i c b V B N S w M x E J 3 U r 1 q / q h 6 9 B I v g q e y K o M e i F 4 8 V b C 2 0 S 8 m m 2 W 5 o k l 2 S r F C W / g U v H h T x 6 h / y 5 r 8 x 2 + 5 B W x 8 M P N 6 b Y W Z e m A p u r O d 9 o 8 r a + s b m V n W 7 t r O 7 t 3 9 Q P z z q m i T T l H V o I h L d C 4 l h g i v W s d w K 1 k s 1 I z I U 7 D G c 3 B b + 4 x P T h i f q w U 5 T F k g y V j z i l N h C G q Q x H 9 Y b X t O b A 6 8 S v y Q N K N E e 1 r 8 G o 4 R m k i l L B T G m 7 3 u p D X K i L a e C z W q D z L C U 0 A k Z s 7 6 j i k h m g n x + 6 w y f O W W E o 0 S 7 U h b P 1 d 8 T O Z H G T G X o O i W x s V n 2 C v E / r 5 / Z 6 D r I u U o z y x R d L I o y g W 2 C i 8 f x i G t G r Z g 6 Q q j m 7 l Z M Y 6 I J t S 6 e m g v B X 3 5 5 l X Q v m r 7 X 9 O 8 v G 6 2 b M o 4 q n M A p n I M P V 9 C C O 2 h D B y j E 8 A y v 8 I Y k e k H v 6 G P R W k H l z D H 8 A f r 8 A R O I j j 8 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " v M C m C B v r Q q v M k c Z e 5 c 7 8 y K 1 9 q u 0 = " &gt; A A A B 6 3 i c b V B N S w M x E J 3 U r 1 q / q h 6 9 B I v g q e y K o M e i F 4 8 V b C 2 0 S 8 m m 2 W 5 o k l 2 S r F C W / g U v H h T x 6 h / y 5 r 8 x 2 + 5 B W x 8 M P N 6 b Y W Z e m A p u r O d 9 o 8 r a + s b m V n W 7 t r O 7 t 3 9 Q P z z q m i T T l H V o I h L d C 4 l h g i v W s d w K 1 k s 1 I z I U 7 D G c 3 B b + 4 x P T h i f q w U 5 T F k g y V j z i l N h C G q Q x H 9 Y b X t O b A 6 8 S v y Q N K N E e 1 r 8 G o 4 R m k i l L B T G m 7 3 u p D X K i L a e C z W q D z L C U 0 A k Z s 7 6 j i k h m g n x + 6 w y f O W W E o 0 S 7 U h b P 1 d 8 T O Z H G T G X o O i W x s V n 2 C v E / r 5 / Z 6 D r I u U o z y x R d L I o y g W 2 C i 8 f x i G t G r Z g 6 Q q j m 7 l Z M Y 6 I J t S 6 e m g v B X 3 5 5 l X Q v m r 7 X 9 O 8 v G 6 2 b M o 4 q n M A p n I M P V 9 C C O 2 h D B y j E 8 A y v 8 I Y k e k H v 6 G P R W k H l z D H 8 A f r 8 A R O I j j 8 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " v M C m C B v r Q q v M k c Z e 5 c 7 8 y K 1 9 q u 0 = " &gt; A A A B 6 3 i c b V B N S w M x E J 3 U r 1 q / q h 6 9 B I v g q e y K o M e i F 4 8 V b C 2 0 S 8 m m 2 W 5 o k l 2 S r F C W / g U v H h T x 6 h / y 5 r 8 x 2 + 5 B W x 8 M P N 6 b Y W Z e m A p u r O d 9 o 8 r a + s b m V n W 7 t r O 7 t 3 9 Q P z z q m i T T l H V o I h L d C 4 l h g i v W s d w K 1 k s 1 I z I U 7 D G c 3 B b + 4 x P T h i f q w U 5 T F k g y V j z i l N h C G q Q x H 9 Y b X t O b A 6 8 S v y Q N K N E e 1 r 8 G o 4 R m k i l L B T G m 7 3 u p D X K i L a e C z W q D z L C U 0 A k Z s 7 6 j i k h m g n x + 6 w y f O W W E o 0 S 7 U h b P 1 d 8 T O Z H G T G X o O i W x s V n 2 C v E / r 5 / Z 6 D r I u U o z y x R d L I o y g W 2 C i 8 f x i G t G r Z g 6 Q q j m 7 l Z M Y 6 I J t S 6 e m g v B X 3 5 5 l X Q v m r 7 X 9 O 8 v G 6 2 b M o 4 q n M A p n I M P V 9 C C O 2 h D B y j E 8 A y v 8 I Y k e k H v 6 G P R W k H l z D H 8 A f r 8 A R O I j j 8 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " v M C m C B v r Q q v M k c Z e 5 c 7 8 y K 1 9 q u 0 = " &gt; A A A B 6 3 i c b V B N S w M x E J 3 U r 1 q / q h 6 9 B I v g q e y K o M e i F 4 8 V b C 2 0 S 8 m m 2 W 5 o k l 2 S r F C W / g U v H h T x 6 h / y 5 r 8 x 2 + 5 B W x 8 M P N 6 b Y W Z e m A p u r O d 9 o 8 r a + s b m V n W 7 t r O 7 t 3 9 Q P z z q m i T T l H V o I h L d C 4 l h g i v W s d w K 1 k s 1 I z I U 7 D G c 3 B b + 4 x P T h i f q w U 5 T F k g y V j z i l N h C G q Q x H 9 Y b X t O b A 6 8 S v y Q N K N E e 1 r 8 G o 4 R m k i l L B T G m 7 3 u p D X K i L a e C z W q D z L C U 0 A k Z s 7 6 j i k h m g n x + 6 w y f O W W E o 0 S 7 U h b P 1 d 8 T O Z H G T G X o O i W x s V n 2 C v E / r 5 / Z 6 D r I u U o z y x R d L I o y g W 2 C i 8 f x i G t G r Z g 6 Q q j m 7 l Z M Y 6 I J t S 6 e m g v B X 3 5 5 l X Q v m r 7 X 9 O 8 v G 6 2 b M o 4 q n M A p n I M P V 9 C C O 2 h D B y j E 8 A y v 8 I Y k e k H v 6 G P R W k H l z D H 8 A f r 8 A R O I j j 8 = &lt; / l a t e x i t &gt; ↵ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " J t D q a C S Y H d U s A r J l V i G Z O Y t H m 8 o = " &gt; A A A B 7 X i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o M e g F 4 8 R z A O S J f R O J s m Y 2 Z l l Z l Y I S / 7 B i w d F v P o / 3 v w b J 8 k e N L G g o a j q p r s r S g Q 3 1 v e / v c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q G p V q y h p U C a X b E R o m u G Q N y 6 1 g 7 U Q z j C P B W t H 4 d u a 3 n p g 2 X M k H O 0 l Y G O N Q 8 g G n a J 3 U 7 K J I R t g r V / y q P w d Z J U F O K p C j 3 i t / d f u K p j G T l g o 0 p h P 4 i Q 0 z 1 J Z T w a a l b m p Y g n S M Q 9 Z x V G L M T J j N r 5 2 S M 6 f 0 y U B p V 9 K S u f p 7 I s P Y m E k c u c 4 Y 7 c g s e z P x P 6 + T 2 s F 1 m H G Z p J Z J u l g 0 S A W x i s x e J 3 2 u G b V i 4 g h S z d 2 t h I 5 Q I 7 U u o J I L I V h + e Z U 0 L 6 q B X w 3 u L y u 1 m z y O I p z A K Z x D A F d Q g z u o Q w M o P M I z v M K b p 7 w X 7 9 3 7 W L Q W v H z m G P 7 A + / w B i 4 G P G A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " J t D q a C S Y H d U s A r J l V i G Z O Y t H m 8 o = " &gt; A A A B 7 X i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o M e g F 4 8 R z A O S J f R O J s m Y 2 Z l l Z l Y I S / 7 B i w d F v P o / 3 v w b J 8 k e N L G g o a j q p r s r S g Q 3 1 v e / v c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q G p V q y h p U C a X b E R o m u G Q N y 6 1 g 7 U Q z j C P B W t H 4 d u a 3 n p g 2 X M k H O 0 l Y G O N Q 8 g G n a J 3 U 7 K J I R t g r V / y q P w d Z J U F O K p C j 3 i t / d f u K p j G T l g o 0 p h P 4 i Q 0 z 1 J Z T w a a l b m p Y g n S M Q 9 Z x V G L M T J j N r 5 2 S M 6 f 0 y U B p V 9 K S u f p 7 I s P Y m E k c u c 4 Y 7 c g s e z P x P 6 + T 2 s F 1 m H G Z p J Z J u l g 0 S A W x i s x e J 3 2 u G b V i 4 g h S z d 2 t h I 5 Q I 7 U u o J I L I V h + e Z U 0 L 6 q B X w 3 u L y u 1 m z y O I p z A K Z x D A F d Q g z u o Q w M o P M I z v M K b p 7 w X 7 9 3 7 W L Q W v H z m G P 7 A + / w B i 4 G P G A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " J t D q a C S Y H d U s A r J l V i G Z O Y t H m 8 o = " &gt; A A A B 7 X i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o M e g F 4 8 R z A O S J f R O J s m Y 2 Z l l Z l Y I S / 7 B i w d F v P o / 3 v w b J 8 k e N L G g o a j q p r s r S g Q 3 1 v e / v c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q G p V q y h p U C a X b E R o m u G Q N y 6 1 g 7 U Q z j C P B W t H 4 d u a 3 n p g 2 X M k H O 0 l Y G O N Q 8 g G n a J 3 U 7 K J I R t g r V / y q P w d Z J U F O K p C j 3 i t / d f u K p j G T l g o 0 p h P 4 i Q 0 z 1 J Z T w a a l b m p Y g n S M Q 9 Z x V G L M T J j N r 5 2 S M 6 f 0 y U B p V 9 K S u f p 7 I s P Y m E k c u c 4 Y 7 c g s e z P x P 6 + T 2 s F 1 m H G Z p J Z J u l g 0 S A W x i s x e J 3 2 u G b V i 4 g h S z d 2 t h I 5 Q I 7 U u o J I L I V h + e Z U 0 L 6 q B X w 3 u L y u 1 m z y O I p z A K Z x D A F d Q g z u o Q w M o P M I z v M K b p 7 w X 7 9 3 7 W L Q W v H z m G P 7 A + / w B i 4 G P G A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " J t D q a C S Y H d U s A r J l V i G Z O Y t H m 8 o = " &gt; A A A B 7 X i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o M e g F 4 8 R z A O S J f R O J s m Y 2 Z l l Z l Y I S / 7 B i w d F v P o / 3 v w b J 8 k e N L G g o a j q p r s r S g Q 3 1 v e / v c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q G p V q y h p U C a X b E R o m u G Q N y 6 1 g 7 U Q z j C P B W t H 4 d u a 3 n p g 2 X M k H O 0 l Y G O N Q 8 g G n a J 3 U 7 K J I R t g r V / y q P w d Z J U F O K p C j 3 i t / d f u K p j G T l g o 0 p h P 4 i Q 0 z 1 J Z T w a a l b m p Y g n S M Q 9 Z x V G L M T J j N r 5 2 S M 6 f 0 y U B p V 9 K S u f p 7 I s P Y m E k c u c 4 Y 7 c g s e z P x P 6 + T 2 s F 1 m H G Z p J Z J u l g 0 S A W x i s x e J 3 2 u G b V i 4 g h S z d 2 t h I 5 Q I 7 U u o J I L I V h + e Z U 0 L 6 q B X w 3 u L y u 1 m z y O I p z A K Z x D A F d Q g z u o Q w M o P M I z v M K b p 7 w X 7 9 3 7 W L Q W v H z m G P 7 A + / w B i 4 G P G A = = &lt; / l a t e x i t &gt; K + 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " x b 7 A v I 1 W s o 4 M B U i M T 0 W y q R 5 Y / y Y = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S I I Q k l U 0 G P R i + C l o v 2 A N p T N d t M u 3 W z C 7 k Q o o T / B i w d F v P q L v P l v 3 L Y 5 a O u D g c d 7 M 8 z M C x I p D L r u t 7 O 0 v L K 6 t l 7 Y K G 5 u b e / s l v b 2 G y Z O N e N 1 F s t Y t w J q u B S K 1 1 G g 5 K 1 E c x o F k j e D 4 c 3 E b z 5 x b U S s H n G U c D + i f S V C w S h a 6 e H u 1 O u W y m 7 F n Y I s E i 8 n Z c h R 6 5 a + O r 2 Y p R F X y C Q 1 p u 2 5 C f o Z 1 S i Y 5 O N i J z U 8 o W x I + 7 x t q a I R N 3 4 2 P X V M j q 3 S I 2 G s b S k k U / X 3 R E Y j Y 0 Z R Y D s j i g M z 7 0 3 E / 7 x 2 i u G V n w m V p M g V m y 0 K U 0 k w J p O / S U 9 o z l C O L K F M C 3 s r Y Q O q K U O b T t G G 4 M 2 / v E g a Z x X v v O L e X 5 S r 1 3 k c B T i E I z g B D y 6 h C r d Q g z o w 6 M M z v M K b I 5 0 X 5 9 3 5 m L U u O f n M A f y B 8 / k D e L m N Q Q = = &lt; / l a t e x i t &gt; The Generalized Latent Class-Conditional Noise model. x and y is the observed training pair. z is the partially observed latent label (the observed samples are for semi-supervised learning). φ ∈ R (K+1)xK is the unknown noise transition (the extra dimension is for outlier learning). α is a Dirichlet parameter. N is the sample number and K is the class number.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>The training loss (left) and the test accuracy (right) of LCCN on the CIFAR-10 dataset with different noise rates r = 0.1, 0.3, 0.5, 0.7, 0.9.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .Fig. 8 .</head><label>78</label><figDesc>The test accuracy of LCCN and S-adaptation in the training on CIFAR-10 with r=0.5 (left), and the corresponding histograms for the change of noise transition φ via a mini-batch of samples (right). The label correction ratio in the training of LCCN on CIFAR-10 with r=0.5 as well as some negatively corrected samples (the red box) and some positively corrected samples (the green box) with the high probabilities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>2019 images classifier network Bayesian noise modeling sampling labels noisy labels Wolf Car Mouse Rose Safeguarded transition update LCCN</head><label></label><figDesc></figDesc><table /><note>arXiv:1903.02152v1 [cs.LG] 6 Mar</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I THE</head><label>I</label><figDesc>AVERAGE ACCURACY (%)OVER  5 TRIALS ON CIFAR-10 AND CIFAR-100 WITH DIFFERENT NOISE LEVELS. 88.12 76.93 59.01 56.85 66.15 64.31 60.11 51.68 33.37 2 Bootstrapping 90.73 88.12 76.29 57.04 56.79 66.48 64.61 63.01 55.27 TRIALS ON OUTLIER-CORRUPTED CIFAR-10 AND CIFAR-100 WITH DIFFERENT NOISE LEVELS.</figDesc><table><row><cell></cell><cell>Dataset</cell><cell></cell><cell></cell><cell>CIFAR-10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CIFAR-100</cell><cell></cell><cell></cell></row><row><cell>#</cell><cell>Method \ Noise Ratio</cell><cell>0.1</cell><cell>0.3</cell><cell>0.5</cell><cell>0.7</cell><cell>0.9</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell></row><row><cell>1</cell><cell>CE</cell><cell cols="10">90.10 34.52</cell></row><row><cell>3</cell><cell>Forward</cell><cell cols="5">90.86 89.03 82.47 67.11 57.29</cell><cell cols="5">65.43 62.72 61.28 52.64 33.82</cell></row><row><cell>4</cell><cell>S-adaptation</cell><cell cols="5">91.02 88.83 86.79 72.74 60.92</cell><cell cols="5">65.52 64.11 62.39 52.74 30.07</cell></row><row><cell>5</cell><cell>LCCN</cell><cell cols="5">91.35 89.33 88.41 79.48 64.82</cell><cell cols="4">67.83 67.63 66.86 65.52</cell><cell>33.71</cell></row><row><cell>6</cell><cell>CE with the clean data</cell><cell></cell><cell></cell><cell>91.63</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>69.41</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE II</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">THE AVERAGE ACCURACY (%) OVER 5 Dataset</cell><cell></cell><cell>CIFAR-10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CIFAR-100</cell><cell></cell><cell></cell></row><row><cell>#</cell><cell>Method \ Noise Ratio</cell><cell>0.1</cell><cell>0.3</cell><cell>0.5</cell><cell>0.7</cell><cell>0.9</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell></row><row><cell>1</cell><cell>CE</cell><cell cols="5">89.13 87.06 74.63 62.29 57.07</cell><cell cols="5">62.94 59.73 54.71 45.57 31.74</cell></row><row><cell>2</cell><cell>Bootstrapping</cell><cell>90.13</cell><cell cols="4">84.58 74.76 54.87 55.56</cell><cell cols="5">63.73 60.88 59.77 40.23 31.86</cell></row><row><cell>3</cell><cell>Forward</cell><cell cols="5">88.63 84.97 78.47 58.23 56.52</cell><cell cols="5">63.69 62.63 61.86 51.47 35.71</cell></row><row><cell>4</cell><cell>S-adaptation</cell><cell cols="5">88.58 87.28 61.17 57.12 56.73</cell><cell cols="5">63.51 61.50 60.59 53.22 32.19</cell></row><row><cell>5</cell><cell>LCCN</cell><cell cols="5">88.63 88.06 82.15 69.48 55.12</cell><cell cols="5">63.97 62.84 61.79 60.34 33.52</cell></row><row><cell>6</cell><cell>LCCN*</cell><cell>89.59</cell><cell cols="3">88.43 84.34 72.33</cell><cell>56.28</cell><cell cols="5">64.71 63.05 62.48 62.02 32.37</cell></row><row><cell>7</cell><cell>LCCN+</cell><cell cols="4">90.30 88.93 88.21 87.42</cell><cell>86.33</cell><cell cols="5">65.67 64.24 63.52 63.19 62.39</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Table II presents the results of LCCN and baselines on the outlier-corrupted datasets. Compared to those in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III THE</head><label>III</label><figDesc>AVERAGE ACCURACY OVER 5 TRIALS ON CLOTHING1M.</figDesc><table><row><cell>#</cell><cell>Method</cell><cell>Accuracy</cell></row><row><cell>1</cell><cell>CE</cell><cell>68.94</cell></row><row><cell>2</cell><cell>Bootstrapping</cell><cell>69.12</cell></row><row><cell>3</cell><cell>Forward</cell><cell>69.84</cell></row><row><cell>4</cell><cell>S-adaptation</cell><cell>70.36</cell></row><row><cell>5</cell><cell>Joint Optimization</cell><cell>72.16</cell></row><row><cell></cell><cell>LCCN</cell><cell>71.63</cell></row><row><cell>6</cell><cell>LCCN warmed-up by φ in [35]</cell><cell>73.07</cell></row><row><cell></cell><cell>LCCN*</cell><cell>72.80</cell></row><row><cell>7</cell><cell>CE on the clean data</cell><cell>75.28</cell></row><row><cell>8</cell><cell>Forward+</cell><cell>80.38</cell></row><row><cell>9</cell><cell>LCCN+</cell><cell>81.25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Table IIIlists the performance of LCCN and baselines on the large-scale Clothing1M. According to the results, we can see that Forward does not show the significant improvement</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV THE</head><label>IV</label><figDesc>LEARNED NOISE TRANSITION ON CLOTHING1M BY LCCN.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V THE</head><label>V</label><figDesc>AVERAGE ACCURACY OVER 5 TRIALS ON WEBVISION.</figDesc><table><row><cell>#</cell><cell>Method</cell><cell>Accuracy@1</cell><cell>Accuracy@5</cell></row><row><cell>1</cell><cell>CE</cell><cell>58.61</cell><cell>80.94</cell></row><row><cell>2</cell><cell>Bootstrapping</cell><cell>58.48</cell><cell>80.81</cell></row><row><cell>3</cell><cell>Forward</cell><cell>58.93</cell><cell>81.06</cell></row><row><cell>4</cell><cell>S-adaptation</cell><cell>58.00</cell><cell>80.16</cell></row><row><cell>5</cell><cell>LCCN LCCN*</cell><cell>58.73 59.09</cell><cell>81.25 81.33</cell></row><row><cell>6</cell><cell>CE on the clean data</cell><cell>53.52</cell><cell>77.84</cell></row><row><cell>7</cell><cell>LCCN+</cell><cell>59.72</cell><cell>80.34</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that ¬ means removing the current object statistic from the whole collection of all object statistics.<ref type="bibr" target="#b1">2</ref> The probabilistic prediction from the model is clipped between ξ and 1−ξ for the computational stability, where ξ is set to 10 −20 in this paper.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://en.wikipedia.org/wiki/Risk difference</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Due to the reason of time and the computational resource, in this paper, we only use the original WebVision 1.0 dataset. The newest version, namely WebVision 2.0 dataset, contains more images and more classes.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal processing magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Robust web image annotation via exploring multi-facet and structural knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4871" to="4884" />
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Exploring weakly labeled images for video object segmentation with submodular proposal selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4245" to="4259" />
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recognition from web data: A progressive filtering approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5303" to="5315" />
			<date type="published" when="2018-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep learning from noisy image labels with quality embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1909" to="1922" />
			<date type="published" when="2019-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A closer look at memorization in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Kanwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dimensionalitydriven learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Houle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wijewickrema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to reweight examples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Curriculumnet: Weakly supervised learning from large-scale web images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Making deep neural networks robust to label noise: A loss correction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Training deep neuralnetworks using a noise adaptation layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ben-Reuven</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Training convolutional networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">ICLR</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Seeing through the Human Reporting Bias: Visual Classifiers from Noisy Human-Centric Labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Masking: A new perspective of noisy supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Classification with noisy labels by importance reweighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="447" to="461" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<title level="m">Training deep neural networks on noisy labels with bootstrapping</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">ICLR</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning from noisy labels with distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Iterative learning with open-set noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<title level="m">Mentornet: Regularizing very deep neural networks on corrupted labels</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Collaborative learning for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1837" to="1846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Co-teaching: Robust training deep neural networks with extremely noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">ICLR</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Joint optimization framework for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ikami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">ICLR</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural variational inference and learning in belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1791" to="1799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Markov chains and mixing times</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peres</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>American Mathematical Soc</publisher>
			<biblScope unit="volume">107</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Fast mixing for latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.02960v2</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Rademacher and gaussian complexities: Risk bounds and structural results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mendelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="463" to="482" />
			<date type="published" when="2002-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Domain adaptation: Learning bounds and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rostamizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">COLT</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning from noisy large-scale datasets with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Domain adaptation for visual applications: A comprehensive survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<idno>abs/1702.05374</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Citeseer, Tech. Rep</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Webvision database: Visual learning and understanding from web data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02862</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2016. PLACE PHOTO HERE Michael Shell Biography text here</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Jane Doe Biography text here</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
