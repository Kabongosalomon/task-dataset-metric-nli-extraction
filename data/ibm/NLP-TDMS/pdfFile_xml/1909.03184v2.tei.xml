<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Auto-GNN: Neural Architecture Search of Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaixiong</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Texas A&amp;M University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingquan</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Texas A&amp;M University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Huang</surname></persName>
							<email>xhuang@tamu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Texas A&amp;M University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Hu</surname></persName>
							<email>xiahu@tamu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Texas A&amp;M University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Auto-GNN: Neural Architecture Search of Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Graph neural networks</term>
					<term>neural architecture search</term>
					<term>node classifica- tion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph neural networks (GNN) has been successfully applied to operate on the graph-structured data. Given a specific scenario, rich human expertise and tremendous laborious trials are usually required to identify a suitable GNN architecture. It is because the performance of a GNN architecture is significantly affected by the choice of graph convolution components, such as aggregate function and hidden dimension. Neural architecture search (NAS) has shown its potential in discovering effective deep architectures for learning tasks in image and language modeling. However, existing NAS algorithms cannot be directly applied to the GNN search problem. First, the search space of GNN is different from the ones in existing NAS work. Second, the representation learning capacity of GNN architecture changes obviously with slight architecture modifications. It affects the search efficiency of traditional search methods. Third, widely used techniques in NAS such as parameter sharing might become unstable in GNN.</p><p>To bridge the gap, we propose the automated graph neural networks (AGNN) framework, which aims to find an optimal GNN architecture within a predefined search space. A reinforcement learning based controller is designed to greedily validate architectures via small steps. AGNN has a novel parameter sharing strategy that enables homogeneous architectures to share parameters, based on a carefully-designed homogeneity definition. Experiments on real-world benchmark datasets demonstrate that the GNN architecture identified by AGNN achieves the best performance, comparing with existing handcrafted models and tradistional search methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph neural networks (GNN) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> has been demonstrated that it could achieve superior performance in modeling graph-structured data, within various domains such as social media <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref> and bioinformatics <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. Following the message passing strategy <ref type="bibr" target="#b8">[9]</ref>, GNN iteratively learns a node's embedding representations via aggregating representations of its neighbors and itself. The learned node representations could be employed by downstream machine learning algorithms to perform different tasks efficiently.</p><p>However, the success of GNN is accompanied with laborious work of neural architecture tuning, aiming to adapt GNN to different graph-structure data. For example, the attention heads in the graph attention networks <ref type="bibr" target="#b9">[10]</ref> are selected carefully for citation networks and protein-protein interactions. GraphSAGE <ref type="bibr" target="#b8">[9]</ref> has been shown to <ref type="bibr">0</ref> Preprint. Under review. be sensitive to hidden dimensions. These handcrafted architectures not only require extensive search in the design space through many trials, but also tend to obtain suboptimal performance when they are transferred to other graph-structured datasets. Naturally, there is a raising demand for automated GNN search to identify the optimal architecture for different real-world scenarios.</p><p>Recently, neural architecture search (NAS) has attracted increasing research interests <ref type="bibr" target="#b10">[11]</ref>. Its goal is to find the optimal neural architecture in the predefined search space to maximize model performance on a given task. The deep architectures discovered by NAS algorithms have outperformed the handcrafted ones at the domains including image classification <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref>, semantic image segmentation <ref type="bibr" target="#b21">[22]</ref>, and image generation <ref type="bibr" target="#b22">[23]</ref>. Motivated by the success of NAS, we extend NAS studies beyond the image domains to node classification.</p><p>However, the direct application of NAS algorithms to find GNN architectures is non-trivial due to three major challenges as follows. First, the search space of GNN architecture is different with the ones in existing NAS work. Taking the search of convolutional neural network (CNN) based architectures <ref type="bibr" target="#b11">[12]</ref> as an example, the convolution operation is specified only by the kernel size. In contrast, the message-passing based graph convolution in GNN is described by a sequence of actions, including aggregation, combination, and activation. Second, the traditional controller is inefficient to discover the potentially well-performed GNN architecture. It is because the representation learning capacity of GNN architecture varies significantly with slight architecture modification. In contrast, the widely-used controller samples a complete neural architecture at each search step, and gets update after validating the new architecture. It would be hard for the traditional controller to learn the following causality: which part of the architecture modification improves or degrades the model performance. For example, the traditional controller changes the action sequence in new GNN architecture, and cannot distinguish the improvement brought only by replacing the aggregate function of max pooling with summation <ref type="bibr" target="#b23">[24]</ref>. Third, the widely-used techniques in NAS such as parameter sharing is not suitable to GNN architecture. The parameter sharing transfers weight trained from one architecture to another one, aiming to avoid training from scratch. But it would lead to unstable training when sharing parameters among heterogeneous GNN architectures. We say that two neural architectures are heterogeneous if they have different shape of trainable weight or output statistics. The weights of architectures with different shapes cannot be directly shared. Output statistics <ref type="bibr" target="#b24">[25]</ref> is defined as the mean, variance, or interval of the output value in each graph convolutional layer of GNN architecture. Suppose that we have parameters deeply trained in a layer with Sigmoid activation function, bounding the output within interval [0, 1]. If we transfer the parameter to another layer possessing Linear function, the output value may be too large to be backpropagated steadily in the gradient decent optimizer.</p><p>To tackle the abovementioned challenges, we investigate the automated graph neural architecture search problem. Specifically, it could be separated as two research questions. (i) How to define the search space of GNN architecture, and explore it efficiently? (ii) How to constrain the parameter sharing among the heterogeneous GNN architectures to make training more stably? In summary, our major contributions are described below.</p><p>• We formally define the neural architecture search problem tailored to graph neural networks. • We design a more efficient controller by considering a key property of GNN architecture-the variation of representation learning capacity with slight architecture modification. • We define the heterogeneous GNN architectures in the context of parameter sharing, to train the architecture more stable with shared weight. • The experiments show that the discovered neural architecture consistently outperforms state-of-the-art handcrafted models and other search methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PROBLEM STATEMENT</head><p>We formally define the graph neural architecture search problem as follows. Given search space F , training set D train , validation set D valid and evaluation metric M, we aims to find the optimal GNN architecture f * ∈ F accompanied with the best metric M * on set D valid . Mathematically, it is written as follows.</p><formula xml:id="formula_0">f * = argmax f ∈ F M(f (θ * ), D valid ) θ * = argmin θ L(f (θ ), D train ).<label>(1)</label></formula><p>θ * denotes the parameter learned for architecture f and L denotes the loss function. Metric M could be represented by F1 score or accuracy for node classification task. The characteristics of GNN search problem could be viewed from three aspects. First, search space F is constructed based graph convolutions. Second, an efficient controller is required to consider the relationship between model performance and slight architecture modification in GNN. Third, the parameter sharing needs to promise weight could be transferred stably among heterogeneous GNN architectures. We propose an efficient and effective framework named AGNN to handle the GNN search problem. <ref type="figure" target="#fig_2">Figure 1</ref> illustrates its core idea via a 3-layer GNN architecture search example. In the search space, each graph convolutional layer is specified by an action sequence as listed in the left box. There are totally six action classes, which cover a wide-variety of state-of-the-art GNN models. Instead of resampling a completely new neural architecture, we have independent RNN encoders to decide the new action for each class, e.g., the hidden dimension and activation function. Controller keeps the best architecture found so far, and makes slight architecture modification to it on specific classes. As shown in the right hand of figure, we change the activation functions in all 3 layers of the retained architecture to ELU, ReLU and Tanh, respectively. In this way, we are able update each RNN encoder independently to learn the affect of specific action class to model performance. A tailored parameter sharing strategy is designed. It defines homogeneous GNN architectures via three constraints. Weight only shares from the homogeneous ancestor architecture, helping the offspring architecture train stably. We will update the best architecture if the offspring architecture outperforms it; otherwise, we continue the search by reusing the old best architecture. Next, we introduce the search space, controller, and parameter sharing in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SEARCH SPACE</head><p>In this section, we describe the designed search space for the general GNN architecture, which is composed of layers of message-passing based graph convolutions. Formally, the k-th layer</p><formula xml:id="formula_1">h (k ) i = AGGREGATE({a (k ) i j W (k ) x (k −1) j : j ∈ N (i)}), x (k ) i = ACT(COMBINE(W (k ) x (k −1) i , h (k ) i )).</formula><p>(2)</p><p>x (k ) i denotes the embedding of node i at the k-th layer. N (i) denotes the set of nodes adjacent to node i.W (k ) denotes the trainable matrix used to transform embedding dimension. a (k ) i j denotes the attention coefficient between nodes i and j obtained from the additional attention layer. Function AGGREGATE is applied to aggregate neighbor representations and prepare intermediate embedding h i , and function ACT is used to activate the node embedding. Based on the messagepassing graph convolutions defined in Equation <ref type="formula">(2)</ref>, we decompose the search space into the following 6 classes of actions:</p><p>• Hidden dimension: Trainable matrix W (k ) extracts representative features from embedding x (k −1) i of the last layer, and maps the embedding to a d-dimensional space. The choice of dimension d is crucial to the final node classification performance. We collect the set of dimensions that are widely adopted by existing work as the candidates, i.e., {4,8,16,32,64,128,256}. • Attention function: The real-world graph-structured data could be both complex and noisy <ref type="bibr" target="#b25">[26]</ref>, which may lead to the inefficient information aggregation. The attention mechanism helps to focus on the most relevant neighbors to improve the representative learning of node embedding. Following NAS framework in <ref type="bibr" target="#b26">[27]</ref>, we collect the set of attention functions as shown in <ref type="table" target="#tab_0">Table 1</ref> to compute coefficient a (k ) i j . • Attention head: It is found that the multi-head attention could be beneficial to stabilize the learning process <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b27">28]</ref>. We select the number of attention heads within the set: {1,2,4,6,8,16}. • Aggregate function: As shown in <ref type="bibr" target="#b23">[24]</ref>, aggregate function is crucial to capture neighborhood structure for learning node representation. Herein GNN architecture is developed based on package Pytorch Geometric <ref type="bibr" target="#b28">[29]</ref>. The package provides the following available aggregate functions:</p><formula xml:id="formula_2">{SUMMATION, MEAN, MAXPOOLING}. • Combine function: Embeddings W (k ) x (k −1) i and h (k )</formula><p>i are usually concatenated to combine information from node itself and neighbors. A differentiable function could then be applied to enhance the node representation learning. We design to select from two types of combine functions:   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention Mechanisms</head><p>Equations</p><formula xml:id="formula_3">CONSTANT 1 GCN 1 √ | N(i) | | N(j) | GAT LeakyReLU(ì a(W (k ) x (k −1) i ||W (k ) x (k −1) j )) SYM-GAT a (k ) i j + a (k ) ji based on GAT COS ì a(W (k ) x (k −1) i ||W (k ) x (k−1) j ) LINEAR tanh( ì a l W (k ) x (k −1) i + ì a r W (k) x (k −1) i ) GERE-LINEAR W G tanh(W (k ) x (k−1) i + W (k ) x (k −1) i ) {IDENTITY, MLP}.</formula><p>Herein MLP is a 2-layer perceptron with a fixed hidden dimension of 128. • Activation function: The set of available activation functions in our AGNN is listed as follows: {Sigmoid, Tanh, ReLU, Linear, Softplus, LeakyReLU, ReLU6, ELU} Note that a wide-variety of state-of-the-art model fall into the above message-passing based GNN architecture, including Chebyshev <ref type="bibr" target="#b29">[30]</ref>, GCN <ref type="bibr" target="#b30">[31]</ref>, GraphSAGE <ref type="bibr" target="#b8">[9]</ref>, GAT <ref type="bibr" target="#b9">[10]</ref> and LGCN <ref type="bibr" target="#b31">[32]</ref>. We apply the fixed skip connection as those in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b30">31]</ref>. The skip connection action could be easily incorporated into search space if necessary. Equipped with the above design, a GNN architecture could be specified by a string of length 6n, where n denotes the number of graph convolutional layers. For each layer, cardinalities of the above six action classes are 7, 7, 6, 3, 2, 8, respectively, which provides 7 × 7 × 6 × 3 × 2 × 8 = 14112 possible combinations in total. Suppose we target at searching a three-layer GNN architecture, i.e., n = 3, which is commonly accepted in GNN models. The number of unique architectures within our search space is (14112) 3 ≈ 2.8 × 10 12 , which is quite large and multifarious.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">REINFORCED CONSERVATIVE CONTROLLER</head><p>In this section, we elaborate the proposed controller aiming to search GNN architecture efficiently. The controller framework is built up upon RL-based exploration guided with conservative exploitation. In traditional RL-based NAS, RNN is applied to specify the variable-length neural architecture, and generate a new candidate architecture at each search step. All of the action components in the neural architecture will be resampled and replaced with the new ones. After validating the new architecture, a scalar reward is made use to update the RNN. However, it could be problematic to directly apply this traditional controller to find potentially well-performed GNN architectures. The main reason is that the representation learning capacity of GNN architecture varies significantly with slight modification of some action classes. Taking the aggregate function as example, the classification performance of GNN architecture may improve by only replacing the function of max pooling with summation <ref type="bibr" target="#b23">[24]</ref>. It would be hard for the conventional controller to learn about which part of architecture modification contributes more to the performance improvement. In order to tackle the above challenge, we propose a new searching algorithm named reinforced conservative neural architecture search (RCNAS). It consists of three components: (1) A conservative explorer, which screens out the best architecture found so far. (2) A guided architecture modifier, which slightly mutates certain actions in the retained best architecture. (3) A reinforcement learning trainer that learns the architecture modification causality. In the following, we introduce the details of these three components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Conservative Explorer</head><p>As the key exploitation component, the conservative explorer is applied to maintain the best neural architecture found so far. In this way, the following architecture modification is performed based on a reliable well-performed architecture, which ensures a fast exploitation towards better architectures among the offsprings generated from slight architecture modification. If the offspring architecture outperforms its parent one, we will update the best neural architecture; otherwise, the best one will be kept and reused to generate the next offspring architecture. In practice, multiple starting points could be randomly initialized to enhance the exploration ability and avoid trapping in local minimums.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Guided Architecture Modifier</head><p>The main role of the guided architecture modifier is to modify the best architecture found so far via selecting and mutating the action classes that wait for exploration. As shown in the right hand of <ref type="figure" target="#fig_2">Figure 1</ref>, assume the class of activation function is selected. Correspondingly, the actions of activation function in the 3-layer GNN architecture are resampled and changed to ELU, ReLU and Tanh, respectively. This will facilitate controller to learn the affect of architecture modification on specific action class.</p><p>To be specific, the architecture modification is realized by three steps: (1) For each class, an independent RNN encoder decides a sequence of new actions. (2) An action guider receives the decision entropy and selects the action classes to be modified. (3) An architecture modification generates the final offspring architecture. Details are introduced as follows. <ref type="figure" target="#fig_2">Figure 1</ref>, for each class, an independent RNN encoder is implemented to decide a sequence of new actions. First, a subarchitecture string of length 5n is generated by removing n actions of concerned class. For example, considering the 3-layer neural architecture in <ref type="figure" target="#fig_2">Figure 1</ref>, the subarchitecture of class activation function is obtained by removing activations existing in all 3 convolutional layers of the best architecture. Second, following an embedding layer, the subarchitecture string is taken as input to RNN encoder. This string represents the input status that asks for action padding of concerned class. Third, RNN encoder iteratively outputs the candidate action; and the output is then fed into next step as input. Note that the candidate action is sampled by feeding hidden state h i into a softmax classifier. The length of each RNN encoder is n, coupling with the number of layers to be searched in the architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">RNN Encoders: As shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Action Guider:</head><p>It is responsible to receive the decision entropy of each RNN encoder, and select some classes to be modified on the retained architecture. Consider the decision entropy of class c. At step i of RNN encoder, hidden state h i is fed into the softmax classifier, and a probability vector ì P i is given as output. The j-th element P i j represents the probability of sampling action j. The decision entropy of class c is then given by:</p><formula xml:id="formula_4">E c ≜ n i=1</formula><p>m c j=1 −P i j log P i j , where m c denote the action cardinality of class c. Decision entropy E c represents the uncertainty of current subarchitecture to explore along action class c.</p><p>Given decision entropy list {E 1 , · · · , E 6 } of the six action classes, the action guider samples classes C = {c 1 , · · · , c s } with size s, which would be used to modify network architecture. For example, class activation function is selected as shown in <ref type="figure" target="#fig_2">Figure 1</ref>, where C = {Activation function}, s = 1. The larger the decision entropy E c is, the larger the probability class c are desired to be sampled. The action guider help controller search the potential networks along the direction with most uncertainty, which performs similar to the Bayesian optimization method <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Architecture Modification:</head><p>The best architecture found so far is modified via replacing the corresponding actions of each class in list C. In <ref type="figure" target="#fig_2">Figure 1</ref>, action list {ELU, ReLU, Tanh} is applied to replace the activation functions existing in all of the 3 graph convolutional layers. When list C includes only one class, we modify the retained neural architecture at a minimum level. If size s = 6, our controller resamples actions in the whole architecture similar to the traditional controller.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Reinforcement Learning Trainer</head><p>We use the REINFORCE rule of policy gradient <ref type="bibr" target="#b32">[33]</ref> to update parameters θ c for RNN encoder of class c ∈ C. Let {a 1 , · · · , a n } denote the decided action list of class c. We have the following update rule <ref type="bibr" target="#b11">[12]</ref>:</p><formula xml:id="formula_5">∇ θ c J (θ c ) = n t =1 E[(R c − b c )∇ θ c log P(a t |a t −1 ; θ c )],<label>(3)</label></formula><p>where R c denotes the reward for taking decisions {a 1 , · · · , a n } of class c, and b c denotes the baseline of class c for variance reduction. Let M b and M o denote the model performances of the best architecture found so far and its offspring one, respectively. We propose the following reward shaping:</p><formula xml:id="formula_6">R c ≜ M o − M b</formula><p>, which represents the performance variation brought by modifying the retained architecture on the class c.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONSTRAINED PARAMETER SHARING</head><p>Compared to training from scratch, parameter sharing reduces the computation cost via forcing the offspring architecture to share weight already trained well in the ancestor architecture. However, the traditional strategy cannot be directly applied to share weight among the heterogeneous GNN architectures. We say that two neural architectures are heterogeneous if they have different shapes of trainable weight or output statistics. First, the distinct weight shape in the offspring architecture prevents the direct transfer from an ancestor architecture. Second, weight is deeply trained and coupled in the ancestor architecture. The shared weight from heterogeneous architecture with different output statistics may lead to output explosion and unstable training <ref type="bibr" target="#b24">[25]</ref>. Consider the output intervals of activation functions Sigmoid and Linear, which are given by [0, 1] and [−∞, +∞], respectively. The shared wight is unsuitable to the architecture possessing function Linear when it is transferred from the one possessing function Sigmoid. Third, the shared weights in the connection layer may not be effective and adaptive to the offspring architecture immediately. The connection layer is given by the batch normalization or skip connection, and may be uncoupled to the offspring architecture. To tackle the above challenges, we propose the constrained parameter sharing strategy to limit how the offspring architecture inheriting parameter from ancestor architectures found before. As shown in <ref type="figure">Figure 2</ref>, we explain the three constraints as follows:</p><p>• The ancestor and offspring architectures have the same shape of input and output tensors for the graph convolutional layer. Based on the graph convolutions defined in Equation <ref type="formula">(2)</ref>,  <ref type="figure">Figure 2</ref>: An illustration of the constrained parameter sharing strategy between the ancestor and offspring architectures. The trainable parameter of a convolutional layer could only be shared when they have the same weight shape (constraint 1), attention and activation functions (constraint 2). Constraint 3 removes the parameter sharing for batch normalization (BN) and skip connection (SC).</p><p>both trainable matrix W (k ) and transform weight used in the attention function could be shared directly only if they have the same shape. • The ancestor and offspring architectures have the same attention function and activation function for the graph convolutional layer. The attention function defines the neighbor information to be aggregated, and the activation function squashes the output to a specific interval. Hence both attention function and activation function greatly determines the output statistics of a graph convolutional layer. It is expected to void output explosion and improve the training stability via sharing parameter from homogeneous architecture with similar output statistics. • The parameters of batch normalization (BN) and skip connection (SC) will not be shared. It is because we do not know the exact output statistics of each layer in the offspring architecture in advance. The shared parameters of BN and SC may cannot bridge the two successive layers well. We train the whole offspring architecture with a few epochs (e.g., 5 or 20 epochs in our experiment), to adapt these parameters to the new architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head><p>We apply our method to find the optimal GNN architecture given the node classification task, to answer the following four questions: More details about the datasets, baseline methods, experimental configuration and results are introduced as follows. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Datasets</head><p>We consider both transductive and inductive learning settings for the node classification task. Under the transductive learning, the unlabeled data used for validation and testing are accessible during training. This means the training process could make use of the complete graph structure and node features, except for node labels on the held-out validation and testing sets. Under the inductive learning, the training process has no idea about the graph structure and node features on both validation and testing sets. We utilize Cora, Citeseer and Pubmed <ref type="bibr" target="#b33">[34]</ref> for the transductive learning, and use PPI for the inductive learning <ref type="bibr" target="#b6">[7]</ref>. These benchmark datasets are commonly used for studying the node classification task. The dataset statistics is given in <ref type="table" target="#tab_3">Table 2</ref>. The three datasets evaluated under transductive learning are citation networks, where node corresponds to document and edge corresponds to citation relation. Node feature is given by bag-of-words representation of a document, and each node is associated with a class label. Following the same experimental setting as those in baseline methods, we allow for 20 nodes per class to be used for training, and use 500 and 1000 nodes for validation and testing, respectively. PPI dataset evaluated under inductive learning consists of graphs corresponding to different human tissues. There are 50 features for each node, including the positional gene sets, motif gene sets and immunological signatures. Each node has several labels simultaneously collected from total of 121 classes. We use 20 graphs for training, 2 graphs for validation and 2 graphs for testing. The model metric is given by classification accuracy and micro-averaged F1 score for transductive learning and inductive learning, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Baseline Methods</head><p>In order to evaluate our method designed specifically for finding GNN architecture, we consider the baselines of both state-of-the-art handcrafted architectures as well as other NAS approaches.</p><p>• Handcrafted architectures: Herein we only consider the message-passing based GNNs as shown in Equation <ref type="formula">(2)</ref> for fair comparison, except the one combined with pooling layer.</p><p>The following baseline methods are included: Chebyshev <ref type="bibr" target="#b29">[30]</ref>, GCN <ref type="bibr" target="#b30">[31]</ref>, GraphSAGE <ref type="bibr" target="#b8">[9]</ref>, GAT <ref type="bibr" target="#b9">[10]</ref>, LGCN <ref type="bibr" target="#b31">[32]</ref>. Note that both Chebyshev and GCN perform information aggregation based on the Laplacian or adjacent matrix of the complete graph. Hence they are only evaluated under the transductive learning setting. Baseline GraphSAGE aggregates information via sampling neighbors pf fixed size, which will be compared only under the inductive learning setting. We consider a variety of GraphSAGE possessing different aggregate functions, including GraphSAGE-GCN, GraphSAGE-mean, GraphSAGE-pool and GraphSAGE-LSTM. • NAS approaches: We compare with the previous NAS approaches based on reinforcement learning and random search. The former one utilizes RNN to sample the whole neural architecture, and applies reinforcement rule to update controller. GraphNAS proposed in <ref type="bibr" target="#b26">[27]</ref> applies this approach directly to search GNN architecture. The later one samples architecture randomly, serving as baseline to evaluate the efficiency of our controller.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Training Details</head><p>We train the sampled neural architecture on the training set, and update the controller via receiving reward from the validation set. Following the model configurations in baselines <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b31">32]</ref>, the training experiments are set up according to transductive learning and inductive learning, respectively. We have an unified model configuration of controller. More details about our experimental procedure are introduced as follows.</p><p>6.3.1 Transductive Learning. Herein we explore a two-layer GNN architecture in the predefined search space. Except that the neural architecture is updated iteratively during the search progress, we have the same training environment to those in the baselines. To deal with the issue of small training set, we apply L2 regularization with λ = 0.0005. Dropout rate of 0.6 is applied to both layersâĂŹ inputs as well as the attention coefficients during training. For Pubmed dataset, L2 regularization is strengthened to λ = 0.001. Foe each sampled architecture, weight is initialized using Glorot initialization <ref type="bibr" target="#b34">[35]</ref> and trained with Adam optimizer <ref type="bibr" target="#b35">[36]</ref> to minimize the cross-entropy loss. We set the initial learning rate of 0.01 for Pubmed and 0.005 for Cora and Citeseer. We have two different settings to train a new offspring architecture: with parameter sharing and without weight sharing. The former one has a small warm-up epochs of 20, while the later one has 200 training epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">Inductive</head><p>Learning. Herein we explore a three-layer GNN architecture. The skip connection between the intermediate graph convolutional layers is included to improve the representation learning. Since dataset PPI is sufficiently large for training, the L2 regularization and random dropout are removed from GNN model. The batch size of 2 graphs is employed during training.</p><p>We have the same parameter initialization and optimizer as the transductive learning. The initial learning rate is set to 0.005. The warm-up epoch number is 5 under the setting with parameter sharing, and it is 20 under the setting without parameter sharing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6.3.3</head><p>Controller. For each action class, RNN encoder is realized by an one-layer LSTM with 100 hidden units. Weights are initialized uniformly in [−0.1, 0.1], and trained with Adam optimizer at a learning rate of 3.5×10 −4 . Following the controller configurations in the previous NAS work, we use a tanh constant of 2.5 and a sample temperature of 5.0 to the hidden output. Totally 1000 architectures are explored iteratively during the search progress, and evaluated to obtain reward for updating controller. Reward to the policy gradient is given by the following combination: the validation performance and the controller entropy weighted by 1.0 × 10 −4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Results</head><p>In this section, we show the comparative evaluation experiments to answer the above four research questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.1">Test Performance Comparison.</head><p>We compare the architecture discovered by our AGNN with the handcrafted ones and those found by other search methods, aiming to provide positive answer for research question Q1. Considering the architecture modification in AGNN, the default size s of class list C is set to 1. All of NAS approaches find the optimal architecture achieving the best performance on the separate held-out validation set. Then, it is evaluated on the testing set only once. Two comprehensive lists of architecture information and model performance are presented in <ref type="table" target="#tab_4">Tables 3 and 4</ref> for transductive learning and inductive learning, respectively. The test performance of NAS approaches is averaged via randomly initializing the optimal architecture 5 times, and those of handcrafted architectures are reported directly from their papers.</p><p>As can be seen from <ref type="table" target="#tab_4">Tables 3 and 4</ref>, the neural architecture discovered by AGNN outperforms the handcrafted ones and other search methods. Compared with the handcrafted architectures, the discovered models generally improve the classification performance accompanied with the increment of parameter size. During the search process, the larger ones of attention head and hidden dimension are explored to improve the representation learning capacity of GNN. The whole neural architecture is sampled and reconstructed in GraphNAS and random search at each step, similar to the previous NAS frameworks. In contrast, our AGNN explores the offspring architecture via only modifying specific action class. The best architecture are retained to provide a good start for architecture modification. This will facilitate the controller to learn the causality between architecture modification and model performance variation, and find the better architecture more potentially.</p><p>It is observed that the architectures found without parameter sharing generally outperform the ones found with parameter sharing. It is because the shared parameter may be uncoupled to the offspring architecture, although several epochs are applied to warm up. Running on a single Nvidia GTX 1080Ti GPU, it takes about 0.5 GPU days to find the best architecture without parameter sharing, which is a few times that with parameter sharing. There is a trade-off between model performance and computation time cost.    As can be seen from <ref type="figure" target="#fig_4">Figure 3</ref>, AGNN is more efficient to find the well-performed architectures during the search progress. The top-10 architectures discovered by AGNN have better averaged performance on PPI and Citeseer. It is because the best architecture found so far is retained and prepared for slight architecture modification in the next step. Only some actions are resampled to generate the offspring architecture. This will accelerate the search progress toward the better neural architectures among the offsprings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.3">Effectiveness Validation of Parameter</head><p>Sharing. Herein we study whether or not the shared parameter could be effective in the offspring architecture to help achieve good classification performance, aiming to provide answer for research question Q3. We consider AGNN equipped with different parameter sharing strategies: the proposed constrained one, the relaxed one in GraphNAS, and training from scratch without parameter sharing. Note that the relaxed parameter sharing in GraphNAS is similar to that in the previous NAS framework, at which the offspring architecture shares weight of the same shape directly without any constraint. The cumulative distribution of validation performance is compared for the 1000 discovered architectures in <ref type="figure" target="#fig_6">Figure 4</ref>.</p><p>As can be seen from <ref type="figure" target="#fig_6">Figure 4</ref>, most of the neural architectures found by the constrained parameter sharing have better performance than those found by relaxed strategy. That is because the manually-designed constraints limit the parameter sharing only between the homogeneous architectures with similar output statistics. Combined with a few epochs to warm up weight in batch normalization and skip connection, the shared parameter could be effective to the newly sampled architecture. In addition, the    offspring architecture is generated with slight architecture modification to the best architecture found so far, which means that they potentially have the similar architecture and output statistics. Hence the well-trained weight could be transferred to the offspring architecture stably. Although the strategy of training from scratch couples the weight to each architecture perfectly, it needs to pay much more computation cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6.4.4</head><p>Influence of Architecture Modification. We study how does different scales of architecture modification affect the search efficiency, in order to provide answer to research question Q4. Note that the action class in list C are exploited to modify the retained architecture, and the size of list C is denoted by s. When s = 1, we perform the architecture modification at the minimum level, at which actions of one specific class will be resampled. When s = 6, we modify the retained network completely similar to the traditional controller. Considering s = 1, 3, and 6, we show the progression of top-10 architectures under the setting of parameter sharing in <ref type="figure" target="#fig_8">Figure 5</ref>.</p><p>As can be seen from <ref type="figure" target="#fig_8">Figure 5</ref>, the architecture search progress tends to be more efficient with the decrease of s. The top-10 neural architectures found by s = 1 achieves the best averaged performance on PPI and Citeseer. The efficient progression of smaller s benefits from the following two facts. First, the offspring architecture tends to have similar structure and output statistics with the retained one. It is more possible for the shared weight being effective in the offspring architecture. Second, the independent RNN encoder can exactly learn causality between performance variation and architecture modification of its own class, and tends to sample well-performed architecture at the next step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RELATED WORK</head><p>Our work is related to the graph neural networks and neural architecture search.</p><p>Graph Neural Networks. A wide variety of GNNs have been proposed to learn the node representation effectively, e.g., recursive neural networks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, graph convolutional networks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b36">37]</ref> and graph attention networks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b27">28]</ref>. Most of these approaches are built up based on message-passing based graph convolutions. The underlying graph is viewed as a computation graph, at which node embedding is generated via message passing, information transformation, neighbor aggregation and self update.</p><p>Neural Architecture Search. Most of NAS frameworks are built up based on one of the two basic algorithms: RL <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref> and EA <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref>. For the former one, a RNN controller is applied to specify the variable-length strings of neural architecture. Then the controller is updated with policy gradient after evaluating the sampled architecture on validation set. For the latter one, a population of architectures are initialized first and evolved with mutation and crossover. The architectures with competitive performance will be retained during the search progress. A new framework combines these two search algorithms to improve the search efficiency <ref type="bibr" target="#b43">[44]</ref>. Parameter sharing <ref type="bibr" target="#b14">[15]</ref> is proposed to transfer the well-trained weight before to a sampled architecture, to avoid training the offspring architecture from scratch to convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>In this paper, we present AGNN to find the optimal neural architecture given a node classification task. The search space, RCNAS controller and constrained parameter sharing strategy together are designed specifically suited for the message-passing based GNN. Experiment results show the discovered neural architectures achieve quite competitive performance on both transductive and inductive learning tasks. The proposed RCNAS controller search the wellperformed architectutres more efficiently, and the shared weight could be effective in the offspring network under constraints. For future work, first we will try to apply AGNN to discover architectures for more applications such as graph classification and link prediction. Second, we plan to consider more advanced techniques of graph convolutions in the search space, to facilitate neural architecture search in different applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>addition, function COMBINE is used to combine information from node itself as well as intermediate embedding h(k )    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of AGNN with 3-layer GNN search. Controller takes the best architecture found so far as input, and removes one of the six classes in turns to generate six subarchitectures. Their strings are fed to RNN encoders to determinate the best alternative action for the missing class. We select the new best architecture from all completed subarchitectures, based the accompanied decision entropy. Herein action guider selects class list C = {Activation function}. The retained architecture is modified via replacing activation functions with ELU, ReLU, and Tanh, in all 3 graph convolutional layers, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>6. 4 . 2</head><label>42</label><figDesc>Search Efficiency Comparison. We compare the progression of top-10 averaged performance of our AGNN, GraphNAS and random search, in order to provide positive answer to the research question Q2. All of the search methods are performed without parameter sharing to only study the efficiencies of different controllers. For each search method, totally 1000 architectures are explored in the same search space. The progression comparisons on the four datasets are shown inFigure 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Progression of top-10 averaged performance of different search methods, i.e., AGNN, GraphNAS, and random search.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>The cumulative distribution of validation performance for AGNN under different parameter sharing strategies: the proposed constrained one, the relaxed one in GraphNAS, and training from scratch without parameter sharing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>The progression of top-10 averaged performance of AGNN under different architecture modification: s = 1, 3, and 6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>The set of attention functions, where symbol || de- notes the concatenation operation, ì a, ì a l and ì a r denote the trainable vectors, and W G denotes the trainable matrix.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Statistics of datasets Cora, Citeseer, Pubmed, and PPI<ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b31">32]</ref>, where T and I denote the transductive and inductive learning, respectively.</figDesc><table><row><cell></cell><cell cols="3">Cora Citeseer Pubmed</cell><cell>PPI</cell></row><row><cell>Setting</cell><cell>T</cell><cell>T</cell><cell>T</cell><cell>I</cell></row><row><cell>#Nodes</cell><cell>2708</cell><cell>3327</cell><cell>19717</cell><cell>56944</cell></row><row><cell>#Features</cell><cell>1433</cell><cell>3703</cell><cell>500</cell><cell>50</cell></row><row><cell>#Classes</cell><cell>7</cell><cell>6</cell><cell>3</cell><cell>121</cell></row><row><cell>#Training Nodes</cell><cell>140</cell><cell>120</cell><cell>60</cell><cell>44906 (20 graphs)</cell></row><row><cell cols="2">#Validation Nodes 500</cell><cell>500</cell><cell>500</cell><cell>6514 (2 graphs)</cell></row><row><cell>#Testing Nodes</cell><cell>1000</cell><cell>1000</cell><cell>1000</cell><cell>5524 (2 graphs)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Test performance comparison for architectures under the transductive learning setting: the state-of-the-art handcrafted architectures, the optimal ones found by NAS baselines, the optimal ones found by AGNN.</figDesc><table><row><cell>Baseline Class</cell><cell>Model</cell><cell>#Layers</cell><cell>#Params</cell><cell>Cora Accuracy</cell><cell cols="2">Citeseer #Params Accuracy</cell><cell cols="2">Pubmed #Params Accuracy</cell></row><row><cell></cell><cell>Chebyshev</cell><cell>2</cell><cell>0.09M</cell><cell>81.2%</cell><cell>0.09M</cell><cell>69.8%</cell><cell>0.09M</cell><cell>74.4%</cell></row><row><cell>Handcrafted</cell><cell>GCN</cell><cell>2</cell><cell>0.02M</cell><cell>81.5%</cell><cell>0.05M</cell><cell>70.3%</cell><cell>0.02M</cell><cell>79.0.5%</cell></row><row><cell>Architectures</cell><cell>GAT</cell><cell>2</cell><cell>0.09M</cell><cell>83.0 ± 0.7%</cell><cell>0.23M</cell><cell>72.5 ± 0.7%</cell><cell>0.03M</cell><cell>79.0 ± 0.3%</cell></row><row><cell></cell><cell>LGCN</cell><cell>3 ∼ 4</cell><cell>0.06M</cell><cell>83.3 ± 0.5%</cell><cell>0.05M</cell><cell>73.0 ± 0.6%</cell><cell>0.05M</cell><cell>79.5 ± 0.2%</cell></row><row><cell></cell><cell>GraphNAS-w/o share</cell><cell>2</cell><cell>0.09M</cell><cell>82.7 ± 0.4%</cell><cell>0.23M</cell><cell>73.5 ± 1.0%</cell><cell>0.03M</cell><cell>78.8 ± 0.5%</cell></row><row><cell>NAS Baselines</cell><cell>GraphNAS-with share Random-w/o share</cell><cell>2 2</cell><cell>0.07M 0.37M</cell><cell>83.3 ± 0.6% 81.4 ± 1.1%</cell><cell>1.91M 0.95M</cell><cell>72.4 ± 1.3% 72.9 ± 0.2%</cell><cell>0.07M 0.13M</cell><cell>78.1 ± 0.8% 77.9 ± 0.5%</cell></row><row><cell></cell><cell>Random-with share</cell><cell>2</cell><cell>2.95M</cell><cell>82.3 ± 0.5%</cell><cell>0.95M</cell><cell>69.9 ± 1.7%</cell><cell>0.13M</cell><cell>77.9 ± 0.4%</cell></row><row><cell>AGNN</cell><cell>AGNN-w/o share AGNN-with share</cell><cell>2 2</cell><cell>0.05M 0.37M</cell><cell>83.6 ± 0.3% 82.7 ± 0.6%</cell><cell>0.71M 1.90M</cell><cell>73.8 ± 0.7% 72.7 ± 0.4%</cell><cell>0.07M 0.03M</cell><cell>79.7 ± 0.4% 79.0 ± 0.5%</cell></row><row><cell>(a) PPI</cell><cell></cell><cell>(b) Cora</cell><cell></cell><cell cols="2">(c) Citeseer</cell><cell></cell><cell cols="2">(d) Pubmed</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Test performance comparison of our AGNN to stateof-the-art handcrafted architectures and other search approaches under the inductive learning setting.</figDesc><table><row><cell>Baseline Class</cell><cell>Model</cell><cell>Layers</cell><cell cols="2">PPI Params F1 score</cell></row><row><cell></cell><cell>GraphSAGE-GCN</cell><cell>2</cell><cell>0.11M</cell><cell>0.500</cell></row><row><cell></cell><cell>GraphSAGE-mean</cell><cell>2</cell><cell>0.11M</cell><cell>0.598</cell></row><row><cell>Hand-</cell><cell>GraphSAGE-pool</cell><cell>2</cell><cell>0.36M</cell><cell>0.600</cell></row><row><cell>crafted</cell><cell>GraphSAGE-LSTM</cell><cell>2</cell><cell>0.39M</cell><cell>0.612</cell></row><row><cell></cell><cell>GAT</cell><cell>3</cell><cell cols="2">0.89M 0.973 ± 0.002</cell></row><row><cell></cell><cell>LGCN</cell><cell>4</cell><cell cols="2">0.85M 0.772 ± 0.002</cell></row><row><cell></cell><cell>GraphNAS-w/o share</cell><cell>3</cell><cell cols="2">4.1M 0.985 ± 0.004</cell></row><row><cell>NAS</cell><cell cols="2">GraphNAS-with share 3</cell><cell cols="2">1.4M 0.960 ± 0.036</cell></row><row><cell cols="2">Baselines Random-w/o share</cell><cell>3</cell><cell cols="2">1.4M 0.984 ± 0.004</cell></row><row><cell></cell><cell>Random-with share</cell><cell>3</cell><cell cols="2">1.4M 0.977 ± 0.011</cell></row><row><cell>AGNN</cell><cell>AGNN-w/o share AGNN-with share</cell><cell>3 3</cell><cell cols="2">4.6M 0.992 ± 0.001 1.6M 0.991 ± 0.001</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Conference'17, July 2017, Washington, DC, USA Kaixiong Zhou, Qingquan Song, Xiao Huang, Xia Hu</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks, 2005. IJCNN&apos;05. Proceedings. 2005 IEEE International Joint Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on world wide web</title>
		<meeting>the 24th international conference on world wide web</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Structural deep network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1225" to="1234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Predicting multicellular function through multi-layer tissue networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="190" to="198" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning graph representations with recurrent neural network autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanya</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Wolf Aynaz</forename><surname>Taheri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;18 Deep Learning Day</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Hendrik</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.05377</idno>
		<title level="m">Neural architecture search: A survey</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<title level="m">Neural architecture search with reinforcement learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chrisantha</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00436</idno>
		<title level="m">Hierarchical representations for efficient architecture search</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Efficient neural architecture search via parameter sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Melody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03268</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Auto-keras: Efficient neural architecture search with network morphism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingquan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.10282</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural architecture optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renqian</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7816" to="7827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="19" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09055</idno>
		<title level="m">Darts: Differentiable architecture search</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01569</idno>
		<title level="m">Exploring randomly wired neural networks for image recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural architecture search with bayesian optimisation and optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirthevasan</forename><surname>Kandasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willie</forename><surname>Neiswanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2016" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="82" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Agan: Towards automated design of generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Huan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.11080</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">How powerful are graph neural networks? CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno>abs/1810.00826</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Single path one-shot neural architecture search with uniform sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyuan</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00420</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Attention models in graphs: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John Boaz</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungchul</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nesreen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunyee</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.07984</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Graphnas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09981</idno>
		<title level="m">Graph neural architecture search with reinforcement learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with PyTorch Geometric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Large-scale learnable graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1416" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Policy gradient methods for reinforcement learning with function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Satinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishay</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="1057" to="1063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Reinforcement learning for architecture search by network transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.04873</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otkrist</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02167</idno>
		<title level="m">Nikhil Naik, and Ramesh Raskar. Designing neural network architectures using reinforcement learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Large-scale evolution of image classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Selle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><forename type="middle">Leon</forename><surname>Suematsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kurakin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2902" to="2911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Evolving deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Risto Miikkulainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elliot</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Meyerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Rawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Fink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bala</forename><surname>Francon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hormoz</forename><surname>Raju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arshak</forename><surname>Shahrzad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Navruzyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duffy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence in the Age of Neural Networks and Brain Computing</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="293" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Genetic cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1379" to="1388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4780" to="4789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Reinforced evolutionary neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaofeng</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiming</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisen</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.00193</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
