<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GCDT: A Global Context Enhanced Deep Transition Architecture for Sequence Labeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijin</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fandong</forename><surname>Meng</surname></persName>
							<email>fandongmeng@tencent.com</email>
							<affiliation key="aff1">
								<orgName type="department">Pattern Recognition Center</orgName>
								<address>
									<addrLine>WeChat AI</addrLine>
									<region>Tencent Inc</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinchao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Pattern Recognition Center</orgName>
								<address>
									<addrLine>WeChat AI</addrLine>
									<region>Tencent Inc</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinan</forename><surname>Xu</surname></persName>
							<email>jaxu@bjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufeng</forename><surname>Chen</surname></persName>
							<email>chenyf@bjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Pattern Recognition Center</orgName>
								<address>
									<addrLine>WeChat AI</addrLine>
									<region>Tencent Inc</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GCDT: A Global Context Enhanced Deep Transition Architecture for Sequence Labeling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Current state-of-the-art systems for the sequence labeling tasks are typically based on the family of Recurrent Neural Networks (RNNs). However, the shallow connections between consecutive hidden states of RNNs and insufficient modeling of global information restrict the potential performance of those models. In this paper, we try to address these issues, and thus propose a Global Context enhanced Deep Transition architecture for sequence labeling named GCDT. We deepen the state transition path at each position in a sentence, and further assign every token with a global representation learned from the entire sentence. Experiments on two standard sequence labeling tasks show that, given only training data and the ubiquitous word embeddings (Glove), our GCDT achieves 91.96 F 1 on the CoNLL03 NER task and 95.43 F 1 on the CoNLL2000 Chunking task, which outperforms the best reported results under the same settings. Furthermore, by leveraging BERT as an additional resource, we establish new stateof-the-art results with 93.47 F 1 on NER and 97.30 F 1 on Chunking 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sequence labeling tasks, including part-of-speech tagging (POS), syntactic chunking and named entity recognition (NER), are fundamental and challenging problems of Natural Language Processing (NLP). Recently, neural models have become the de-facto standard for high-performance systems. Among various neural networks for sequence labeling, bi-directional RNNs (BiRNNs), especially BiLSTMs <ref type="bibr" target="#b11">(Hochreiter and Schmidhuber, 1997)</ref> have become a dominant method on multiple benchmark datasets <ref type="bibr" target="#b3">Chiu and Nichols, 2016;</ref><ref type="bibr" target="#b15">Lample et al., 2016;</ref><ref type="bibr" target="#b24">Peters et al., 2017)</ref>.</p><p>However, there are several natural limitations of the BiLSTMs architecture. For example, at each time step, the BiLSTMs consume an incoming word and construct a new summary of the past subsequence. This procedure should be highly nonlinear, to allow the hidden states to rapidly adapt to the mutable input while still preserving a useful summary of the past <ref type="bibr" target="#b21">(Pascanu et al., 2014)</ref>. While in BiLSTMs, even stacked BiLSTMs, the transition depth between consecutive hidden states are inherently shallow. Moreover, global contextual information, which has been shown highly useful for model sequence <ref type="bibr">(Zhang et al., 2018)</ref>, is insufficiently captured at each token position in BiLSTMs. Subsequently, inadequate representations flow into the final prediction layer, which leads to the restricted performance of BiLSTMs.</p><p>In this paper, we present a global context enhanced deep transition architecture to eliminate the mentioned limitations of BiLSTMs. In particular, we base our network on the deep transition (DT) RNN <ref type="bibr" target="#b21">(Pascanu et al., 2014)</ref>, which increases the transition depth between consecutive hidden states for richer representations. Furthermore, we assign each token an additional representation, which is a summation of hidden states of a specific DT over the whole input sentence, namely global contextual embedding. It's beneficial to make more accurate predictions since the combinatorial computing between diverse token embeddings and global contextual embedding can capture useful representations in a way that improves the overall system performance.</p><p>We evaluate our GCDT on both CoNLL03 and CoNLL2000. Extensive experiments on two benchmarks suggest that, merely given training data and publicly available word embeddings arXiv:1906.02437v1 [cs.CL] 6 Jun 2019 (Glove), our GCDT surpasses previous state-ofthe-art systems on both tasks. Furthermore, by exploiting BERT as an extra resource, we report new state-of-the-art F 1 scores with 93.47 on CoNLL03 and 97.30 on CoNLL2000. The main contributions of this paper can be summarized as follows:</p><p>• We are the first to introduce the deep transition architecture for sequence labeling, and further enhance it with the global contextual representation at the sentence level, named GCDT.</p><p>• GCDT substantially outperforms previous systems on two major tasks of NER and Chunking. Moreover, by leveraging BERT as an extra resource to enhance GCDT, we report new state-of-the-art results on both tasks.</p><p>• We conduct elaborate investigations of global contextual representation, model complexity and effects of various components in GCDT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Given a sequence of X = {x 1 , x 2 , · · · , x N } with N tokens and its corresponding linguistic labels Y = {y 1 , y 2 , · · · , y N } with the equal length, the sequence labeling tasks aim to learn a parameterized mapping function f θ : X → Y from input tokens to task-specific labels. Typically, the input sentence is firstly encoded into a sequence of distributed representations X = {x 1 , x 2 , · · · , x N } by character-aware and pretrained word embeddings. The majority of highperformance models use bidirectional RNNs, BiL-STMs in particular, to encode the token embeddings X into context-sensitive representations for the final prediction.</p><p>Additionally, it's beneficial to model and predict labels jointly, thus a subsequent conditional random field <ref type="bibr">(CRF Lafferty et al., 2001)</ref> is commonly utilized as a decoder layer. At the training stage, those models maximize the log probability of the correct sequence of tags as follows:</p><formula xml:id="formula_0">log(p(y|X)) = s(X, y) − log( y∈Yx e s(X, y) )<label>(1)</label></formula><p>where s(·) is the score function and Y x is the set of all possible sequence of tags. Typically, the Viterbi algorithm <ref type="bibr" target="#b8">(Forney, 1973)</ref> is utilized to search the label sequences with maximum score when decoding:</p><formula xml:id="formula_1">y * = arg max y∈Yx s(x, y)<label>(2)</label></formula><p>3 GCDT</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>In this section, we start with a brief overview of our presented GCDT and then proceed to structure the following sections with more details about each submodule. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, there are three deep transition modules in our model, namely global contextual encoder, sequence labeling encoder and decoder accordingly.</p><p>Token Representation Given a sentence X = {x 1 , x 2 , ..., X N } with N tokens, our model first captures each token representation x t by concatenating three primary embeddings: 3. Global contextual embedding g is extracted from bidirectional DT, and more details will be described in the following paragraphs.</p><formula xml:id="formula_2">x t = [c t ; w t ; g]<label>(3)</label></formula><p>The global embedding g is computed by mean pooling over all hidden states {h g 1 , h g 2 , · · · , h g N } of global contextual encoder (right part in <ref type="figure" target="#fig_0">Figure  1</ref>). For simplicity, we can take "DT" as a reinforced Gated Recurrent Unit (GRU <ref type="bibr" target="#b4">Chung et al., 2014)</ref>, and more details about DT will be described in the next section. Thus g is computed as follows:</p><formula xml:id="formula_3">g = 1 N n t=1 h g t (4) h g t = [ − → h g t ; ← − h g t ] (5) − → h g t = − − → DT g (c t , w t ; θ− − → DT g ) (6) ← − h g t = ← − − DT g (c t , w t ; θ← − − DT g )<label>(7)</label></formula><p>Mean Pooling Sequence Labeling Encoder Subsequently, the concatenated token embeddings x t (Eq. 3) is fed into the sequence labeling encoder (bottom left part in <ref type="figure" target="#fig_0">Figure 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global Contextual Encoder Sequence Labeling Encoder</head><formula xml:id="formula_4">h t = [ − → h t ; ← − h t ] (8) − → h t = − − → DT en (x t , − → h t−1 ; θ− − → DT en ) (9) ← − h t = ← − − DT en (x t , ← − h t−1 ; θ← − − DT en )<label>(10)</label></formula><p>Sequence Labeling Decoder Considering the t-th word in this sentence, the output of sequence labeling encoder h t along with the past label embedding y t−1 are fed into the decoder (top left part in <ref type="figure" target="#fig_0">Figure 1</ref>). Subsequently, the output of decoder s t is transformed into l t for the final softmax over the tag vocabulary. Formally, the label of word x t is predicted as the probabilistic equation (Eq. 13)</p><formula xml:id="formula_5">s t = DT de (h t , y t−1 ; θ DT de ) (11) l t = s t W l + b l (12) P (y t = j|x) = sof tmax(l t )[j]<label>(13)</label></formula><p>As we can see from the above procedures and <ref type="figure">Figure</ref> 1, our GCDT firstly encodes the global contextual representation along the sequential axis by DT, which is utilized to enrich token representations. At each time step, we encode the past label information jointly using the sequence labeling decoder instead of resorting to CRF. Additionally, we employ beam search algorithm to infer the most probable sequence of labels when testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Deep Transition RNN</head><p>Deep transition RNNs extend conventional RNNs by increasing the transition depth of consecutive hidden states. Previous studies have shown the superiority of this architecture on both language modeling <ref type="bibr" target="#b21">(Pascanu et al., 2014)</ref> and machine translation (Barone et al., 2017; <ref type="bibr" target="#b19">Meng and Zhang, 2019)</ref>. Particularly, <ref type="bibr" target="#b19">Meng and Zhang (2019)</ref> propose to maintain a linear transformation path throughout the deep transition procedure with a linear gate to enhance the transition structure. Following <ref type="bibr" target="#b19">Meng and Zhang (2019)</ref>, the deep transition block in our hierarchical model is composed of two key components, namely Linear Transformation enhanced GRU (L-GRU) and Transition GRU (T-GRU). At each time step, L-GRU first encodes each token with an additional linear transformation of the input embedding, then the hidden state of L-GRU is passed into a chain of T-GRU connected merely by hidden states. Afterwards, the output "state" of the last T-GRU for the current time step is carried over as "state" input of the first L-GRU for the next time step. Formally, in a unidirectional network with transition number of L, the hidden state of the t-th token in a sentence is computed as:</p><formula xml:id="formula_6">h 0 i = L-GRU(x i , h L i−1 ) (14) h j i = T-GRU j (h j−1 i ) 1 ≤ j ≤ L (15)</formula><p>Linear Transformation Enhanced GRU L-GRU extends the conventional GRU by an additional linear transformation of the input token embeddings. At time step t, the hidden state of L-GRU is computed as follows:</p><formula xml:id="formula_7">h t = (1 − z t ) h t−1 + z t h t (16) h t = tanh(W xh x t + r t (W hh h t−1 )) + l t W x x t<label>(17)</label></formula><p>where W xh and W hh are parameter matrices, and reset gate r t and update gate z t are same as GRU:</p><formula xml:id="formula_8">r t = σ(W xr x t + W hr h t−1 ) (18) z t = σ(W xz x t + W hz h t−1 )<label>(19)</label></formula><p>The linear transformation W x x t in candidate hidden state h t (Eq. 17) is regulated by the linear gate l t , which is computed as follows:</p><formula xml:id="formula_9">l t = σ(W xl x t + W hl h t−1 )<label>(20)</label></formula><p>Transition GRU T-GRU is a special case of conventional GRU, which only takes hidden states from the adjacent lower layer as inputs. At time step t at transition depth l, the hidden state of T-GRU is computed as follows:</p><p>Reset gate r t and update gate z t also only take hidden states as input, which are computed as:</p><formula xml:id="formula_10">r l = σ(W l r h l−1 )<label>(23)</label></formula><formula xml:id="formula_11">z t = σ(W l z h l−1 )<label>(24)</label></formula><p>As indicated above, at each time step of our deep transition block, there is a L-GRU in the bottom and several T-GRUs on the top of L-GRU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Local Word Representation</head><p>Charater-aware word embeddings It has been demonstrated that character level information (such as capitalization, prefix and suffix) <ref type="bibr" target="#b6">(Collobert et al., 2011;</ref><ref type="bibr" target="#b30">dos Santos and Zadrozny, 2014)</ref> is crucial for sequence labeling tasks. In our GCDT, the character sets consist of all unique characters in datasets besides the special symbol "PAD" and "UNK". We use one layer of CNN followed by max pooling to generate character-aware word embeddings.</p><p>Pre-trained word embeddings The pre-trained word embeddings have been indicated as a standard component of neural network architectures for various NLP tasks. Since the capitalization feature of words is crucial for sequence labeling tasks <ref type="bibr" target="#b6">(Collobert et al., 2011)</ref>, we adopt word embeddings trained in the case sensitive schema.</p><p>Both the character-aware and pre-trained word embeddings are context-insensitive, which are called local word representations compared with global contextual embedding in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Global Contextual Embedding</head><p>We adopt an independent deep transition RNN named global contextual encoder (right part in <ref type="figure" target="#fig_0">Figure 1</ref>) to capture global features. In particular, we transform the hidden states of global contextual encoder into a fixed-size vector with various strategies, such as mean pooling, max pooling and self-attention mechanism <ref type="bibr" target="#b33">(Vaswani et al., 2017)</ref>. According to the preliminary experiments, we choose mean pooling strategy considering the balance between effect and efficiency.</p><p>In conventional BiRNNs, the global contextual feature is insufficiently modeled at each position, as the nature of recurrent architecture makes RNN partial to the most recent input token. While our context-aware representation is incorporated with local word embeddings directly, which assists in capturing useful representations through combinatorial computing between diverse local word embeddings and the global contextual embedding. We further investigate the effects on positions where the global embedding is used. (Section 5.1) Metric We adopt the BIOES tagging scheme for both tasks instead of the standard BIO2, since previous studies have highlighted meaningful improvements with this scheme <ref type="bibr" target="#b26">(Ratinov and Roth, 2009</ref>). We take the official conlleval 3 as the token-level F 1 metric. Since the data size if relatively small, we train each final model for 5 times with different parameter initialization and report the mean and standard deviation F 1 value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>All trainable parameters in our model are initialized by the method described by <ref type="bibr" target="#b9">Glorot and Bengio (2010)</ref>. We apply dropout <ref type="bibr" target="#b32">(Srivastava et al., 2014)</ref> to embeddings and hidden states with a rate of 0.5 and 0.3 respectively. All models are optimized by the Adam optimizer (Kingma and Ba, 2014) with gradient clipping of 5 <ref type="bibr" target="#b22">(Pascanu et al., 2013)</ref>. The initial learning rate α is set to 0.008, and decrease with the growth of training steps. We monitor the training process on the development set and report the final result on the test set. One layer CNN with a filter of size 3 is utilized to generate 128-dimension word embeddings by max pooling. The cased, 300d Glove is adapted to initialize word embeddings, which is frozen in all models. In the auxiliary experiments, the output hidden states of BERT are taken as additional word embeddings and kept fixed all the time.</p><p>Empirically, We assign the following hyperparameters with default values except mentioned later. We set batch size to 4096 at the token level, transition number to 4, hidden size of sequence labeling encoder and decoder to 256, hidden size of global contextual encoder to 128.  <ref type="bibr" target="#b37">(Yang et al., 2017b)</ref> 94.66 <ref type="bibr" target="#b38">(Zhai et al., 2017)</ref> 94.72 <ref type="bibr" target="#b10">(Hashimoto et al., 2017)</ref> 95.02 <ref type="bibr" target="#b31">(Søgaard and Goldberg, 2016)</ref> 95.28 <ref type="bibr" target="#b34">(Xin et al., 2018)</ref> 95.29 ± 0.08 GCDT 95.43 ± 0.06 GCDT + BERT LARGE 97.30 ± 0.03 <ref type="table">Table 2</ref>: F 1 scores on CoNLL2000 Chunking task. * refers to adopting external task-specific resources (like Gazetteers or annotated data).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Main Results</head><p>The main results of our GCDT on the CoNLL03 and CoNLL2000 are illustrated in <ref type="table">Table 1</ref> and <ref type="table">Table 2</ref> respectively. Given only standard training data and publicly available word embeddings, our GCDT achieves state-of-the-art results on both tasks. It should be noted that some results on NER are not comparable to ours directly, as their final models are trained on both training and development data 4 . More notably, our GCDT surpasses the models that exploit additional task-specific resources or annotated corpora <ref type="bibr" target="#b17">(Luo et al., 2015;</ref><ref type="bibr" target="#b37">Yang et al., 2017b;</ref><ref type="bibr" target="#b3">Chiu and Nichols, 2016)</ref>. Additionally, we conduct experiments by leveraging the well-known BERT as an external resource for relatively fair comparison with models Models F 1 <ref type="bibr" target="#b27">(Rei, 2017)</ref> 86.26 <ref type="bibr" target="#b16">(Liu et al., 2017)</ref> 91.71 ± 0.10 (Peters et al., 2017) † 91.93 ± 0.19 <ref type="bibr" target="#b25">(Peters et al., 2018)</ref> 92.20  92.61 <ref type="formula" target="#formula_0">(2018)</ref>   <ref type="table">Table 3</ref>: F 1 scores on the CoNL03 NER task by leveraging language model, † refers to models trained on both training and development data. We establish new state-of-the-art result on this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>F 1 <ref type="bibr" target="#b27">(Rei, 2017)</ref> 93.88 <ref type="bibr" target="#b16">(Liu et al., 2017)</ref> 95.96 ± 0.08 <ref type="bibr" target="#b24">(Peters et al., 2017)</ref> 96.37 ± 0.05 <ref type="bibr" target="#b0">(Akbik et al., 2018)</ref> 96.72 ± 0.05  97.00 GCDT + BERT LARGE 97.30 ± 0.03 <ref type="table">Table 4</ref>: F 1 scores on the CoNLL2000 Chunking task by leveraging language model. We establish new stateof-the-art result on this task. that utilize external language models trained on massive corpora. Especially, <ref type="bibr" target="#b27">Rei (2017)</ref> and <ref type="bibr" target="#b16">Liu et al. (2017)</ref> build task-specific language models only on supervised data. <ref type="table">Table 3</ref> and <ref type="table">Table 4</ref> show that our GCDT outperforms previous state-of-theart results substantially at 93.47 (+0.38) on NER and 97.30 (+0.30) on Chunking when contrasted with a collection of highly competitive baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>We choose the CoNLL03 NER task as example to elucidate the properties of our GCDT and conduct several additional experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Where to Use the Global Representation?</head><p>In this experiment, we investigate the effects of locations on the global contextual embedding in our hierarchical model. In particular, we use the global embedding g to augment:</p><p>• input of final softmax layer ;</p><p>x  • input of sequence labeling encoder;</p><p>x encoder k = [w k ; c k ; g] <ref type="table" target="#tab_5">Table 5</ref> shows that the global embedding g improves performance when utilized at the relative low layer (row 3) , while g may do harm to performances when adapted at the higher layers (row 0 vs. row 1 &amp; 2). In the last option, g is incorporated to enhance the input token representation for sequence labeling encoder, the combinatorial computing between the multi-granular local word embeddings (w k and c k ) and global embedding g can capture more specific and richer representations for the prediction of each token, and thus improves overall system performance. While the other two options (row 1, 2) concatenate the highly abstract g with hidden states (h encoder k or h decoder k ) from the higher layers, which may bring noise to token representation due to the similar feature spaces and thus hurt task-specific predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparing with Stacked RNNs</head><p>Although our proposed GCDT bears some resemblance to the conventional stacked RNNs, they are very different from each other. Firstly, although the stacked RNNs can process very deep architectures, the transition depth between consecutive hidden states in the token level is still shallow.</p><p>Secondly, in the stacked RNNs, the hidden states along the sequential axis are simply fed into the corresponding positions of the higher layers, namely only position-aware features are transmitted in the deep architecture. While in GCDT, the internal states in all token position of the global contextual encoder are transformed into a fixedsize vector. This contextual-aware representation provides more general and informative features of the entire sentence compared with stacked RNNs.</p><p>To obtain rigorous comparisons, we stack two layers of deep transition RNNs instead of conventional RNNs with similar parameter numbers of GCDT. According to the results in <ref type="table" target="#tab_7">Table 6</ref>, the stacked-DT improves the performance of the orig-  inal DT slightly, while there is still a large margin between GCDT and the stacked-DT. As we can see, our GCDT achieves a much better performance than stacked-DT with a smaller parameter size, which further verifies that our GCDT can effectively leverage global information to learn more useful representations for sequence labeling tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Experiments</head><p>We conduct ablation experiments to investigate the impacts of various components in GCDT. More specifically, we remove one kind of token embedding from char-aware, pre-trained and global embeddings for sequence labeling encoder each time, and utilize DT or conventional GRU with similar model sizes 5 . Results of different combinations are presented in <ref type="table" target="#tab_9">Table 7</ref>. Given the same input embeddings, DT surpasses the conventional GRU substantially in most cases, which further demonstrates the superiority of DT in sequence labeling tasks. Our observations on character-level and pre-trained word embeddings suggest that they have a significant impact on highly competitive results (row 1 &amp; 3 vs. row 5), which is consistent with previous work (dos <ref type="bibr" target="#b30">Santos and Zadrozny, 2014;</ref><ref type="bibr" target="#b15">Lample et al., 2016)</ref>. Furthermore, the global contextual embedding substantially improves the performances on both DT and GRU based models <ref type="bibr">(row 6 &amp; 7 vs. row 4 &amp; 5)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Effect of BERT</head><p>WordPiece is adopted to tokenize sequence in BERT, which may cut a word into pieces, such as converting "Johanson" into "Johan ##son". Therefore, additional efforts should be taken to maintain alignments between input tokens and their corresponding labels. Three strategies are conducted to obtain the exclusive BERT embedding of each token in a sequence. Firstly, we take the first subword as the whole word embedding after tokenization, which is employed in the original paper of   <ref type="table">Table 8</ref>: Comparison of CoNLL03 F 1 scores when various types, layers and pooling strategies of BERT are employed. "first" indicates the first sub-word embedding, "mean" and "max" refer to mean and max pooling correspondingly.</p><p>BERT <ref type="bibr" target="#b7">(Devlin et al., 2018)</ref>. Mean and max poolings are used as the latter two strategies. Results of various combinations of BERT type, layer and pooling strategy are illustrated in <ref type="table">Table 8</ref>. It's reasonable that BERT trained on large model surpasses the smaller one in most cases due to the larger model capacity and richer contextual representation. For the pooling strategy, "mean" is considered to capture more comprehensive representations of rare words than "first" and "max", thus better average performances. Additionally, we hypothesize that the higher layers in BERT encode more abstract and semantic features, while the lower ones prefer general and syntax infor-  mation, which is more helpful for our NER and Chunking tasks. These hypotheses are consistent with results emerged in <ref type="table">Table 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Model Complexity</head><p>One way of measuring the complexity of a neural model is through the total number of trainable parameters. In GCDT, the global contextual encoder increases parameter numbers of the sequence labeling encoder due to the enlargement of input dimensions, thus we run additional experiments to verify whether the increment of parameters has a great affection on performances. Empirically, we replace DT with conventional GRU in the global contextual encoder and sequence labeling module (both encoder and decoder) respectively. Results of various combinations are shown in <ref type="table" target="#tab_11">Table 9</ref>. Observations on parameter numbers show that DT outperforms GRU substantially, with a smaller size (row 4 &amp; 5 vs. row 0). From the perspective of global contextual encoder, DT gives slightly better result compared with GRU (row 3 vs. row 0). We observe similar results in the sequence labeling module (row 1 &amp; 2 vs. row 0). Intuitively, it should further improve performance when utilizing DT in both modules, which is consistent with the observations in <ref type="table" target="#tab_11">Table 9</ref> (row 4 &amp; 5 vs. row 0).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Neural Sequence Labeling <ref type="bibr" target="#b6">Collobert et al. (2011)</ref> propose a seminal neural architecture for sequence labeling, which learns useful representation from pre-trained word embeddings instead of hand-crafted features.  develop the outstanding BiLSTMs-CRF architecture, which is improved by incorporating character-level LSTM <ref type="bibr" target="#b15">(Lample et al., 2016)</ref>, GRU <ref type="bibr" target="#b36">(Yang et al., 2016)</ref>, <ref type="bibr">CNN (dos Santos and Zadrozny, 2014;</ref><ref type="bibr" target="#b34">Xin et al., 2018)</ref>, IntNet <ref type="bibr" target="#b34">(Xin et al., 2018)</ref>. The shallow connections between consecutive hidden states in those models inspire us to deepen the transition path for richer representation.</p><p>More recently, there has been a growing body of work exploring to leverage language model trained on massive corpora in both character level <ref type="bibr" target="#b24">(Peters et al., 2017</ref><ref type="bibr" target="#b25">(Peters et al., , 2018</ref><ref type="bibr" target="#b0">Akbik et al., 2018)</ref> and token level <ref type="bibr" target="#b7">(Devlin et al., 2018)</ref>. Inspired by the effectiveness of language model embeddings, we conduct auxiliary experiments by leveraging the well-known BERT as an additional feature.</p><p>Exploit Global Information Chieu and Ng (2002) explore the usage of global feature in the whole document by the co-occurrence of each token, which is fed into a maximum entropy classifier. With the widespread application of distributed word representations  and neural networks <ref type="bibr" target="#b6">(Collobert et al., 2011;</ref> in sequence labeling tasks, the global information is encoded into hidden states of BiRNNs. Specially, <ref type="bibr" target="#b35">Yang et al. (2017a)</ref> leverage global sentence patterns for NER reranking. Inspired by the global sentence-level representation in S-LSTM <ref type="bibr">(Zhang et al., 2018)</ref>, we propose a more concise approach to capture global information, which has been demonstrated more effective on sequence lableing tasks.</p><p>Deep Transition RNN Deep transition recurrent architecture extends conventional RNNs by increasing the transition depth between consecutive hidden states. Previous studies have shown the superiority of this architecture on both language model <ref type="bibr" target="#b21">(Pascanu et al., 2014)</ref> and machine translation <ref type="bibr" target="#b1">(Barone et al., 2017;</ref><ref type="bibr" target="#b19">Meng and Zhang, 2019)</ref>. We follow the deep transition architecture in <ref type="bibr" target="#b19">(Meng and Zhang, 2019)</ref>, and extend it into a hierarchical model with the global contextual representation at the sentence level for sequence labeling tasks.</p><p>We propose a novel hierarchical neural model for sequence labeling tasks (GCDT), which is based on the deep transition architecture and motivated by global contextual representation at the sentence level. Empirical studies on two standard datasets suggest that GCDT outperforms previous state-ofthe-art systems substantially on both CoNLL03 NER task and CoNLL2000 Chunking task without additional corpora or task-specific resources. Furthermore, by leveraging BERT as an external resource, we report new state-of-the-art F 1 scores of 93.47 on CoNLL03 and 97.30 on CoNLL2000.</p><p>In the future, we would like to extend GCDT to other analogous sequence labeling tasks and explore its effectiveness on other languages.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overview of GCDT. The global contextual encoder (on the right) serves as an enhancement of token representation. The sequence labeling encoder and decoder (on the left) take charge of the task-specific predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Datasets and Metric NER The CoNLL03 NER task (Sang and De Meulder, 2003) is tagged with four linguistic entity types (PER, LOC, ORG, MISC). Standard data includes train, development and test sets. NP, VP, PP, etc.). Standard data includes train and test sets.</figDesc><table><row><cell>4 Experiments</cell></row><row><cell>4.1 Chunking The CoNLL2000 Chunking task</cell></row><row><cell>(Sang and Buchholz, 2000) defines 11 syntactic</cell></row><row><cell>chunk types (</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>BERT LARGE 93.47 ± 0.03</figDesc><table><row><cell>BERT BASE</cell><cell>92.40</cell></row><row><cell>(2018) BERT LARGE</cell><cell>92.80</cell></row><row><cell>(Akbik et al., 2018) †</cell><cell>93.09</cell></row><row><cell>GCDT +</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Input of sequence labeling decoder 91.45 3 Input of sequence labeling encoder 91.96</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell># Use global embedding at</cell><cell>F 1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0 None</cell><cell>91.60</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1 Input of final softmax</cell><cell>91.48</cell></row><row><cell></cell><cell></cell><cell></cell><cell>2</cell></row><row><cell cols="2">sof tmax k</cell><cell cols="2">= [h decoder k</cell><cell>; y k−1 ; g]</cell></row><row><cell cols="4">• input of sequence labeling decoder;</cell></row><row><cell>x decoder k</cell><cell cols="2">= [h encoder k</cell><cell>; y k−1 ; g]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comparison of CoNLL03 test F 1 when the global contextual embedding is used at different layers.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Comparison of CoNLL03 test F 1 between stacked RNNs and GCDT.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell cols="4">: Ablation experiments on the CoNLL03 to in-</cell></row><row><cell cols="4">vestigate the impacts of various components, where</cell></row><row><cell cols="4">"char" indicates character-aware word embeddings,</cell></row><row><cell cols="4">"Glove" indicates pre-trained word embeddings, and</cell></row><row><cell cols="4">"global" indicates global contextual embedding.</cell></row><row><cell>Type</cell><cell>BERT Layer</cell><cell>Pooling</cell><cell>F 1</cell></row><row><cell></cell><cell></cell><cell>first</cell><cell>92.70</cell></row><row><cell></cell><cell>6</cell><cell>max</cell><cell>92.88</cell></row><row><cell>BASE</cell><cell></cell><cell>mean first</cell><cell>92.99 92.89</cell></row><row><cell></cell><cell>12</cell><cell>max</cell><cell>92.74</cell></row><row><cell></cell><cell></cell><cell>mean</cell><cell>92.92</cell></row><row><cell></cell><cell></cell><cell>first</cell><cell>92.88</cell></row><row><cell></cell><cell>12</cell><cell>max</cell><cell>93.23</cell></row><row><cell></cell><cell></cell><cell>mean</cell><cell>93.36</cell></row><row><cell></cell><cell></cell><cell>first</cell><cell>93.18</cell></row><row><cell>LARGE</cell><cell>18</cell><cell>max</cell><cell>93.07</cell></row><row><cell></cell><cell></cell><cell>mean</cell><cell>93.47</cell></row><row><cell></cell><cell></cell><cell>first</cell><cell>92.57</cell></row><row><cell></cell><cell>24</cell><cell>max</cell><cell>92.60</cell></row><row><cell></cell><cell></cell><cell>mean</cell><cell>92.83</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>F 1 scores on the CoNLL03 and parameter sizes of various models, where "GRU-384" indicates the conventional GRU with hidden size of 384, while "DT2-128" refers to deep transition RNN with transition number of 2 and hidden size of 128, similarly for "DT4-256".</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://nlp.stanford.edu/projects/glove/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">h l t = (1 − z l t ) h l−1 t + z l t h t l (21) h t l = tanh(r l t (W l h h l−1 t ))(22)</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://www.clips.uantwerpen.be/conll2000/chunking/ conlleval.txt</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We achieve F1 score of 92.18 when training on both training and development data without extra resources.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">To avoid the effect of various model size, we fine tuning hidden size of each model, and more details in Section 5.5</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Liu, Xu, and Chen are supported by the National Nature Science Foundation of China (Contract 61370130, 61473294 and 61502149), and Beijing Natural Science Foundation under Grant No. 4172047, and the Fundamental Research Funds for the Central Universities (2015JBM033), and the International Science and Technology Cooperation Program of China under grant No. 2014DFA11350. We sincerely thank the anonymous reviewers for their thorough reviewing and valuable suggestions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Yue Zhang, <ref type="bibr">Qi Liu, and Linfeng Song. 2018</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Contextual string embeddings for sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duncan</forename><surname>Blythe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1638" to="1649" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deep architectures for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Valerio Miceli Barone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindrich</forename><surname>Helcl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno>abs/1707.07631</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Named entity recognition: A maximum entropy approach using global information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Leong Chieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2002: The 19th International Conference on Computational Linguistics</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Named entity recognition with bidirectional lstm-cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nichols</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="357" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Aglar Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence modeling with cross-view training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1914" to="1925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The viterbi algorithm. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Forney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1973" />
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="268" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A joint manytask model: Growing a neural network for multiple nlp tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1206</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1923" to="1933" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jrgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Bidirectional LSTM-CRF models for sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno>abs/1508.01991</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando Cn</forename><surname>Pereira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1030</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Empower sequence labeling with task-aware neural language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><forename type="middle">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno>abs/1709.04109</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Joint entity recognition and disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiqing</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="879" to="888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">End-to-end sequence labeling via bi-directional lstm-cnns-crf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1101</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1064" to="1074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">DTMT: A novel deep transition architecture for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinchao</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">How to construct deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Lexicon infused phrase embeddings for named entity resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineet</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/W14-1609</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Eighteenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="78" to="86" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence tagging with bidirectional language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Power</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1756" to="1765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Design challenges and misconceptions in named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth conference on computational natural language learning</title>
		<meeting>the thirteenth conference on computational natural language learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="147" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semi-supervised multitask learning for sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Rei</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1194</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2121" to="2130" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Erik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Buchholz</surname></persName>
		</author>
		<title level="m">Introduction to the conll-2000 shared task: Chunking. arXiv preprint cs/0009008</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Erik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fien</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De Meulder</surname></persName>
		</author>
		<idno>cs/0306050</idno>
		<title level="m">Introduction to the conll-2003 shared task: Languageindependent named entity recognition</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning character-level representations for part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cícero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bianca</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zadrozny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep multi-task learning with low level tasks supervised at lower layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-2038</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="231" to="235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Learning better internal structure of words for sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhuti</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-David</forename><surname>Ruvini</surname></persName>
		</author>
		<idno>abs/1810.12443</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Neural reranking for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Dong</surname></persName>
		</author>
		<idno type="DOI">10.26615/978-954-452-049-6_101</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference Recent Advances in Natural Language Processing</title>
		<meeting>the International Conference Recent Advances in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="784" to="792" />
		</imprint>
	</monogr>
	<note>INCOMA Ltd</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Multi-task cross-lingual sequence tagging from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06270</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Transfer learning for sequence tagging with hierarchical recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Neural models for sequence chunking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feifei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saloni</forename><surname>Potdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<idno>abs/1701.04027</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
