<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generalized State-Dependent Exploration for Deep Reinforcement Learning in Robotics</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonin</forename><surname>Raffin</surname></persName>
							<email>antonin.raffin@dlr.de</email>
							<affiliation key="aff0">
								<orgName type="department">Robotics and Mechatronics Center (RMC) German Aerospace Center (DLR)</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Freek</forename><surname>Stulp</surname></persName>
							<email>freek.stulp@dlr.de</email>
							<affiliation key="aff1">
								<orgName type="department">Robotics and Mechatronics Center (RMC) German Aerospace Center (DLR)</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Generalized State-Dependent Exploration for Deep Reinforcement Learning in Robotics</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Robotics</term>
					<term>Reinforcement Learning</term>
					<term>Exploration</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Reinforcement learning (RL) enables robots to learn skills from interactions with the real world. In practice, the unstructured step-based exploration used in Deep RL -often very successful in simulation -leads to jerky motion patterns on real robots. Consequences of the resulting shaky behavior are poor exploration, or even damage to the robot. We address these issues by adapting state-dependent exploration (SDE) [1] to current Deep RL algorithms. To enable this adaptation, we propose three extensions to the original SDE, which leads to a new exploration method generalized state-dependent exploration (gSDE). We evaluate gSDE both in simulation, on PyBullet continuous control tasks, and directly on a tendon-driven elastic robot. gSDE yields competitive results in simulation but outperforms the unstructured exploration on the real robot. The code is available at https://github.com/DLR-RM/stable-baselines3/tree/sde.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>One of the first robots that used artificial intelligence methods was called "Shakey", because it would shake a lot during operation <ref type="bibr" target="#b1">[2]</ref>. Shaking has now again become quite prevalent in robotics, but for a very different reason. When learning robotic skills with deep reinforcement learning (Deep RL), the de facto standard for exploration is to sample a noise vector t from a Gaussian distribution independently at each time step t, and then adding it to the policy output.</p><formula xml:id="formula_0">t ∼ N (0, σ 2 )</formula><p>Noise sampled from Gaussian at each time step (1) a t = µ(s t ; θ µ ) + t Perturb policy output (action) at each time step <ref type="bibr" target="#b1">(2)</ref> This approach can be very effective in simulation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>, and has therefore also been applied to robotics <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>. But for experiments on real robots, such unstructured exploration has many drawbacks, which have been pointed out by the robotics community <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>:</p><p>challenging for DDPG <ref type="bibr" target="#b19">[20]</ref>. Despite hyperparameter optimization, the problem cannot be solved without external noise <ref type="bibr" target="#b0">1</ref> . Because of the unstructured exploration, the commanded power oscillates at high frequency (cf <ref type="figure" target="#fig_0">Figure 1</ref>), making the velocity stay around the initial value of zero. The policy thus converges to a local minimum of doing nothing, which minimizes the consumed energy. In robotics, multiple solutions have been proposed to counteract this inefficient exploration strategy. These include correlated noise <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16]</ref>, low-pass filters <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>, action repeat <ref type="bibr" target="#b22">[23]</ref> or lower level controllers <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b8">9]</ref>. A more principled solution is to perform exploration in parameter space, rather than in action space <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>. This approach usually requires fundamental changes in the algorithm, and is harder to tune when the number of parameters is high.</p><p>State Dependent Exploration (SDE) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12]</ref> was proposed as a compromise between exploring in parameter and action space. SDE replaces the sampled noise with a state-dependent exploration function, which during an episode returns the same action for a given state. This results in smoother exploration and less variance per episode. To the best of our knowledge, no Deep RL algorithm has yet been successfully combined with SDE. We surmise that this is because the problem that it solves -shaky, jerky movement -is not as noticeable in simulation, which is the current focus of the community.</p><p>Going back to the MountainCar problem, SAC with SDE can solve it with many different hyperparameter configurations 2 . Looking at the taken actions during early stage of training (cf <ref type="figure" target="#fig_0">Figure 1)</ref>, it is clear that State Dependent Exploration provides a smoother and more consistent exploration, permitting to drive up the hill.</p><p>In this paper, we aim at reviving interest in SDE as an effective method for addressing exploration issues that arise from using independently sampled Gaussian noise on real robots. Our concrete contributions, which also determine the structure of the paper, are:</p><p>1. highlighting the issues with unstructured Gaussian exploration (Section 1). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>In reinforcement learning, an agent interacts with its environment, usually modeled as a Markov Decision Process (MDP) (S, A, p, r) where S is the state space, A the action space and p(s |s, a) the transition function. At every step t, the agent performs an action a in state s following its policy π : S → A. It then receives a feedback signal in the next state s : the reward r(s, a). The objective of the agent is to maximize the long-term reward. More formally, the goal is to maximize the expectation of the sum of discounted reward, over the trajectories ρ π generated using its policy π:</p><formula xml:id="formula_1">t E (st,at)∼ρπ γ t r(s t , a t )<label>(3)</label></formula><p>where γ ∈ [0, 1] is the discount factor and represents a trade-off between maximizing short-term and long-term rewards. The agent-environment interactions are often broken down into sequences called episodes, that end when the agent reaches a terminal state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Exploration in action or policy parameter space</head><p>In the case of continuous actions, the exploration is commonly done in the action space <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b5">6]</ref>. At each time-step, a noise vector t is independently sampled from a Gaussian distribution and then added to the controller output.</p><formula xml:id="formula_2">a t = µ(s t ; θ µ ) + t , t ∼ N (0, σ 2 )<label>(4)</label></formula><p>where µ(s t ) is the deterministic policy and π(a t |s t ) ∼ N (µ(s t ), σ 2 ) is the resulting stochastic policy, used for exploration. θ µ denotes the parameters of the deterministic policy. For simplicity, throughout the paper, we will only consider Gaussian distributions with diagonal covariance matrices. Hence, here, σ is a vector with the same dimension as the action space A.</p><p>Alternatively, the exploration can also be done in the parameter space <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">32]</ref>. At the beginning of an episode, the perturbation is sampled and added to the policy parameters θ µ . This usually results in more consistent exploration but becomes challenging with an increasing number of parameters <ref type="bibr" target="#b23">[24]</ref>.</p><formula xml:id="formula_3">a t = µ(s t ; θ µ + ), ∼ N (0, σ 2 )<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">State Dependent Exploration</head><p>State Dependent Exploration (SDE) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12]</ref> is an intermediate solution that consists in adding noise as a function of the state s t , to the deterministic action µ(s t ). At the beginning of an episode, the parameters θ of that exploration function are drawn from a Gaussian distribution. The resulting action a t is as follows:</p><formula xml:id="formula_4">a t = µ(s t ; θ µ ) + (s t ; θ ), θ ∼ N (0, σ 2 )<label>(6)</label></formula><p>In the linear case, i. e. with a linear policy and a noise matrix, parameter space exploration and SDE are equivalent:</p><formula xml:id="formula_5">a t = µ(s t ; θ µ ) + (s t ; θ ), θ ∼ N (0, σ 2 ) = θ µ s t + θ s t = (θ µ + θ )s t</formula><p>This episode-based exploration is smoother and more consistent than the unstructured step-based exploration. Thus, during one episode, instead of oscillating around a mean value, the action a for a given state s will be the same.</p><p>In the remainder of this paper, to avoid overloading notation, we drop the time subscript t, i. e. we now write s instead of s t . s j or a j now refer to an element of the state or action vector.</p><p>In the case of a linear exploration function (s; θ ) = θ s, by operation on Gaussian distributions, Rückstieß et al. <ref type="bibr" target="#b0">[1]</ref> show that the action element a j is normally distributed:</p><formula xml:id="formula_6">π j (a j |s) ∼ N (µ j (s),σ j 2 )<label>(7)</label></formula><p>whereσ is a diagonal matrix with elementsσ j = i (σ ij s i ) 2 We can then obtain the derivative of the log-likelihood log π(a|s) with respect to the variance σ:</p><formula xml:id="formula_7">∂ log π(a|s) ∂σ ij = k ∂ log π k (a k |s) ∂σ j ∂σ j ∂σ ij (8) = ∂ log π j (a j |s) ∂σ j ∂σ j ∂σ ij (9) = (a j − µ j ) 2 −σ j 2 σ j 3 s 2 i σ iĵ σ j<label>(10)</label></formula><p>This can be easily plugged into the likelihood ratio gradient estimator <ref type="bibr" target="#b32">[33]</ref>, which allows to adapt σ during training. SDE is therefore compatible with standard policy gradient methods, while addressing most shortcomings of the unstructured exploration.</p><p>For a non-linear exploration function, the resulting distribution π(a|s) is most of the time unknown. Thus, computing the exact derivative w.r.t. the variance is not trivial and may require approximate inference. As we focus on simplicity, we leave this extension for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Generalized State Dependent Exploration (gSDE)</head><p>Considering Equations <ref type="formula" target="#formula_6">(7)</ref> and <ref type="formula">(8)</ref>, some limitations of the original formulation are apparent:</p><p>i the variance of the policyσ j = i (σ ij s i ) 2 depends on the state space dimension (it grows with it), which means that the initial σ must be tuned for each problem. ii there is only a linear dependency between the state and the exploration noise, which limits the possibilities. iii the state must be normalized, as the gradient and the noise magnitude depend on the state magnitude, otherwise one may have gradient issues. iv the noise does not change during one episode, which is problematic <ref type="bibr" target="#b33">[34]</ref> if the episode length is long, because the exploration will be limited.</p><p>To mitigate the mentioned issues and adapt it to Deep RL algorithms, we propose three improvements:</p><p>1. instead of the state s, we can in fact use any features. We chose policy features z µ (s; θ zµ ) (last layer before the deterministic output µ(s) = θ µ z µ (s; θ zµ )) as input to the noise function (s; θ ) = θ z µ (s). 2. we sample the parameters θ of the exploration function every n steps instead of every episode. 3. when applicable (here, for A2C <ref type="bibr" target="#b28">[29]</ref> and PPO <ref type="bibr" target="#b29">[30]</ref>), we make use of parallelization and have multiple exploration matrices. That is to say, for each worker <ref type="bibr" target="#b28">[29]</ref>, we draw different parameters of the exploration function.</p><p>Using policy features allows to mitigate issues i, ii and iii: the variance of the policy only depends on the network architecture and the relationship between the state s and the noise is non-linear. This permits for instance to use images as input. Also, because we can back-propagate through z µ (s) (using the reparametrization trick <ref type="bibr" target="#b34">[35]</ref>), the features can be learned. This formulation is therefore more general and includes the original SDE description. In practice, as encountered during our early experiments, relying on policy features makes the algorithm easier to tune and avoid the use of normalization: the weights of the policy are usually small at the beginning of training and evolve slowly, which mitigates gradient problem.</p><p>Sampling the parameters θ every n steps tackles the issue iv. and yields a unifying framework <ref type="bibr" target="#b33">[34]</ref> which encompasses both unstructured exploration (n = 1) and original SDE (n = episode length). This formulation follows the description of Deep RL algorithms that update their parameters every m step. In the remainder of the paper, n is always the same (except for PPO) as the update frequency (n = m). This avoids having an additional hyperparameter.</p><p>Finally, using multiple exploration matrices for A2C and PPO favor exploration and generally yield better results (cf Section 4.2).</p><p>We call the resulting approach generalized State Dependent Exploration (gSDE).</p><p>Deep RL algorithms Integrating this updated version of SDE into recent Deep RL algorithms, such as those listed in Appendix A.1, is straightforward. For A2C, PPO and SAC, that rely on a probability distribution, we can replace the original Gaussian distribution by the one derived in Equation <ref type="formula" target="#formula_6">(7)</ref>, where the analytical form of the log-likelihood is known (cf Equation <ref type="formula">(8)</ref>). Regarding TD3 <ref type="bibr" target="#b5">[6]</ref>, which doesn't need any distribution, there is even more freedom in the choice of the exploration function. We chose an on-policy exploration based on A2C gradient update, as it allows to adapt the noise magnitude automatically, instead of relying on a scheduler for instance. We provide pseudo-code for SAC with gSDE in the Appendix A.2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>The goal of this section is to investigate the performance of gSDE compared to unstructured exploration in simulation and on a real system. We first evaluate the two strategies on a set of simulated continuous control tasks. Then, we perform an ablation study to assess the usefulness and robustness of the proposed modifications. Finally, we apply gSDE directly on a real tendon-driven robot and compared it to a model-based controller</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Continuous Control Simulated Environments</head><p>Experiment setup In order to compare gSDE to unstructured exploration in simulation, we chose 4 locomotion tasks from the PyBullet <ref type="bibr" target="#b25">[26]</ref> environments: HALFCHEETAH, ANT, HOPPER and WALKER2D. They are similar to the one found in OpenAI Gym <ref type="bibr" target="#b18">[19]</ref> but the simulator is open source and they are harder to solve 3 .</p><p>We fix the budget to 1 Million steps for off-policy algorithms (SAC, TD3), and to 2 Million for on-policy methods (A2C, PPO) because they require less time to train but are sample inefficient. We report the average score over 10 runs and the associated variance. This variance corresponds to the 68% confidence interval for the estimation of the mean. For each run, we test the learned policy on 10 evaluation episodes every 10000 steps, using the deterministic controller µ(s t ). In all learning curve figures, unless specified otherwise, the x-axis represents the number of steps performed in the environment.</p><p>Regarding the implementation 4 , we use a PyTorch <ref type="bibr" target="#b35">[36]</ref> version of Stable-Baselines <ref type="bibr" target="#b36">[37]</ref>, with performances matching the ones published in the RL zoo <ref type="bibr" target="#b37">[38]</ref>.</p><p>The methodology we follow to tune the hyperparameters can be found in Appendix A.5. PPO and TD3 hyperparameters for unstructured exploration are reused from the original papers <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b5">6]</ref>. For SAC, the optimized hyperparameters for gSDE are performing better than the ones from Haarnoja et al. <ref type="bibr" target="#b16">[17]</ref>, so we keep them for the unstructured exploration to have a fair comparison. No hyperparameters are available for A2C in Mnih et al. <ref type="bibr" target="#b28">[29]</ref> so we use the tuned one from Raffin <ref type="bibr" target="#b37">[38]</ref>. Full hyperparameters details are listed in Appendix A.6.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2C PPO</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Environments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The results in <ref type="table" target="#tab_2">Table 1</ref> show that on-policy algorithms with gSDE perform much better than with the unstructured exploration. This difference may be explained by better hyperparameters, as gSDE main advantage is on a real robot. PPO reaches higher scores than A2C which confirms results previously published.</p><p>Regarding off-policy algorithms in <ref type="table" target="#tab_3">Table 2</ref>, the performance of gSDE is on-par with their independent exploration equivalent. As expected, no real difference is seen in simulation. The essential improvement of gSDE is shown on a real system (cf Section 4.3). The off-policy algorithms are also much more sample efficient compared to their on-policy counterparts: they attain higher performances using half the budget. Those results comfort our choice of SAC for experiments on a real robot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study</head><p>In this section, we investigate the contribution of the proposed modifications to the original SDE: using policy features as input to the noise function, sampling the exploration function parameters every n steps and different exploration parameters per worker. We also examine how sensitive SAC is to the initial exploration variance σ, which is the only additional hyperparameter introduced by SDE. This study is missing in the original paper.</p><p>Initial Exploration Variance Robustness to hyperparameter choice is important for experiments in the real world, as hyperparameter tuning would be quite costly. Therefore, we investigate the influence of the initial exploration variance log σ on PyBullet environments. The results for SAC on the HOPPER task is displayed in <ref type="figure" target="#fig_1">Figure 2a</ref>. SAC is working for a wide range of initial values: from log σ = −4 (σ ≈ 0.018) to log σ = 0 (σ ≈ 1). This is also the case for the other PyBullet tasks, as shown in Appendix A.4. Sampling frequency gSDE is a n-step version of SDE, where n is set to the be the same as the update frequency (except for PPO). This n-step version allows to interpolate between the unstructured exploration n = 1 and the original SDE per-episode formulation. <ref type="figure" target="#fig_1">Figure 2b</ref> shows the importance of that parameter for PPO on the WALKER2D task. If the sampling interval is too large, the agent won't explore enough during long episodes. On the other hand, with a high sampling frequency n ≈ 1, the issues mentioned in Section 1 arise.    <ref type="figure" target="#fig_3">Figure 3a</ref> shows the effect of changing the exploration function input for SAC and PPO. Although it varies from task to task, using policy features is usually beneficial, especially for PPO. It also requires less tuning and no normalization as it depends only on the policy network architecture. Here, the PyBullet tasks are low dimensional and the state space size is of the same order, so no careful per-task tuning is needed. Relying on features also allows to learn directly from pixels, which is not possible in the original formulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Policy features as input</head><p>Parallel Sampling The effect of sampling a set of noise parameters per worker is shown for PPO in <ref type="figure" target="#fig_3">Figure 3b</ref>. This modification improves the performance for each task, as it allows a more diverse exploration. Although less significant, we observe the same outcome for A2C on PyBullet environments (cf <ref type="figure">Figure 8</ref>). Thus, making use of parallel workers improves both exploration and the final performance. Experiment setup To assess the usefulness of gSDE, we apply it on a real system. The task is to control a tendon-driven elastic continuum neck <ref type="bibr" target="#b38">[39]</ref> (see <ref type="figure" target="#fig_4">Figure 4a</ref>) to a given target pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Learning to Control a Tendon-Driven Elastic Robot</head><p>Controlling such soft robot is challenging as the deformation of the structure needs to be modeled accurately, which is computationally expensive <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref> and requires assumptions.</p><p>The system is under-actuated (there are only 4 tendons), hence, the desired pose is a 4D vector: 3 angles for the rotation θ x , θ y , θ z and one for the position x. The input is a 16D vector composed of: the measured tendon lengths (4D), the current tendon forces (4D), the current pose (4D) and the target pose (4D). The reward is a weighted sum between the negative geodesic distance to the desired orientation and the negative Euclidean distance to the desired position. The weights are chosen such that the two components have the same magnitude. The agent receives an additional reward of +2 when reaching and staying at the target pose for half a second. The action space consists in desired delta in tendon forces, limited to 5N. For safety reasons, the tendon forces are clipped below 10N and above 40N. An episode terminates either when the agent reaches the desired pose or after a timeout of 5s, i. e. each episode has a maximum length of 200 steps. The episode is considered successful if the desired pose is reached within a threshold of 2mm for the position and 1deg for the orientation. The agent controls the tendons forces at 30Hz, while a PD controller monitors the motor current at 3KHz on the robot. The gradient update was directly done on a 4-core laptop, after each episode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We first ran the unstructured exploration on the robot but had to stop the experiment early: the high-frequency noise in the command was damaging the tendons and would have broken them due to their friction on the bearings. Then, we trained a controller using SAC with gSDE for two hours. After one hour, the learned policy could already reach successfully 98% of the desired poses (cf <ref type="figure" target="#fig_4">Figure 4b</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Exploration is a key topic in reinforcement learning <ref type="bibr" target="#b17">[18]</ref>. It has been extensively studied in the discrete case and most recent papers still focus on discrete actions <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45]</ref>.</p><p>Several works tackle the issues of unstructured exploration for continuous control by replacing it with correlated noise. Korenkevych et al. <ref type="bibr" target="#b15">[16]</ref> use an autoregressive process and introduce two variables that allows to control the smoothness of the exploration. In the same vein, van Hoof et al. <ref type="bibr" target="#b33">[34]</ref> rely on a temporal coherence parameter to interpolate between the step-or episode-based exploration, making use of a Markov chain to correlate the noise. This smoothed noise comes at a cost: it requires an history, which changes the problem definition.</p><p>Exploring in parameter space <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48]</ref> is an orthogonal approach that also solves some issues of the unstructured exploration. It was successfully applied to real robot but relied on motor primitives <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b12">13]</ref>, which requires expert knowledge. Plappert et al. <ref type="bibr" target="#b23">[24]</ref> adapt parameter exploration to Deep RL by defining a distance in the action space and applying layer normalization to handle high-dimensional space. This approach however adds both complexity, as defining a distance in the action space is not trivial, and computational load.</p><p>Population based algorithms, such as Evolution strategies (ES) or Genetic Algorithms (GA), also explore in parameter space. Thanks to massive parallelization, they were shown to be competitive <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51]</ref> with RL in terms of training time, at the cost of being sample inefficient. To address this problem, recent works <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b24">25]</ref> proposed to combine ES exploration with RL gradient update. This combination, although powerful, unfortunately adds numerous hyperparameters and a nonnegligible computational overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Motivated by a simple failure case, we highlighted several issues that arise from the unstructured exploration in Deep RL algorithms for continuous control. Due to those issues, these algorithms cannot be directly applied to real-world robotic problems.</p><p>To address these issues, we adapt State Dependent Exploration to Deep RL algorithms by extending the original formulation: we replace the exploration function input by learned features, sample the parameters every n steps, and make use of parallelism. This generalized version (gSDE), provides a simple and efficient alternative to unstructured Gaussian exploration.</p><p>gSDE achieves very competitive results on several continuous control benchmarks. We also investigate the contribution of each modification by performing an ablation study. Our proposed exploration strategy, combined with SAC, is robust to hyperparameter choice, which makes it suitable for robotics applications. To demonstrate it, we successfully apply SAC with gSDE directly on a tendon-driven elastic robot. The trained controller matches the performance of a model-based approach in less than two hours.</p><p>Although much progress is being made in sim2real approaches, we believe there is still much truth in Rodney Brooks' assessment that "the world is its own best model". Reinforcement learning on real robots does not require the modeling of interaction forces, friction due to wear and tear, or sensor errors and failures; all of which are also difficult to cover with domain randomization. For these reasons, we believe more effort should be invested in learning on real systems, even if this poses challenges in terms of safety and duration of learning. This paper is meant as a step towards this goal, and we hope that it will revive interest in developing exploration methods that can be directly applied to real robots.</p><p>[58] A. Rajeswaran, K. Lowrey, E. V. Todorov, and S. M. Kakade. Towards generalization and simplicity in continuous control. In Advances in Neural Information Processing Systems, pages 6550-6561, 2017.</p><p>[59] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Algorithms</head><p>In this section, we shortly present the algorithms used in this paper. They correspond to state of the art methods in model-free RL for continuous control, either in terms of sample efficiency or wall-clock time.</p><p>A2C A2C is the synchronous version of Asynchronous Advantage Actor-Critic (A3C) <ref type="bibr" target="#b28">[29]</ref>. It is an actor-critic method that uses parallel rollouts of n-steps to update the policy. It relies on the REINFORCE <ref type="bibr" target="#b32">[33]</ref> estimator to compute the gradient. A2C is fast but not sample efficient.</p><p>PPO A2C gradient update does not prevent large changes that lead to huge drop in performance.</p><p>To tackle this issue, Trust Region Policy Optimization (TRPO) <ref type="bibr" target="#b26">[27]</ref> introduces a trust-region in the policy parameter space, formulated as a constrained optimization problem: it updates the policy while being close in terms of KL divergence to the old policy. Its successor, Proximal Policy Optimization (PPO) <ref type="bibr" target="#b29">[30]</ref> relaxes the constrain (which requires costly conjugate gradient step) by clipping the objective using importance ratio. PPO makes also use of workers (as in A2C) and Generalized Advantage Estimation (GAE) <ref type="bibr" target="#b51">[52]</ref> for computing the advantage.</p><p>TD3 Deep Deterministic Policy Gradient (DDPG) <ref type="bibr" target="#b27">[28]</ref> combines the deterministic policy gradient algorithm <ref type="bibr" target="#b52">[53]</ref> with the improvements from Deep Q-Network (DQN) <ref type="bibr" target="#b53">[54]</ref>: using a replay buffer and target networks to stabilize training. Its direct successor, Twin Delayed DDPG (TD3) <ref type="bibr" target="#b5">[6]</ref> brings three major tricks to tackle issues coming from function approximation: clipped double Q-Learning (to reduce overestimation of the Q-value function), delayed policy update (so the value function converges first) and target policy smoothing (to prevent overfitting). Because the policy is deterministic, DDPG and TD3 rely on external noise for exploration.</p><p>SAC Soft Actor-Critic <ref type="bibr" target="#b30">[31]</ref>, successor of Soft Q-Learning (SQL) <ref type="bibr" target="#b16">[17]</ref> optimizes the maximumentropy objective, that is slightly different compared to the classic RL objective:</p><formula xml:id="formula_8">J(π) = T t=0 E (st,at)∼ρπ [r(s t , a t ) + αH(π( · |s t ))] .<label>(11)</label></formula><p>where H is the policy entropy and α is the entropy temperature and allows to have a trade-off between the two objectives.</p><p>SAC learns a stochastic policy, using a squashed Gaussian distribution, and incorporates the clipped double Q-learning trick from TD3. In its latest iteration <ref type="bibr" target="#b7">[8]</ref>, SAC automatically adjusts the entropy coefficient α, removing the need to tune this crucial hyperparameter.</p><p>Which algorithm for robotics? A2C and PPO are both on-policy algorithms and can be easily parallelized, resulting in relatively small training time. On the other hand, SAC and TD3 are offpolicy and run on a single worker, but are much more sample efficient than the two previous methods, achieving equivalent performances with a fraction of the samples.</p><p>Because we are focusing on robotics applications, having multiple robots is usually not possible, which makes TD3 and SAC the methods of choice. Although TD3 and SAC are very similar, SAC embeds the exploration directly in its objective function, making it easier to tune. We also found, during our experiments in simulation, that SAC works for a wide range of hyperparameters. As a result, we adopt that algorithm for the experiment on a real robot and for the ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Implementation Details</head><p>We used a PyTorch <ref type="bibr" target="#b35">[36]</ref> version of Stable-Baselines <ref type="bibr" target="#b36">[37]</ref> library, with results matching the ones published in the RL zoo <ref type="bibr" target="#b37">[38]</ref>. The training scripts are available at https://github. com/DLR-RM/rl-baselines3-zoo/tree/sde and implementation at https://github.com/ DLR-RM/stable-baselines3/tree/sde. It uses the common implementations tricks for PPO <ref type="bibr" target="#b54">[55]</ref> for the version using independent Gaussian noise.</p><p>For SAC, to ensure numerical stability, we clip the mean to be in range [−2, 2], as it was causing infinite values. In the original implementation, a regularization L 2 loss on the mean and standard deviation was used instead. The algorithm for SAC with gSDE is described in Algorithm 1.</p><p>Compared to the original SDE paper, we did not have to use the expln trick <ref type="bibr" target="#b0">[1]</ref> to avoid exploding variance for PyBullet tasks. However, we found it useful on specific environment like BipedalWalkerHardcore-v2. The original SAC implementation clips this variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Soft Actor-Critic with gSDE</head><p>Initialize parameters θ µ , θ Q , σ, α Initialize replay buffer D for each iteration do θ ∼ N (0, σ 2 ) Sample noise function parameters for each environment step do a t = π(s t ) = µ(s t ; θ µ ) + (s t ; θ ) Get the noisy action s t+1 ∼ p(s t+1 |s t , a t )</p><p>Step in the environment D ← D ∪ {(s t , a t , r(s t , a t ), s t+1 )} Update the replay buffer end for for each gradient step do θ ∼ N (0, σ 2 ) Sample noise function parameters Sample a minibatch from the replay buffer D Update the entropy temperature α Update parameters using ∇J Q and ∇J π Update actor µ, critic Q and noise variance σ Update target networks end for end for A.3 Learning Curves <ref type="figure" target="#fig_6">Figure 5</ref> and <ref type="figure" target="#fig_7">Figure 6</ref> show the learning curves for off-policy and on-policy algorithms on the four PyBullet tasks, using gSDE or unstructured Gaussian exploration. <ref type="figure">Figure 7</ref> displays the ablation study on remaining PyBullet tasks. It shows that SAC is robust against initial exploration variance, and PPO results highly depend on the sampling frequency. <ref type="figure">Figure 8</ref> shows the effect of parallel sampling for A2C. The benefit is only clearly visible for the HALFCHEETAH task. On the other, this parameter does not really affects the final performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Ablation Study: Additional Plots</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Hyperparameter Optimization</head><p>To tune the hyperparameters, we use a TPE sampler and a median pruner from Optuna <ref type="bibr" target="#b55">[56]</ref> library. We give a budget of 500 candidates with a maximum of 3 · 10 5 time-steps on the HALFCHEETAH environment. Some hyperparameters are then manually adjusted (e. g. increasing the replay buffer size) to improve the stability of the algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 Hyperparameters</head><p>For all experiments with a time limit, as done in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr">58,</ref><ref type="bibr" target="#b36">37]</ref>, we augment the observation with a time feature (remaining time before the end of an episode) to avoid breaking Markov assumption. This feature has a great impact on performance, as shown in <ref type="figure" target="#fig_9">Figure 9b</ref>.         gSDE number of steps per rollout 8 initial log σ -3.62 learning rate 9 · 10 −4 GAE coefficient <ref type="bibr" target="#b51">[52]</ref> (λ) 0.9 orthogonal initialization <ref type="bibr" target="#b54">[55]</ref> no Unstructured Exploration number of steps per rollout 32 initial log σ 0.0 learning rate 2 · 10 −3 GAE coefficient [52] (λ) 1.0 orthogonal initialization <ref type="bibr" target="#b54">[55]</ref> yes  <ref type="bibr" target="#b36">[37]</ref> True clip range value function <ref type="bibr" target="#b54">[55]</ref> no normalization observation and reward <ref type="bibr" target="#b36">[37]</ref>   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The MountainCar problem. (a) An underpowered car must drive up the mountain to the flag on the right. This requires driving back-and-forth to build up momentum. (b) and (c) illustrate the exploration during the first 500 steps. Unstructured exploration (b) produces high-frequency noise while SDE (c) provides smooth and consistent exploration, allowing the top of the mountain to be reached. The action executed is decomposed into its deterministic and exploratory component.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>exploration variance logσ on Hopper (a) Initial exploration variance log σ (SAC on HOPPER) sampling frequency on Walker2D (b) Sampling frequency (PPO on WALKER2D) Sensitivity of SAC and PPO to selected hyperparameters on PyBullet tasks. (a) SAC works for a wide range of initial exploration variance (b) The frequency of sampling the noise function parameters is crucial for PPO with gSDE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Parallel sampling of the noise parameters</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>(a) Influence of the input to the exploration function (s; θ ) for SAC and PPO on PyBullet environments: using latent features from the policy zµ (Latent) is usually better than using the state s (Original). (b) Parallel sampling of the noise matrix has a positive impact for PPO on PyBullet tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Training success rate on the real robot (a) The tendon-driven robot<ref type="bibr" target="#b38">[39]</ref> used for the experiment. The tendons are highlighted in orange. (b) Training success rate on the real robot. The blue line is a moving average over 100 episodes and the x-axis is the wall-clock time in hours.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Learning curves for on-policy algorithms on PyBullet tasks. The line denotes the mean over 10 runs of 2 million steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>FigureFigure 6 :</head><label>6</label><figDesc>9a displays the influence of the network architecture for SAC on PyBullet tasks. A bigger network usually yields better results but the gain is minimal passed a certain complexity (here, a two layers neural network with 256 unit per layer). Learning curves for off-policy algorithms on PyBullet tasks. The line denotes the mean over 10 runs of 1 million steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Initial exploration variance log σ (SAC) Sampling frequency (PPO) Sensitivity of SAC and PPO to selected hyperparameters on PyBullet tasks Effect of parallel sampling for A2C on PyBullet tasks time feature SAC-no-time-feature SAC-with-time-feature PPO-no-time-feature PPO-with-time-feature (b) Influence of the time feature</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>(a) Influence of the network architecture (same for actor and critic) for SAC on PyBullet environments. The labels displays the number of units per layer. (b) Influence of including the time or not in the observation for PPO and SAC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Final performance (higher is better) of A2C and PPO on 4 environments with gSDE and unstructured Gaussian exploration (higher is better). We report the mean over 10 runs of 2 million steps. For each benchmark, we highlight the results of the method with the best mean.</figDesc><table><row><cell></cell><cell>SAC</cell><cell></cell><cell cols="2">TD3</cell></row><row><cell>Environments</cell><cell>gSDE</cell><cell>Gaussian</cell><cell>gSDE</cell><cell>Gaussian</cell></row><row><cell cols="3">HALFCHEETAH 2945 +/-95 2883 +/-57</cell><cell>2578 +/-44</cell><cell>2687 +/-67</cell></row><row><cell>ANT</cell><cell cols="2">3106 +/-61 2859 +/-329</cell><cell>3267 +/-34</cell><cell>2865 +/-278</cell></row><row><cell>HOPPER</cell><cell cols="2">2515 +/-50 2477 +/-117</cell><cell>2353 +/-78</cell><cell>2470 +/-111</cell></row><row><cell>WALKER2D</cell><cell cols="2">2270 +/-28 2215 +/-92</cell><cell cols="2">1989 +/-153 2106 +/-67</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Final performance of SAC and TD3 on 4 environments with gSDE and unstructured Gaussian exploration. We report the mean over 10 runs of 1 million steps.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: SAC Hyperparameters</cell><cell></cell></row><row><cell>Parameter</cell><cell>Value</cell></row><row><cell>Shared</cell><cell></cell></row><row><cell>optimizer</cell><cell>Adam [59]</cell></row><row><cell>learning rate</cell><cell>7.3 · 10 −4</cell></row><row><cell>learning rate schedule</cell><cell>constant</cell></row><row><cell>discount (γ)</cell><cell>0.98</cell></row><row><cell>replay buffer size</cell><cell>3 · 10 5</cell></row><row><cell cols="2">number of hidden layers (all networks) 2</cell></row><row><cell>number of hidden units per layer</cell><cell>[400, 300]</cell></row><row><cell>number of samples per minibatch</cell><cell>256</cell></row><row><cell>non-linearity</cell><cell>ReLU</cell></row><row><cell>entropy coefficient (α)</cell><cell>auto</cell></row><row><cell>target entropy</cell><cell>−dim(A)</cell></row><row><cell>target smoothing coefficient (τ )</cell><cell>0.02</cell></row><row><cell>target update interval</cell><cell>64</cell></row><row><cell>train frequency</cell><cell>64</cell></row><row><cell>gradient steps</cell><cell>64</cell></row><row><cell>warm-up steps</cell><cell>10 000</cell></row><row><cell>normalization</cell><cell>None</cell></row><row><cell>gSDE</cell><cell></cell></row><row><cell>initial log σ</cell><cell>-3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>SAC Environment Specific Parameters</figDesc><table><row><cell>Environment</cell><cell>Learning rate schedule</cell></row><row><cell>HopperBulletEnv-v0</cell><cell>linear</cell></row><row><cell>Walker2dBulletEnv-v0</cell><cell>linear</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>: TD3 Hyperparameters</cell><cell></cell></row><row><cell>Parameter</cell><cell>Value</cell></row><row><cell>Shared</cell><cell></cell></row><row><cell>optimizer</cell><cell>Adam [59]</cell></row><row><cell>discount (γ)</cell><cell>0.98</cell></row><row><cell>replay buffer size</cell><cell>2 · 10 5</cell></row><row><cell cols="2">number of hidden layers (all networks) 2</cell></row><row><cell>number of hidden units per layer</cell><cell>[400, 300]</cell></row><row><cell>number of samples per minibatch</cell><cell>100</cell></row><row><cell>non-linearity</cell><cell>ReLU</cell></row><row><cell>target smoothing coefficient (τ )</cell><cell>0.005</cell></row><row><cell>target policy noise</cell><cell>0.2</cell></row><row><cell>target noise clip</cell><cell>0.5</cell></row><row><cell>policy delay</cell><cell>2</cell></row><row><cell>warm-up steps</cell><cell>10 000</cell></row><row><cell>normalization</cell><cell>None</cell></row><row><cell>gSDE</cell><cell></cell></row><row><cell>initial log σ</cell><cell>-3.62</cell></row><row><cell>learning rate for TD3</cell><cell>6 · 10 −4</cell></row><row><cell>target update interval</cell><cell>64</cell></row><row><cell>train frequency</cell><cell>64</cell></row><row><cell>gradient steps</cell><cell>64</cell></row><row><cell>learning rate for gSDE</cell><cell>1.5 · 10 −3</cell></row><row><cell>Unstructured Exploration</cell><cell></cell></row><row><cell>learning rate</cell><cell>1 · 10 −3</cell></row><row><cell>action noise type</cell><cell>Gaussian</cell></row><row><cell>action noise std</cell><cell>0.1</cell></row><row><cell>target update interval</cell><cell>every episode</cell></row><row><cell>train frequency</cell><cell>every episode</cell></row><row><cell>gradient steps</cell><cell>every episode</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell cols="2">: A2C Hyperparameters</cell></row><row><cell>Parameter</cell><cell>Value</cell></row><row><cell>Shared</cell><cell></cell></row><row><cell>number of workers</cell><cell>4</cell></row><row><cell>optimizer</cell><cell>RMSprop with = 1 · 10 −5</cell></row><row><cell>discount (γ)</cell><cell>0.99</cell></row><row><cell>number of hidden layers (all networks)</cell><cell>2</cell></row><row><cell>number of hidden units per layer</cell><cell>[64, 64]</cell></row><row><cell cols="2">shared network between actor and critic False</cell></row><row><cell>non-linearity</cell><cell>T anh</cell></row><row><cell>value function coefficient</cell><cell>0.4</cell></row><row><cell>entropy coefficient</cell><cell>0.0</cell></row><row><cell>max gradient norm</cell><cell>0.5</cell></row><row><cell>learning rate schedule</cell><cell>linear</cell></row><row><cell>normalization</cell><cell>observation and reward [37]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>PPO HyperparametersParameterValue</figDesc><table><row><cell>Shared</cell><cell></cell></row><row><cell>optimizer</cell><cell>Adam [59]</cell></row><row><cell>discount (γ)</cell><cell>0.99</cell></row><row><cell>value function coefficient</cell><cell>0.5</cell></row><row><cell>entropy coefficient</cell><cell>0.0</cell></row><row><cell>number of hidden layers (all networks)</cell><cell>2</cell></row><row><cell cols="2">shared network between actor and critic False</cell></row><row><cell>max gradient norm</cell><cell>0.5</cell></row><row><cell>learning rate schedule</cell><cell>constant</cell></row><row><cell>advantage normalization</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>PPO Environment Specific ParametersEnvironmentLearning rate schedule Clip range schedule initial log σ</figDesc><table><row><cell>gSDE</cell><cell></cell><cell></cell><cell></cell></row><row><cell>AntBulletEnv-v0</cell><cell>default</cell><cell>default</cell><cell>-1</cell></row><row><cell>HopperBulletEnv-v0</cell><cell>default</cell><cell>linear</cell><cell>-1</cell></row><row><cell>Walker2dBulletEnv-v0</cell><cell>default</cell><cell>linear</cell><cell>default</cell></row><row><cell>Unstructured Exploration</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Walker2dBulletEnv-v0</cell><cell>linear</cell><cell>default</cell><cell>default</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">See issue on the original SAC repository https://frama.link/original-sac-mountaincar 2 See report https://frama.link/MountainCarSDEHyperparametersReport 2</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://frama.link/PyBullet-harder-than-MuJoCo-envs<ref type="bibr" target="#b3">4</ref> The code is available at https://github.com/DLR-RM/stable-baselines3/tree/sde</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The work described in this paper was partially funded by the project "Reduced Complexity Models" from the "Helmholtz-Gemeinschaft Deutscher Forschungszentren".</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">State-dependent exploration for policy gradient methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rückstieß</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="234" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Shakey the robot</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Nilsson</surname></persName>
		</author>
		<idno>323</idno>
		<ptr target="http://www.ai.sri.com/shakey/" />
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Center, SRI International</title>
		<imprint>
			<date type="published" when="1984" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Benchmarking deep reinforcement learning for continuous control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1329" to="1338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deepmimic: Example-guided deep reinforcement learning of physics-based character skills</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van De Panne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">143</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learning dexterous in-hand manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chociej</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pachocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Petron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.00177</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Addressing function approximation error in actorcritic methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hoof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.09477</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning agile and dynamic motor skills for legged robots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hwangbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bellicoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tsounis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08652</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hartikainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.05905</idno>
		<title level="m">Soft actor-critic algorithms and applications</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to drive in a day</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hawke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Janz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mazur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V.-D</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8248" to="8254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The ingredients of real world robotic reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hartikainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJe2syrtvS" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Policy search for motor primitives in robotics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kober</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="849" to="856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Exploring parameter space in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rückstiess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sehnke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Paladyn, Journal of Behavioral Robotics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="14" to="24" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Robot skill learning: From reinforcement learning to evolution strategies. Paladyn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stulp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sigaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Behavioral Robotics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="61" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A survey on policy search for robotics. Foundations and Trends R in Robotics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Deisenroth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning to drive smoothly in minutes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sokolkov</surname></persName>
		</author>
		<ptr target="https://github.com/araffin/learning-to-drive-in-5-minutes/" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Autoregressive policies for continuous control deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Korenkevych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.11524</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01290</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pettersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01540</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Openai gym. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Colas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sigaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-Y</forename><surname>Oudeyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05054</idno>
		<title level="m">Gep-pg: Decoupling exploration and exploitation in deep reinforcement learning algorithms</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning to walk via deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.11103</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning to walk in the real world with minimal human effort</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08550</idno>
		<imprint>
			<biblScope unit="volume">02</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neunert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdolmaleki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wulfmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lampe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Buchli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.00449</idno>
		<title level="m">Continuous-discrete reinforcement learning for hybrid control in robotics</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sidor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Asfour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrychowicz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.01905</idno>
		<title level="m">Parameter space noise for exploration</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Cem-rl: Combining evolutionary and gradient-based methods for policy search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pourchot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sigaud</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.01222</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Pybullet, a python module for physics simulation for games, robotics and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Coumans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<ptr target="http://pybullet.org" />
		<imprint>
			<biblScope unit="page" from="2016" to="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Trust region policy optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1889" to="1897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.02971</idno>
		<title level="m">Continuous control with deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Asynchronous methods for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1928" to="1937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<title level="m">Proximal policy optimization algorithms</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Reinforcement learning with deep energybased policies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1352" to="1361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Evolution-guided policy gradient in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khadka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tumer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1188" to="1200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Generalized exploration in policy search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hoof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tanneberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="1705" to="1724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ernestus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gleave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanervisto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dormann</surname></persName>
		</author>
		<ptr target="https://github.com/DLR-RM/stable-baselines3" />
		<title level="m">Stable baselines3</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ernestus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gleave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanervisto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Traore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sidor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<ptr target="https://github.com/hill-a/stable-baselines" />
		<title level="m">Stable baselines</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Rl baselines zoo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raffin</surname></persName>
		</author>
		<ptr target="https://github.com/araffin/rl-baselines-zoo" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A structurally flexible humanoid spine based on a tendon-driven elastic continuum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Reinecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Deutschmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fehrenbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4714" to="4721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Position control of an underactuated continuum mechanism using a reduced nonlinear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Deutschmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dietrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE 56th Annual Conference on Decision and Control (CDC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5223" to="5230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Six-dof pose estimation for a tendon-driven continuum mechanism without a deformation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Deutschmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chalon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Reinecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3425" to="3432" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unifying count-based exploration and intrinsic motivation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Saxton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1471" to="1479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep exploration via bootstrapped dqn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4026" to="4034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Noisy networks for exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Legg</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rywHCPkAW" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Randomized prior functions for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aslanides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cassirer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8617" to="8629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Parameterexploring policy gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sehnke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Osendorfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rückstieß</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="551" to="559" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Simple random search provides a competitive approach to reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07055</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Policy search in continuous action domains: an overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sigaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stulp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Reinforcement learning of motor skills with policy gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="682" to="697" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sidor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03864</idno>
		<title level="m">Evolution strategies as a scalable alternative to reinforcement learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Conti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.06567</idno>
		<title level="m">Deep neuroevolution: Genetic algorithms are a competitive alternative for training deep neural networks for reinforcement learning</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">High-dimensional continuous control using generalized advantage estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02438</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deterministic policy gradient algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Degris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
	<note>ICML14, page I387I395. JMLR.org</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5602</idno>
		<title level="m">Playing atari with deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Implementation matters in deep {rl}: A case study on {ppo} and {trpo}</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Janoos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1etN1rtPB" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Optuna: A next-generation hyperparameter optimization framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Akiba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yanase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 25rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tavakoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Levdik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kormushev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.00378</idno>
		<title level="m">Time limits in reinforcement learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
