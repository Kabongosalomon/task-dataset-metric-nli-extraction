<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Few-Shot Adaptive Gaze Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonwook</forename><surname>Park</surname></persName>
							<email>spark@inf.ethz.ch</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shalini</forename><surname>De Mello</surname></persName>
							<email>shalinig@nvidia.com</email>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
							<email>pmolchanov@nvidia.com</email>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
							<email>uiqbal@nvidia.com</email>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otmar</forename><surname>Hilliges</surname></persName>
							<email>otmarh@inf.ethz.ch</email>
							<affiliation key="aff1">
								<orgName type="institution">ETH Zürich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
							<email>jkautz@nvidia.com</email>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Few-Shot Adaptive Gaze Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Inter-personal anatomical differences limit the accuracy of person-independent gaze estimation networks. Yet there is a need to lower gaze errors further to enable applications requiring higher quality. Further gains can be achieved by personalizing gaze networks, ideally with few calibration samples. However, over-parameterized neural networks are not amenable to learning from few examples as they can quickly over-fit. We embrace these challenges and propose a novel framework for Few-shot Adaptive GaZE Estimation (FAZE) for learning person-specific gaze networks with very few (≤ 9) calibration samples. FAZE learns a rotationaware latent representation of gaze via a disentangling encoder-decoder architecture along with a highly adaptable gaze estimator trained using meta-learning. It is capable of adapting to any new person to yield significant performance gains with as few as 3 samples, yielding state-of-theart performance of 3.18 • on GazeCapture, a 19% improvement over prior art. We open-source our code at https: //github.com/NVlabs/few_shot_gaze 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Estimation of human gaze has numerous applications in human-computer interaction <ref type="bibr" target="#b7">[8]</ref>, virtual reality <ref type="bibr" target="#b33">[34]</ref>, automotive <ref type="bibr" target="#b46">[47]</ref> and content creation <ref type="bibr" target="#b51">[52]</ref>, among others. Many of these applications require high levels of accuracy (cf. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b1">2]</ref>). While appearance-based gaze estimation techniques that use Convolutional Neural Networks (CNN) have significantly surpassed classical methods <ref type="bibr" target="#b56">[57]</ref> for inthe-wild settings, there still remains a significant gap towards applicability in high-accuracy domains. The currently lowest reported person-independent error of 4.3 • <ref type="bibr" target="#b6">[7]</ref> is equivalent to 4.7cm at a distance of 60cm, which restricts use of such techniques to public display interactions <ref type="bibr" target="#b59">[60]</ref> or crowd-sourced attention analysis <ref type="bibr" target="#b29">[30]</ref>.</p><p>High-accuracy gaze estimation from images is difficult because it requires either explicit or implicit fitting of a * The first two authors contributed equally. <ref type="bibr" target="#b0">1</ref> This includes a real-time demo which takes &lt; 10 seconds to record 9 calibration points for a new user and ∼ 1 minute to train a personalized network on a laptop with an NVIDIA GTX GeForce 1060 GPU.  <ref type="figure">Figure 1</ref>: Overview of the FAZE framework. Given a set of training images with ground-truth gaze direction information, we first learn a latent feature representation, which is tailored specifically for the task of gaze estimation. Given the features, we then learn an adaptable gaze estimation network, AdaGEN, using meta-learning which can be adapted easily to a robust person-specific gaze estimation network (PS-GEN) with very little calibration data.</p><p>person-specific eye-ball model to the image data and the estimation of their visual and optical axes. Moreover, it is well understood that inter-subject anatomical differences affect gaze estimation accuracy <ref type="bibr" target="#b10">[11]</ref>. Classical model-based techniques can often be personalized via few (9 or less) samples (e.g., <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13]</ref>) but are not robust to image variations in uncontrolled settings. While feasible, subject-specific training of CNNs requires thousands of samples and is clearly impractical <ref type="bibr" target="#b58">[59]</ref>. Few-shot personalization of CNNs is difficult because training of highly overparametrized models with only few training samples will lead to over-fitting.</p><p>We tackle these many-fold challenges by proposing FAZE, a framework for learning gaze estimation networks for new subjects using very few calibration samples ( <ref type="figure">Fig. 1</ref>). It consists of: i) learning a rotation-aware latent represen-tation of gaze via a disentangling transforming encoderdecoder architecture ii) with these features learning a highly adaptable gaze estimator using meta-learning, and iii) adapting it to any new person to yield significant performance gains with as few as 3 samples.</p><p>In order to learn a robust representation for gaze, we take inspiration from recent work on transforming encoderdecoder architectures <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b52">53]</ref> and design a rotationequivariant pair of encoder-decoder functions. We disentangle the factors of appearance, gaze and head pose in the latent space and enforce equivariance by decoding explicitly rotated latent codes to images of the same person but with a different gaze direction compared to the input (via a 1 reconstruction loss). The equivariance property of our gaze representation further allows us to devise a novel embedding consistency loss term that further minimizes the intra-person differences in the gaze representation. We then leverage the proposed latent embedding to learn personspecific gaze estimators from few samples. To this end we use a meta-learning algorithm to learn how to learn such estimators. We take inspiration from the recent success of meta-learning <ref type="bibr" target="#b0">[1]</ref> for few-shot learning in several other vision tasks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b30">31]</ref>. To the best of our knowledge, we are the first to cast few-shot learning of person-specific gaze estimators as one of multi-task learning where each subject is seen as a new task in the meta-learning sense.</p><p>We evaluate the proposed framework on two benchmark datasets and show that our meta-learned network with its latent gaze features can be successfully adapted with very few (k ≤ 9) samples to produce accurate person-specific models. We demonstrate the validity of our design choices with detailed empirical evidence, and demonstrate that our proposed framework outperforms state-of-the-art methods by significant margins. In particular, we demonstrate improvements of 13% (3.94 • → 3.42 • ) on the MPIIGaze dataset, and 19% (3.91 • → 3.18 • ) on the GazeCapture dataset over the approach of <ref type="bibr" target="#b25">[26]</ref> using just 3 calibration samples.</p><p>To summarize, the main contributions of our work are:</p><p>• FAZE, a novel framework for learning person-specific gaze networks with few calibration samples, fusing the benefits of rotation-equivariant feature learning and meta-learning. • A novel encoder-decoder architecture that disentangles gaze direction, head pose and appearance factors. • A novel and effective application of meta-learning to the task of few-shot personalization. • State-of-the-art performance (3.14 • with k = 9 on MPIIGaze), with consistent improvements over existing methods for 1 ≤ k ≤ 256.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Gaze Estimation. Appearance-based gaze estimation <ref type="bibr" target="#b45">[46]</ref> methods that map images directly to gaze have recently surpassed classical model-based approaches <ref type="bibr" target="#b12">[13]</ref> for in-thewild settings. Earlier approaches in this direction assume images captured in restricted laboratory settings and use direct regression methods <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b26">27]</ref> or learning-by-synthesis approaches combined with random forests to separate headpose clusters <ref type="bibr" target="#b44">[45]</ref>. More recently, the availability of large scale datasets such as MPIIGaze <ref type="bibr" target="#b56">[57]</ref> and GazeCapture <ref type="bibr" target="#b21">[22]</ref>, and progress in CNNs have rapidly moved the field forward. MPIIGaze has become a benchmark dataset for in-the-wild gaze estimation. For the competitive personindependent within-MPIIGaze leave-one-person-out evaluation, gaze errors have progressively decreased from 6.3 • for naively applying a LeNet-5 architecture to eye-input <ref type="bibr" target="#b56">[57]</ref> to the current best of 4.3 • with an ensemble of multimodal networks based on VGG-16 <ref type="bibr" target="#b6">[7]</ref>. Proposed advancements include the use of more complex CNNs <ref type="bibr" target="#b58">[59]</ref>; more meaningful use of face <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b21">22]</ref> and multi-modal input <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b53">54]</ref>; explicit handling of differences in the two eyes <ref type="bibr" target="#b3">[4]</ref>; greater robustness to head pose <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b35">36]</ref>; improvements in data normalization <ref type="bibr" target="#b54">[55]</ref>; learning more informed intermediate representations <ref type="bibr" target="#b31">[32]</ref>; using ensembles of networks <ref type="bibr" target="#b6">[7]</ref>; and using synthetic data <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36]</ref>. However, person-independent gaze errors are still insufficient for many applications <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b1">2]</ref>. While significant gains can be obtained by training person-specific models, it requires many thousands of training images per subject <ref type="bibr" target="#b58">[59]</ref>. On the other hand, CNNs are prone to over-fitting if trained with very few (k ≤ 9) samples. In order to address this issue, existing approaches try to adapt personindependent CNN-based features <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b32">33]</ref> or points-ofregard (PoR) <ref type="bibr" target="#b55">[56]</ref> to person-specific ones via hand-designed heuristic functions. Some methods also train a Siamese network with pairs of images of the same subject <ref type="bibr" target="#b25">[26]</ref>.</p><p>Learned Equivariance. Generalizing models learned for regression tasks to new data is a challenging problem. However, recent works show improvements from enforcing the learning of equivariant mappings between input, latent features, and label spaces <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b38">39]</ref>, via so-called transforming encoder-decoder architectures <ref type="bibr" target="#b14">[15]</ref>. In <ref type="bibr" target="#b52">[53]</ref>, this idea is expanded to learn complex phenomena such as the orientation of synthetic light sources and in <ref type="bibr" target="#b38">[39]</ref> the method is applied to real-world multi-view imagery to improve semisupervised human pose estimation. In contrast, we learn from very noisy real-world data while successfully disentangling the two noisily-labeled phenomena of gaze direction and head orientation.</p><p>Few-shot Learning. Few-shot learning aims to learn a new task with very few examples <ref type="bibr" target="#b22">[23]</ref>. This is a non-trivial prob-lem for highly over-parameterized deep networks as it leads to over-fitting. Recently, several promising meta-learning <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b36">37]</ref> techniques have been proposed that learn how to learn unique but similar tasks in a fewshot manner using CNNs. They have been shown to be successful for various few-shot visual learning tasks including object recognition <ref type="bibr" target="#b5">[6]</ref>, segmentation <ref type="bibr" target="#b34">[35]</ref>, viewpoint estimation <ref type="bibr" target="#b47">[48]</ref> and online adaptation of trackers <ref type="bibr" target="#b30">[31]</ref>. Inspired by their success, we use meta-learning to learn how to learn person-specific gaze networks from few examples. To the best of our knowledge we are the first to cast person-specific gaze estimation as a multi-task problem in the context of meta-learning, where each subject is seen as a new task for the meta-learner. Our insight is that meta-learning lends itself well to few-shot gaze personalization and leads to performance improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we describe how we perform gaze estimation from challenging in-the-wild imagery, with minimal burden to the user. The latter objective can be fulfilled by designing our framework to adapt well even with very few calibration samples (k ≤ 9). We first provide an overview of the FAZE framework and its three stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The FAZE framework</head><p>We design FAZE ( <ref type="figure">Fig. 1</ref>) with the understanding that a person-specific gaze estimator must encode factors particular to the person, yet at the same time, leverage insights from observing the eye-region appearance variations across a large number of people with different head pose and gaze direction configurations. The latter is important for building models that are robust to extraneous factors such as poor image quality. Thus, the first step in FAZE is to learn a generalizable latent embedding space that encodes information pertaining to the gaze-direction, including person-specific aspects. We detail this in Sec. 3.2.</p><p>Provided that good and consistent features can be learned, we can leverage meta-learning to learn how to learn few-shot person-specific gaze estimators for these features. This results in few-shot learners which generalize better to new people (tasks) without overfitting. Specifically, we use the MAML meta-learning algorithm <ref type="bibr" target="#b5">[6]</ref>. For our task, MAML learns a set of initial network weights which allow for fine-tuning without the usual over-fitting issues that occur with low k. Effectively, it produces a highly Adaptable Gaze Estimation Network (AdaGEN). The final step concerns the adaptation of MAML-initialized weights to produce person-specific models (PS-GEN) for each user. We describe this in Sec. 3.3.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Gaze-Equivariant Feature Learning</head><p>In this section, we explain how the learning of a function, which understands equivalent rotations in input data and output label can lead to better generalization in the context of our final task of person-specific gaze estimation. In addition, we: (a) show how to disentangle eyeball and head rotation factors leading to better distillation of gaze information, and (b) introduce a frontalized embedding consistency loss term to specifically aid in learning consistent frontal gaze codes for a particular subject.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Architecture Overview</head><p>In learning a generalizable latent embedding space representing gaze, we apply the understanding that a relative change in gaze direction is easier to learn in a personindependent manner <ref type="bibr" target="#b25">[26]</ref>. We extend the transforming encoder-decoder architecture <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b52">53]</ref> to consider three distinct factors apparent in our problem setting: gaze direction, head orientation, and other factors related to the appearance of the eye region in given images ( <ref type="figure" target="#fig_1">Fig. 2</ref>). We disentangle the three factors by explicitly applying separate and known differences in rotations (eye gaze and head orientation) to the respective sub-codes. We refer to this architecture as the Disentangling Transforming Encoder-Decoder (DT-ED).</p><p>For a given input image x , we define an encoder E : x → z and a decoder D : z →x such that D (E(x)) =x. We consider the latent space embedding z as being formed of 3 parts representing: appearance (z a ), gaze direction or eyeball rotation (z g ), and head pose (z h ), which can be expressed as: z = z a ; z g ; z h where gaze and head codes are flattened to yield a single column. We define z g as having dimensions (3 × F g ) and z h as having dimensions 3 × F h with F ∈ N. With these dimensions, it is possible to apply a rotation matrix to explicitly rotate these 3D latent space embeddings using rotation matrices.</p><p>(a) Only varying gaze direction, <ref type="figure">Figure 3</ref>: Our disentangled rotation-aware embedding spaces for gaze direction and head pose are demonstrated by predicting embeddingsẑ g ,ẑ h from a given sample, rotating it to 15 points each, and then decoding them.</p><formula xml:id="formula_0">(θ g , φ g ) ∈ [−25 • , 25 • ] (b) Only varying head orientation, θ h , φ h ∈ [−20 • , 20 • ]</formula><p>The frontal orientation of eyes and heads in our setting can be represented as (0, 0) in Euler angles notation for azimuth and elevation, respectively assuming no roll, and using the x − y convention. Then, the rotation of the eyes and the head from the frontal orientation can be described using (θ g , φ g ) and θ h , φ h in Euler angles and converted to rotation matrices defined as,</p><formula xml:id="formula_1">R (θ, φ) =   cos φ 0 sin φ 0 1 0 − sin φ 0 cos φ     1 0 0 0 cos θ − sin θ 0 sin θ cos θ   .</formula><p>(1) While training DT-ED, we input a pair of images of a person x a and x b . We can calculate R g ba = R g b (R g a ) −1 to describe the change in gaze direction in going from sample a to sample b of the same person. Likewise for head rotation,</p><formula xml:id="formula_2">R h ba = R h b R h a −1 .</formula><p>This can be done using the ground-truth labels for gaze (g a and g b ) and head pose (h a and h b ) for the pair of input samples. The rotation of the latent code z g a can then be expressed via the operation z g b = R g ab z g a . At training time, we enforce this code to be equivalent to the one extracted from image x b , via a reconstruction loss (Eq. 3). We assume the rotated codesẑ h b and z g b , along with the appearance-code z a a , to be sufficient for reconstructing x b through the decoder function such that,</p><formula xml:id="formula_3">D (ẑ b ) = x b .</formula><p>More specifically, given the encoder output E (x a ) = z a = z a a ; z g a ; z h a , we assume the rotated version of x a to match the embedding of x b , that is we assume <ref type="figure" target="#fig_1">Fig. 2</ref>). This approach indeed applies successfully to noisy realworld imagery, as shown in <ref type="figure">Fig. 3</ref> where we map a sample into the gaze and head pose latent spaces, rotate to the frontal orientation, and then again rotate by a pre-defined set of 15 yaw and pitch values and reconstruct the image via the decoder. We can see that the factors of gaze direction and head pose are fully disentangled and DT-ED succeeds in the challenging task of eye-region frontalization and redirection from monocular RGB input.</p><formula xml:id="formula_4">ẑ a b ;ẑ g b ;ẑ h b = z a a ; R g ba z g a ; R h ba z h a (See</formula><p>We train the FAZE transforming encoder-decoder archi-tecture using a multi-objective loss function defined as,</p><formula xml:id="formula_5">L full = λ recon L recon + λ EC L EC + λ gaze L gaze , (2)</formula><p>where we empirically set λ recon = 1, λ EC = 2, and λ gaze = 0.1. The individual loss terms are explained in the following sub-sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Reconstruction Loss</head><p>To guide learning of the encoding-decoding process, we apply a simple 1 reconstruction loss. Given an input image x b and reconstructedx b obtained by decoding the rotated embeddingsẑ b of image x a , the loss term is defined as,</p><formula xml:id="formula_6">L recon (x b ,x b ) = 1 |x b | u∈x b ,û∈x b |û − u| ,<label>(3)</label></formula><p>where u andû are pixels of images x b andx b respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Embedding consistency Loss</head><p>We introduce a novel embedding consistency term, which ensures that the encoder network always embeds images of a person with different appearance but identical gaze direction to similar features. Usually this would require paired images with only gaze directions changed. However, it is intractable to collect such data in the real world, so we instead exploit the learned equivariance of DT-ED. Before measuring the consistency between latent gaze features from different samples, we first frontalize them by applying the inverse of the rotation matrix R g a using ground-truth gaze direction g a . It should be noted that naively enforcing all gaze features to be similar across persons may disregard the inter-subject anatomical differences which should result in different latent embeddings. Hence, we apply the embedding consistency to intra-subject pairs of images only. We validate this choice through experiments in Sec. B.1.</p><p>Given a batch of B image samples during training, we formally compute the embedding consistency loss using,</p><formula xml:id="formula_7">L EC = 1 B B i=1 max j=1...B id(i)=id(j) d f (z g i ), f (z g j ) ,<label>(4)</label></formula><p>where f (z g ) = (R g ) −1 z g corresponds to frontalized latent gaze features. The function id(i) provides the personidentity of the i-th sample in the batch, and d is a function based on mean column-wise angular distance (between 3D vectors). The max function minimizes differences between intra-person features that are furthest apart, and is similar to the idea of batch-hard online triplet mining <ref type="bibr" target="#b40">[41]</ref>.</p><p>During training, we linearly increase λ EC from 0 until sufficient mini-batches to cover 10 6 images have been processed, to allow for more natural embeddings to arise before applying consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Gaze Direction Loss</head><p>Lastly, we add the additional objective of gaze estimation via G : z g →ĝ, parameterized by a simple multi-layer perceptron (MLP). The gaze direction loss is calculated using,</p><formula xml:id="formula_8">L gaze (ĝ, g) = arccos ĝ · g ĝ g .<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Person-specific Gaze Estimation</head><p>Having learned a robust feature extractor, which is tailored specifically for gaze estimation, our final goal is to learn a personalized gaze estimator with as few calibration samples as possible. A straightforward solution for doing this is to train a person-independent model with the training data used to train DT-ED and simply fine-tune it with the available calibration samples for the given subject. However, in practical setups where only a few calibration samples are available, this approach can quickly lead to overfitting (see experiments in <ref type="figure" target="#fig_8">Fig. 7a</ref>). In order to alleviate this problem, we propose to use the meta-learning method MAML <ref type="bibr" target="#b5">[6]</ref>, which learns a highly adaptable gaze network (AdaGEN).</p><p>Adaptable Gaze Estimator (AdaGEN) Training. We wish to learn weights θ * for the AdaGEN gaze prediction model M such that it becomes a successful few-shot learner. In other words, when M θ * is fine-tuned with only a few "calibration" examples of a new person P not present in the training set, it can generalize well to unseen "validation" examples of the same person. We achieve this by training it with the MAML meta learning algorithm adapted for few-shot learning.</p><p>In conventional CNN training the objective is to minimize the training loss for all the examples of all training subjects. In contrast, for few-shot learning, MAML explicitly minimizes the generalization loss of a network after minimizing its training loss for a few examples of a particular subject via a standard optimization algorithm, e.g., stochastic gradient descent (SGD). Additionally, MAML repeats this procedure for all subjects in the training set and hence learns from several closely related "tasks" (subjects) to become a successful few shot learner for any new unseen task (subject). We identify that personspecific factors may have few parameters, with only slight but important variations across people such that all people constitute a set of closely related tasks. Our insight is that meta-learning lends itself well to such a problem of personalization.</p><p>The overall procedure of meta-learning to learn the optimal θ * is as follows. We divide the entire set of persons S into meta-training (S train ) and meta-testing (S test ) subsets of non-overlapping subjects. During each meta-training iteration n, we randomly select one person The first step in the meta-learning procedure is to compute the loss for the few-shot calibration set D train c and update the weights θ n at step n via one (or more) gradient steps and a learning rate α as,</p><formula xml:id="formula_9">θ n = f (θ n ) = θ n − α∇L c P train (θ n ).<label>(6)</label></formula><p>With the updated weights θ n , we then compute the loss for the validation set D train v of the subject P train as L v P train (θ n ) = L v P train (f (θ n )) and its gradients w.r.t the initial weights of the network θ n at that training iteration n. Lastly, we update θ n with a learning rate of η to explicitly minimize the validation loss as,</p><formula xml:id="formula_10">θ n+1 = θ n − η∇L v P train (f (θ n )).<label>(7)</label></formula><p>We repeat these training iterations until convergence to get the optimal weights θ * .</p><p>Final Person-specific Adaptation. Having learned our encoder and our optimal few-shot person-specific learner M θ * , we are now well poised to produce person-specific models for each new person P test from S test . We fine-tune M θ * with the k calibration images D test c to create a personalized model for P test as</p><formula xml:id="formula_11">θ P test = θ * − α∇L c P test (θ * ),<label>(8)</label></formula><p>and test the performance of the personalized model</p><formula xml:id="formula_12">M θ P test on person P test 's validation set D test v .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data pre-processing</head><p>Our data pre-processing pipeline is based on <ref type="bibr" target="#b54">[55]</ref>, a revision of the data normalization procedure introduced in <ref type="bibr" target="#b44">[45]</ref>. In a nutshell, the data normalization procedure ensures that a common virtual camera points at the same reference point in space with the head upright. This requires the rotation, tilt, and forward translation of the virtual camera. Please refer to <ref type="bibr" target="#b54">[55]</ref> for a formal and complete description, and our supplementary materials for a detailed list of changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Neural Network Configurations</head><p>DT-ED. The functions E and D in our transforming encoder-decoder architecture can be implemented with any CNN architecture. We select the DenseNet architecture <ref type="bibr" target="#b17">[18]</ref> both for our DT-ED as well as for our re-implementation of state-of-the-art person-specific gaze estimation methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b55">56]</ref>. The latent codes z a , z g , and z h are defined to have dimensions (64), (3 × 2), and (3 × 16) respectively. Please refer to supplementary materials for further details.</p><p>Gaze MLP. Our gaze estimation function G is parameterized by a multi-layer perceptron with 64 hidden layer neurons and SELU <ref type="bibr" target="#b20">[21]</ref> activation. The MLP outputs 3dimensional unit gaze direction vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Training</head><p>DT-ED. Following <ref type="bibr" target="#b8">[9]</ref>, we use a batch size of 1536 and apply linear learning rate scaling and ramp-up for the first 10 6 training samples. We use NVIDIA's Apex library 2 for mixed-precision training. and train our model for 50 epochs with a base learning rate of 5 × 10 −5 , l 2 weight regularization of 10 −4 , and use instance normalization <ref type="bibr" target="#b48">[49]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Datasets</head><p>GazeCapture <ref type="bibr" target="#b21">[22]</ref> is the largest available in-the-wild gaze dataset. We mined camera intrinsic parameters from the web for the devices used, and applied our pre-processing pipeline (Sec. A.1) to yield input images. For training the DT-ED as well as for meta-learning, we use data from 993 people in the training set specified in <ref type="bibr" target="#b21">[22]</ref>, each with 1766 samples, on average, for a total of 1.7M samples. To ensure within-subject diversity of sampled image-pairs at training time, we only select subjects with ≥ 400 samples. For computing our final evaluation metric, we use the last 500 entries from 109 subjects that have at least 1000 samples each. We select the k-shot samples for meta-training and fine-tuning randomly from the remaining samples.</p><p>MPIIGaze <ref type="bibr" target="#b56">[57]</ref> is the most established benchmark dataset for in-the-wild gaze estimation. In comparison to GazeCapture it has higher within-person variations in appearance including illumination, make-up, and facial hair changes, potentially making it more challenging. We use the images 2 https://github.com/NVIDIA/apex specified in the MPIIFaceGaze subset <ref type="bibr" target="#b57">[58]</ref> only for evaluation purposes. The MPIIFaceGaze subset consists of 15 subjects each with 2500 samples on average. We reserve the last 500 images of each subject for final evaluations as is done in <ref type="bibr" target="#b58">[59]</ref> and select k calibration samples for personalization by sampling randomly from the remaining samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>For all methods, we report person-specific gaze estimation errors for a range of k calibration samples. For each data point, we perform the evaluation 10 times using k randomly chosen calibration samples. Each evaluation or trial yields a mean gaze estimation error in degrees over all subjects in the test set. The mean error over all such trials is plotted, with its standard deviation represented by the shaded areas above and below the curves. The values at k = 0 are determined via G (z g ). We train this MLP on the GazeCapture training subset, without any person-specific adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Ablation Study</head><p>We evaluate our method under different settings to better understand the impact of our various design choices. For all experiments, we train the models using the GazeCapture dataset's training set and test on the MPIIGaze dataset. This challenging experiment allows us to demonstrate the generalization capability of our approach across different datasets. The ablation studies are summarized in <ref type="figure" target="#fig_8">Fig. 7</ref>. We provide additional plots of the results of this ablation study on the test partition of the GazeCapture dataset in the supplementary material.</p><p>MAML vs. Finetuning. In <ref type="figure" target="#fig_8">Fig. 7a</ref>, we first evaluate the impact of meta-learning a few-shot person-adaptive gaze estimator using MAML (Sec. 3.3) and compare its performance with naive finetuning. When no person-specific adaptation is performed (i.e., k = 0), the person-independent baseline model G (z g ) with the features learned using a vanilla autoencoder (AE) results in a mean test error of 7.17 • . Using MAML for person-specific adaptation with only one calibration sample decreases the error to 6.61 • . The error reduces further as we increase k and reaches a mean value of 5.38 • for k = 32. In contrast, naively finetuning (AE-Finetuning) the gaze estimator results in severe over-fitting and yields very high test errors, in particular, for very low k values. In fact, for k ≤ 3, the error increases to above the person-independent baseline model. Since the model initialized with weights learned by MAML clearly outperforms vanilla finetuning, in the rest of this section, we always use MAML unless specified otherwise.</p><p>Impact of feature representation. <ref type="figure" target="#fig_8">Fig. 7a</ref> also evaluates the impact of the features used to learn the gaze estimation model. Our proposed latent gaze features (Sec. 3.2) signif-  <ref type="formula" target="#formula_7">4)</ref>). We provide additional results for the test partition of the GazeCapture dataset in the supplementary material.</p><p>icantly decrease the error, e.g., 4.87 • vs. 5.62 • with k = 9 for DT-ED (MAML) and AE (MAML), respectively. Note that the gain remains consistent across all values of k. The only difference between DT-ED and AE is that the latent codes are rotated in DT-ED before decoding. Despite this more difficult task, the learned code clearly better informs the final task of person-specific gaze estimation, showing that disentangling gaze, head pose, and appearance is importance for gaze estimation.</p><p>Contribution of loss terms. We evaluate the impact of each loss term described in Eq. (2) (Sec. 3.2) by incorporating them one at a time into the total loss used to train DT-ED. <ref type="figure" target="#fig_8">Fig. 7b</ref> summarizes the results. Using only the image reconstruction loss L recon in Eq. (3), the learned latent gaze features result in an error of 4.87 • at k = 9. Incorporating gaze supervision L gaze in Eq. (5) to obtain features that are more informed of the ultimate task of gaze-estimation gives an improvement of 26% from 4.87 • to 3.60 • . Adding the person-specific embedding consistency term L EC in Eq. (4) to L recon also reduces the error significantly from 4.87 • to 3.32 • at k = 9 (an improvement of over 30%). Finally, combining all loss terms improves the performance even further to 3.14 • (in total, an improvement of 36%).</p><p>Analysis of embedding consistency. In order to validate our choice of the embedding consistency loss, in <ref type="figure" target="#fig_8">Fig. 7c</ref>, we compare its performance with two other possible variants.</p><p>As described in Sec. 3.2.3, the embedding consistency loss term minimizes the intra-person differences of the frontalized latent gaze features. The main rationale behind this is that the gaze features for a unique person should be consistent while they can be different across subjects due to inter-subject anatomical differences. We further conjecture that preserving these inter-personal differences as opposed to trying to remove them by learning person-invariant embeddings is indeed important to obtaining high accuracy for gaze estimation. In order to validate this observation, we introduce a person-independent embedding consistency term which also minimizes the inter-person latent gaze feature differences. As is evident from <ref type="figure" target="#fig_8">Fig. 7c</ref>, enforcing person-  independent embedding consistency of the latent gaze features results in increased mean errors. In fact it performs worse than only using the reconstruction loss (L recon ).</p><p>One may argue the complete opposite i.e., the latent gaze features should be hugely different for every subject for the best possible subject-specific accuracy, but we did not find this to be the case. To demonstrate this, we apply a triplet loss (L triplet ) <ref type="bibr" target="#b40">[41]</ref>, which explicitly maximizes the interpersonal differences in gaze features in addition to minimizing the intra-person ones. As is evident from <ref type="figure" target="#fig_8">Fig 7c this</ref> results in a significant increase in the error. This suggests that perhaps factors that quantify the overall appearance of a person's face and define their unique identity may not necessarily be correlated to the anatomical properties that define "person-uniqueness" for the task of gaze estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison with CNN + Meta-Learning</head><p>An alternative baseline to FAZE can be created by replacing the DT-ED with a standard CNN architecture. We take an identically configured DenseNet (to FAZE) and a VGG-16 architecture for the convolutional layers, then add 2 fully-connected layers each with 256 neurons and train the networks with the gaze objective (Eq. 5). The output of the convolutional layers are used as input to a gaze estimation network trained via MAML to yield the results in <ref type="figure" target="#fig_6">Fig. 5</ref>. Having been directly trained on the (cross-person) gaze estimation objective, it is expected that the encoder network would make better use of its model capacity as it does not have to satisfy a reconstruction objective. Thus, we can  <ref type="figure">Figure 6</ref>: Comparison of FAZE against state-of-the-art person-specific gaze estimation methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b55">56]</ref> call these highly competitive baselines. FAZE outperforms these baselines with statistical significance, demonstrating that the DT-ED training and our loss terms yield features which are more amenable to meta-learning, and thus to the final objective of personalized gaze estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparison with State-of-the-Art</head><p>Few-shot personalization of CNN models in the context of gaze estimation for very low k is very challenging. Two recent approaches <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b25">26]</ref> are the most relevant in this direction, and we provide evaluations on highly competitive re-implementations. Our results are presented in <ref type="figure">Fig. 6</ref> for both the test partition of the GazeCapture dataset and the MPIIGaze dataset. Overall, we show statistically significantly better mean errors over the entire range of 1 ≤ k ≤ 256 than all the existing state-of-the-art methods. In addition, our performance between trials is more consistent as shown by the narrower error bands. This indicates robustness to the choice of the k calibration samples.</p><p>Ours vs Polynomial fit to PoR. In <ref type="bibr" target="#b55">[56]</ref>, Zhang et al. fit a 3rd order polynomial function to correct initial pointof-regard (PoR) estimates from a person-independent gaze CNN. To re-implement their method, we train a DenseNet CNN (identical to FAZE) and intersect the predicted gaze ray (defined by gaze origin and direction in 3D with respect to the original camera) with the z = 0 plane to estimate the initial PoR and later refine it with a person-specific 3rd order polynomial function. Though this approach performs respectably with k = 9, yielding 4.19 • on MPIIGaze <ref type="figure">(Fig. 6b</ref>), it suffers with lower k especially on GazeCapture. Nonetheless, its performance converges to our performance at k ≥ 128 showing its effectiveness at higher k despite its apparent simplicity.</p><p>Ours vs Differential Gaze Estimation. Liu et al. <ref type="bibr" target="#b25">[26]</ref> introduce a CNN architecture for learning to estimate the differences in the gaze yaw and pitch values between pairs of images of the same subject. That is, in order to estimate the gaze their network always requires one reference image of a subject with known gaze values. Then given a reference image I a with a known gaze label g a and another image I b with unknown gaze label, their approach outputs a ∆g ba , from which the absolute gaze for I b can be computed asŷ b = y a + ∆g ba . Their original paper states a within-MPIIGaze error with k = 9 at 4.67 • using a simple LeNet-5 style Siamese network and a pair of eye images as input. We use 256 × 64 eye-region images from GazeCapture as input and use a DenseNet-based architecture to make their approach more comparable to ours. Our re-implementation yields 3.53 • for their method at k = 9 on MPIIGaze, a 1.2 • improvement despite dataset differences. We show statistically significant improvements to <ref type="bibr" target="#b25">[26]</ref> across all ranges of k in our MPIIGaze evaluations, with our method only requiring 4 calibration samples to compete with their best performance at k = 256 (see the red and green curves in <ref type="figure">Fig. 6</ref>). The improvement from our final approach is further emphasized in <ref type="figure">Fig. 6a</ref> with evaluations on the test subset of Gaze-Capture. At k = 4, we yield a performance improvement of 20.5% or 0.8 • over <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper we presented the first practical approach to deep-learning based high-accuracy personalized gaze estimation requiring only few calibration samples. Our FAZE framework consists of a disentangling encode-decoder network that learns a compact person-specific latent representation of gaze, head pose and appearance. Furthermore, we show that these latent embeddings can be used in a metalearning context to learn a person-specific gaze estimation network from very few (as low as k = 3) calibration points. We experimentally showed that our approach outperforms other state-of-the-art approaches by significant margins and produces the currently lowest reported personalized gaze errors on both the GazeCapture and MPIIGaze datasets.</p><p>(with α = 0.01) throughout the network as they proved to improve performance for all architectures.</p><p>To project CNN features back from latent features z, we apply a fully-connected layer to output values equivalent to 32 feature maps of width 8 and height 2. The DenseNet decoder that we use to model D is identical in construction to a usual DenseNet but uses deconvolutional layers (with stride 1) in the place of normal convolutions, and 3 × 3 deconvolutions (with stride 2) instead of average pooling at the transition layers. To be faithful to the original implementation, we do not apply bias layers to convolutions in our DenseNet-based DT-ED. We initialize all layers' weights with MSRA initialization <ref type="bibr" target="#b13">[14]</ref>, while biases of the fullyconnected layers are initialized with zeros.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Results</head><p>We provide additional results of the ablation study on the test partition of the GazeCapture dataset and evaluate the within-dataset performance of FAZE on the MPIIGaze dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Ablation Study on GazeCapture</head><p>In the main paper, we provide the results of the ablation study on the MPIIGaze dataset ( <ref type="figure" target="#fig_4">Fig. 4</ref> in the main paper). Our evaluation setting is a cross-dataset evaluation, where we train on the training partition of the GazeCapture dataset <ref type="bibr" target="#b21">[22]</ref> and test on the test partition of the same dataset as well as on MPIIGaze <ref type="bibr" target="#b56">[57]</ref>. Here we show additional results for the GazeCapture test partition <ref type="figure" target="#fig_8">(Fig. 7)</ref>.</p><p>In <ref type="figure" target="#fig_8">Fig. 7a</ref> we observe the same trends for the Gaze-Capture test dataset that we observed for MPIIGaze. Our proposed DT-ED architecture learns latent representations that are better suited for gaze estimation than those learned by a naive encoder-decoder architecture. Additionally, for few-shot personalization significant gains in accuracy are obtained with meta-learning an adaptable network, as we propose, versus naively fine-tuning a network designed for person-independent gaze estimation (Fine-tuning versus MAML). The latter approach also leads to over-fits at very low k. <ref type="figure" target="#fig_8">Fig. 7b</ref> shows the value of our proposed loss terms of embedding consistency and of computing gaze from the latent representations while training DT-ED, for GazeCapture. Finally, <ref type="figure" target="#fig_8">Fig. 7c</ref> shows the consistent improvements obtained for the GazeCapture dataset by preserving interperson differences versus not doing so.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Within-MPIIGaze Performance</head><p>So far Liu et al. <ref type="bibr" target="#b25">[26]</ref> report the best known accuracy of 4.67 • with 9 calibration samples on MPIIGaze with their differential network architecture. They use the within MPI-IGaze leave-one-subject out evaluation protocol for their experiments. To directly compare against their method, we evaluate the performance of our FAZE framework for this experimental protocol <ref type="figure" target="#fig_9">(Fig. 8</ref>). With 9 calibration samples FAZE obtains a gaze error of 3.88, which is a 17% improvement over Liu et al.'s method. Note, also, that within-MPIIGaze training performs worse than training with Gaze-Capture (see <ref type="figure">Fig. 6</ref> in the main paper). This is expected, given the significantly larger diversity of subjects present in the GazeCapture training subset (993) versus MPIIGaze (14 in a leave-one-out setting), which benefits both DT-ED and MAML. This observation corroborates with similar ones previously made in <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Sensitivity Analysis</head><p>We show the influence of various design parameters on the overall performance of our algorithm. These analyses help to determine the parameters' optimal values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Latent Gaze Code</head><p>Dimension Our latent gaze code has the dimensions of 3 × F g . In order to empirically select the optimal value of F g , we evaluate the performance of FAZE for several different values of F g = {16, 3, 2} shown in <ref type="figure" target="#fig_10">Fig. 9</ref>, while keeping the dimensions of the appearance and head pose codes fixed at 64 and 16 respectively. Empirically we find F g = 2 to be optimal for both datasets and hence select it for our final implementation.</p><p>Normalization In general we find that is beneficial to normalize our 3 × F g -sized latent gaze code to achieve the low- est gaze errors. We experiment with various methods for normalization, which involve computing an 2 norm along a particular dimension and dividing all the observed values for that dimension with the norm. We compute norms along the F g dimension resulting in 3 norms. Alternatively, one can normalize along the 3 dimension, resulting in F g norms. We observe that normalizing along the F g dimension, produces lower gaze errors for GazeCapture and equivalent ones for MPIIGaze, versus the alternate approach ( <ref type="figure">Fig. 10</ref>). Hence, we use it for our final implementation.   <ref type="figure">Figure 10</ref>: Performance of FAZE for normalizing the 3 × F g -dimensional gaze code along the 3 or F g dimensions, respectively.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Disentangling appearance, gaze and head pose variations from an image with our Disentangling Transforming Encoder-Decoder (DT-ED). We learn to translate between pairs of images of the same person by rotating the gaze and head pose codes. The encoder-decoder are supervised by a pixel-wise L 1 loss (Eq. 3), with the gaze embedding additionally supervised via gaze regression (Eq. 5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>P</head><label></label><figDesc>train from S train and create a meta-training sample for the person (via random sampling), defined as P train = {D train c , D train v }, containing a calibration set D train c = {(z g i , g i )|i = 1 . . . k} of k training examples, and a validation set D train v = {(z g j , g j )|j = 1 . . . l} of another l examples for the same person. Here, z g and g refer to the latent gaze representation learned by DT-ED and the ground-truth 3D gaze vector, respectively. Both k and l are typically small (≤ 20) and k represents the "shot" size used in few-shot learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Gaze MLP. During meta-learning, we use α = 10 −5 with SGD (Eq. 6), and η = 10 −3 (Eq. 7) with the Adam optimizer (α and β in [6]), and do 5 updates per inner loop iteration. For sampling D train v we set l = 100. During standard eye-tracker calibration, one cannot assume the knowledge of extra ground-truth beyond the k samples. Thus, we perform the final fine-tuning operation (Eq. 8) for 1000 steps for all values of k and for all people.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>+ Ltriplet DT-ED + LEC (person-independent) DT-ED + LEC (person-specific) Ablation Study: Impact of (a) learning the few-shot gaze estimator using MAML (Sec. 3.3) and using the transforming encoder-decoder for feature learning (Sec. 3.2); (b) different loss terms in Eq. (2) for training the transforming encoder-decoder; and (c) comparison of the different variants of embedding consistency loss term (Eq. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Comparison of FAZE against competitive CNN + MAML baselines, evaluated on MPIIGaze.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>+ Ltriplet DT-ED + LEC (person-independent) DT-ED + LEC (person-specific) Ablation Study on GazeCapture (test): Impact of (a) learning the few-shot gaze estimator using MAML and using the transforming encoder-decoder for feature learning; (b) different loss terms for training the transforming encoder-decoder; and (c) comparison of the different variants of embedding consistency loss term.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Gaze errors of FAZE for within-MPII leave-oneperson out training (blue); and training on GazeCapture's training partition and testing one MPIIGaze (orange).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Performance of FAZE for different dimensions F g of the 3 × F g -dimensional latent gaze code.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/cydonia999/Tiny_Faces_in_Tensorflow 4 https://github.com/jiankangdeng/Face_Detection_Alignment</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. Seonwook Park carried out this work during his internship at Nvidia. This work was supported in part by the ERC Grant OPTINT (StG-2016-717054).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>Due to constraints on the space available in the main paper, we were unable to include all the details there. Here we provide additional implementation details pertaining to our (a) data pre-processing pipeline and (b) the configuration of our DT-ED network. We also show additional results of the ablation study (Section 5.1 in the main paper) on the test partition of the GazeCapture dataset and the performance of FAZE for the within MPIIGaze leave-one-person out setting. Finally, we show the sensitivity of FAZE to various design configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>We describe further details in how we pre-process the datasets used, and the configuration of the DT-ED architecture. A reference implementation of both can be found as open-source software at https://github. com/NVlabs/few_shot_gaze.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Data Pre-processing</head><p>We employ a normalization procedure based on <ref type="bibr" target="#b54">[55]</ref>, which is a revision of <ref type="bibr" target="#b44">[45]</ref>, but with a few small changes. We utilize state-of-the-art open-source implementations for face detection <ref type="bibr" target="#b2">3</ref>  <ref type="bibr" target="#b16">[17]</ref> and facial landmarks detection 4 <ref type="bibr" target="#b4">[5]</ref>, respectively. We use the Surrey Face Model <ref type="bibr" target="#b19">[20]</ref> as the reference 3D face model, and select 4 eye corners and 9 nose landmarks as described by the Multi-PIE 68-points markup <ref type="bibr" target="#b9">[10]</ref> for PnP-based <ref type="bibr" target="#b24">[25]</ref> head pose estimation. This is in contrast to <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b54">55]</ref> which instead use the 4 eye corners and 2 mouth corners. This is motivated by our observation that the mouth corner landmarks are not sufficiently static due to facial expression changes, and that the inherent ambiguity in determining head yaw with very few co-planar landmarks in 3D leads to less reliable head pose estimation.</p><p>In our work, we utilize a single image as input which contains both eyes. For this purpose, we select the mean of the 2 inner eye corner landmarks in 3D as the origin of our normalized camera coordinate system. We use a focal length of 1300mm for the normalized camera intrinsic parameters, and a distance of 600mm from the face to produce image patches of size 256 × 64 to use as input for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Configuration of Disentangling Transforming</head><p>Encoder-Decoder (DT-ED)</p><p>We use the DenseNet architecture to parameterize our encoder-decoder network <ref type="bibr" target="#b17">[18]</ref>. We configure the DenseNet with a growth-rate of 32, 4 dense blocks (each with 4 composite layers), and a compression factor of 1.0. We neither use dropout nor 1 × 1 convolutional layers. We use instance normalization <ref type="bibr" target="#b48">[49]</ref> and leaky ReLU activation functions</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning to learn by gradient descent by gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando De</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freitas</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The camera mouse: visual tracking of body features to provide computer access for people with severe disabilities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margrit</forename><surname>Betke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Gips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Fleming</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on neural systems and Rehabilitation Engineering</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Biedert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Buscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Schwarz</surname></persName>
		</author>
		<title level="m">Jörn Hees, and Andreas Dengel. Text 2.0. In CHI</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Appearancebased gaze estimation via evaluation-guided asymmetric regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihua</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xucong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cascade multiview hourglass model for robust 3d face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zaferiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In FG</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Modelagnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">RT-GENE: Real-Time Eye Gaze Estimation in Natural Environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hyung Jin Chang, and Yiannis Demiris</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ECCV</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cognitive load estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lex</forename><surname>Fridman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Reimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruce</forename><surname>Mehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: training imagenet in 1 hour</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno>abs/1706.02677</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Baker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010-05" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="807" to="813" />
		</imprint>
	</monogr>
	<note type="report_type">Multi-pie. IVC</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">General theory of remote gaze estimation using the pupil center and corneal reflections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elias</forename><surname>Daniel Guestrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moshe</forename><surname>Eizenman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on biomedical engineering</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1124" to="1133" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Few-shot human motion prediction via meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Liang-Yan Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">In the eye of the beholder: A survey of models for eyes and gaze</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Witzner Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Ji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>TPAMI</publisher>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="478" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Transforming auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Geoffrey E Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida D</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICANN</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improving landmark localization with semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Finding tiny faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiyun</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Stressclick: Sensing stress from gaze-click patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajia</forename><surname>Michael Xuelin Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong Va</forename><surname>Ngai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A multiresolution 3d morphable face model and fitting framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrik</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Tena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pouria</forename><surname>Mortazavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koppen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Christmas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Ratsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VISIGRAPP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Self-normalizing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Günter</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Eye Tracking for Everyone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Krafka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Kellnhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harini</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suchendra</forename><surname>Bhandarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Brenden M Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Simu-lated+unsupervised learning with adaptive data generation and bidirectional mappings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kangwook</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changho</forename><surname>Suh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Epnp: An accurate o (n) solution to the pnp problem. International journal of computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page">155</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A differential approach for gaze estimation with calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth Alberto Funes</forename><surname>Mora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Marc</forename><surname>Odobez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A head pose-free approach for appearance-based gaze estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takahiro</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoichi</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Inferring human gaze from appearance via adaptive linear regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takahiro</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoichi</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">On firstorder meta-learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Searchgazer: Webcam eye tracking for remote studies of web search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Papoutsaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Laskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHIIR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Meta-tracker: Fast and robust online adaptation for visual object trackers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunbyung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep Pictorial Gaze Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonwook</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Spurr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otmar</forename><surname>Hilliges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Andreas Bulling, and Otmar Hilliges. Learning to find eye region landmarks for remote gaze estimation in unconstrained settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonwook</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xucong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM ETRA</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Perceptually-based foveated virtual reality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anjul</forename><surname>Patney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joohwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Salvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Kaplanyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Wyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nir</forename><surname>Benty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Lefohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luebke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Few-shot segmentation propagation with guided networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Rakelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.07373</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Lightweight head pose invariant gaze tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Shalini De Mello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR -Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">One-shot generalization in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unsupervised geometry-aware representation for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Meta-learning with memory-augmented neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning from simulated and unsupervised images through adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenda</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The reading assistant: eye gaze triggered auditory prompting for reading remediation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehmet</forename><surname>Sibert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">A</forename><surname>Gokturk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lavine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM UIST</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="101" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning-by-Synthesis for Appearance-based 3D Gaze Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuyuki</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoichi</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Appearance-based eye gaze estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kar-Han</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Where is the driver looking: Analysis of head, eye and iris for robust gaze zone estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Tawari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan M</forename><surname>Kuo Hao Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ITSC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Fewshot viewpoint estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yu</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shalini</forename><forename type="middle">De</forename><surname>Mello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tremblay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sifei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Birchfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<title level="m">stance normalization: The missing ingredient for fast stylization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A hierarchical generative model for eye image synthesis and eye gaze estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A review of eye-tracking research in marketing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Wedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Pieters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Review of marketing research</title>
		<imprint>
			<publisher>Emerald Group Publishing Limited</publisher>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Interpretable transformations with encoder-decoder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><forename type="middle">J</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniyar</forename><surname>Garbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Turmukhambetov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brostow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deep multitask gaze estimation with a constrained landmark-gaze model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Marc</forename><surname>Odobez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV -Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Revisiting data normalization for appearance-based gaze estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xucong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Bulling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ETRA</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Evaluation of appearance-based methods and implications for gaze-based applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xucong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Bulling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Appearance-based gaze estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xucong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Bulling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">It&apos;s written all over your face: Full-face appearancebased gaze estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xucong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Bulling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR -Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Mpiigaze: Real-world dataset and deep appearancebased gaze estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xucong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Bulling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Gazehorizon: Enabling passers-by to interact with public displays by gaze</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanxia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><forename type="middle">Ki</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Bulling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Gellersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM UbiComp</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Monocular free-head 3d gaze tracking with deep learning and geometry constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangjiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoping</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
