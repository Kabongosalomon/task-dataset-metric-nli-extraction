<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TENSOR DECOMPOSITIONS FOR TEMPORAL KNOWLEDGE BASE COMPLETION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-04-10">10 Apr 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothee</forename><surname>Lacroix</surname></persName>
							<email>timothee.lax@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">ENPC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Obozinski</surname></persName>
							<email>guillaume.obozinski@epfl.chusunier@fb.com</email>
							<affiliation key="aff2">
								<orgName type="department">Swiss Data Science Center</orgName>
								<orgName type="institution">EPFL &amp; ETH ZÃijrich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TENSOR DECOMPOSITIONS FOR TEMPORAL KNOWLEDGE BASE COMPLETION</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-04-10">10 Apr 2020</date>
						</imprint>
					</monogr>
					<note>arXiv:2004.04926v1 [stat.ML] Published as a conference paper at ICLR 2020</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most algorithms for representation learning and link prediction in relational data have been designed for static data. However, the data they are applied to usually evolves with time, such as friend graphs in social networks or user interactions with items in recommender systems. This is also the case for knowledge bases, which contain facts such as (US, has president, B. Obama, [2009<ref type="bibr" target="#b11">-2017</ref>) that are valid only at certain points in time. For the problem of link prediction under temporal constraints, i.e., answering queries such as (US, has president, ?, 2012), we propose a solution inspired by the canonical decomposition of tensors of order 4. We introduce new regularization schemes and present an extension of Com-plEx (Trouillon et al., 2016)  that achieves state-of-the-art performance. Additionally, we propose a new dataset for knowledge base completion constructed from Wikidata, larger than previous benchmarks by an order of magnitude, as a new reference for evaluating temporal and non-temporal link prediction methods. * UniversitÃl' Paris-Est, Equipe Imagine, LIGM (UMR8049) Ecole des Ponts ParisTech, Marne-la-VallÃl'e</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Link prediction in relational data has been the subject of interest, given the widespread availability of such data and the breadth of its use in bioinformatics <ref type="bibr" target="#b28">(Zitnik et al., 2018)</ref>, recommender systems <ref type="bibr" target="#b14">(Koren et al., 2009)</ref> or Knowledge Base completion <ref type="bibr" target="#b19">(Nickel et al., 2016a)</ref>. Relational data is often temporal, for example, the action of buying an item or watching a movie is associated to a timestamp. Some medicines might not have the same adverse side effects depending on the subject's age. The task of temporal link prediction is to find missing links in graphs at precise points in time.</p><p>In this work, we study temporal link prediction through the lens of temporal knowledge base completion, which provides varied benchmarks both in terms of the underlying data they represent, but also in terms of scale. A knowledge base is a set of facts <ref type="bibr">(subject, predicate, object)</ref> about the world that are known to be true. Link prediction in a knowledge base amounts to answer incomplete queries of the form <ref type="bibr">(subject, predicate, ?)</ref> by providing an accurate ranking of potential objects. In temporal knowledge bases, these facts have some temporal metadata attached. For example, facts might only hold for a certain time interval, in which case they will be annotated as such. Other facts might be event that happened at a certain point in time. Temporal link prediction amounts to answering queries of the form <ref type="bibr">(subject, predicate, ?, timestamp)</ref>. For example, we expect the ranking of queries <ref type="bibr">(USA, president, ?, timestamp)</ref> to vary with the timestamps.</p><p>As tensor factorization methods have proved successful for Knowledge Base Completion <ref type="bibr" target="#b19">(Nickel et al., 2016a;</ref><ref type="bibr" target="#b25">Trouillon et al., 2016;</ref><ref type="bibr" target="#b15">Lacroix et al., 2018)</ref>, we express our Temporal Knowledge Base Completion problem as an order 4 tensor completion problem. That is, timestamps are discretized and used to index a 4-th mode in the binary tensor holding <ref type="bibr">(subject, predicate, object, timestamps)</ref> facts.</p><p>First, we introduce a ComplEx <ref type="bibr" target="#b25">(Trouillon et al., 2016)</ref> decomposition of this order 4 tensor, and link it with previous work on temporal Knowledge Base completion. This decomposition yields embeddings for each timestamps. A natural prior is for these timestamps representation to evolve slowly over time. We are able to introduce this prior as a regularizer for which the optimum is a variation on the nuclear p-norm. In order to deal with heterogeneous temporal knowledge bases where a significant amount of relations might be non-temporal, we add a non-temporal component to our decomposition.</p><p>Experiments on available benchmarks show that our method outperforms the state of the art for similar number of parameters. We run additional experiments for larger, regularized models and obtain improvements of up to 0.07 absolute Mean Reciprocal Rank (MRR).</p><p>Finally, we propose a dataset of 400k entities, based on Wikidata, with 7M train triples, of which 10% contain temporal validity information. This dataset is larger than usual benchmarks in the Knowledge Base completion community and could help bridge the gap between the method designed and the envisaged web-scale applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Matrices and tensors are upper case letters. The i-th row of U is denoted by u i while it's j − th column is denoted by U :,j . The tensor product of two vectors is written ⊗ and the hadamard (elementwise) product ⊙.</p><p>Static link prediction methods Standard tensor decomposition methods have lead to good results <ref type="bibr" target="#b27">(Yang et al., 2014;</ref><ref type="bibr" target="#b25">Trouillon et al., 2016;</ref><ref type="bibr" target="#b15">Lacroix et al., 2018;</ref><ref type="bibr" target="#b1">Balažević et al., 2019)</ref> in Knowledge Base completion. The Canonical Polyadic (CP) Decomposition <ref type="bibr" target="#b9">(Hitchcock, 1927)</ref> is the tensor equivalent to the low-rank decomposition of a matrix. A tensor X of canonical rank R can be written as:</p><formula xml:id="formula_0">X = R r=1 U :,r ⊗V :,r ⊗W :,r = [[U, V, W ]] ⇐⇒ ∀(i, j, k), X i,j,k = R r=1 u i,r v j,r w k,r = u i , v j , w k</formula><p>Setting U = W leads to the Distmult <ref type="bibr" target="#b27">(Yang et al., 2014)</ref> model, which has been successful, despite only being able to represent symmetric score functions. In order to keep the parameter sharing scheme but go beyond symmetric relations, <ref type="bibr" target="#b25">Trouillon et al. (2016)</ref> use complex parameters and set W to the complex conjugate of U , U . Regularizing this algorithm with the variational form of the tensor nuclear norm as well as a slight transformation to the learning objective (also proposed in <ref type="bibr" target="#b12">Kazemi &amp; Poole (2018)</ref>) leads to state of the art results in <ref type="bibr" target="#b15">Lacroix et al. (2018)</ref>.</p><p>Other methods are not directly inspired from classical tensor decompositions. For example, TransE <ref type="bibr" target="#b2">(Bordes et al., 2013)</ref> models the score as a distance of the translated subject to an object representation. This method has lead to many variations <ref type="bibr" target="#b10">(Ji et al., 2015;</ref><ref type="bibr" target="#b17">Nguyen et al., 2016;</ref><ref type="bibr" target="#b26">Wang et al., 2014)</ref>, but is limited in the relation systems it can model <ref type="bibr" target="#b12">(Kazemi &amp; Poole, 2018)</ref> and does not lead to state of the art performances on current benchmarks. Finally <ref type="bibr" target="#b22">Schlichtkrull et al. (2018)</ref> propose to generate the entity embeddings of a CP-like tensor decomposition by running a forward pass of a Graph Neural Network over the training Knowledge Base. The experiments included in this work did not lead to better link prediction performances than the same decomposition (Distmult) directly optimized <ref type="bibr" target="#b11">(Kadlec et al., 2017)</ref>.</p><p>Temporal link prediction methods <ref type="bibr" target="#b21">Sarkar &amp; Moore (2006)</ref> describes a bayesian model and learning method for representing temporal relations. The temporal smoothness prior used in this work is similar to the gradient penalty we describe in Section 3.3. However, learning one embedding matrix per timestamp is not applicable to the scales considered in this work. <ref type="bibr" target="#b0">Bader et al. (2007)</ref> uses a tensor decomposition called ASALSAN to express temporal relations. This decomposition is related to RESCAL <ref type="bibr" target="#b18">(Nickel et al., 2011)</ref> which underperforms on recent benchmarks due to overfitting <ref type="bibr" target="#b20">(Nickel et al., 2016b)</ref>.</p><p>For temporal knowledge base completion, <ref type="bibr" target="#b8">Goel et al. (2020)</ref> learns entity embeddings that change over time, by masking a fraction of the embedding weights with an activation function of learned frequencies. Based on the Tucker decomposition, ConT <ref type="bibr" target="#b16">(Ma et al., 2018)</ref> learns one new core tensor for each timestamp. Finally, viewing the time dimension as a sequence to be predicted, <ref type="bibr" target="#b7">García-Durán et al. (2018)</ref> use recurrent neural nets to transform the embeddings of standard models such as TransE or Distmult to accomodate the temporal data.</p><p>Published as a conference paper at ICLR 2020  <ref type="bibr" target="#b15">Lacroix et al. (2018)</ref> by studying and extending a regularized CP decomposition of the training set seen as an order 4 tensor. We propose and study several regularizer suited to our decompositions.</p><formula xml:id="formula_1">DE-SimplE 2r ((3γ + (1 − γ))|E| + |P |) TComplEx 2r(|E| + |T | + 2|P |) TNTComplEx 2r(|E| + |T | + 4|P |)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MODEL</head><p>In this section, we are given facts (subject, predicate, object) annotated with timestamps, we discretize the timestamp range (eg. by reducing timestamps to years) in order to obtain a training set of 4-tuple (subject, predicate, object, time) indexing an order 4 tensor. We will show in Section 5.1 how we reduce each datasets to this setting. Following <ref type="bibr" target="#b15">Lacroix et al. (2018)</ref>, we minimize, for each of the train tuples (i, j, k, l), the instantaneous multiclass loss :</p><formula xml:id="formula_2">ℓ(X; (i, j, k, l)) = −X i,j,k,l + log k ′ exp X i,j,k ′ ,l .</formula><p>(1)</p><p>Note that this loss is only suited to queries of the type (subject, predicate, ?, time), which is the queries that were considered in related work. We consider another auxiliary loss in Section 6 which we will use on our Wikidata dataset. For a training set S (augmented with reciprocal relations <ref type="bibr" target="#b15">(Lacroix et al., 2018;</ref><ref type="bibr" target="#b12">Kazemi &amp; Poole, 2018)</ref>), and parametric tensor estimateX(θ), we minimize the following objective, with a weighted regularizer Ω:</p><formula xml:id="formula_3">L(X(θ)) = 1 |S| (i,j,k,l)∈S ℓ(X(θ); (i, j, k, l)) + λΩ(θ; (i, j, k, l)) .</formula><p>The ComplEx <ref type="bibr" target="#b25">(Trouillon et al., 2016)</ref> decomposition can naturally be extended to this setting by adding a new factor T , we then have:</p><formula xml:id="formula_4">X(U, V, T ) = Re [[U, V, U , T ]] ⇐⇒X(U, V, T ) i,j,k,l = Re ( u i , v j , u k , t l )<label>(2)</label></formula><p>We call this decomposition TComplEx. Intuitively, we added timestamps embedding that modulate the multi-linear dot product. Notice that the timestamp can be used to equivalently modulate the objects, predicates or subjects to obtain time-dependent representation:</p><formula xml:id="formula_5">u i , v j , u k , t l = u i ⊙ t l , v j , u k = u i , v j ⊙ t l , u k = u i , v j , u k ⊙ t l .</formula><p>Contrary to DE-SimplE <ref type="bibr" target="#b8">(Goel et al., 2020)</ref>, we do not learn temporal embeddings that scale with the number of entities (as frequencies and biases), but rather embeddings that scale with the number of timestamps. The number of parameters for the two models are compared in <ref type="table" target="#tab_0">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">NON-TEMPORAL PREDICATES</head><p>Some predicates might not be affected by timestamps. For example, Malia and Sasha will always be the daughters of Barack and Michelle Obama, whereas the "has occupation" predicate between two entities might very well change over time. In heterogeneous knowledge bases, where some predicates might be temporal and some might not be, we propose to decompose the tensorX as the sum of two tensors, one temporal, and the other non-temporal:</p><formula xml:id="formula_6">X = Re [[U, V t , U , T ]] + [[U, V, U , 1]] ⇐⇒X i,j,k,l = Re u i , v t j ⊙ t l + v j , u k<label>(3)</label></formula><p>Published as a conference paper at ICLR 2020</p><p>We call this decomposition TNTComplEx. <ref type="bibr" target="#b8">Goel et al. (2020)</ref> suggests another way of introducing a non-temporal component, by only allowing a fraction γ of components of the embeddings to be modulated in time. By allowing this sharing of parameters between the temporal and non-temporal part of the tensor, our model removes one hyperparameter. Moreover, preliminary experiments showed that this model outperforms one without parameter sharing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">REGULARIZATION</head><p>Any order 4 tensor can be considered as an order 3 tensor by unfolding modes together. For a tensor X ∈ R N1×N2×N3×N4 , unfolding modes 3 and 4 together will lead to tensorX ∈ R N1×N2×N3N4 <ref type="bibr" target="#b13">(Kolda &amp; Bader, 2009</ref>).</p><p>We can see both decompositions ( <ref type="formula" target="#formula_4">(2)</ref> and <ref type="formula" target="#formula_6">(3)</ref>) as order 3 tensors by unfolding the temporal and predicate modes together. Considering the decomposition implied by these unfoldings (see Appendix 8.1) leads us to the following weighted regularizers <ref type="bibr" target="#b15">(Lacroix et al., 2018)</ref>:</p><formula xml:id="formula_7">Ω 3 (U, V, T ; (i, j, k, l)) = 1 3 u i 3 3 + u k 3 3 + v k ⊙ t l 3 3 (4) Ω 3 (U, V t , V, T ; (i, j, k, l)) = 1 3 2 u i 3 3 + 2 u k 3 3 + v t j ⊙ t l 3 3 + v j 3 3</formula><p>The first regularizer weights objects, predicates and pairs (predicate, timestamp) according to their respective marginal probabilities. This regularizer is a variational form of the weighted nuclear 3norm on an order 4 tensor (see subsection 3.4 and Appendix 8.3 for details and proof). The second regularizer is the sum of the nuclear 3 penalties on tensors</p><formula xml:id="formula_8">[[U, V t , U , T ]] and [[U, V, U ]].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">SMOOTHNESS OF TEMPORAL EMBEDDINGS</head><p>We have more a priori structure on the temporal mode than on others. Notably, we expect smoothness of the application i → t i . In words, we expect neighboring timestamps to have close representations. Thus, we penalize the norm of the discrete derivative of the temporal embeddings :</p><formula xml:id="formula_9">Λ p (T ) = 1 |T | − 1 |T |−1 i=1 t i+1 − t i p p .<label>(5)</label></formula><p>We show in Appendix 8.2 that the sum of Λ p and the variational form of the nuclear p norm (6) lead to a variational form of a new tensor atomic norm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">NUCLEAR p-NORMS OF TENSORS AND THEIR VARIATIONAL FORMS</head><p>As was done in <ref type="bibr" target="#b15">Lacroix et al. (2018)</ref>, we aim to use tensor nuclear p-norms as regularizers. The definition of the nuclear p-norm of a tensor <ref type="bibr" target="#b6">(Friedland &amp; Lim, 2018)</ref> of order D is:</p><formula xml:id="formula_10">X p * = inf α,R,U (1) ,...,U (D) α 1 | X = R r=1 α r U (1) :,r ⊗ · · · ⊗ U (D) :,r , ∀r, d U (d) :,r p = 1 .</formula><p>This formulation of the nuclear p-norm writes a tensor as a sum over atoms which are the rank-1 tensors of unit p-norm factors. The nuclear p-norm is NP-hard to compute <ref type="bibr" target="#b6">(Friedland &amp; Lim, 2018)</ref>. Following <ref type="bibr" target="#b15">Lacroix et al. (2018)</ref>, a practical solution is to use the equivalent formulation of nuclear p-norm using their variational form, which can be conveniently written for p = D:</p><formula xml:id="formula_11">X D * = 1 D inf X=[[U (1) ,...,U (D) ]] D d=1 R r=1 U (d) :,r D D .<label>(6)</label></formula><p>For the equality above to hold, the infimum should be over all possible R. The practical solution is to fix R to the desired rank of the decomposition. Using this variational formulation as a regularizer leads to state of the art results for order-3 tensors <ref type="bibr" target="#b15">(Lacroix et al., 2018)</ref> and is convenient in a stochastic gradient setting because it separates over each model coefficient.</p><p>In addition, this formulation makes it easy to introduce a weighting as recommended in <ref type="bibr" target="#b24">Srebro &amp; Salakhutdinov (2010)</ref>; <ref type="bibr" target="#b5">Foygel et al. (2011)</ref>. In order to learn under non-uniform sampling distributions, one should penalize the weighted norm : </p><formula xml:id="formula_12">√ M (1) ⊗ √ M (2) ⊙ X 2 * , where M<label>(</label></formula><p>iD for observed triple (i 1 , . . . , i D ) in stochastic gradient descent. More precisely for D = 2 and N (d) the vectors holding the observed count of each index over each mode d:</p><formula xml:id="formula_14">1 |S| (i,j)∈S u i 2 2 + v j 2 2 = i N (1) i S u i 2 2 + j N (2) j S v j 2 2 = i M (1) i u i 2 2 + j M (2) j v j 2 2 .</formula><p>In subsection 3.3, we add another penalty in Equation <ref type="formula" target="#formula_9">(5)</ref> which changes the norm of our atoms.In subsection 3.2, we introduced another variational form in Equation <ref type="formula" target="#formula_12">(4)</ref> which allows to easily penalize the nuclear 3-norm of an order 4 tensor. This regularizer leads to different weighting. By considering the unfolding of the timestamp and predicate modes, we are able to weight according to the joint marginal of timestamps and predicates, rather than by the product of the marginals. This can be an important distinction if the two are not independent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">EXPERIMENTAL IMPACT OF THE REGULARIZERS</head><p>We study the impact of regularization on the ICEWS05-15 dataset, for the TNTComplEx model. For details on the experimental set-up, see Section 5.1. The first effect we want to quantify is the effect of the regularizer Λ p . We run a grid search for the strength of both Λ p and Ω 3 and plot the convex hull as a function of the temporal regularization strength. As shown in <ref type="figure" target="#fig_2">Figure 1</ref>, imposing smoothness along the time mode brings an improvement of over 2 MRR point.</p><p>The second effect we wish to quantify is the effect of the choice of regularizer Ω. A natural regularizer for TNTComplEx would be : ∆ 2 is the common weight-decay frequently used in deep-learning. Such regularizers have been used in knowledge base completion <ref type="bibr" target="#b18">(Nickel et al., 2011;</ref><ref type="bibr" target="#b20">2016b;</ref><ref type="bibr" target="#b25">Trouillon et al., 2016)</ref>, however, <ref type="bibr" target="#b15">Lacroix et al. (2018)</ref> showed that the infimum of this penalty is non-convex over tensors.</p><p>∆ 3 matches the order used in the Ω 3 regularizer, and in previous work on knowledge base completion <ref type="bibr" target="#b15">(Lacroix et al., 2018)</ref>. However, by the same arguments, its minimization does not lead to a convex penalty over tensors.</p><p>∆ 4 is the sum of the variational forms of the Nuclear 4-norm for the two tensors of order 4 in the TNTComplEx model according to equation <ref type="formula" target="#formula_11">(6)</ref>.</p><p>Detailed results of the impact of regularization on the performances of the model are given in <ref type="figure" target="#fig_2">Figure 1</ref>. The two regularizers ∆ 4 and Ω 3 are the only regularizers that can be interpreted as sums of tensor norm variational forms and perform better than their lower order counterparts.</p><p>There are two differences between ∆ 4 and Ω 3 . First, whereas the first is a variational form of the nuclear 4-norm, the second is a variational form of the nuclear 3-norm which is closer to the nuclear 2-norm. Results for exact recovery of tensors have been generalized to the nuclear 2-norm, and to the extent of our knowledge, there has been no formal study of generalization properties or exact recovery under the nuclear p-norm for p greater than two.</p><p>Second, the weighting in ∆ 4 is done separately over timestamps and predicates, whereas it is done jointly for Ω 3 . This leads to using the joint empirical marginal as a weighting over timestamps and predicates. The impact of weighting on the guarantees that can be obtained are described more precisely in <ref type="bibr" target="#b5">Foygel et al. (2011)</ref>.</p><p>The contribution of all these regularizers over a non-regularized model are summarized in <ref type="table" target="#tab_2">Table 3</ref>. Note that careful regularization leads to a 0.05 MRR increase. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">A NEW DATASET FOR TEMPORAL AND NON-TEMPORAL KNOWLEDGE BASE COMPLETION</head><p>A dataset based on Wikidata was proposed by <ref type="bibr" target="#b7">García-Durán et al. (2018)</ref>. However, upon inspection, this dataset contains numerical data as entities, such as ELO rankings of chess players, which are not representative of practically useful link prediction problems. Also, in this dataset, temporal informations is specified in the form of "OccursSince" and "OccursUntil" statements appended to triples, which becomes unwieldy when a predicate holds for several intervals in time. Moreover, this dataset contains only 11k entities and 150k which is insufficient to benchmark methods at scale.</p><p>The GDelt dataset described in <ref type="bibr" target="#b16">Ma et al. (2018)</ref>; <ref type="bibr" target="#b8">Goel et al. (2020)</ref> holds many triples (2M ), but does not describe enough entities (500). In order to adress these limitations, we created our own dataset from Wikidata, which we make available along with the code for this paper at https://github.com/facebookresearch/tkbc. Starting from Wikidata, we removed all entities that were instance of scholarly articles, proteins and others. We also removed disambiguation, template, category and project pages from wikipedia. Then, we removed all facts for which the object was not an entity. We iteratively filtered out entities that had degree at least 5 and predicates that had at least 50 occurrences. With this method, we obtained a dataset of 432715 entities, 407 predicates and 1724 timestamps (we only kept the years). Each datum is a triple (subject, predicate, object) together a timestamp range (begin, end) where begin, end or both can be unspecified. Our train set contains 7M such tuples, with about 10% partially specified temporal tuples. We kept a validation and test set of size 50k each.</p><p>At train and test time, for a given datum <ref type="bibr">(subject, predicate, object, [begin, end]</ref>), we sample a timestamp (appearing in the dataset) uniformly at random, in the range <ref type="bibr">[begin, end]</ref>. For datum without a temporal range, we sample over the maximum date range. Then, we rank the objects for the partial query (subject, predicate, ?, timestamp).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">EXPERIMENTAL SET-UP</head><p>We follow the experimental set-up in <ref type="bibr" target="#b7">García-Durán et al. (2018)</ref>; <ref type="bibr" target="#b8">Goel et al. (2020)</ref>. We use models from <ref type="bibr" target="#b7">García-Durán et al. (2018)</ref> and <ref type="bibr" target="#b8">Goel et al. (2020)</ref> as baselines since they are the best performing algorithms on the datasets considered. We report the filtered Mean Reciprocal Rank (MRR) defined in <ref type="bibr" target="#b20">Nickel et al. (2016b)</ref>. In order to obtaiqn comparable results, we use <ref type="table" target="#tab_0">Table 1</ref> and dataset statistics to compute the rank for each (model, dataset) pair that matches the number of parameters used in <ref type="bibr" target="#b8">Goel et al. (2020)</ref>. We also report results at ranks 10 times higher. This higher rank setup gives an estimation of the best possible performance attainable on these datasets, even though the dimension used might be impractical for applied systems. All our models are optimized with Adagrad <ref type="bibr" target="#b4">(Duchi et al., 2011)</ref>, with a learning rate of 0.1, a batch-size of 1000. More details on the grid-search, actual ranks used and hyper-parameters are given in Appendix 8.  <ref type="table">Table 2</ref>: Results for TA <ref type="bibr" target="#b7">(García-Durán et al., 2018)</ref> and DE-SimplE <ref type="bibr" target="#b8">(Goel et al., 2020)</ref> are the best numbers reported in the respective papers. Our models have as many parameters as DE-SimplE. Numbers in parentheses are for ranks multiplied by 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reg. MRR</head><p>No regularizer 0.62 ∆ 2 0.63 ∆ 3 0.63 ∆ 4 0.64 Ω 3 0.65 Ω 3 + Λ 4 0.67  <ref type="bibr" target="#b2">(Bordes et al., 2013)</ref> which adds "occursSince" and "occursUntil" timestamps to each triples. Following the evaluation setting of <ref type="bibr" target="#b7">García-Durán et al. (2018)</ref>, during evaluation, the incomplete triples to complete are of the form (subject, predicate, ?, occursSince | occursUntil, timestamp) (with reciprocal predicates). Rather than deal with tensors of order 5, we choose to unfold the (occursSince, occursUntil) and the predicate mode together, multiplying its size by two.</p><p>Some relations in Wikidata are highly unbalanced (eg. <ref type="figure">(?, InstanceOf, Human)</ref>). For such relations, a ranking evaluation would not make much sense. Instead, we only compute the Mean Reciprocal Rank for missing right hand sides, since the data is such that highly unbalanced relations happen on the left-hand side. However, we follow the same training scheme as for all the other dataset, including reciprocal relations in the training set. The cross-entropy loss evaluated on 400k entities puts a restriction on the dimensionality of embeddings at about d = 100 for a batch-size of 1000.</p><p>We leave sampling of this loss, which would allow for higher dimensions to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">RESULTS</head><p>We compare ComplEx with the temporal versions described in this paper. We report results in <ref type="table">Table 2</ref>. Note that ComplEx has performances that are stable through a tenfold increase of its number of parameters, a rank of 100 is enough to capture the static information of these datasets. For temporal models however, the performance increases a lot with the number of parameters. It is always beneficial to allow a separate modeling of non-temporal predicates, as the performances of TNTComplex show. Finally, our model match or beat the state of the art on all datasets, even at identical number of parameters. Since these datasets are small, we also report results for higher ranks (10 times the number of parameters used for DE-SimplE).   <ref type="formula" target="#formula_12">(7)</ref> and is thus more flexible on the queries it can answer.</p><p>Training TNTComplEx on Wikidata with a rank of d = 100 with the full cross-entropy on a Quadro GP 100, we obtain a speed of 5.6k triples per second, leading to experiments time of 7.2 hours. This is to be compared with 5.8k triples per second when training ComplEx for experiments time of 6.9 hours. The additional complexity of our model does not lead to any real impact on runtime, which is dominated by the computation of the cross-entropy over 400k entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">QUALITATIVE STUDY</head><p>The instantaneous loss described in equation <ref type="formula" target="#formula_12">(1)</ref>, along with the timestamp sampling scheme described in the previous section only enforces correct rankings along the "object" tubes of our order-4 tensor. In order to enforce a stronger temporal consistency, and be able to answer queries of the type (subject, predicate, object, ?), we propose another cross-entropy loss along the temporal tubes:</p><p>ℓ(X; (i, j, k, l)) = −X i,j,k,l + log l ′ exp X i,j,k,l ′ .</p><p>We optimize the sum of ℓ defined in Equation 1 andl defined in Equation 7. Doing so, we only lose 1 MRR point overall. However, we make our model better at answering queries along the time axis. The macro area under the precision recall curve is 0.92 for a TNTComplEx model learned with ℓ alone and 0.98 for a TNTComplEx model trained with ℓ +l.</p><p>We plot in <ref type="figure">Figure 2</ref> the scores along time for train triples (president of the french republic, office holder, {Jacques Chirac | Nicolas Sarkozy | FranÃgois Hollande | Emmanuel Macron}, <ref type="bibr">[1980,</ref><ref type="bibr">2020]</ref>). The periods where a score is highest matches closely the ground truth of start and end dates of these presidents mandates which is represented as a colored background. This shows that our models are able to learn rankings that are correct along time intervals despite our training method only ever sampling timestamps within these intervals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>Tensor methods have been successful for Knowledge Base completion. In this work, we suggest an extension of these methods to Temporal Knowledge Bases. Our methodology adapts well to the various form of these datasets : point-in-time, beginning and endings or intervals. We show that our methods reach higher performances than the state of the art for similar number of parameters. For several datasets, we also provide performances for higher dimensions. We hope that the gap between low-dimensional and high-dimensional models can motivate further research in models that have increased expressivity at lower number of parameters per entity. Finally, we propose a large scale temporal dataset which we believe represents the challenges of large scale temporal completion in knowledge bases. We give performances of our methods for low-ranks on this dataset. We believe that, given its scale, this dataset could also be an interesting addition to non-temporal knowledge base completion.    <ref type="bibr" target="#b8">(Goel et al., 2020)</ref> are the best numbers reported in the respective papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.6">STANDARD DEVIATIONS</head><p>We give the standard deviations for the MRR computed over 5 runs of TNTComplEx on all datasets: ICEWS14 ICEWS15-05 Yago15k Wikidata (T) Wikidata (NT) TNTComplEx 0.0016 0.0011 0.00076 0.0035 0.0012 8.7 GRID SEARCH For ICEWS14, ICEWS05-15 and Yago15k, we follow the grid-search below :</p><p>Using <ref type="table" target="#tab_0">Table 1</ref> to compute the number of parameters and the dataset statistics in </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>1) and M (2) are the empirical row and column marginal of the distribution. The variational form (6) makes this easy, by simply penalizing rows U</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>We compare ∆ 4 , ∆ 3 and ∆ 2 with Ω 3 . The comparison is done with a temporal regularizer of 0 to reduce the experimental space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Impact of the temporal (left) regularizer and embeddings (right) regularizer on a TNT-ComplEx model trained on ICEWS05-15.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Number of parameters for each models consideredThis work follows</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Impact of regu-larizers on ICEWS05-15</cell></row><row><cell>for TNTComplEx.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>On Wikidata, 90% of the triples have no temporal data attached. This leads to ComplEx outperforming all temporal models in term of average MRR, since the Non-Temporal MRR (NT-MRR) far</figDesc><table><row><cell>ComplEx</cell><cell cols="3">MRR NT-MRR T-MRR 0.45 0.48 0.29</cell></row><row><cell cols="2">TComplEx TNTComplEx 0.44 0.42</cell><cell>0.45 0.47</cell><cell>0.30 0.32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Results on wikidata for entity dimension d = 100.</figDesc><table><row><cell></cell><cell>15.0</cell><cell></cell><cell></cell><cell></cell><cell>Jacques Chirac</cell></row><row><cell>Score</cell><cell>10.0 12.5</cell><cell></cell><cell></cell><cell></cell><cell>Nicolas Sarkozy François Hollande</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Emmanuel Macron</cell></row><row><cell></cell><cell>7.5</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1980</cell><cell>1990</cell><cell>2000</cell><cell>2010</cell><cell>2020</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Year</cell><cell></cell></row><row><cell cols="6">Figure 2: Scores for triples (President of the French republic, office holder, {Jacques Chirac | Nicolas</cell></row><row><cell cols="6">Sarkozy | FranÃgois Hollande | Emmanuel Macron}, [1980, 2020])</cell></row><row><cell cols="6">outweighs the Temporal MRR (T-MRR). A breakdown of the performances is available in table 4. TNTComplEx obtains performances that are comparable to ComplEx on non-temporal triples, but</cell></row><row><cell cols="6">are better on temporal triples. Moreover, TNTComplEx can minimize the temporal cross-entropy</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>DATASET STATISTICS Statistics of all the datasets used in this work are gathered inTable 5.</figDesc><table><row><cell>8 APPENDIX 8.4 ICEWS14 ICEWS05-15 Yago15k Wikidata Entities 6869 10094 15403 432715 Predicates 460 502 102 814 Timestamps 365 4017 170 1726 |S| 72826 368962 110441 7224361</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc>TNTComplEx (x10) 0.62 0.52 0.66 0.76 0.67 0.59 0.71 0.81 0.37 0.29 0.39 0.54</figDesc><table><row><cell>: Dataset statistics</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Results for TA (García-Durán et al., 2018) and DE-SimplE</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5</head><label>5</label><figDesc>, we use the following ranks to match the number of parameters of DE-SimplE in dimension 100:</figDesc><table><row><cell>DE-SimplE</cell><cell cols="3">ICEWS14 ICEWS05-15 Yago15k 100 100 100</cell></row><row><cell>ComplEx TComplEx TTComplEx</cell><cell>182 174 156</cell><cell>186 136 128</cell><cell>196 194 189</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">More information can be found at http://www.icews.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">. Note that for D pred/time = D pred D time , there would also be equality of the weighted norms. However, in the application considered, time and predicate are most likely not independent, leading to different weightings of the norms.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">UNFOLDING AND THE CP DECOMPOSITION</head><p>Let X = [[U, V, W, T ]], that is X i,j,k,l = u i , v j , w k , t l . Then according to <ref type="bibr" target="#b13">Kolda &amp; Bader (2009)</ref>, unfolding along modes 3 and 4 leads to an order three tensor of decompositionX = [[U, V, W • T ]]. Where • is the Khatri-Rao product <ref type="bibr" target="#b23">(Smilde et al., 2005)</ref>, which is the column-wise Kronecker product : W • T = (W :,1 ⊗ T :,1 , . . . , W :,R ⊗ T :,R ).</p><p>Note that for a fourth mode of size L: (W • T ) L(k−1)+l = w k ⊙ t l . This justifies the regularizers used in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">TEMPORAL REGULARIZER AND NUCLEAR NORMS</head><p>Consider the penalty:</p><p>Let us define a new norm on vectors:</p><p>· τ 4 is a norm and lets us rewrite:</p><p>Following the proof in <ref type="bibr" target="#b15">Lacroix et al. (2018)</ref> which only uses homogeneity of the norms, we can show that Ω(U, V, W, T ) is a variational form of an atomic norm with atoms :</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">NUCLEAR NORMS ON UNFOLDINGS</head><p>We consider the regularizer :</p><p>Let D subj (resp. obj, pred/time) the diagonal matrix containing the cubic-roots of the marginal probabilities of each subject (resp. obj, pred/time) in the dataset. We denote by • the Kathri-Rao product between two matrices (the columnwise Kronecker product). Summing over the entire dataset, we obtain the penalty:</p><p>Dropping the weightings to simplify notations, we state the equivalence between this regularizer and a variational form of the nuclear 3-norm of an order 4 tensor: The proof follows <ref type="bibr" target="#b15">Lacroix et al. (2018)</ref>, noting that u </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Temporal analysis of semantic graphs using asalsan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">A</forename><surname>Bader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">G</forename><surname>Harshman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kolda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Seventh IEEE international conference on data mining (ICDM 2007)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="33" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivana</forename><surname>Balažević</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tucker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09590</idno>
		<title level="m">Tensor factorization for knowledge graph completion</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Translating Embeddings for Modeling Multi-relational Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Icews coded event data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Boschee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Lautenschlager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Oâȃźbrien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Shellman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Starz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Harvard Dataverse</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning with the weighted trace-norm under arbitrary sampling distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rina</forename><surname>Foygel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ohad</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nati</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2133" to="2141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shmuel</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lek-Heng</forename><surname>Lim</surname></persName>
		</author>
		<title level="m">Nuclear norm of higher-order tensors. Mathematics of Computation</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="1255" to="1281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning sequence encoders for temporal knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>García-Durán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastijan</forename><surname>Dumančić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.03202</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Diachronic embedding for temporal knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishab</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Seyed Mehran Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Brubaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poupart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The expression of a tensor or a polyadic as a sum of products</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hitchcock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Studies in Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="164" to="189" />
			<date type="published" when="1927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding via dynamic mapping matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="687" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Knowledge base completion: Baselines strike back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bajgar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kleindienst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Representation Learning for NLP</title>
		<meeting>the 2nd Workshop on Representation Learning for NLP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="69" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Simple embedding for link prediction in knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Seyed Mehran Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="4289" to="4300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Tensor decompositions and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">G</forename><surname>Kolda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brett</forename><forename type="middle">W</forename><surname>Bader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM review</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="455" to="500" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Canonical tensor decomposition for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothée</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Obozinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning (ICML-18)</title>
		<meeting>the 35th International Conference on Machine Learning (ICML-18)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2863" to="2872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Embedding models for episodic knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">A</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daxberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Web Semantics</title>
		<imprint>
			<biblScope unit="page">100490</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Stranse: a novel embedding model of entities and relationships in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kairit</forename><surname>Dat Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Sirts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08140</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A Review of Relational Machine Learning for Knowledge Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="33" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Holographic embeddings of knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tomaso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poggio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dynamic social network analysis using latent space models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Purnamrita</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew W</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1145" to="1152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Multi-way analysis: applications in the chemical sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Age</forename><surname>Smilde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rasmus</forename><surname>Bro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Geladi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Collaborative filtering in a non-uniform world: Learning with the weighted trace norm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2056" to="2064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Théo</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Éric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Eighth AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6575</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Modeling polypharmacy side effects with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monica</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="457" to="466" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
