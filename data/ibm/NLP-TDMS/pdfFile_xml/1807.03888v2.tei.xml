<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimin</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Korea Advanced Institute of Science and Technology (KAIST)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kibok</forename><surname>Lee</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<addrLine>3 Google Brain 4 AItrics</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Korea Advanced Institute of Science and Technology (KAIST)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<addrLine>3 Google Brain 4 AItrics</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Detecting test samples drawn sufficiently far away from the training distribution statistically or adversarially is a fundamental requirement for deploying a good classifier in many real-world machine learning applications. However, deep neural networks with the softmax classifier are known to produce highly overconfident posterior distributions even for such abnormal samples. In this paper, we propose a simple yet effective method for detecting any abnormal samples, which is applicable to any pre-trained softmax neural classifier. We obtain the class conditional Gaussian distributions with respect to (low-and upper-level) features of the deep models under Gaussian discriminant analysis, which result in a confidence score based on the Mahalanobis distance. While most prior methods have been evaluated for detecting either out-of-distribution or adversarial samples, but not both, the proposed method achieves the state-of-the-art performances for both cases in our experiments. Moreover, we found that our proposed method is more robust in harsh cases, e.g., when the training dataset has noisy labels or small number of samples. Finally, we show that the proposed method enjoys broader usage by applying it to class-incremental learning: whenever out-of-distribution samples are detected, our classification rule can incorporate new classes well without further training deep models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep neural networks (DNNs) have achieved high accuracy on many classification tasks, e.g., speech recognition <ref type="bibr" target="#b0">[1]</ref>, object detection <ref type="bibr" target="#b8">[9]</ref> and image classification <ref type="bibr" target="#b11">[12]</ref>. However, measuring the predictive uncertainty still remains a challenging problem <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. Obtaining well-calibrated predictive uncertainty is indispensable since it could be useful in many machine learning applications (e.g., active learning <ref type="bibr" target="#b7">[8]</ref> and novelty detection <ref type="bibr" target="#b17">[18]</ref>) as well as when deploying DNNs in real-world systems <ref type="bibr" target="#b1">[2]</ref>, e.g., self-driving cars and secure authentication system <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>The predictive uncertainty of DNNs is closely related to the problem of detecting abnormal samples that are drawn far away from in-distribution (i.e., distribution of training samples) statistically or adversarially. For detecting out-of-distribution (OOD) samples, recent works have utilized the confidence from the posterior distribution <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b20">21]</ref>. For example, Hendrycks &amp; Gimpel <ref type="bibr" target="#b12">[13]</ref> proposed the maximum value of posterior distribution from the classifier as a baseline method, and it is improved by processing the input and output of DNNs <ref type="bibr" target="#b20">[21]</ref>. For detecting adversarial samples, confidence scores were proposed based on density estimators to characterize them in feature spaces of DNNs <ref type="bibr" target="#b6">[7]</ref>. More recently, Ma et al. <ref type="bibr" target="#b21">[22]</ref> proposed the local intrinsic dimensionality (LID) and empirically showed that the characteristics of test samples can be estimated effectively using the 32nd Conference on Neural Information Processing Systems (NIPS 2018), Montr√©al, Canada. arXiv:1807.03888v2 [stat.ML] 27 Oct 2018 LID. However, most prior works on this line typically do not evaluate both OOD and adversarial samples. To best of our knowledge, no universal detector is known to work well on both tasks.</p><p>Contribution. In this paper, we propose a simple yet effective method, which is applicable to any pre-trained softmax neural classifier (without re-training) for detecting abnormal test samples including OOD and adversarial ones. Our high-level idea is to measure the probability density of test sample on feature spaces of DNNs utilizing the concept of a "generative" (distance-based) classifier. Specifically, we assume that pre-trained features can be fitted well by a class-conditional Gaussian distribution since its posterior distribution can be shown to be equivalent to the softmax classifier under Gaussian discriminant analysis (see Section 2.1 for our justification). Under this assumption, we define the confidence score using the Mahalanobis distance with respect to the closest classconditional distribution, where its parameters are chosen as empirical class means and tied empirical covariance of training samples. To the contrary of conventional beliefs, we found that using the corresponding generative classifier does not sacrifice the softmax classification accuracy. Perhaps surprisingly, its confidence score outperforms softmax-based ones very strongly across multiple other tasks: detecting OOD samples, detecting adversarial samples and class-incremental learning.</p><p>We demonstrate the effectiveness of the proposed method using deep convolutional neural networks, such as DenseNet <ref type="bibr" target="#b13">[14]</ref> and ResNet <ref type="bibr" target="#b11">[12]</ref> trained for image classification tasks on various datasets including CIFAR <ref type="bibr" target="#b14">[15]</ref>, SVHN <ref type="bibr" target="#b27">[28]</ref>, ImageNet <ref type="bibr" target="#b4">[5]</ref> and LSUN <ref type="bibr" target="#b31">[32]</ref>. First, for the problem of detecting OOD samples, the proposed method outperforms the current state-of-the-art method, ODIN <ref type="bibr" target="#b20">[21]</ref>, in all tested cases. In particular, compared to ODIN, our method improves the true negative rate (TNR), i.e., the fraction of detected OOD (e.g., LSUN) samples, from 45.6% to 90.9% on ResNet when 95% of in-distribution (e.g., CIFAR-100) samples are correctly detected. Next, for the problem of detecting adversarial samples, e.g., generated by four attack methods such as FGSM <ref type="bibr" target="#b9">[10]</ref>, BIM <ref type="bibr" target="#b15">[16]</ref>, DeepFool <ref type="bibr" target="#b25">[26]</ref> and CW <ref type="bibr" target="#b2">[3]</ref>, our method outperforms the state-of-the-art detection measure, LID <ref type="bibr" target="#b21">[22]</ref>. In particular, compared to LID, ours improves the TNR of CW from 82.9% to 95.8% on ResNet when 95% of normal CIFAR-10 samples are correctly detected.</p><p>We also found that our proposed method is more robust in the choice of its hyperparameters as well as against extreme scenarios, e.g., when the training dataset has some noisy, random labels or a small number of data samples. In particular, Liang et al. <ref type="bibr" target="#b20">[21]</ref> tune the hyperparameters of ODIN using validation sets of OOD samples, which is often impossible since the knowledge about OOD samples is not accessible a priori. We show that hyperparameters of the proposed method can be tuned only using in-distribution (training) samples, while maintaining its performance. We further show that the proposed method tuned on a simple attack, i.e., FGSM, can be used to detect other more complex attacks such as BIM, DeepFool and CW.</p><p>Finally, we apply our method to class-incremental learning <ref type="bibr" target="#b28">[29]</ref>: new classes are added progressively to a pre-trained classifier. Since the new class samples are drawn from an out-of-training distribution, it is natural to expect that one can classify them using our proposed metric without re-training the deep models. Motivated by this, we present a simple method which accommodates a new class at any time by simply computing the class mean of the new class and updating the tied covariance of all classes. We show that the proposed method outperforms other baseline methods, such as Euclidean distance-based classifier and re-trained softmax classifier. This evidences that our approach have a potential to apply to many other related machine learning tasks, such as active learning <ref type="bibr" target="#b7">[8]</ref>, ensemble learning <ref type="bibr" target="#b18">[19]</ref> and few-shot learning <ref type="bibr" target="#b30">[31]</ref>.</p><p>2 Mahalanobis distance-based score from generative classifier Given deep neural networks (DNNs) with the softmax classifier, we propose a simple yet effective method for detecting abnormal samples such as out-of-distribution (OOD) and adversarial ones. We first present the proposed confidence score based on an induced generative classifier under Gaussian discriminant analysis (GDA), and then introduce additional techniques to improve its performance. We also discuss how the confidence score is applicable to incremental learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Why Mahalanobis distance-based score?</head><p>Derivation of generative classifiers from softmax ones. Let x ‚àà X be an input and y ‚àà Y = {1, ¬∑ ¬∑ ¬∑ , C} be its label. Suppose that a pre-trained softmax neural classifier is given: </p><formula xml:id="formula_0">P (y = c|x) = exp(w c f (x)+bc) c exp(w c f (x)+b c )</formula><p>, where w c and b c are the weight and the bias of the softmax classifier for class c, and f (¬∑) denotes the output of the penultimate layer of DNNs. Then, without any modification on the pre-trained softmax neural classifier, we obtain a generative classifier assuming that a class-conditional distribution follows the multivariate Gaussian distribution. Specifically, we define C class-conditional Gaussian distributions with a tied covariance Œ£:</p><formula xml:id="formula_1">P (f (x)|y = c) = N (f (x)|¬µ c , Œ£) ,</formula><p>where ¬µ c is the mean of multivariate Gaussian distribution of class c ‚àà {1, ..., C}. Here, our approach is based on a simple theoretical connection between GDA and the softmax classifier: the posterior distribution defined by the generative classifier under GDA with tied covariance assumption is equivalent to the softmax classifier (see the supplementary material for more details). Therefore, the pre-trained features of the softmax neural classifier f (x) might also follow the class-conditional Gaussian distribution.</p><p>To estimate the parameters of the generative classifier from the pre-trained softmax neural classifier, we compute the empirical class mean and covariance of training samples {(x 1 , y 1 ), . . . , (x N , y N )}:</p><formula xml:id="formula_2">¬µ c = 1 N c i:yi=c f (x i ), Œ£ = 1 N c i:yi=c (f (x i ) ‚àí ¬µ c ) (f (x i ) ‚àí ¬µ c ) ,<label>(1)</label></formula><p>where N c is the number of training samples with label c. This is equivalent to fitting the classconditional Gaussian distributions with a tied covariance to training samples under the maximum likelihood estimator.</p><p>Mahalanobis distance-based confidence score. Using the above induced class-conditional Gaussian distributions, we define the confidence score M (x) using the Mahalanobis distance between test sample x and the closest class-conditional Gaussian distribution, i.e.,</p><formula xml:id="formula_3">M (x) = max c ‚àí (f (x) ‚àí ¬µ c ) Œ£ ‚àí1 (f (x) ‚àí ¬µ c ) .<label>(2)</label></formula><p>Note that this metric corresponds to measuring the log of the probability densities of the test sample.</p><p>Here, we remark that abnormal samples can be characterized better in the representation space of DNNs, rather than the "label-overfitted" output space of softmax-based posterior distribution used in the prior works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b20">21]</ref> for detecting them. It is because a confidence measure obtained from the posterior distribution can show high confidence even for abnormal samples that lie far away from the softmax decision boundary. Feinman et al. <ref type="bibr" target="#b6">[7]</ref> and Ma et al. <ref type="bibr" target="#b21">[22]</ref> process the DNN features for detecting adversarial samples in a sense, but do not utilize the Mahalanobis distance-based metric, i.e., they only utilize the Euclidean distance in their scores. In this paper, we show that Mahalanobis distance is significantly more effective than the Euclidean distance in various tasks.</p><p>Experimental supports for generative classifiers. To evaluate our hypothesis that trained features of DNNs support the assumption of GDA, we measure the classification accuracy as follows: </p><formula xml:id="formula_4">y(x) = arg min c (f (x) ‚àí ¬µ c ) Œ£ ‚àí1 (f (x) ‚àí ¬µ c ) .<label>(3)</label></formula><formula xml:id="formula_5">(f (x) ‚àí ¬µ ,c ) Œ£ ‚àí1 (f (x) ‚àí ¬µ ,c )</formula><p>Add small noise to test sample:  We remark that this corresponds to predicting a class label using the posterior distribution from generative classifier with the uniform class prior. Interestingly, we found that the softmax accuracy (red bar) is also achieved by the Mahalanobis distance-based classifier (blue bar), while conventional knowledge is that a generative classifier trained from scratch typically performs much worse than a discriminative classifier such as softmax. For visual interpretation, <ref type="figure" target="#fig_0">Figure 1</ref>(a) presents embeddings of final features from CIFAR-10 test samples constructed by t-SNE <ref type="bibr" target="#b22">[23]</ref>, where the colors of points indicate the classes of the corresponding objects. One can observe that all ten classes are clearly separated in the embedding space, which supports our intuition. In addition, we also show that Mahalanobis distance-based metric can be very useful in detecting out-of-distribution samples. For evaluation, we obtain the receiver operating characteristic (ROC) curve using a simple thresholdbased detector by computing the confidence score M (x) on a test sample x and decide it as positive (i.e., in-distribution) if M (x) is above some threshold. The Euclidean distance, which only utilizes the empirical class means, is considered for comparison. We train ResNet on CIFAR-10, and Tiny-ImageNet dataset <ref type="bibr" target="#b4">[5]</ref> is used for an out-of-distribution. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>(c), the Mahalanobis distance-based metric (blue bar) performs better than Euclidean one (green bar) and the maximum value of the softmax distribution (red bar).</p><formula xml:id="formula_6">x = x ‚àí Œµsign x (f (x) ‚àí ¬µ , c ) Œ£ ‚àí1 (f (x) ‚àí ¬µ , c ) Computing confidence score: M = max c ‚àí (f ( x) ‚àí ¬µ ,c ) Œ£ ‚àí1 (f ( x) ‚àí ¬µ ,c )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Calibration techniques</head><p>Input pre-processing. To make in-and out-of-distribution samples more separable, we consider adding a small controlled noise to a test sample. Specifically, for each test sample x, we calculate the pre-processed sample x by adding the small perturbations as follows:</p><formula xml:id="formula_7">x = x + Œµsign ( x M (x)) = x ‚àí Œµsign x (f (x) ‚àí ¬µ c ) Œ£ ‚àí1 (f (x) ‚àí ¬µ c ) ,<label>(4)</label></formula><p>where Œµ is a magnitude of noise and c is the index of the closest class. Next, we measure the confidence score using the pre-processed sample. We remark that the noise is generated to increase the proposed confidence score (2) unlike adversarial attacks <ref type="bibr" target="#b9">[10]</ref>. In our experiments, such perturbation can have stronger effect on separating the in-and out-of-distribution samples. We remark that similar input pre-processing was studied in <ref type="bibr" target="#b20">[21]</ref>, where the perturbations are added to increase the softmax score of the predicted label. However, our method is different in that the noise is generated to increase the proposed metric. </p><formula xml:id="formula_8">N C+1 i f (x i ) Compute the covariance of the new class: Œ£ C+1 ‚Üê 1 N C+1 i (f (x i ) ‚àí ¬µ C+1 )(f (x i ) ‚àí ¬µ C+1 ) Update the shared covariance: Œ£ ‚Üê C C+1 Œ£ + 1 C+1 Œ£ C+1 return Mean and covariance of all classes { ¬µ c : ‚àÄc = 1 . . . C + 1}, Œ£</formula><p>Feature ensemble. To further improve the performance, we consider measuring and combining the confidence scores from not only the final features but also the other low-level features in DNNs. Formally, given training data, we extract the -th hidden features of DNNs, denoted by f (x), and compute their empirical class means and tied covariances, i.e., ¬µ ,c and Œ£ . Then, for each test sample x, we measure the confidence score from the -th layer using the formula in <ref type="bibr" target="#b1">(2)</ref>. One can expect that this simple but natural scheme can bring an extra gain in obtaining a better calibrated score by extracting more input-specific information from the low-level features. We measure the area under ROC (AUROC) curves of the threshold-based detector using the confidence score in (2) computed at different basic blocks of DenseNet <ref type="bibr" target="#b13">[14]</ref> trained on CIFAR-10 dataset, where the overall trends on ResNet are similar. <ref type="figure" target="#fig_2">Figure 2</ref> shows the performance on various OOD samples such as SVHN <ref type="bibr" target="#b27">[28]</ref>, LSUN <ref type="bibr" target="#b31">[32]</ref>, TinyImageNet and adversarial samples generated by DeepFool <ref type="bibr" target="#b25">[26]</ref>, where the dimensions of the intermediate features are reduced using average pooling (see Section 3 for more details). As shown in <ref type="figure" target="#fig_2">Figure 2</ref>, the confidence scores computed at low-level features often provide better calibrated ones compared to final features (e.g., LSUN, TinyImageNet and DeepFool). To further improve the performance, we design a feature ensemble method as described in Algorithm 1. We first extract the confidence scores from all layers, and then integrate them by weighted averaging: Œ± M (x), where M (¬∑) and Œ± is the confidence score at the -th layer and its weight, respectively. In our experiments, following similar strategies in <ref type="bibr" target="#b21">[22]</ref>, we choose the weight of each layer Œ± by training a logistic regression detector using validation samples. We remark that such weighted averaging of confidence scores can prevent the degradation on the overall performance even in the case when the confidence scores from some layers are not effective: the trained weights (using validation) would be nearly zero for those ineffective layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Class-incremental learning using Mahalanobis distance-based score</head><p>As a natural extension, we also show that the Mahalanobis distance-based confidence score can be utilized in class-incremental learning tasks <ref type="bibr" target="#b28">[29]</ref>: a classifier pre-trained on base classes is progressively updated whenever a new class with corresponding samples occurs. This task is known to be challenging since one has to deal with catastrophic forgetting <ref type="bibr" target="#b23">[24]</ref> with a limited memory. To this end, recent works have been toward developing new training methods which involve a generative model or data sampling, but adopting such training methods might incur expensive back-and-forth costs. Based on the proposed confidence score, we develop a simple classification method without the usage of complicated training methods. To do this, we first assume that the classifier is well pre-trained with a certain amount of base classes, where the assumption is quite reasonable in many practical scenarios. <ref type="bibr" target="#b0">1</ref> In this case, one can expect that not only the classifier can detect OOD samples well, but also might be good for discriminating new classes, as the representation learned with the base classes can characterize new ones. Motivated by this, we present a Mahalanobis distance-based classifier based on (3), which tries to accommodate a new class by simply computing and updating the class mean and covariance, as described in Algorithm 2. The class-incremental adaptation of our confidence score shows its potential to be applied to a wide range of new applications in the future.  <ref type="table">Table 1</ref>: Contribution of each proposed method on distinguishing in-and out-of-distribution test set data. We measure the detection performance using ResNet trained on CIFAR-10, when SVHN dataset is used as OOD. All values are percentages and the best results are indicated in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental results</head><p>In this section, we demonstrate the effectiveness of the proposed method using deep convolutional neural networks such as DenseNet <ref type="bibr" target="#b13">[14]</ref> and ResNet <ref type="bibr" target="#b11">[12]</ref> on various vision datasets: CIFAR <ref type="bibr" target="#b14">[15]</ref>, SVHN <ref type="bibr" target="#b27">[28]</ref>, ImageNet <ref type="bibr" target="#b4">[5]</ref> and LSUN <ref type="bibr" target="#b31">[32]</ref>. Due to the space limitation, we provide the more detailed experimental setups and results in the supplementary material. Our code is available at https: //github.com/pokaxpoka/deep_Mahalanobis_detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Detecting out-of-distribution samples</head><p>Setup. For the problem of detecting out-of-distribution (OOD) samples, we train DenseNet with 100 layers and ResNet with 34 layers for classifying CIFAR-10, CIFAR-100 and SVHN datasets. The dataset used in training is the in-distribution (positive) dataset and the others are considered as OOD (negative). We only use test datasets for evaluation. In addition, the TinyImageNet (i.e., subset of ImageNet dataset) and LSUN datasets are also tested as OOD. For evaluation, we use a thresholdbased detector which measures some confidence score of the test sample, and then classifies the test sample as in-distribution if the confidence score is above some threshold. We measure the following metrics: the true negative rate (TNR) at 95% true positive rate (TPR), the area under the receiver operating characteristic curve (AUROC), the area under the precision-recall curve (AUPR), and the detection accuracy. For comparison, we consider the baseline method <ref type="bibr" target="#b12">[13]</ref>, which defines a confidence score as a maximum value of the posterior distribution, and the state-of-the-art ODIN <ref type="bibr" target="#b20">[21]</ref>, which defines the confidence score as a maximum value of the processed posterior distribution.</p><p>For our method, we extract the confidence scores from every end of dense (or residual) block of DenseNet (or ResNet). The size of feature maps on each convolutional layers is reduced by average pooling for computational efficiency: F √ó H √ó W ‚Üí F √ó 1, where F is the number of channels and H √ó W is the spatial dimension. As shown in Algorithm 1, the output of the logistic regression detector is used as the final confidence score in our case. All hyperparameters are tuned on a separate validation set, which consists of 1,000 images from each in-and out-of-distribution pair. Similar to Ma et al. <ref type="bibr" target="#b21">[22]</ref>, the weights of logistic regression detector are trained using nested cross validation within the validation set, where the class label is assigned positive for in-distribution samples and assigned negative for OOD samples. Since one might not have OOD validation datasets in practice, we also consider tuning the hyperparameters using in-distribution (positive) samples and corresponding adversarial (negative) samples generated by FGSM <ref type="bibr" target="#b9">[10]</ref>.</p><p>Contribution by each technique and comparison with ODIN. <ref type="table">Table 1</ref> validates the contributions of our suggested techniques under the comparison with the baseline method and ODIN. We measure the detection performance using ResNet trained on CIFAR-10, when SVHN dataset is used as OOD.</p><p>We incrementally apply our techniques to see the stepwise improvement by each component. One can note that our method significantly outperforms the baseline method without feature ensembles and input pre-processing. This implies that our method can characterize the OOD samples very effectively compared to the posterior distribution. By utilizing the feature ensemble and input preprocessing, the detection performance are further improved compared to that of ODIN. The left-hand column of <ref type="table" target="#tab_4">Table 2</ref> reports the detection performance with ODIN for all in-and out-of-distribution   dataset pairs. Our method outperforms the baseline and ODIN for all tested cases. In particular, our method improves the TNR, i.e., the fraction of detected LSUN samples, compared to ODIN: 41.2% ‚Üí 91.4% using DenseNet, when 95% of CIFAR-100 samples are correctly detected.</p><p>Comparison of robustness. In order to evaluate the robustness of our method, we measure the detection performance when all hyperparameters are tuned only using in-distribution and adversarial samples generated by FGSM <ref type="bibr" target="#b9">[10]</ref>. As shown in the right-hand column of <ref type="table" target="#tab_4">Table 2</ref>, ODIN is working poorly compared to the baseline method in some cases (e.g., DenseNet trained on SVHN), while our method still outperforms the baseline and ODIN consistently. We remark that our method validated without OOD but adversarial samples even outperforms ODIN validated with OOD. We also verify the robustness of our method under various training setups. Since our method utilizes empirical class mean and covariance of training samples, there is a caveat such that it can be affected by the properties of training data. In order to verify the robustness, we measure the detection performance when we train ResNet by varying the number of training data and assigning random label to training data on CIFAR-10 dataset. As shown in <ref type="figure" target="#fig_3">Figure 3</ref>, our method (blue bar) maintains high detection performances even for small number of training data or noisy one, while baseline (red bar) and ODIN (yellow bar) do not. Finally, we remark that our method using softmax neural classifier trained by standard cross entropy loss typically outperforms the ODIN using softmax neural classifier trained by confidence loss <ref type="bibr" target="#b19">[20]</ref> which involves jointly training a generator and a classifier to calibrate the posterior distribution even though training such model is computationally more expensive (see the supplementary material for more details).  positive samples to measure the performance. We use adversarial images as the negative samples generated by the following attack methods: FGSM <ref type="bibr" target="#b9">[10]</ref>, BIM <ref type="bibr" target="#b15">[16]</ref>, DeepFool <ref type="bibr" target="#b25">[26]</ref> and CW <ref type="bibr" target="#b2">[3]</ref>, where the detailed explanations can be found in the supplementary material. For comparison, we use a logistic regression detector based on combinations of kernel density (KD) <ref type="bibr" target="#b6">[7]</ref> and predictive uncertainty (PU), i.e., maximum value of posterior distribution. We also compare the state-of-theart local intrinsic dimensionality (LID) scores <ref type="bibr" target="#b21">[22]</ref>. Following the similar strategies in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b21">22]</ref>, we randomly choose 10% of original test samples for training the logistic regression detectors and the remaining test samples are used for evaluation. Using nested cross-validation within the training set, all hyper-parameters are tuned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Detecting adversarial samples</head><p>Comparison with LID and generalization analysis. The left-hand column of <ref type="table" target="#tab_6">Table 3</ref> reports the AUROC score of a logistic regression detectors for all normal and adversarial pairs. One can note that the proposed method outperforms all tested methods in most cases. In particular, ours improves the AUROC of LID from 82.2% to 95.8% when we detect CW samples using ResNet trained on the CIFAR-10 dataset. Similar to <ref type="bibr" target="#b21">[22]</ref>, we also evaluate whether the proposed method is tuned on a simple attack can be generalized to detect other more complex attacks. To this end, we measure the detection performance when we train the logistic regression detector using samples generated by FGSM. As shown in the right-hand column of <ref type="table" target="#tab_6">Table 3</ref>, our method trained on FGSM can accurately detect much more complex attacks such as BIM, DeepFool and CW. Even though LID can also generalize well, our method still outperforms it in most cases. A natural question that arises is whether the LID can be useful in detecting OOD samples. We indeed compare the performance of our method with that of LID in the supplementary material, where our method still outperforms LID in all tested case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Class-incremental learning</head><p>Setup. For the task of class-incremental learning, we train ResNet with 34 layers for classifying CIFAR-100 and downsampled ImageNet <ref type="bibr" target="#b3">[4]</ref>. As described in Section 2.3, we assume that a classifier is pre-trained on a certain amount of base classes and new classes with corresponding datasets are incrementally provided one by one. Specifically, we test two different scenarios: in the first scenario, half of CIFAR-100 classes are bases classes and the rest are new classes. In the second scenario, all classes in CIFAR-100 are considered to be base classes and 100 of ImageNet classes are new classes. All scenarios are tested five times and then averaged. Class splits are randomly generated for each trial. For comparison, we consider a softmax classifier, which is fine-tuned whenever new class data come in, and a Euclidean classifier <ref type="bibr" target="#b24">[25]</ref>, which tries to accommodate a new class by only computing the class mean. For the softmax classifier, we only update the softmax layer to achieve near-zero cost training <ref type="bibr" target="#b24">[25]</ref>, and follow the memory management in Rebuffi &amp; Kolesnikov <ref type="bibr" target="#b28">[29]</ref>: a small number of samples from old classes are kept in the limited memory, where the size of the Comparison with other classifiers. <ref type="figure" target="#fig_4">Figure 4</ref> compares the incremental learning performance of methods in terms of AUC in the two scenarios mentioned above. In each sub-figure, AUC with respect to the number of learned classes (left) and the base-new class accuracy curve after the last new classes is added (right) are drawn. Our proposed Mahalanobis distance-based classifier outperforms the other methods by a significant margin, as the number of new classes increases, although there is a crossing in the right figure of <ref type="figure" target="#fig_4">Figure 4</ref>(b) in small regimes (due to the catastrophic forgetting issue). In particular, the AUC of our proposed method is 40.0% (22.1%), which is better than 32.7% (15.6%) of the softmax classifier and 32.9% (17.1%) of the Euclidean distance classifier after all new classes are added in the first (second) experiment. We also report the experimental results in the supplementary material for the case when classes of CIFAR-100 are base classes and those of CIFAR-10 are new classes, where the overall trend is similar. The experimental results additionally demonstrate the superiority of our confidence score, compared to other plausible ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we propose a simple yet effective method for detecting abnormal test samples including both out-of-distribution and adversarial ones. In essence, our main idea is inducing a generative classifier under LDA assumption, and defining new confidence score based on it. With calibration techniques such as input pre-processing and feature ensemble, our method performs very strongly across multiple tasks: detecting out-of-distribution samples, detecting adversarial attacks and classincremental learning. We also found that our proposed method is more robust in the choice of its hyperparameters as well as against extreme scenarios, e.g., when the training dataset has some noisy, random labels or a small number of data samples. We believe that our approach have a potential to apply to many other related machine learning tasks, e.g., active learning <ref type="bibr" target="#b7">[8]</ref>, ensemble learning <ref type="bibr" target="#b18">[19]</ref> and few-shot learning <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material: A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks A Preliminaries for Gaussian discriminant analysis</head><p>In this section, we describe the basic concept of the discriminative and generative classifier <ref type="bibr" target="#b26">[27]</ref>. Formally, denote the random variable of the input and label as x ‚àà X and y ‚àà Y = {1, ¬∑ ¬∑ ¬∑ , C}, respectively. For the classification task, the discriminative classifier directly defines a posterior distribution P (y|x), i.e., learning a direct mapping between input x and label y. A popular model for discriminative classifier is softmax classifier which defines the posterior distribution as follows:</p><formula xml:id="formula_9">P (y = c|x) = exp(w c x+bc) c exp(w c x+b c )</formula><p>, where w c and b c are weights and bias for a class c, respectively.</p><p>In contrast to the discriminative classifier, the generative classifier defines the class conditional distribution P (x|y) and class prior P (y) in order to indirectly define the posterior distribution by specifying the joint distribution P (x, y) = P (y) P (x|y). Gaussian discriminant analysis (GDA) is a popular method to define the generative classifier by assuming that the class conditional distribution follows the multivariate Gaussian distribution and the class prior follows Bernoulli distribution:</p><formula xml:id="formula_10">P (x|y = c) = N (x|¬µ c , Œ£ c ) , P (y = c) = Œ≤c c Œ≤ c ,</formula><p>where ¬µ c and Œ£ c are the mean and covariance of multivariate Gaussian distribution, and Œ≤ c is the unnormalized prior for class c. This classifier has been studied in various machine learning areas (e.g., semi-supervised learning <ref type="bibr" target="#b16">[17]</ref> and incremental learning <ref type="bibr" target="#b28">[29]</ref>).</p><p>In this paper, we focus on the special case of GDA, also known as the linear discriminant analysis (LDA). In addition to Gaussian assumption, LDA further assumes that all classes share the same covariance matrix, i.e., Œ£ c = Œ£. Since the quadratic term is canceled out with this assumption, the posterior distribution of generative classifier can be represented as follows: </p><formula xml:id="formula_11">= exp ¬µ c Œ£ ‚àí1 x ‚àí 1 2 ¬µ c Œ£ ‚àí1 ¬µ c + log Œ≤ c c exp ¬µ c Œ£ ‚àí1 x ‚àí 1 2 ¬µ c Œ£ ‚àí1 ¬µ c + log Œ≤ c .</formula><p>One can note that the above form of posterior distribution is equivalent to the softmax classifier by considering ¬µ c Œ£ ‚àí1 and ‚àí 1 2 ¬µ c Œ£ ‚àí1 ¬µ c + log Œ≤ c as weight and bias of it, respectively. This implies that x might be fitted in Gaussian distribution during training a softmax classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Experimental setup</head><p>In this section, we describe detailed explanation about all the experiments described in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Experimental setups in detecting out-of-distribution</head><p>Detailed model architecture and training. We consider two state-of-the-art neural network architectures: DenseNet <ref type="bibr" target="#b13">[14]</ref> and ResNet <ref type="bibr" target="#b11">[12]</ref>. For DenseNet, our model follows the same setup as in Huang &amp; Liu <ref type="bibr" target="#b13">[14]</ref>: 100 layers, growth rate k = 12 and dropout rate 0. Also, we use ResNet with 34 layers and dropout rate 0. <ref type="bibr" target="#b1">2</ref> The softmax classifier is used, and each model is trained by minimizing the cross-entropy loss using SGD with Nesterov momentum. Specifically, we train DenseNet for 300 epochs with batch size 64 and momentum 0.9. For ResNet, we train it for 200 epochs with batch size 128 and momentum 0.9. The learning rate starts at 0.1 and is dropped by a factor of 10 at 50% and 75% of the training progress, respectively. The test set errors of DenseNet and ResNet on CIFAR-10, CIFAR-100 and SVHN are reported in <ref type="table">Table 4</ref>.</p><p>Datasets. We train DenseNet and ResNet for classifying CIFAR-10 (or 100) and SVHN datasets: the former consists of 50,000 training and 10,000 test images with 10 (or 100) image classes, and the latter consists of 73,257 training and 26,032 test images with 10 digits. <ref type="bibr" target="#b2">3</ref> The corresponding test dataset is used as the in-distribution (positive) samples to measure the performance. We use realistic images as the out-of-distribution (negative) samples: the TinyImageNet consists of 10,000 test images with 200 image classes from a subset of ImageNet images. The LSUN consists of 10,000 test images of 10 different scenes. We downsample each image of TinyImageNet and LSUN to size <ref type="bibr">32 √ó 32. 4</ref> Tested methods. In this paper, we consider the baseline method <ref type="bibr" target="#b12">[13]</ref> and ODIN <ref type="bibr" target="#b20">[21]</ref> for comparison. The confidence score in Hendrycks &amp; Gimpel <ref type="bibr" target="#b12">[13]</ref> is a maximum value of softmax posterior distribution, i.e., max y P (y|x). The key idea of ODIN is the temperature scaling which is defined as follows:</p><formula xml:id="formula_12">P (y = y|x; T ) = exp (f y (x)/T ) y exp (f y (x)/T ) ,</formula><p>where T &gt; 0 is the temperature scaling parameter and f = (f 1 , . . . , f K ) is final feature vector of deep neural networks. For each data x, ODIN first calculates the pre-processed image x by adding the small perturbations as follows:</p><p>x = x ‚àí Œµ odin sign (‚àí x log P Œ∏ (y = y|x; T )) , where Œµ odin is a magnitude of noise and y is the predicted label. Next, ODIN feeds the pre-processed data into the classifier, computes the maximum value of scaled predictive distribution, i.e., max y P Œ∏ (y|x ; T ), and classifies it as positive (i.e., in-distribution) if the confidence score is above some threshold Œ¥. For ODIN, the perturbation noise Œµ odin is chosen from {0, 0.0005, 0.001, 0.0014, 0.002, 0.0024, 0.005, 0.01, 0.05, 0.1, 0.2}, and the temperature T is chosen from {1, 10, 100, 1000}.</p><p>Hyper parameters for our method. There are two hyper parameters in our method: the magnitude of noise in (4) and layer indexes for feature ensemble. For all experiments, we extract the confidence scores from every end of dense (or residual) block of DenseNet (or ResNet). The size of feature maps on each convolutional layers is reduced by average pooling for computational efficiency: F √ó H √ó W ‚Üí F √ó1, where F is the number of channels and H√óW is the spatial dimension. The magnitude of noise in (4) is chosen from {0, 0.0005, 0.001, 0.0014, 0.002, 0.0024, 0.005, 0.01, 0.05, 0.1, 0.2}.</p><p>Performance metrics. For evaluation, we measure the following metrics to measure the effectiveness of the confidence scores in distinguishing in-and out-of-distribution images.</p><p>‚Ä¢ True negative rate (TNR) at 95% true positive rate (TPR). Let TP, TN, FP, and FN denote true positive, true negative, false positive and false negative, respectively. We measure TNR = TN / (FP+TN), when TPR = TP / (TP+FN) is 95%. ‚Ä¢ Area under the receiver operating characteristic curve (AUROC). The ROC curve is a graph plotting TPR against the false positive rate = FP / (FP+TN) by varying a threshold. ‚Ä¢ Area under the precision-recall curve (AUPR). The PR curve is a graph plotting the precision = TP / (TP+FP) against recall = TP / (TP+FN) by varying a threshold. AUPR-IN (or -OUT) is AUPR where in-(or out-of-) distribution samples are specified as positive. ‚Ä¢ Detection accuracy. This metric corresponds to the maximum classification probability over all possible thresholds Œ¥:</p><formula xml:id="formula_13">1 ‚àí min Œ¥ P in (q (x) ‚â§ Œ¥) P (x is from P in ) + P out (q (x) &gt; Œ¥) P (x is from P out ) ,</formula><p>where q(x) is a confident score. We assume that both positive and negative examples have equal probability of appearing in the test set, i.e., P (x is from P in ) = P (x is from P out ).</p><p>Note that AUROC, AUPR and detection accuracy are threshold-independent evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Experimental setups in detecting adversarial samples</head><p>Adversarial attacks. For the problem of detecting adversarial samples, we consider the following attack methods: fast gradient sign method (FGSM) <ref type="bibr" target="#b9">[10]</ref>, basic iterative method (BIM) <ref type="bibr" target="#b15">[16]</ref>, Deep-Fool <ref type="bibr" target="#b25">[26]</ref> and Carlini-Wagner (CW) <ref type="bibr" target="#b2">[3]</ref>. The FGSM directly perturbs normal input in the direction of the loss gradient. Formally, non-targeted adversarial examples are constructed as x adv = x + Œµ F GSM sign ( x (y * , P (y|x))) , CW 0.08 0.00% 0.08 0.01% 0.15 0.04% <ref type="table">Table 4</ref>: The L ‚àû mean perturbation and classification accuracy on clean and adversarial samples.</p><p>where Œµ F GSM is a magnitude of noise, y * is the ground truth label and is a loss function to measure the distance between the prediction and the ground truth. The BIM is an iterative version of FGSM, which applies FGSM multiple times with a smaller step size. Formally, non-targeted adversarial examples are constructed as</p><formula xml:id="formula_14">x 0 adv = x, x n+1 adv = Clip Œµ BIM x {x n adv + Œ± BIM sign x n adv (y * , P (y|x n adv )) },</formula><p>where Clip Œµ BIM x means we clip the resulting image to be within the Œµ BIM -ball of x. DeepFool works by finding the closest adversarial examples with geometric formulas. CW is an optimization-based method which arguably the most effective method. Formally, non-targeted adversarial examples are constructed as arg min</p><formula xml:id="formula_15">x adv Œªd(x, x adv ) ‚àí (y * , P (y|x adv )),</formula><p>where Œª is penalty parameter and d(¬∑, ¬∑) is a metric to quantify the distance between an original image and its adversarial counterpart. However, compared to FGSM and BIM, this method is much slower in practice. For all experiments, L 2 distance is used as a constraint. We used the library from FaceBook <ref type="bibr" target="#b10">[11]</ref> for generating adversarial samples. <ref type="bibr" target="#b4">5</ref>  <ref type="table">Table 4</ref> tatistics of adversarial attacks including the L ‚àû mean perturbation and classification accuracy on adversarial attacks.</p><p>Tested methods. Ma et al. <ref type="bibr" target="#b21">[22]</ref> proposed to characterize adversarial subspaces by using local intrinsic dimensionality (LID). Given a test sample x, LID is defined as follows:</p><formula xml:id="formula_16">LID = ‚àí 1 k i log r i (x) r k (x) ,</formula><p>where r i (x) denotes the distance between x and its i-th nearest neighbor within a sample of points drawn from in-distribution, and r k (x) denotes the maximum distance among k nearest neighbors. We commonly extract the LID scores from every end of dense (or residual) block of DenseNet (or ResNet) similar to ours. Given test sample x and the set X c of training samples with label c, the Gaussian kernel density with bandwidth œÉ is defined as follows:</p><formula xml:id="formula_17">KD(x) = 1 |X c | xi‚ààXc k œÉ (x i , x),</formula><p>where k œÉ (x, y) ‚àù exp(‚àí||x ‚àí y|| 2 /œÉ 2 ). For LID and KD, we used the library from Ma et al. <ref type="bibr" target="#b21">[22]</ref>.</p><p>Hyper-parameters and training. Following the similar strategies in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b21">22]</ref>  parameter for KD, the number of nearest neighbors for LID, and input noise for our method are tuned. Specifically, the value of k is chosen from {10, 20, 30, 40, 50, 60, 70, 80, 90} with respect to a minibatch of size 100, and the bandwidth was chosen from {0.1, 0.25, 0.5, 0.75, 1}. The magnitude of noise in (4) is chosen from {0, 0.0005, 0.001, 0.0014, 0.002, 0.0024, 0.005, 0.01, 0.05, 0.1, 0.2}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C More experimental results</head><p>In this section, we provide more experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Robustness of our method in detecting adversarial samples</head><p>In order to verify the robustness, we measure the detection performance when we train ResNet by varying the number of training data and assigning random label to training data on CIFAR-10 dataset. As shown in <ref type="figure">Figure 5</ref>, our method (blue bar) outperforms LID (green bar) for all experiments.</p><p>C.2 Class-incremental learning <ref type="figure">Figure 6</ref> compares the AUCs of tested methods when CIFAR-100 is pre-trained and CIFAR-10 is used as new classes. Our proposed Mahalanobis distance-based classifier outperforms the other methods by a significant margin, as the number of new classes increases. The AUC of our proposed method is 47.7%, which is better than 41.0% of the softmax classifier and 43.0% of the Euclidean distance classifier after all new classes are added.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Experimental results on joint confidence loss</head><p>In addition, we remark that the proposed detector using softmax neural classifier trained by standard cross entropy loss typically outperforms the ODIN detector using softmax neural classifier trained by confidence loss <ref type="bibr" target="#b18">[19]</ref> which involves jointly training a generator and a classifier to calibrate the posterior distribution. Also, our detector provides further improvement if one use it with model trained by confidence loss. In other words, our proposed method can improve any pre-trained softmax neural classifier.    <ref type="figure" target="#fig_8">Figure 8 and 9</ref> shows the performance of the ODIN <ref type="bibr" target="#b20">[21]</ref>, LID <ref type="bibr" target="#b21">[22]</ref> and Mahalanobis detector for each in-and out-of-distribution pair. We remark that the proposed method outperforms all tested methods.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 LID for detecting out-of-distribution samples</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Evaluation on ImageNet dataset</head><p>In this section, we verify the performance of the proposed method using the ImageNet 2012 classification dataset <ref type="bibr" target="#b4">[5]</ref> that consists of 1000 classes. The models are trained on the 1.28 million training images, and evaluated on the 50k validation images. For all experiments, we use the pre-trained ResNet <ref type="bibr" target="#b11">[12]</ref> which is available at https://github.com/pytorch/vision/blob/ master/torchvision/models/resnet.py. First, we measure the classification accuracy of generative classifier from the pre-trained model as follows:</p><formula xml:id="formula_18">y(x) = arg min c (f (x) ‚àí ¬µ c ) Œ£ ‚àí1 (f (x) ‚àí ¬µ c ) + log Œ≤ c ,</formula><p>where Œ≤ c = Nc N is an empirical class prior. We remark that this corresponds to predicting a class label using the posterior distribution from generative with LDA assumption. <ref type="table" target="#tab_13">Table 7</ref> shows the top-1 classification accuracy on ImageNet 2012 dataset. One can note that the proposed generative classifier can perform reasonably well even though the softmax classifier outperforms it in all cases. However, we remark that the gap between them is decreasing as the training accuracy increases, i.e., the pre-trained model learned more strong representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Softmax <ref type="formula">(</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Review comment</head><p>‚Ä¢ Generating the garbage images matching mean &amp; variance</p><p>‚Ä¢ Initialize random vector X ‚Ä¢ Find X which optimize the following objective (final layer attack):</p><p>‚Ä¢ Experimental results</p><p>‚Ä¢ Model: ResNet-34 trained on CIFAR-10 (Success rate: 89.00 %)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>Class 0 class 1. class 2. class 3. class 4. class 5. class 6. class 7. class 8. class 9. <ref type="figure" target="#fig_0">Figure 11</ref>: The generated garbage sample and its target class.</p><p>detecting adversarial samples generated by FGSM <ref type="bibr" target="#b9">[10]</ref> and BIM <ref type="bibr" target="#b15">[16]</ref>. Similar to Section 3.2, we extract the confidence scores from every end of residual block of ResNet. <ref type="figure" target="#fig_0">Figure 10</ref>(a) and <ref type="bibr">10(b)</ref> show the performance of various detectors. One can note that the proposed Mahalanobis distancebased detector outperforms all tested methods including LID. These results imply that our method can be working well for the large-scale datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Adaptive attacks against Mahalanobis distance-based detector</head><p>In this section, we evaluate the robustness of our method by generating the garbage images which may fool the Mahalanobis distance-based detector in a white-box setting, i.e., one can access to the parameters of the classifier and that of the Mahalanobis distance-based detector. Here, we remark that accessing the parameters of the Mahalanobis distance-based detector, i.e., sample means and covariance, is not mild assumption since the information about training data is required to compute them. To attack our method, we generate a garbage images x g by minimizing the Mahalanobis distance as follows:</p><p>arg min</p><formula xml:id="formula_19">xg (f (x g ) ‚àí ¬µ c ) Œ£ ‚àí1 (f (x g ) ‚àí ¬µ c ) ,</formula><p>where c is a target class. We test two different scenarios using ResNet with 34 layers trained on CIFAR-10 dataset. In the first scenario, we generate the garbage images only using a penultimate layer of DNNs. In the second scenario, we attack every end of residual block of ResNet. <ref type="figure" target="#fig_0">Figure 11</ref> shows the generated samples by minimizing the Mahalanobis distance. Even though the generated sample looks like the random noise, it successfully fools the pre-trained classifier, i.e., it is classified as the target class. We measure the detection performance of the baseline <ref type="bibr" target="#b12">[13]</ref>, ODIN <ref type="bibr" target="#b20">[21]</ref>, LID <ref type="bibr" target="#b21">[22]</ref> and the proposed Mahalanobis distance-based detector. As shown in <ref type="figure" target="#fig_0">Figure 10</ref>(c) and 10(d), our method can distinguish CIFAR-10 test and garbage images for both scenarios better than the tested methods. In particular, we remark that the input pre-processing is very useful in detecting such garbage samples. These results imply that our proposed method is robust to the attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Hybrid inference of generative and discriminative classifiers</head><p>In this paper, we show that the generative classifier can be very useful in characterizing the abnormal samples such as OOD and adversarial samples. Here, a caveat is that the generative classifier might degrade the classification performance. In order to handle this issue, we introduce a hybrid inference of generative and discriminative classifiers. Given a generative classifier with GDA assumptions, the posterior distribution of generative classifier via Bayes rule is given as: To match this with a standard softmax classifier's weights, the parameters of the generative classifier have to satisfy the following conditions:</p><formula xml:id="formula_20">P</formula><formula xml:id="formula_21">¬µ c = Œ£w c , log Œ≤ c ‚àí 0.5¬µ c Œ£ ‚àí1 ¬µ c = b c ,</formula><p>where w c and b c are weights and bias for a class c, respectively. Using the empirical covariance Œ£ as shown in <ref type="formula" target="#formula_2">(1)</ref>, one can induce the parameters of another generative classifier which has same decision boundary with the softmax classifier as follows:</p><formula xml:id="formula_22">¬µ c = Œ£w c ,Œ≤ c = exp(0.5Œº c Œ£ ‚àí1Œº c ‚àí b c ) c exp(0.5Œº c Œ£ ‚àí1Œº c ‚àí b c )</formula><p>.</p><p>Here, we normalize theŒ≤ c to satisfy cŒ≤ c = 1. Then, using this generative classifier, we define new hybrid posterior distribution which combines the softmax-and sample-based generative classifiers:</p><formula xml:id="formula_23">P h (y|x) = exp Œª ¬µ c Œ£ ‚àí1 x ‚àí 0.5 ¬µ c Œ£ ‚àí1 ¬µ c + log Œ≤ c + (1 ‚àí Œª) Œº c Œ£ ‚àí1 x ‚àí 0.5Œº c Œ£ ‚àí1Œº c + logŒ≤ c c exp Œª ¬µ c Œ£ ‚àí1 x ‚àí 0.5 ¬µ c Œ£ ‚àí1 ¬µ c + log Œ≤ c + (1 ‚àí Œª) Œº c Œ£ ‚àí1 x ‚àí 0.5Œº c Œ£ ‚àí1Œº c + logŒ≤ c ,</formula><p>where Œª ‚àà [0, 1] is a hyper-parameter. This hybrid model can be interpreted as ensemble of softmax and generative classifiers, and one can expect that it can improve the classification performance. <ref type="table" target="#tab_15">Table 8</ref> compares the classification accuracy of softmax, generative and hybrid classifiers. One can note that the hybrid model improves the classification accuracy, where we determine the optimal tuning parameter between the two objectives using the validation set. We also remark that such hybrid model can be useful in detecting the abnormal samples, where we pursue these tasks in the future.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Experimental results under the ResNet with 34 layers. (a) Visualization of final features from ResNet trained on CIFAR-10 by t-SNE, where the colors of points indicate the classes of the corresponding objects. (b) Classification test set accuracy of ResNet on CIFAR-10, CIFAR-100 and SVHN datasets. (c) Receiver operating characteristic (ROC) curves: the x-axis and y-axis represent the false positive rate (FPR) and true positive rate (TPR), respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>AUROC (%) of threshold-based detector using the confidence score in (2) computed at different basic blocks of DenseNet trained on CIFAR-10 dataset. We measure the detection performance using (a) TinyImageNet, (b) LSUN, (c) SVHN and (d) adversarial (DeepFool) samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Comparison of AUROC (%) under extreme scenarios: (a) small number of training data, where the x-axis represents the number of training data. (b) Random label is assigned to training data, where the x-axis represents the percentage of training data with random label.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Experimental results of class-incremental learning on CIFAR-100 and ImageNet datasets. In each experiment, we report (left) AUC with respect to the number of learned classes and, (right) the base-new class accuracy curve after the last new classes is added. memory is matched with that for keeping the parameters for Mahalanobis distance-based classifier. Namely, the number of old exemplars kept for training the softmax classifier is chosen as the sum of the number of learned classes and the dimension (512 in our experiments) of the hidden features. For evaluation, similar to<ref type="bibr" target="#b17">[18]</ref>, we first draw base-new class accuracy curve by adjusting an additional bias to the new class scores, and measure the area under curve (AUC) since averaging base and new class accuracy may cause an imbalanced measure of the performance between base and new classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>P</head><label></label><figDesc>(y = c|x) = P (y = c) P (x|y = c) c P (y = c ) P (x|y = c )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 : 10 Figure 6 :</head><label>5106</label><figDesc>Comparison of AUROC (%) under different training data. To evaluate the robustness of proposed method, we train ResNet (a) by varying the number of training data and (b) assigning random label to training data on CIFAR-10 dataset. Base: CIFAR-100 / New: CIFAR-Experimental results of class-incremental learning on CIFAR-100 and CIFAR-10 datasets. We report (left) AUC with respect to the number of learned classes and, (right) the base-new class accuracy curve after the last new classes is added.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Cross entropy loss + ODIN Cross entropy loss + baselineCross entropy loss + Mahalanobis (ours) Joint confidence loss + ODIN Joint confidence loss + baseline Joint confidence loss + Mahalanobis (ours) Performances of the baseline detector<ref type="bibr" target="#b12">[13]</ref>, ODIN detector<ref type="bibr" target="#b20">[21]</ref> and Mahalanobis detector under various training losses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Distinguishing in-and out-of-distribution test set data for image classification using DenseNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>2 Figure 10 :</head><label>210</label><figDesc>(a)/(b) Distinguishing clean and adversarial samples using ResNet with 18 layers on ImageNet 2012 validation set. (c)/(d) Distinguishing clean and garbage samples using ResNet 18 layers trained on CIFAR-10 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>(y = c|x) = P (y = c) P (x|y = c) c P (y = c ) P (x|y = c )= exp ¬µ c Œ£ ‚àí1 x ‚àí 1 2 ¬µ c Œ£ ‚àí1 ¬µ c + log Œ≤ c c exp ¬µ c Œ£ ‚àí1 x ‚àí 1 2 ¬µ c Œ£ ‚àí1 ¬µ c + log Œ≤ c .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1 Computing the Mahalanobis distance-based confidence score. Input: Test sample x, weights of logistic regression detector Œ± , noise Œµ and parameters of Gaussian distributions { ¬µ ,c , Œ£ : ‚àÄ , c} Initialize score vectors: M(x) = [M : ‚àÄ ] for each layer ‚àà 1, . . . , L do Find the closest class: c = arg min c</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Algorithm 2 Updating Mahalanobis distance-based classifier for class-incremental learning. Input: set of samples from a new class {x i : ‚àÄi = 1 . . . N C+1 }, mean and covariance of observed classes { ¬µ c : ‚àÄc = 1 . . . C}, Œ£ Compute the new class mean: ¬µ C+1 ‚Üê 1</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>/ 86.2 / 90.8 89.9 / 95.5 / 98.1 83.2 / 91.4 / 93.9 40.2 / 70.5 / 89.6 89.9 / 92.8 / 97.6 83.2 / 86.5 / 92.6 TinyImageNet 58.9 / 92.4 / 95.0 94.1 / 98.5 / 98.8 88.5 / 93.9 / 95.0 58.9 / 87.1 / 94.9 94.1 / 97.2 / 98.8 88.5 / 92.1 / 95.0 LSUN 66.6 / 96.2 / 97.2 95.4 / 99.2 / 99.3 90.3 / 95.7 / 96.3 66.6 / 92.9 / 97.2 95.4 / 98.5 / 99.2 90.3 / 94.3 / 96.2 / 70.6 / 82.5 82.7 / 93.8 / 97.2 75.6 / 86.6 / 91.5 26.7 / 39.8 / 62.2 82.7 / 88.2 / 91.8 75.6 / 80.7 / 84.6 TinyImageNet 17.6 / 42.6 / 86.6 71.7 / 85.2 / 97.4 65.7 / 77.0 / 92.2 17.6 / 43.2 / 87.2 71.7 / 85.3 / 97.0 65.7 / 77.2 / 91.8 LSUN 16.7 / 41.2 / 91.4 70.8 / 85.5 / 98.0 64.9 / 77.1 / 93.9 16.7 / 42.1 / 91.4 70.8 / 85.7 / 97.9 64.9 / 77.3 / 93.8 / 71.7 / 96.8 91.9 / 91.4 / 98.9 86.6 / 85.8 / 95.9 69.3 / 69.3 / 97.5 91.9 / 91.9 / 98.8 86.6 / 86.6 / 96.3 TinyImageNet 79.8 / 84.1 / 99.9 94.8 / 95.1 / 99.9 90.2 / 90.4 / 98.9 79.8 / 79.8 / 99.9 94.8 / 94.8 / 99.8 90.2 / 90.2 / 98.9 LSUN 77.1 / 81.1 / 100 94.1 / 94.5 / 99.9 89.1 / 89.2 / 99.3 77.1 / 77.1 / 100 94.1 / 94.1 / 99.9 89.1 / 89.1 / 99.2 / 86.6 / 96.4 89.9 / 96.7 / 99.1 85.1 / 91.1 / 95.8 32.5 / 40.3 / 75.8 89.9 / 86.5 / 95.5 85.1 / 77.8 / 89.1 TinyImageNet 44.7 / 72.5 / 97.1 91.0 / 94.0 / 99.5 85.1 / 86.5 / 96.3 44.7 / 69.6 / 95.5 91.0 / 93.9 / 99.0 85.1 / 86.0 / 95.4 LSUN 45.4 / 73.8 / 98.9 91.0 / 94.1 / 99.7 85.3 / 86.7 / 97.7 45.4 / 70.0 / 98.1 91.0 / 93.7 / 99.5 85.3 / 85.8 / 97.2 / 62.7 / 91.9 79.5 / 93.9 / 98.4 73.2 / 88.0 / 93.7 20.3 / 12.2 / 41.9 79.5 / 72.0 / 84.4 73.2 / 67.7 / 76.5 TinyImageNet 20.4 / 49.2 / 90.9 77.2 / 87.6 / 98.2 70.8 / 80.1 / 93.3 20.4 / 33.5 / 70.3 77.2 / 83.6 / 87.9 70.8 / 75.9 / 84.6 LSUN 18.8 / 45.6 / 90.9 75.8 / 85.6 / 98.2 69.9 / 78.3 / 93.5 18.8 / 31.6 / 56.6 75.8 / 81.9 / 82.3 69.9 / 74.6 / 79.7 98.4 92.9 / 92.1 / 99.3 90.0 / 89.4 / 96.9 78.3 / 79.8 / 94.1 92.9 / 92.1 / 97.6 90.0 / 89.4 / 94.6 TinyImageNet 79.0 / 82.1 / 99.9 93.5 / 92.0 / 99.9 90.4 / 89.4 / 99.1 79.0 / 80.5 / 99.2 93.5 / 92.9 / 99.3 90.4 / 90.1 / 98.8 LSUN 74.3 / 77.3 / 99.9 91.6 / 89.4 / 99.9 89.0 / 87.2 / 99.5 74.3 / 76.3 / 99.9 91.6 / 90.7 / 99.9 89.0 / 88.2 / 99.5</figDesc><table><row><cell>In-dist (model)</cell><cell>OOD</cell><cell>Validation on OOD samples TNR at TPR 95% AUROC Detection acc. Baseline [13] / ODIN [21] / Mahalanobis (ours)</cell><cell>Validation on adversarial samples TNR at TPR 95% AUROC Detection acc. Baseline [13] / ODIN [21] / Mahalanobis (ours)</cell></row><row><cell cols="3">CIFAR-10 (DenseNet) 40.2 CIFAR-100 SVHN (DenseNet) SVHN 26.7 SVHN (DenseNet) CIFAR-10 69.3 CIFAR-10 (ResNet) SVHN 32.5 CIFAR-100 (ResNet) SVHN 20.3 SVHN CIFAR-10 78.3 / 79.8 /</cell><cell></cell></row><row><cell>(ResNet)</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Distinguishing in-and out-of-distribution test set data for image classification under various validation setups. All values are percentages and the best results are indicated in bold.</figDesc><table><row><cell>100</cell><cell>Out-of-distribution: SVHN</cell><cell>100</cell><cell>Out-of-distribution: TinyImageNet</cell><cell>100</cell><cell>Out-of-distribution: SVHN</cell><cell>100</cell><cell>Out-of-distribution: TinyImageNet</cell></row><row><cell>90</cell><cell></cell><cell>90</cell><cell></cell><cell>90</cell><cell></cell><cell>90</cell><cell></cell></row><row><cell>80</cell><cell></cell><cell>80</cell><cell></cell><cell>80</cell><cell></cell><cell>80</cell><cell></cell></row><row><cell>70</cell><cell>Baseline ODIN</cell><cell>70</cell><cell>Baseline ODIN</cell><cell>70</cell><cell>Baseline ODIN</cell><cell>70</cell><cell>Baseline ODIN</cell></row><row><cell>60</cell><cell>Mahalanobis</cell><cell>60</cell><cell>Mahalanobis</cell><cell>60</cell><cell>Mahalanobis</cell><cell>60</cell><cell>Mahalanobis</cell></row><row><cell></cell><cell>5K 10K 20K 30K 40K 50K</cell><cell></cell><cell>5K 10K 20K 30K 40K 50K</cell><cell></cell><cell>0% 10% 20% 30% 40%</cell><cell></cell><cell>0% 10% 20% 30% 40%</cell></row><row><cell></cell><cell cols="3">(a) Small number of training data</cell><cell></cell><cell cols="3">(b) Training data with random labels</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Setup. For the problem of detecting adversarial samples, we train DenseNet and ResNet for classifying CIFAR-10, CIFAR-100 and SVHN datasets, and the corresponding test dataset is used as the</figDesc><table><row><cell>Model</cell><cell>Dataset (model)</cell><cell>Score</cell><cell cols="3">Detection of known attack FGSM BIM DeepFool</cell><cell>CW</cell><cell cols="3">Detection of unknown attack FGSM (seen) BIM DeepFool</cell><cell>CW</cell></row><row><cell></cell><cell></cell><cell>KD+PU [7]</cell><cell>85.96 96.80</cell><cell>68.05</cell><cell cols="2">58.72</cell><cell>85.96</cell><cell>3.10</cell><cell>68.34</cell><cell>53.21</cell></row><row><cell></cell><cell>CIFAR-10</cell><cell>LID [22]</cell><cell>98.20 99.74</cell><cell>85.14</cell><cell cols="2">80.05</cell><cell>98.20</cell><cell>94.55</cell><cell>70.86</cell><cell>71.50</cell></row><row><cell></cell><cell></cell><cell cols="2">Mahalanobis (ours) 99.94 99.78</cell><cell>83.41</cell><cell cols="2">87.31</cell><cell>99.94</cell><cell>99.51</cell><cell>83.42</cell><cell>87.95</cell></row><row><cell></cell><cell></cell><cell>KD+PU [7]</cell><cell>90.13 89.69</cell><cell>68.29</cell><cell cols="2">57.51</cell><cell>90.13</cell><cell>66.86</cell><cell>65.30</cell><cell>58.08</cell></row><row><cell>DenseNet</cell><cell>CIFAR-100</cell><cell>LID [22]</cell><cell>99.35 98.17</cell><cell>70.17</cell><cell cols="2">73.37</cell><cell>99.35</cell><cell>68.62</cell><cell>69.68</cell><cell>72.36</cell></row><row><cell></cell><cell></cell><cell cols="2">Mahalanobis (ours) 99.86 99.17</cell><cell>77.57</cell><cell cols="2">87.05</cell><cell>99.86</cell><cell>98.27</cell><cell>75.63</cell><cell>86.20</cell></row><row><cell></cell><cell></cell><cell>KD+PU [7]</cell><cell>86.95 82.06</cell><cell>89.51</cell><cell cols="2">85.68</cell><cell>86.95</cell><cell>83.28</cell><cell>84.38</cell><cell>82.94</cell></row><row><cell></cell><cell>SVHN</cell><cell>LID [22]</cell><cell>99.35 94.87</cell><cell>91.79</cell><cell cols="2">94.70</cell><cell>99.35</cell><cell>92.21</cell><cell>80.14</cell><cell>85.09</cell></row><row><cell></cell><cell></cell><cell cols="2">Mahalanobis (ours) 99.85 99.28</cell><cell>95.10</cell><cell cols="2">97.03</cell><cell>99.85</cell><cell>99.12</cell><cell>93.47</cell><cell>96.95</cell></row><row><cell></cell><cell></cell><cell>KD+PU [7]</cell><cell>81.21 82.28</cell><cell>81.07</cell><cell cols="2">55.93</cell><cell>83.51</cell><cell>16.16</cell><cell>76.80</cell><cell>56.30</cell></row><row><cell></cell><cell>CIFAR-10</cell><cell>LID [22]</cell><cell>99.69 96.28</cell><cell>88.51</cell><cell cols="2">82.23</cell><cell>99.69</cell><cell>95.38</cell><cell>71.86</cell><cell>77.53</cell></row><row><cell></cell><cell></cell><cell cols="2">Mahalanobis (ours) 99.94 99.57</cell><cell>91.57</cell><cell cols="2">95.84</cell><cell>99.94</cell><cell>98.91</cell><cell>78.06</cell><cell>93.90</cell></row><row><cell></cell><cell></cell><cell>KD+PU [7]</cell><cell>89.90 83.67</cell><cell>80.22</cell><cell cols="2">77.37</cell><cell>89.90</cell><cell>68.85</cell><cell>57.78</cell><cell>73.72</cell></row><row><cell>ResNet</cell><cell>CIFAR-100</cell><cell>LID [22]</cell><cell>98.73 96.89</cell><cell>71.95</cell><cell cols="2">78.67</cell><cell>98.73</cell><cell>55.82</cell><cell>63.15</cell><cell>75.03</cell></row><row><cell></cell><cell></cell><cell cols="2">Mahalanobis (ours) 99.77 96.90</cell><cell>85.26</cell><cell cols="2">91.77</cell><cell>99.77</cell><cell>96.38</cell><cell>81.95</cell><cell>90.96</cell></row><row><cell></cell><cell></cell><cell>KD+PU [7]</cell><cell>82.67 66.19</cell><cell>89.71</cell><cell cols="2">76.57</cell><cell>82.67</cell><cell>43.21</cell><cell>84.30</cell><cell>67.85</cell></row><row><cell></cell><cell>SVHN</cell><cell>LID [22]</cell><cell>97.86 90.74</cell><cell>92.40</cell><cell cols="2">88.24</cell><cell>97.86</cell><cell>84.88</cell><cell>67.28</cell><cell>76.58</cell></row><row><cell></cell><cell></cell><cell cols="2">Mahalanobis (ours) 99.62 97.15</cell><cell>95.73</cell><cell cols="2">92.15</cell><cell>99.62</cell><cell>95.39</cell><cell>72.20</cell><cell>86.73</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Comparison of AUROC (%) under various validation setups. For evaluation on unknown attack, FGSM samples denoted by "seen" are used for validation. For our method, we use both feature ensemble and input pre-processing. The best results are indicated in bold.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>, we randomly choose 10% of original test samples for training the logistic regression detectors and the remaining test samples are used for evaluation. The training sets consists of three types of examples: adversarial, normal and noisy. Here, noisy examples are generated by adding random noise to normal examples. Using nested cross validation within the training set, all hyper-parameters including the bandwidth</figDesc><table><row><cell>100</cell><cell cols="4">Adversarial attack: FGSM</cell><cell>100</cell><cell></cell><cell cols="4">Adversarial attack: BIM</cell><cell>100</cell><cell></cell><cell cols="4">Adversarial attack: DeepFool</cell><cell>100</cell><cell></cell><cell cols="4">Adversarial attack: CW</cell><cell></cell></row><row><cell>90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>5K</cell><cell>10K</cell><cell>20K</cell><cell>30K</cell><cell>40K</cell><cell>50K</cell><cell>5K</cell><cell>10K</cell><cell>20K</cell><cell>30K</cell><cell>40K</cell><cell>50K</cell><cell>5K</cell><cell>10K</cell><cell>20K</cell><cell>30K</cell><cell>40K</cell><cell>50K</cell><cell>5K</cell><cell>10K</cell><cell>20K</cell><cell>30K</cell><cell>40K</cell><cell>50K</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="16">(a) Small training data: the x-axis represents the number of training data</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>100</cell><cell cols="4">Adversarial attack: FGSM</cell><cell>100</cell><cell></cell><cell cols="3">Adversarial attack: BIM</cell><cell></cell><cell>100</cell><cell></cell><cell cols="4">Adversarial attack: DeepFool</cell><cell>100</cell><cell></cell><cell cols="3">Adversarial attack: CW</cell><cell></cell><cell></cell></row><row><cell>90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0%</cell><cell>10%</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell><cell></cell><cell>0%</cell><cell>10%</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell><cell></cell><cell>0%</cell><cell>10%</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell><cell></cell><cell>0%</cell><cell>10%</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell><cell></cell></row><row><cell></cell><cell cols="22">(b) Noisy training data: the x-axis represents the percentage of training data with random label</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Distinguishing in-and out-of-distribution test set data for image classification when we tune the hyper-parameters of ODIN and our method only using in-distribution and adversarial (FGSM) samples. All values are percentages and boldface values indicate relative the better results.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>Top-1 accuracy (%) of ResNets on ImageNet 2012 dataset.</figDesc><table /><note>Next, we also evaluate the detection performance of the Mahalanobis distance-based detector on ImageNet 2012 dataset using ResNets with 18 layers. For evaluation, we consider the problem of</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 8 :</head><label>8</label><figDesc>Classification test set accuracy (%) of DenseNet and ResNet on CIFAR-10, CIFAR-100 and SVHN datasets.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For example, state-of-the-art CNNs trained on large-scale image dataset are off-the-shelf<ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14]</ref>, so they are a starting point in many computer vision tasks<ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">25]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">ResNet architecture is available at https://github.com/kuangliu/pytorch-cifar.<ref type="bibr" target="#b2">3</ref> We do not use the extra SVHN dataset for training.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">LSUN and TinyImageNet datasets are available at https://github.com/ShiyuLiang/odin-pytorch.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">The code is available at https://github.com/facebookresearch/adversarial_image_defenses.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Comparison with ODIN</head><p>In-dist (model)</p><p>Out-of-dist TNR at TPR 95% AUROC Detection accuracy AUPR in AUPR out Baseline <ref type="bibr" target="#b12">[13]</ref> / ODIN <ref type="bibr" target="#b20">[21]</ref>   <ref type="table">Table 5</ref>: Distinguishing in-and out-of-distribution test set data for image classification. We tune the hyper-parameters using validation set of in-and out-of-distributions. All values are percentages and the best results are indicated in bold.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep speech 2: End-to-end speech recognition in english and mandarin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anubhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rishita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jingliang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Case</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guoliang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christiano</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Man√©</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.06565</idno>
		<title level="m">Concrete problems in ai safety</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adversarial examples are not easily detected: Bypassing ten detection methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM workshop on AISec</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A downsampled variant of imagenet as an alternative to the cifar datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patryk</forename><surname>Chrabaszcz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.08819</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li-Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust physical-world attacks on machine learning models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Evtimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eykholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kevin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Earlence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kohno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tadayoshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Rahmati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Detecting adversarial samples from artifacts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reuben</forename><surname>Feinman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">R</forename><surname>Curtin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Shintre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">B</forename><surname>Gardner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00410</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep bayesian active learning with image data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riashat</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mayank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Ciss√©</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00117</idno>
		<title level="m">Laurens. Countering adversarial images using input transformations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaiming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiangyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A baseline for detecting misclassified and out-ofdistribution examples in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adversarial examples in the physical world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Samy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.02533</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Principled hybrids of generative and discriminative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><forename type="middle">A</forename><surname>Lasserre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minka</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hierarchical novelty detection for visual object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kibok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kimin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kyle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Honglak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Confident multiple choice learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kimin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Changho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoungsoo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Training confidence-calibrated classifiers for detecting out-of-distribution samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kimin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Honglak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kibok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Principled detection of out-of-distribution examples in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shiyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Characterizing adversarial subspaces using local intrinsic dimensionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yisen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><forename type="middle">M</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wijewickrema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sudanthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Houle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenebeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Catastrophic interference in connectionist networks: The sequential learning problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neal</forename><forename type="middle">J</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Psychology of learning and motivation</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Distance-based image classification: Generalizing to new classes at near-zero cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deepfool: a simple and accurate method to fool deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moosavi</forename><surname>Dezfooli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyed</forename><surname>Mohsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alhussein</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Machine learning: a probabilistic perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">icarl: Incremental classifier and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sylvestre-Alvise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmood</forename><surname>Sharif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sruti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lujo</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">K</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGSAC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oriol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yinda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shuran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Lsun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03365</idno>
		<title level="m">Construction of a large-scale image dataset using deep learning with humans in the loop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
