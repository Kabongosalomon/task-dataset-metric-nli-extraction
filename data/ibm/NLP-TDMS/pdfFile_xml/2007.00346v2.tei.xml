<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A NOVEL HIGHER-ORDER WEISFEILER-LEHMAN GRAPH CONVOLUTION PREPRINT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Damke</surname></persName>
							<email>cdamke@mail.upb.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">Heinz Nixdorf Institute</orgName>
								<orgName type="institution" key="instit2">Paderborn University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitalik</forename><surname>Melnikov</surname></persName>
							<email>melnikov@mail.upb.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">Heinz Nixdorf Institute</orgName>
								<orgName type="institution" key="instit2">Paderborn University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eyke</forename><surname>Hüllermeier</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">Heinz Nixdorf Institute</orgName>
								<orgName type="institution" key="instit2">Paderborn University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A NOVEL HIGHER-ORDER WEISFEILER-LEHMAN GRAPH CONVOLUTION PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>graph neural networks · Weisfeiler-Lehman test · cycle detection</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Current graph neural network (GNN) architectures use a vertex neighborhood aggregation scheme, which limits their discriminative power to that of the 1dimensional Weisfeiler-Lehman (WL) graph isomorphism test. Here, we propose a novel graph convolution operator that is based on the 2-dimensional WL test. We formally show that the resulting 2-WL-GNN architecture is more discriminative than existing GNN approaches. This theoretical result is complemented by experimental studies using synthetic and real data. On multiple common graph classification benchmarks, we demonstrate that the proposed model is competitive with state-of-the-art graph kernels and GNNs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In 1-WL, a vertex color is refined by combining the colors of neighboring vertices. In k-WL, the color of a k-tuple s ∈ V k G is refined by combining the colors of its neighborhood, which is defined as the set of all k-tuples in which at most one vertex differs from s. Note that each vertex k-tuple has one neighbor for each u ∈ V G , each of which is a k-tuple of vertex k-tuples. For k = 2, this means that each potential edge (v, w) ∈ V 2 G has all possible walks of length 2 from v to w as its neighbors. Even though k-WL refines k-tuple colors, lower-dimensional structures still get their own colors, since a tuple does not have to consist of distinct vertices: In k-WL, the color of a single vertex v ∈ V G is described byχ G,k (s) for s = (v, . . . , v) ∈ V k G ; similarly, every possible edge e ij ∈ V 2 G has at least one color that can encode the adjacency information, i.e., whether e ij ∈ E G .  The output of the k-WL algorithm is the color distribution distχ G,k . Since the way in which WL colorings are refined is vertex order invariant, any difference in the final color distribution of two graphs always implies the non-isomorphism ( ) of the colored graphs, i.e., G k-WL H =⇒ G H. <ref type="figure" target="#fig_1">Figure 1</ref> shows that the opposite does however not necessarily hold. Additionally, it highlights the inability of 1-WL to detect cycles of varying lengths in graphs.</p><p>Definition 2.8. We say that k-WL detects m-cycles iff. k-WL d m , where d m : G → {0, 1} is an indicator function, that determines whether a given graph contains at least one m-cycle.</p><p>Intuitively, Def. 2.8 describes cycle detection as the ability to solve the corresponding decision problem given only the color distributions distχ G,k for all G ∈ G. As already mentioned in the introduction, to detect cycles of length m ≤ 7, a WL dimension of k ≥ 2 is required <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Neural Networks</head><p>In this paper, we will focus on so-called spatial GNNs, which are expressible in terms of repeated vertex neighborhood aggregations. Such a GNN takes a graph G ∈ G with vertex feature vectors</p><p>x G [v] ∈ R d (0) as input; those features are typically represented as a matrix Z (0)</p><formula xml:id="formula_0">G :=    x G [v 1 ] . . . x G [v n ]    ∈</formula><p>R n×d (0) , where n := |V G |. A GNN convolves this vertex feature matrix via a stack of graph convolution operators S (t) :</p><formula xml:id="formula_1">R n×d (t−1) → R n×d (t) s.t. Z (t) G := S (t) (Z (t−1) G ). We use Z (t) G [v] ∈ R d (t) to denote the row vector of Z (t)</formula><p>G , which represents the convolved vertex features of v ∈ V G . After applying T convolutional layers, the convolved vertex features Z (T )</p><formula xml:id="formula_2">G [v]</formula><p>can be used directly for node classification problems, or they can be combined via a pooling layer, e.g., an elementwise mean, to obtain a global graph feature vector which in turn can be used to solve graph classification and regression (GC/GR) problems. In the rest of this paper, GNNs will be discussed in the context of GC/GR.</p><p>To get an intuition for how GNNs relate to the WL algorithm, one should think of the vertex feature vectors Z (t) G as a continuous generalization of WL colors χ (t) G,1 . The graph convolution operators S (t) then directly correspond to WL color refinement steps. This intuition was recently formalized by Xu et al. <ref type="bibr" target="#b9">[10]</ref>, who showed the following upper bound on the discriminative power of GNNs. Proposition 2.9. Any GNN that convolves vertex feature vectors via a convolution operator of the form Z (t)</p><formula xml:id="formula_3">G [v] = h (t) Z (t−1) G [v], { {Z (t−1) G [u] | u ∈ Γ G (v)} } is at most as discriminative as 1-WL, where h (t) : R d (t−1) * → R d (t)</formula><p>is an arbitrary vertex neighborhood hashing function. Moreover, iff.</p><p>h (t) is injective, the GNN has the same DP as 1-WL.</p><p>Among others, this bound applies to the GCN <ref type="bibr" target="#b7">[8]</ref> and GraphSAGE <ref type="bibr" target="#b8">[9]</ref> architectures. Since those approaches use a non-injective hashing function h <ref type="bibr">(t)</ref> , their DP turns out to be strictly lower than that of 1-WL. On the other hand, the graph isomorphism network (GIN) <ref type="bibr" target="#b9">[10]</ref> convolution operator achieves injectivity through the use of a multilayer perceptron (MLP), and therefore has the same DP as 1-WL (cf. Def. 2.5):</p><formula xml:id="formula_4">Z (t) G [v] := MLP (t)   (1 + ε)Z (t−1) G [v] + u∈Γ G (v) Z (t−1) G [u]   with some irrational ε &gt; 0. (1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Limitations of an Existing 2-GNN</head><p>The idea to extend GNNs along the lines of the higher-order WL algorithm, which we shall elaborate on in Section 4 below, is not entirely new. Morris et al. <ref type="bibr" target="#b15">[16]</ref> recently proposed the so-called k-GNN, which adapts the discrete k-WL refinement step (see Def. 2.6) to the continuous convolution setting. However, it turns out that k-GNNs do not preserve some of the desirable properties of k-WL. In particular, unlike 2-WL, 2-GNNs cannot count or even detect m-cycles in graphs. In this section, we give a proof of this statement.</p><p>Similar to k-WL, a k-GNN iteratively refines/convolves the colors/features of combinations of k vertices. To reduce runtime complexity, k-GNNs assign feature vectors to vertex k-multisets instead of k-tuples. Additionally, only a "local" neighborhood of each multiset is considered in k-GNN convolutions, whereas in k-WL each tuple has a "global" neighborhood of size n = |V G |, one neighbor for each vertex u ∈ V G (cf. Def. 2.6). As we will see next, the main difference between k-GNNs and k-WL lies in their respective notion of "neighborhood". More specifically, since 2-WL already has a significantly higher DP than 1-WL, we will analyze how the DP of 2-GNNs compares to that of 1-WL and 2-WL.</p><p>2-GNNs define the neighbors of an edge e ij = (v i , v j ) to be the edges that are incident to either v i or v j . In 2-WL, on the other hand, the neighbors of e ij are the edge pairs {(e il , e lj )} v l ∈V G , i.e., all possible walks of length two that start at v i and end at v j . This difference becomes clear when comparing the definition of convolution in 2-GNNs with that of color refinement in 2-WL:</p><formula xml:id="formula_5">2-GNN 1 : Z (t) G [e ij ] = σ Z (t−1) G [e ij ]W (t) +   v l ∈Γ G (vj ) Z (t−1) G [e il ] + i =j ∧ v l ∈Γ G (vi) Z (t−1) G [e lj ]   W (t) Γ (2) 2-WL: χ (t) G,2 (e ij ) = h χ (t−1) G,2 (e ij ), { {(χ (t−1) G,2 (e il ), χ (t−1) G,2 (e lj )) | v l ∈ V G } } 1</formula><p>Here, σ denotes some nonlinear activation function and</p><formula xml:id="formula_6">W (t) , W (t) Γ ∈ R d (t−1) ×d (t)</formula><p>the weight matrices of the convolution operator. To highlight the relationship between 2-GNNs and 2-WL, a 2-GNN definition that uses two sums over v l ∈ ΓG(vj) resp. v l ∈ ΓG(vi) is shown; this is equivalent to a single sum over the features of the edges</p><formula xml:id="formula_7">{(u, w) ∈ EG | u = vi ∨ w = vj} [see 16]. B B C B A A B C C C 2-GNN 2-GNN 2-WL 2-WL D A B B C C D A B C B C F A B B C C E A B C B C Figure 2</formula><p>: Two edge colorings on which 2-GNNs and 2-WL behave differently. A 2-GNN will refine the "color vector" of e ij to D for both initial colorings. 2-WL on the other hand differentiates both colorings by preserving the color tuple information.</p><p>In order to analyze what those different notions of neighborhood imply for the DP of 2-GNNs in comparison to 2-WL, we first show that the DP of 2-GNNs on all graphs G ∈ G is less than or equal to that of 1-WL on the so-called edge neighborhood graphs G E ∈ G E .</p><formula xml:id="formula_8">Definition 3.1. The edge neighborhood graph of a given graph G = (V G , E G ) is defined as G E := (V G E , E G E ) with the vertices V G E := {{ {v, u} } | (v, u) ∈ E G ∨ v = u} and the edges E G E := (e, e ) ∈ V 2 G E | |e ∩ e | = 1 . Proposition 3.2. The DP of all 2-GNNs h 2 : G → Y is less than or equal to that of 1-WL on edge neighborhood graphs, i.e., ∀ G, H ∈ G : G E 1-WL H E → h 2 (G) = h 2 (H). Proof. By Def. 3.1, Γ G E (e ij ) = {(u, w) ∈ E G | u = v i ∨ w = v j } for all e ij ∈ E G .</formula><p>Therefore, the 2-GNN convolution in Eq. (2) can be rewritten as a vertex neighborhood convolution Proof. Let G and H be two d-regular graphs of size n. Their corresponding edge neighborhood graphs G E and H E both have n E = n + nd 2 vertices, n of which correspond to the vertices of G and H respectively; we will refer to them as loop vertices L G /L H . The remaining nd 2 edge neighborhood vertices correspond to the edges of G and H; we will refer to them as edge vertices</p><formula xml:id="formula_9">Z (t) G [e] = σ   Z (t−1) G [e]W (t) + e ∈Γ G E (e) Z (t−1) G [e ]W<label>(</label></formula><formula xml:id="formula_10">E G /E H .</formula><p>W.l.o.g. we define the initial colors of the loop vertices as</p><formula xml:id="formula_11">χ (0) (v) = A for all v ∈ L G ∪ L H .</formula><p>The initial colors of the edge vertices are defined as } }) =: D. This means that χ (0) and χ <ref type="bibr" target="#b0">(1)</ref> are identical up to the color substitutions A → C and B → D, i.e. χ (0) ≡ χ <ref type="bibr" target="#b0">(1)</ref> , which in turn implies that 1-WL terminates after one iteration. <ref type="bibr">Lemma</ref>  As previously mentioned, the DP of a model by itself is not necessarily relevant for real-world GC/GR problems. However, 2-WL is not only more discriminative than 1-WL, but is also able to detect and count the number of m-cycles in a given graph for all m ≤ 7. We now show that 2-GNNs not only have a lower DP than 2-WL, but are also unable to detect cycles. Proof. Let n be the lowest common multiple of 3 and some m &gt; 3. We define c 3 := n 3 and c m := n m . Based on that, we define the following two graphs: Let G 3 be a graph consisting of c 3 disconnected cycles of length 3, analogously let G m be a graph consisting of c m disconnected cycles of length m. Since both G 3 and G m are 2-regular and have the size n, any 2-GNN h 2 : G → Y must map both of them to the same y ∈ Y by Prop. 3.4, i.e., G 3 h2 G m .</p><formula xml:id="formula_12">χ (0) (e) = B for all e ∈ E G ∪ E H . Note that each loop vertex { {v i , v i } } with v i ∈ V G ∪ V H has d neighbors, the edges incident to v i . Similarly, each edge vertex { {v i , v j } } has</formula><p>Let us assume that h 2 is able to detect cycles of length 3, i.e. triangles. Following Def. 2.8, this would imply that h 2 is at least as discriminative as the triangle detection function</p><formula xml:id="formula_13">d 3 : G → {0, 1}. It follows that d 3 (G 3 ) = 1 ∧ d 3 (G m ) = 0 ⇒ G 3 d3 G m h2 d3 = ==== ⇒ G 3 h2 G m ,</formula><p>which is a contradiction. Conversely, assuming that h 2 is able to detect cycles of length m &gt; 3, the m-cycle detection function d m also distinguishes G 3 and G m , which again results in the contradiction</p><formula xml:id="formula_14">G 3 h2 G m ∧ G 3 h2 G m .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The 2-WL Graph Convolution Operator</head><p>In the previous section, we compared 2-GNNs with the 2-WL algorithm and found that the former have a significantly lower DP than the latter. Motivated by this limitation, we devote this section to a novel, more discriminative convolution operator, which is inspired by the higher-order WL algorithm and meant to overcome the limitations of 1-WL. Our operator is inspired by 2-WL but uses the following simplifications to reduce its computational cost.</p><p>Similar to k-GNNs, or more specifically, 2-GNNs, our novel operator refines/convolves the feature vectors of 2-multisets { {v i , v j } } instead of refining/convolving the feature vectors of 2tuples (v i , v j ). This simplification halves the number of feature vectors without affecting the DP because we assume that graphs are undirected, i.e. e ij and e ji have identical feature vectors x[e ij ] = x[e ji ] ∈ X E and the same 2-WL neighborhood. To simplify the notation, we assume that e ij = e ji = { {v i , v j } } in the rest of the paper.</p><p>After applying the 2-multiset simplification, the 2-WL algorithm refines the color of all multisets e ij ∈ V 2 G by hashing its current color and the colors of all neighbors {{ {e il , e lj } }} v l ∈V G . This means that the time complexity of a single refinement step is O(n 3 ) for n := |V G |, which quickly becomes infeasible for large graphs. To address this issue, we reduce both the number of colored edges as well as the number of neighbors of each edge. This is achieved by only considering the edges that are part of the so-called r-th power of a graph G, where r ∈ N is the freely choosable neighborhood radius.</p><p>Definition 4.1. The r-th power of a graph G is defined as</p><formula xml:id="formula_15">G r := V G , e ij ∈ V 2 G | d SP,G (v i , v j ) ≤ r , where d SP,G (v i , v j ) is the length of the shortest path between v i and v j in G. The distance of a vertex v i ∈ V G to itself is defined as d SP,G (v i , v i ) := 0. Note that G 1 does not generally equal G because G 1 has self-loop edges e ii ∈ E G 1 at all vertices.</formula><p>For the neighborhood radius r = 1, only the self-loop edges {e ii } vi∈V G and the edges E G are considered; for r &gt; 1, edges between indirectly connected vertices are considered as well.</p><p>Through the reduction of the considered edges, the neighbors of each e ij ∈ E G r are in turn reduced to the common r-neighbors</p><formula xml:id="formula_16">of v i and v j , i.e. {{ {e il , e lj } } | v l ∈ Γ G r (v i ) ∩ Γ G r (v j )}.</formula><p>Let us now consider what the reduced number of used edges and the reduced number of edge neighbors implies for the runtime of a refinement step. If G is a sparse graph with the maximum vertex degree d :</p><formula xml:id="formula_17">= max v∈V G |Γ G (v)|, the number of considered edges is bounded by O(nd r ),</formula><p>where each edge has at most O(d r ) neighbors. Consequently, the time complexity of a refinement step becomes O(nd 2r ), which is a significant improvement over the O(n 3 ) bound of a full 2-WL refinement step (assuming d n).</p><p>Based on the 2-multiset and the neighborhood localization simplifications, we now define the 2-WL convolution operator and the corresponding 2-WL-GNN. </p><formula xml:id="formula_18">v i ] ∈ X V = R d V as well as the edge features x[e ij ] ∈ X E = R d E of a given graph G. More specifically, Z (0) G ∈ R |E G r |×(d V +d E ) assigns a row vector Z (0)</formula><p>G [e ij ] to all edges e ij ∈ E G r . Those initial edge feature vectors are defined by the following vector concatenation (⊕):</p><formula xml:id="formula_19">Z (0) G [e ij ] := x[v i ] if i = j 0 else ⊕ x[e ij ] if e ij ∈ E G 0 else Definition 4.3.</formula><p>We define the 2-WL graph convolution operator as</p><formula xml:id="formula_20">Z (t) G [e ij ] := σ   Z (t−1) G [e ij ]W (t) L + v l ∈Γ G r (vi)∩Γ G r (vj ) κ (t) Z (t−1) G [e ij ], { {Z (t−1) G [e il ], Z (t−1) G [e lj ]} }   , with κ (t) (z ij , { {z il , z lj } }) := z ij W (t) F σ Γ (z il + z lj ) W (t) Γ .</formula><p>This operator is parameterized by the three matrices W</p><formula xml:id="formula_21">(t) L , W (t) F , W (t) Γ ∈ R d (t−1) ×d (t)</formula><p>and uses two freely choosable activation functions σ and σ Γ . We use to denote element-wise multiplication. In the following, we will analyze the DP of GNNs using the 2-WL convolution operator. Our goal is to show that such 2-WL-GNNs have a strictly higher DP than 1-WL. We begin by proving that 2-WL-GNNs are at least as discriminative as 1-WL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 4.4.</head><p>A GNN h 1 : G → Y uses weighted vertex neighborhood sums if its convolutional layers can be described by</p><formula xml:id="formula_22">Z (t) G [v i ] = MLP (t)   w ii Z (t−1) G [v i ] + vj ∈Γ G (vi) w ij Z (t−1) G [v l ]   .</formula><p>Definition 4.4 includes GCNs <ref type="bibr" target="#b7">[8]</ref>, where the MLP only consists of a single layer with weights</p><formula xml:id="formula_23">w ij = (|Γ G (v i )| + 1) − 1 2 (|Γ G (v j )| + 1) − 1 2 .</formula><p>GINs also trivially satisfy the definition (see Eq. <ref type="formula" target="#formula_9">(1)</ref>).</p><formula xml:id="formula_24">Theorem 4.5. For each GNN h 1 using weighted vertex neighborhood sums, there is a 2-WL-GNN h 2 that simulates h 1 , i.e., such that ∀G ∈ G : h 1 (G) = h 2 (G).</formula><p>Proof. We prove by construction. Let G ∈ G be an arbitrary input graph with n := |V G | and</p><formula xml:id="formula_25">m := n + |E G |. By definition, h 1 is a GNN of the form Pool 1 (Conv 1 (G)), where Conv 1 is a stack of T weighted vertex neighborhood sum convolutions S (t) : R n×d (t−1) → R n×d (t) T t=1</formula><p>with each corresponding MLP (t) having K layers. Pool 1 combines the vertex feature vectors produced by Conv 1 . Let h 2 be a GNN of the form Pool 2 (Conv 2 (G)), where Conv 2 is a stack of</p><formula xml:id="formula_26">(2 + K)T 2-WL convolution layers S (t,k) : R m×d (t,k−1) → R m×d (t,k) (t,k)∈[T ]×[2+K]</formula><p>(see <ref type="figure" target="#fig_5">Fig. 3</ref>) with the neighborhood radius r := 1. The layers S (t,2+K) T t=1 produce the feature matrices Z (t,2+K) = Z (t+1,0) which are fed as input into the successor layer S (t+1,1) . </p><formula xml:id="formula_27">Pool 1 ({ {z ij | z ij = ϕ Z (T,2+K) G [e ij ] ∧ e ij ∈ E G 1 ∧ z ij = nil} }). Theorem 4.5 then follows if there is a function ϕ s.t. ∀v i ∈ V G : Conv 1 (G)[v i ] = ϕ(Conv 2 (G)[e ii ]) and ∀e ij ∈ E G : ϕ(Conv 2 (G)[e ij ]) = nil.</formula><p>To guarantee that there is such a function ϕ, we now inductively prove the following three invariants, which have to hold for all t ∈ {0, . . . , T }: to decide whether that vector should be mapped to nil.</p><formula xml:id="formula_28">(P1) Z (t,2+K) G [e ij ] 1 = 1[i = j], i.e.,</formula><formula xml:id="formula_29">(P2) Z (t,2+K) G [e ii ] 2,...,(d (t) +1) = Z (t) G [v i ],</formula><p>i.e., the second to (1 + d (t) )-th components of each self-loop feature vector in h 2 contain the corresponding convolved vertex feature vector at layer t in h 1 .</p><formula xml:id="formula_30">(P3) Z (t,2+K) G [e ij ] d (t) +2 = w ij , i.e.</formula><p>, the weights for the vertex neighborhood sums are encoded in the edge and self-loop feature vectors.</p><p>For t = 0, all invariants apply to the initial feature matrix Z</p><formula xml:id="formula_31">(0,2+K) G = Z (1,0) G by Def. 4.2: ∀v i ∈ V G : Z (1,0) G [e ii ] := (1) ⊕ x[v i ] ⊕ (w ii ) and ∀e ij ∈ E G : Z (1,0) G [e ij ] := (0) ⊕ 0 ⊕ (w ij ).</formula><p>Assuming the invariants hold for t − 1, we now show that they also hold for t. The layers S (t,1) and S (t,2) are used to compute the weighted vertex neighborhood sums</p><formula xml:id="formula_32">Z (t,2) [e ii ] 2,...,(1+d (t−1) ) = w ii Z (t,0) [e ii ] 2,...,(1+d (t−1) ) + vj ∈Γ G (vi) w ij Z (t,0) [e jj ] 2,...,(1+d (t−1) ) .</formula><p>We now explicitly define parameter matrices for S (t,1) and S (t,2) s.t. this weighted sum is produced. Note that the weighted vertex neighborhood sum only requires scalar multiplication and vector addition, i.e., the d (t−1) vertex feature dimensions are mutually independent. W.l.o.g. this allows us to simplify notation by treating the vertex feature vectors as if they were scalars in the following definitions, i.e., we can assume</p><formula xml:id="formula_33">d (t−1) = 1 and Z (t,0) [e ii ] = (1, Z (t−1) [v i ], w ii ) ∈ R 3 .</formula><p>Using this simplification, the layer S (t,1) is defined by</p><formula xml:id="formula_34">Z (t,1) [e ij ] = Z (t,0) [e ij ]W (t,1) L + v l ∈Γ G 1 (vi)∩Γ G 1 (vj ) Z (t,0) [e ij ]W (t,1) F Z (t,0) [e il ] + Z (t,0) [e lj ] W (t,1) Γ =      (1, 0, w ii , 0) + 0, 0, 0, 2w ii Z (t−1) [v i ] + v l ∈Γ G 1 (vi) 0, 2 2 Z (t−1) [v i ]w il , 0, 0 if i = j (0, 0, w ij , 0) + 0, 0, 0, w ij (Z (t−1) [v i ] + 0) + 0, 0, 0, w ij (0 + Z (t−1) [v j ]) else =          1, v l ∈Γ G 1 (vi) w il Z (t−1) [v i ], w ii , 2w ii Z (t−1) [v i ]   if i = j 0, 0, w ij , w ij (Z (t−1) [v i ] + Z (t−1) [v j ]) else , with W (t,1) L :=   1 0 0 0 0 0 0 0 0 0 1 0   , W (t,1) F :=   0 0 0 0 0 1 2 0 0 0 0 0 1   , W (t,1) Γ :=   0 0 0 0 0 0 0 1 0 1 0 0   .</formula><p>The vertex neighborhood summation is completed via S (t,2) , which is defined by <ref type="figure">Figure 4</ref>: Intuition for how S (t,1) and S (t,2) compute vertex neighborhood sums in two steps. For simplicity, weights are ignored, i.e. all w ij = 1.</p><formula xml:id="formula_35">Z (t,2) [e ij ] =          1, − v l ∈Γ G 1 (vi) w il Z (t−1) [v i ], w ii   + v l ∈Γ G 1 (vi) 0, w il Z (t−1) [v i ] + Z (t−1) [v l ] , 0 if i = j (0, 0, w ij ) else =          1, w ii Z (t−1) [v i ] + v l ∈Γ G (vi) w il Z (t−1) [v l ], w ii   if i = j (0, 0, w ij ) else , with W (t,2) L :=     1 0 0 0 −1 0 0 0 1 0 0 0     , W (t,2) F :=     0 1 2 0 0 0 0 0 0 0 0 0 0     , W (t,2) Γ :=     0 0 0 0 0 0 0 0 0 0 1 0     . C A B 0 0 C, C 2 A, A 3 C C B B A A A A A A -3 + ( + ) + ( + ) + ( + ) = + + A A B B B B B -2 + ( + ) + ( + ) = + A A C C C C C -2 + ( + ) + ( + ) = + B, B 2 B + A 0 0 C + A</formula><formula xml:id="formula_36">For each self-loop/edge e ij ∈ E G 1 , the set of localized 2-WL neighbors Γ G 1 (v i ) ∩ Γ G 1 (v j )</formula><p>is shown in the middle, after the first step. The colors in this illustration are unrelated to the colored parts in the previous equations.</p><p>Using the two layers S (t,1) and S (t,2) that we just defined, the weighted vertex neighborhood sum for all</p><formula xml:id="formula_37">v i ∈ V G is contained in Z (t,2) [e ii ]. Additionally, for all e ij ∈ E G 1 , the indicators Z (t,2) [e ij ] 1 = 1[i = j] and the weights Z (t,2) [e ij ] d (t) +2 = w ij are preserved.</formula><p>This means that invariants (P1) and (P3) are satisfied after S (t,2) .</p><p>To complete the induction step, it now remains to show that all three invariants hold after applying the layers S (t,2+1) , . . . , S (t,2+K) . Note that a 2-WL convolution layer is reduced to a fully connected layer if W (t) F = 0. Via the universal approximation theorem <ref type="bibr" target="#b16">[17]</ref>, we can therefore use S (t,2+1) , . . . , S (t,2+K) to simulate the K layers of MLP (t) without changing the first and last dimension of each feature vector to preserve invariants (P1) and (P3). The resulting feature matrix Z (t,2+K) then satisfies all three invariants, which completes the induction.</p><p>Using invariants (P1) and (P2) for t = T , we can therefore set</p><formula xml:id="formula_38">ϕ Z (T,2+K) G [e ij ] := Z (T,2+K) G [e ij ] 2,...,(d T +1) if Z (T,2+K) G [e ij ] 1 = 1 nil else .</formula><p>By our previous definition of Pool 2 , this in turn implies that Proof. The corollary directly follows from the fact that 2-WL-GNNs can simulate GINs by Thm. 4.5 and the fact that GINs have the same DP as 1-WL by Prop. 2.9, because they use injective vertex neighborhood hashing functions.</p><formula xml:id="formula_39">Pool 2 (Z (T,2+K) G ) = Pool 1 (Z (T ) G ) ⇐⇒ h 2 (G) = h 1 (G), which concludes the proof.</formula><p>To complete our analysis of the DP of the 2-WL-GNN, we now show that it is not just as discriminative as 1-WL but is in fact more discriminative than 1-WL. Proof. The proposition follows if we choose the six-cycle graph for G and the two three-cycles graph for H (see <ref type="figure" target="#fig_1">Fig. 1</ref>). Let h 2 = Pool • S be a 2-WL-GNN with the neighborhood radius r = 1, which consists of a single 2-WL convolution layer S : R * ×2 → R * ×1 and the pooling layer Pool = min. In accordance with Def. 4.2, we set the initial feature vectors of the vertices v i of G and H to Z (0) [e ii ] := (1, 0) and the initial feature vectors of their edges e ij to Z (0) [e ij ] := (0, 1).</p><p>Let the weight matrices of S be W L := 0 and W F = W Γ This concludes our analysis of the discriminative power of 2-WL-GNNs. The key insight in this section is that 2-WL-GNNs are more discriminative than all vertex neighborhood aggregation GNNs, because the DP of the latter is at most that of 1-WL. Additionally, we can conclude that 2-WL-GNNs are able to distinguish graphs that are indistinguishable by 2-GNNs due to Prop. 3.4 and 4.7.</p><p>Note that no statement regarding the DP of 2-WL-GNNs compared to 2-WL was made. It is easy to see that 2-WL-GNNs generally cannot have the same power as 2-WL due to the neighborhood localization simplification: For a small neighborhood radius of r = 1, nonexistent edges e ij / ∈ E G do not have a feature vector; those missing feature vectors are however required by the proof of 2-WL's m-cycle counting ability for m ≥ 4 [see the proof by 13, Lem. 1 and Thm. 2]. We leave a thorough discussion of the relation between 2-WL-GNNs and 2-WL for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>In our experimental evaluation, we compare the proposed 2-WL convolution layer with other state-of-the-art approaches. We focus on two types of learners: SVMs using graph kernels and GNNs. We evaluate those learners by comparing their test accuracies on multiple binary classification problems. To obtain those accuracies, we use the graph classification benchmarking framework recently proposed by Errica et al. <ref type="bibr" target="#b17">[18]</ref>: We use 10-fold stratified training/test splits; for each split the hyperparameters are tuned via a second 90%/10% validation holdout split of the training data. Experiments were run three times to smooth out differences caused by random weight initialization. To train models that require gradient-based optimization, we use the well-known Adam optimizer and the standard binary cross-entropy loss.</p><p>Using this assessment strategy, we evaluate SVMs with the following graph kernels: WL ST , WL SP , and the so-called 2-LWL and 2-GWL kernels <ref type="bibr" target="#b18">[19]</ref>; the last two are essentially 2-WL variants of the WL ST kernel. We additionally evaluate the following GNNs: 2-WL-GNN (our method), GIN, 2-GNN, and a structure-unaware baseline that applies an MLP to each vertex feature vector x G [v], sums up the resulting vectors and applies another MLP to the sum [see 18].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Synthetic Data</head><p>We begin with an evaluation on a synthetic binary classification dataset, which demonstrates the potential advantages of a higher dimensional WL method. To determine the classes of the graphs in this dataset, a learner has to solve the following unicolored triangle detection problem: Given a graph G with vertices that are colored as either l G [v] = A or as l G [v] = B, the learner has to find the unique triangle</p><formula xml:id="formula_40">(v i , v j , v k ) in G for which l G [v i ] = l G [v j ] = l G [v k ].</formula><p>The class of G is then determined by the color of the vertices (v i , v j , v k ).</p><p>Based on this problem, we generated a synthetic triangle detection dataset. It contains randomly generated graphs with varying vertex counts and vertex color proportions. We use this dataset to evaluate whether a learner is able to ignore varying amounts of noisy random structure and focus on relevant local substructures, in this case unicolored triangles. For the evaluation of 2-WL-GNNs the neighborhood radius r = 2 is used. Looking at the results in Tbl. 1, it can be seen that the structure-unaware baseline method is completely unable to detect triangles, as expected. The structure-aware learners on the other hand all perform better than random guessing and are in fact mostly able to fit the training data perfectly. This shows that all generated graphs are 1-WL distinguishable; the WL subtree kernel SVM, for example, can simply "memorize" the training graphs via their unique 1-WL color distribution after T = 5 refinement steps. However, the ability to distinguish training graphs is not sufficient to also classify previously unseen graphs correctly. Since 1-WL cannot detect triangles, all 1-WL bounded approaches (WL ST , WL SP , Baseline, GIN) are therefore unable to generalize, as suggested by their test accuracies. Performance better than random guessing can be explained by availability of the following proxy indicator: The presence of an A-colored triangle in a graph G implies that there is a local region with a slightly higher density of A-colored vertices than in a B-colored graph H with the same vertex color proportions. This local difference in color density is already detectable in the depth-1 BFS subtrees used by 1-WL after a single refinement step, which explains why WL ST performs similarly for T = 1 and T = 5.</p><p>As for the 2-WL inspired kernels, 2-LWL and 2-GWL, it is interesting to see that both kernels do not appear to generalize better than the 1-WL bounded methods. We explain this by the fairly small size of the triangle detection dataset (228 graphs); even though both kernels embed graphs into a space which contains dimensions that indicate the presence of a unicolored triangle, i.e., their DP is sufficiently high to solve the problem, there are so many of those triangle-indicating embedding dimensions that the relevant indicator dimensions found in a given training split might not overlap with those in the test split.</p><p>Looking at the 2-WL inspired GNNs (2-GNN, 2-WL-GNN), we find that the 2-WL-GNN significantly outperforms all other methods, which is in line with our results from Sections 3 and 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation on Real-World Data</head><p>We evaluate the approaches on five common binary graph classification benchmark datasets, namely the NCI1 <ref type="bibr" target="#b4">[5]</ref>, PROTEINS <ref type="bibr" target="#b19">[20]</ref>, and D&amp;D <ref type="bibr" target="#b21">[21]</ref> datasets from the domain of bioinformatics, and the REDDIT-B and IMDB-B datasets <ref type="bibr" target="#b22">[22]</ref> from social network analysis. Tbl. 2 shows our evaluation results. For the evaluation of 2-WL-GNNs, different neighborhood radii were used for each dataset. In the order of the columns in the table, the results were obtained with the radii r = 8, 5, 2, 1 and 4, respectively. Compared with the triangle detection dataset, the advantage of 2-WL-GNNs over the other approaches is clearly less pronounced. This indicates that the theoretical advantages of 2-WL over 1-WL are not as relevant for the five real-world domains as they are for the synthetic problem. Nonetheless, the test performance of 2-WL-GNNs is generally comparable to that of the other state-of-the-art learners, in the sense that the performance of the evaluated 2-WL-GNN models is within the 2σ confidence interval of the best evaluated model.</p><p>If we look at the enzyme detection problem (PROTEINS and D&amp;D), we observe that all evaluated approaches appear to be unable to leverage structural information for a significant improvement over the baseline learner. On the social network datasets <ref type="figure">(REDDIT-B and IMDB-B)</ref>, on the other hand, the structure aware methods clearly outperform the baseline. This confirms similar results by Errica et al. <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We proposed the novel 2-WL-GNN and showed it to be strictly more discriminative than 1-WL bounded GNNs. This theoretical advantage was clearly confirmed experimentally on synthetic data, while results competitive to state-of-the-art graph kernels and GNNs could be achieved on real-world data. We envision two main directions for future research: First, a more thorough theoretical analysis of the relation between 2-WL-GNNs and 2-WL is required to answer questions such as how the neighborhood radius r relates to the discriminative power of a 2-WL-GNN. Second, evaluations on a broader range of domains and other problem types, such as vertex labeling, link prediction, or graph regression, will help to determine in which contexts the theoretical advantages of 2-WL-GNNs also lead to practical improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Implementation of 2-WL-GNNs on GPGPUs</head><p>In the main paper we focused on the discriminative power of the proposed 2-WL-GNN. In this supplementary section we describe how our approach can be implemented on general purpose graphics processing units (GPGPUs).</p><p>Efficient high-level software libraries for the implementation of vertex neighborhood convolution approaches such as GCN or GIN already exist. They describe convolutions via a message-passing abstraction in which vertex feature vectors are passed along their neighboring edges [see 23]. Since a message-passing model along edges is incompatible with the edge-pair neighborhoods of 2-WL, a custom convolution implementation is required for 2-WL-GNNs.</p><p>For this purpose we propose a sparse 2-WL graph representation which is inspired by the coordinate list adjacency format described by Fey and Lenssen <ref type="bibr" target="#b24">[24]</ref>. Given a neighborhood radius r, we encode a graph G using the following two matrices:</p><formula xml:id="formula_41">1. Z (0) G ∈ R m×d (0) :</formula><p>The initial feature matrix is represented directly as a dense floating point matrix with m := |E G r | rows, each of which encodes the feature vector of an edge e ij ∈ E G r . Edge feature duplicates are prevented by only encoding edges with i ≤ j for some arbitrary vertex ordering of G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">R G ∈ [m]</head><p>γ×3 : The reference matrix R G encodes the edge neighborhood information. It This encoding can also be used to represent graph batches by simply concatenating the rows of each graph's feature and reference matrices while shifting the index pointers to account for the concatenation offsets. <ref type="figure" target="#fig_10">Figure 5</ref> illustrates how such a batch encoding might look like. After idx. edge Z (0)  Using the gather and scatter Σ operators, the 2-WL convolution operator from Def. 4.3 can be computed via the following parallel algorithm:</p><formula xml:id="formula_42">consists of γ := eij ∈E G r |Γ G r (v i ) ∩ Γ G r (v j )| rows,</formula><formula xml:id="formula_43">Algorithm 1 Parallel Implementation of a 2-WL Convolution Layer S (t) 1: function S (t) (Z (t−1) ∈ R m×d (t−1) , R ∈ [m] γ×3 ) 2: Z L := Z (t−1) W (t) L Matrix multiply: R m×d (t−1) → R m×d (t)</formula><p>3:</p><formula xml:id="formula_44">Z F := Z (t−1) W (t) F 4: Z Γ := Z (t−1) W (t) Γ 5: X Γ,1 := gather (Z Γ , R Γ,1 ) Gather: R m×d (t) × [m] γ → R γ×d (t)</formula><p>6:</p><p>X Γ,2 := gather (Z Γ , R Γ,2 ) 7:</p><formula xml:id="formula_45">X Γ := σ Γ (X Γ,1 + X Γ,2 )</formula><p>Element-wise operations 8:</p><formula xml:id="formula_46">Z ΣΓ := scatter Σ (X Γ , R L ) Scatter: R γ×d (t) × [m] γ → R m×d (t) 9</formula><p>:</p><formula xml:id="formula_47">Z (t) := σ (Z L + Z F Z ΣΓ )</formula><p>Element-wise operations <ref type="bibr">10:</ref> return Z (t)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Evaluated Hyperparameter Grids</head><p>To tune the hyperparameters of the evaluated models, we used a regular grid search. Depending on the type of model, different sets of hyperparameter configurations Θ were used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Kernels</head><p>We used the SVM classifier from Scikit-learn to evaluate the graph kernel approaches. We tuned only the regularization parameter C of this classifier; the evaluated values are C ∈ {1, 1 × 10 −1 , 1 × 10 −2 , 1 × 10 −3 , 1 × 10 −4 }. All other parameters were left at the default setting (using scikit-learn 0.22.1).</p><p>Baseline and GIN For the evaluation of the structure unaware baseline learner and GIN, we used the same hyperparameter configurations as Errica et al. <ref type="bibr" target="#b17">[18]</ref>. We therefore refer to their work for a complete list of the tuned hyperparameters for those models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2-GNN and 2-WL-GNN</head><p>We evaluated our implementations of 2-GNNs as well as 2-WL-GNNs on the grid spanned by the following hyperparameter values:</p><p>• Number of convolutional layers T ∈ {3, 5}: This parameter describes only the depth of the stack of convolutional layers. The MLP after the pooling layer is always configured with a single hidden layer.</p><p>• Layer width d ∈ {32, 64}: This parameter describes the output dimensionalities d = d (1) = · · · = d (T ) of the convolutional layers and (if applicable) also the hidden layer width of the final MLP after the pooling layer.</p><p>• Learning rate η ∈ {1 × 10 −2 , 1 × 10 −3 , 1 × 10 −4 } of the Adam optimizer.</p><p>• Activation functions σ and σ Γ are set to the standard logistic function. However, for the evaluation of the synthetic TRIANGLE dataset we used ReLU instead because this choice led to improved and more consistent results in previous exploratory experiments.</p><p>• Number of epochs E and early stopping patience p are set to E = 1000 and p = 100, except for the evaluation of the synthetic TRIANGLE dataset for which we used E = 5000 and p = 1000 to ensure model convergence.</p><p>Both, 2-GNNs and 2-WL-GNNs, were evaluated using two different pooling layers which combine the edge feature vectors {z ij } eij ∈E G r into a graph feature representation z G . The mean pooling layer uses z G = 1</p><formula xml:id="formula_48">|E G r | eij z ij .</formula><p>The weighted mean pooling layer extends this approach by incorporating attention scores w ij ∈ R that are learned alongside z ij for each edge; the graph feature representation is then defined as z G = 1 e ij e w ij eij e wij z ij . TRIANGLE The triangle detection dataset was generated by sampling three graphs with exactly one unicolored triangle uniformly at random for each possible combination of the following parameters: The number of vertices (between 6 and 32), the vertex color proportions (either 50/50%, 75/25% or 25/75% vertices with the colors A/B), the graph density (|V G | −2 |E G | ∈ { 1 /4, 1 /2}) the graph class (add a triangle with either the color A or B).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Dataset Statistics and Descriptions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NCI1</head><p>This dataset was made available by Shervashidze et al. <ref type="bibr" target="#b4">[5]</ref>. It contains a balanced subset of molecule graphs that were originally published by the US National Cancer Institute. In each molecule graph, vertices correspond to atoms and edges to bonds between them. The binary classes in this dataset describe whether a molecule is able to suppress or inhibit the growth of certain lung cancer and ovarian cancer cell lines in humans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PROTEINS and D&amp;D</head><p>The graphs in both the PROTEINS dataset <ref type="bibr" target="#b19">[20]</ref> as well as the D&amp;D dataset <ref type="bibr" target="#b21">[21]</ref> represent proteins. Each vertex corresponds to a so-called secondary structure element (SSE), i.e. a certain molecular substructure. An edge encodes either that two SSEs are neighbors in the protein's amino-acid sequence or that those SSEs are close to each other in 3D space. Each protein graph is classified by whether it is an enzyme or not. The main difference between the two datasets is their selection of vertex features/labels.</p><p>REDDIT This balanced dataset contains graphs that represent online discussion threads on the website Reddit <ref type="bibr" target="#b22">[22]</ref>. Each vertex corresponds to a user; an edge is drawn between two users iff. at least one of them replied to a comment of another. Such social interaction graphs were sampled from two types of subreddits: Question/answer-based and discussion-based. The classification goal is to predict from which type of subreddit a given graph was sampled.</p><p>IMDB This dataset contains so-called ego-networks of movie actors <ref type="bibr" target="#b22">[22]</ref>. Vertices in such networks represent actors and edges encode whether two actors starred in the same movie. The graphs in the dataset are derived from the actors starring in either action or romance movies. The classification goal for each graph is to predict the movie genre it was derived from.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Influence of the Neighborhood Radius on the Predictive Performance</head><p>As described in Section 4, the neighborhood radius r ∈ N determines the number of convolved edges feature vectors, i.e. the number of rows in Z (t) ∈ R |E G r |×d (t) (see <ref type="figure" target="#fig_15">Fig. 6</ref>). We will now analyze the influence of r on the accuracy of 2-WL-GNNs. The theory suggests that the DP of the 2-WL convolution is increased by increasing the radius; e.g., the proof of 2-WL's cycle counting ability <ref type="bibr" target="#b12">[13]</ref> depends heavily on the structural information carried by the indirect edge features/colors that are only present if r &gt; 1. r = 0 r = 1 r = 2 r = 3 <ref type="figure" target="#fig_15">Figure 6</ref>: Illustration of the powers of the six-cycle graph for varying r. The self-loop edge at each of the vertices is not explicitly shown. For r = 3 all possible edges between the six vertices will be considered, just as in the original 2-WL algorithm. <ref type="figure">Figure 7</ref> shows 2 that, in practice, a neighborhood radius r &gt; 1 does correlate with a higher training and test accuracy on the NCI1 and D&amp;D datasets; howerver, on the IMDB and PROTEINS datasets this is not the case. This difference is interesting because NCI1 (molecular structures) and D&amp;D (protein sequences) contain more cyclic graphs, while IMDB (ego-network structures)  <ref type="figure">Figure 7</ref>: Accuracy of the 2-WL-GNN with varying neighborhood radii r. All datasets were evaluated on r = 1 and the highest radius for which the 2-WL graph encodings would still fit into memory; the REDDIT dataset only fit into memory for r = 1, therefore it is not shown here.</p><p>and PROTEINS (protein sequences) consist of more tree-or list-like graphs (see Appendix C). Even though the PROTEINS and D&amp;D datasets both contain protein sequences, we find that the protein sequences in the PROTEINS dataset are very "list-like" with much fewer large cycles than in the proteins structures of the D&amp;D dataset (compare the vertex and edge count statistics of both datasets in Tbl. 3). <ref type="figure" target="#fig_12">Figure 8</ref> illustrates this difference. This leads us the the hypothesis that 2-WL-GNNs with a neighborhood radius of r &gt; 1 are able to improve their real-world performance over that achieved with r = 1 by detecting cyclic constituents in graphs. Due to the limited number of evaluation results, further investigations are required to verify this hypothesis.  In Appendix D we evaluated the relation between the neighborhood radius r and the resulting accuracy. As described in Section 4, the neighborhood radius also affects the 2-WL convolution runtime which is bounded by O(nd 2r ), with n denoting the vertex count and d being the maximum vertex degree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D&amp;D PROTEINS</head><p>We will now verify this bound experimentally. <ref type="figure">Figure 9</ref> shows the duration of a single training epoch of a 2-WL-GNN and a GIN model with the same depth and a similar number of learnable parameters (≈ 2200). The time for each combination of n, d and r was obtained by taking the mean duration of the first 100 training epochs using a dataset of 100 synthesized regular graphs with size n and uniform vertex degree d. The experiment was implemented in TensorFlow and executed on a single Nvidia 1080 Ti GPU. Since the coefficient of variation for the 100 samples of each (n, d, r) combination is ≤ 5%, no error bars are shown for visual clarity.</p><p>In the left plot of <ref type="figure">Fig. 9</ref> we see that the epoch durations are dominated by constant costs for n &lt; 2 10 ; for n ≥ 2 10 the expected linearity in n can be observed. For 2-WL-GNNs we expect that the slope of the epoch durations is described by O(d 2r ). The roughly uniform y-offsets of the r = 1, r = 2 and r = 3 curves in the left log-log plot are in line with this expectation. Additionally, the middle plot confirms the expected power law relation between the epoch duration and the vertex degree d.</p><p>The  <ref type="figure">Fig. 9</ref>. Additionally, as we saw in Appendix D, a neighborhood radius r &gt; 1 is not always necessary to reach optimal predictive performance; the computationally cheap choice of r = 1 should therefore always be considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Fold-wise Accuracy Deltas</head><p>Due to the relatively small sizes of the evaluated benchmark datasets, the variance of the test accuracies across different folds is quite large. When directly comparing the mean accuracies of two learners, it is often impossible to tell whether one consistently outperforms the other. We therefore now list the mean and standard deviations of the fold-wise test accuracy differences of all pairs of learners for all datasets. This effectively removes the variance introduced by "easy" and "hard" folds on which all learners might tend to perform consistently better/worse.</p><p>In the following Matrices 4 to 9 we show accuracy differences as row accuracy minus column accuracy. For each row i and column j, the corresponding cell (i, j) is highlighted in red or green iff. the learner i performs consistently worse (or better respectively) than j with a significance level of 2σ. To compute the deltas for 2-WL-GNNs, the same neighborhood radii as in the evaluation section of the main paper are used, i.e. r = 2 for the synthetic triangle detection dataset and r = 8, 5, 2, 1 and 4 for NCI1, PROTEINS, D&amp;D, REDDIT and IMDB respectively.      </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Definition 2 . 7 .</head><label>27</label><figDesc>The color distribution dist χ G,k : C → N 0 of a k-coloring χ G,k counts each color c ∈ C in the coloring, i.e., distχ G,k (c) := v ∈ V k G | χ G,k (v) = c .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Two simple non-isomorphic graphs that are indistinguishable by 1-WL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>2d neighbors, two of which are the loop vertices { {v i , v i } } and { {v j , v j } } with the remaining 2d − 2 neighbors corresponding to the edges that are incident to e ij . After one color refinement step, we get χ (1) (v) = h(A, { { B, . . . , B d times } }) =: C for all loop vertices v ∈ L G ∪ L H and χ (1) (e) = h(B, { {A, A, B, . . . , B 2d−2 times</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Proposition 3 . 5 . 2 -</head><label>352</label><figDesc>GNNs cannot detect m-cycles for m ≥ 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Definition 4 . 2 .</head><label>42</label><figDesc>The initial feature matrix Z (0) G of the 2-WL convolution operator with the neighborhood radius r ∈ N contains both the vertex features x[</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of the correspondence between Conv 1 and Conv 2 .Let ϕ : R d (T ,2+K) → R d (T ) ∪ {nil}be a function that maps the final 2-WL feature vectors produced by Conv 2 to the output space of Conv 1 or the constant nil. Let Pool 2 Z (T,2+K) G :=</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>the first component of each 2-WL feature vector allows ϕ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Corollary 4 . 6 .</head><label>46</label><figDesc>2-WL-GNNs have at least the same DP as 1-WL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Proposition 4 . 7 .</head><label>47</label><figDesc>There are d-regular graphs G and H of size n, which can be distinguished by 2-WL-GNNs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>one for each 2 -</head><label>2</label><figDesc>WL neighbor { {e il , e lj } } of each edge e ij . Each neighbor row is a vector (r L , r Γ,1 , r Γ,2 ) ∈ [m] 3 of three index pointers to rows in Z (0)G . r L points to the row index of the feature vector of e ij , while r Γ,1 and r Γ,2 point to the indices of e il and e lj respectively. We will refer to the three column vectors of R G as R G,L , R G,Γ,1 and R G,Γ,2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 5 :</head><label>5</label><figDesc>Exemplary 2-WL encoding of a batch of two small graphs. encoding a graph dataset as 2-WL matrices, convolutions can be computed efficiently on GPGPUs via the common gather-scatter pattern from parallel programming<ref type="bibr" target="#b25">[25]</ref>. The so-called gather operator takes two inputs:A list Z of m row vectors and a list R of γ pointers into Z. It returns a list X of γ row vectors X[i] = Z[R[i]] for i ∈ [γ]. The scatter Σ operator can be understood as the opposite of gather . scatter Σ takes a list X of γ row vectors and a list R of γ pointers from the range [m]. It returns a list Z of m row vectors Z[i] = j∈[γ]∧R[j]=i X[j] for i ∈ [m].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 8 :</head><label>8</label><figDesc>Two samples illustrating the difference between the PROTEINS and D&amp;D datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Matrix 4 : 2 -</head><label>42</label><figDesc>Fold-wise accuracy delta means and standard deviations on the triangle dataset. WL-GNN (w.m.) WL ST (T = 3)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Matrix 5 :</head><label>5</label><figDesc>Fold-wise accuracy delta means and standard deviations on NCI1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Matrix 6 :</head><label>6</label><figDesc>Fold-wise accuracy delta means and standard deviations on PROTEINS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>3.3 then directly follows, since both G E and H E have n vertices with the final color C and nd 2 vertices with the final color D, i.e. G E 1-WL H E .</figDesc><table><row><cell>Proposition 3.4. A 2-GNN cannot distinguish regular graphs of the same size and therefore has a</cell></row><row><cell>lower DP than 2-WL.</cell></row><row><cell>Proof. The proposition directly follows from Prop. 3.2, Lem. 3.3 and the fact that 2-WL is able to</cell></row><row><cell>distinguish most regular graphs [12, Cor. 1.8.6].</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>= σ Γ = id. By Def. 4.3, all self-loops e ii of G 1 and H 1 have the three neighbors {{ {e ii , e ii } }, { {e ij , e ji } }, { {e il , e li } }}, i.e., the length-two walk along e ii itself and the length-two walks to and from the two neighboring vertices Γ(v</figDesc><table><row><cell>=</cell><cell>1 1</cell><cell>. For simplicity, we choose the</cell></row><row><cell>identity activation functions σ</cell><cell></cell><cell></cell></row></table><note>i ) = {v j , v l }. Therefore, the convolved feature vector of all self-loops are Z (1) [e ii ] = (1) ((1 + 1) + (1 + 1) + (1 + 1)) = 6. However, for the non-self-loops of G 1 and H 1 , i.e., the edges of G and H, we get differing convolved feature vectors. The 2-WL neighbors of e ij ∈ E G are {{ {e ii , e ij } }, { {e ij , e jj } }}. The 2-WL neighbors of e ij ∈ E H are {{ {e ii , e ij } }, { {e ij , e jj } }, { {e il , e lj } }}, where v l ∈ V H is the common neighbor of v i and v j . The different neighborhood sizes of the edges of G and H imply that ∀e ij ∈ E G : Z (1) [e ij ] = 4, while ∀e ij ∈ E H : Z (1) [e ij ] = 6. Thus h 2 (G) = min{4, 6} = min{6, 6} = h 2 (H), which concludes the proof. Corollary 4.8. The DP of 2-WL-GNNs is strictly higher than that of the 1-WL algorithm. Proof. The corollary directly follows from Cor. 4.6 and Prop. 4.7, since 1-WL cannot distinguish regular graphs [12, Cor. 1.8.5].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Mean accuracies and standard deviations on the triangle detection dataset.</figDesc><table><row><cell></cell><cell>Model (Iterations/Pooling)</cell><cell>Train</cell><cell>Test</cell><cell>Class</cell><cell>A</cell></row><row><cell></cell><cell>WL ST (T = 1)</cell><cell>88.3 ± 6.9</cell><cell>64.8 ± 13.2</cell><cell></cell></row><row><cell>KERNEL</cell><cell>WL ST (T = 3) WL ST (T = 5) WL SP (T = 3) 2-LWL (T = 3)</cell><cell cols="2">98.0 ± 1.7 100.0 ± 0.0 96.9 ± 8.4 68.0 ± 10.7 56.9 ± 11.1 62.6 ± 11.2 97.3 ± 3.3 56.5 ± 6.2</cell><cell></cell></row><row><cell></cell><cell>2-GWL (T = 3)</cell><cell>99.9 ± 0.2</cell><cell>61.8 ± 8.8</cell><cell></cell></row><row><cell></cell><cell>Baseline (sum)</cell><cell>48.8 ± 1.6</cell><cell>44.6 ± 8.1</cell><cell>Class</cell><cell>B</cell></row><row><cell></cell><cell>GIN (sum)</cell><cell>84.2 ± 10.6</cell><cell>70.0 ± 7.4</cell><cell></cell></row><row><cell>GNN</cell><cell>2-GNN (mean) 2-GNN (weighted mean)</cell><cell>93.2 ± 3.1 97.1 ± 2.9</cell><cell>76.8 ± 10.7 81.8 ± 7.6</cell><cell></cell></row><row><cell></cell><cell>2-WL-GNN (mean)</cell><cell>98.3 ± 2.6</cell><cell>92.9 ± 8.4</cell><cell></cell></row><row><cell></cell><cell>2-WL-GNN (weighted mean)</cell><cell>99.8 ± 0.4</cell><cell>99.4 ± 1.3</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Mean test accuracies and standard deviations on real-world data. mean) 73.5 ± 2.9 76.2 ± 3.3 74.7 ± 3.1 89.4 ± 2.6 72.2 ± 3.1</figDesc><table><row><cell></cell><cell>Model (Iter./Pooling)</cell><cell>NCI1</cell><cell>PROTEINS</cell><cell>D&amp;D</cell><cell>REDDIT-B IMDB-B</cell></row><row><cell></cell><cell>WL ST (T = 1)</cell><cell cols="4">73.9 ± 2.6 72.8 ± 3.3 78.9 ± 4.2 76.3 ± 2.5 71.0 ± 2.2</cell></row><row><cell>KERNEL</cell><cell>WL ST (T = 3) WL SP (T = 3) 2-LWL (T = 3)</cell><cell cols="4">84.8 ± 1.6 73.0 ± 2.4 78.8 ± 4.3 78.0 ± 2.7 72.9 ± 2.5 OOM 73.1 ± 3.5 OOM OOM 74.4 ± 3.5 76.7 ± 2.2 69.4 ± 4.6 76.6 ± 3.5 75.8 ± 2.9 72.2 ± 3.3</cell></row><row><cell></cell><cell>2-GWL (T = 3)</cell><cell cols="4">71.6 ± 2.1 73.1 ± 3.6 76.3 ± 3.9 75.4 ± 3.2 70.4 ± 3.2</cell></row><row><cell></cell><cell>Baseline (sum)</cell><cell cols="4">67.7 ± 3.1 74.0 ± 4.9 75.7 ± 2.5 72.1 ± 7.8 50.7 ± 2.4</cell></row><row><cell></cell><cell>GIN (sum)</cell><cell cols="4">77.4 ± 2.9 71.8 ± 3.1 75.2 ± 3.4 87.0 ± 4.4 66.8 ± 3.9</cell></row><row><cell>GNN</cell><cell>2-GNN (mean) 2-GNN (w. mean)</cell><cell cols="3">75.9 ± 2.0 74.8 ± 3.4 72.9 ± 4.1 78.3 ± 1.8 73.8 ± 3.5 69.6 ± 3.9</cell><cell>OOM 71.4 ± 3.6 OOM 70.9 ± 3.2</cell></row><row><cell></cell><cell>2-WL-GNN (mean)</cell><cell>72.</cell><cell></cell><cell></cell></row></table><note>4 ± 2.9 76.5 ± 2.7 75.4 ± 3.3 83.7 ± 5.2 71.2 ± 4.0 2-WL-GNN (w.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Sizes of the evaluated binary classification datasets and their graphs.</figDesc><table><row><cell></cell><cell>no. of</cell><cell>vertex data</cell><cell cols="3">vertex count |VG|</cell><cell cols="3">edge count |EG|</cell><cell>vert. deg.</cell></row><row><cell></cell><cell>graphs</cell><cell>(feat. + lab.)</cell><cell cols="5">min mean max min mean</cell><cell cols="2">max mean ± σ</cell></row><row><cell>TRIANGLE</cell><cell>228</cell><cell>0 + 2</cell><cell>6</cell><cell>18.3</cell><cell>32</cell><cell>5</cell><cell>52.1</cell><cell>164</cell><cell>5.7 ± 2.8</cell></row><row><cell>NCI1</cell><cell>4110</cell><cell>0 + 37</cell><cell>3</cell><cell>29.9</cell><cell>111</cell><cell>2</cell><cell>32.3</cell><cell>119</cell><cell>2.2 ± 0.8</cell></row><row><cell>PROTEINS</cell><cell>1113</cell><cell>29 + 3</cell><cell>4</cell><cell>39.1</cell><cell>620</cell><cell>5</cell><cell>72.8</cell><cell>1049</cell><cell>3.7 ± 1.1</cell></row><row><cell>D&amp;D</cell><cell>1178</cell><cell>0 + 89</cell><cell cols="3">30 284.3 5748</cell><cell cols="3">63 715.7 14267</cell><cell>5.0 ± 1.7</cell></row><row><cell>REDDIT</cell><cell>2000</cell><cell>0 + 1</cell><cell cols="3">6 429.6 3782</cell><cell cols="2">4 497.8</cell><cell cols="2">4071 2.3 ± 20.7</cell></row><row><cell>IMDB</cell><cell>1000</cell><cell>0 + 1</cell><cell>12</cell><cell>19.8</cell><cell>136</cell><cell>26</cell><cell>96.5</cell><cell>1249</cell><cell>9.8 ± 7.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>E Empirical Runtime Evaluation</head><label></label><figDesc>Figure 9: Comparison of the mean training epoch durations of 2-WL-GNN and GIN for varying graph sizes n, vertex degrees d and neighborhood radii r. (Left) Durations for varying vertex counts n ∈ {2 4 , . . . , 2 14 } with a fixed degree d = 2. (Middle) Durations for varying vertex degrees d with a fixed size n = 1024. (Right) Table of the factors by which 2-WL-GNN is slower than GIN for n = 1024.</figDesc><table><row><cell>epoch duration (ms)</cell><cell>10 1 10 2 10 3</cell><cell>2 4</cell><cell>2 8 r = 3 r = 2 r = 1 GIN</cell><cell>2 12</cell><cell>500 1,000 1,500 2,000 0</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>d r = 1 r = 2 r = 3 2 2.0 3.2 4.5 3 2.4 5.2 8.9 4 2.8 8.7 20.7 5 3.1 14.8 35.6 6 3.5 20.4 58.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">vertex count n</cell><cell></cell><cell></cell><cell cols="3">vertex degree d</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>tableon the right shows how much slower the 2-WL-GNN is compared to the 1-WL bounded GIN architecture. Note however that we considered the worst-case scenario in which all vertices reach the upper degree bound d. Many real-world datasets consist of graphs with only a few high-degree vertices. Looking at the NCI1, PROTEINS, D&amp;D, REDDIT and IMDB datasets, which have mean vertex degrees that range between 2.2 and 9.8 (see Tbl. 3), the table inFig. 9would suggest that a 2-WL-GNN with r = 1 is at least 2 or 3 times slower than a GIN on those datasets. However, in reality the slowdown is only228ms 198ms ≈ 1.15 for NCI1, 262ms 206ms ≈ 1.27 for PROTEINS, 765ms 514ms ≈ 1.49 for D&amp;D, 1081ms 804ms ≈ 1.35 for REDDIT and 96ms 61ms ≈ 1.57 for IMDB. For neighborhood radii r &gt; 1, the real-world slowdown factor can be significantly smaller than the worst-case slowdown as well: 964ms 198ms ≈ 4.87 for NCI1 with r = 8, 670ms 206ms ≈ 3.25 for PROTEINS with r = 5, 1386ms 514ms ≈ 2.70 for D&amp;D with r = 2 and 239ms 61ms ≈ 3.92 for IMDB with r = 4. Consequently, 2-WL-GNNs appear to be computationally feasible in practice despite the large worst-case slowdown factors in</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Note that the accuracies in this figure are lower than those in the evaluation section of the main paper because a different 2-WL-GNN architecture is used here; namely, no MLP is applied after pooling.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by German Research Foundation (DFG) within the Collaborative Research Center "On-The-Fly Computing" (SFB 901/3 project no. 160364472).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Machine learning of toxicological big data enables read-across structure activity relationships (RASAR) outperforming animal test reproducibility</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luechtefeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rowlands</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hartung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Toxicological Sciences</title>
		<imprint>
			<biblScope unit="volume">165</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="198" to="212" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Graph Neural Networks for Social Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Web Conf., WWW&apos;19</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Algorithm selection for software validation based on graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hüllermeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jakobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wehrheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automated Software Engineering</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="153" to="186" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The Multiscale Laplacian Graph Kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Int. Conf. on Neural Inf. Proc. Sys., NIPS&apos;16</title>
		<meeting>the 30th Int. Conf. on Neural Inf. Proc. Sys., NIPS&apos;16</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2990" to="2998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Weisfeiler-Lehman Graph Kernels. JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A survey on graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">D</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Network Science</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<title level="m">Spectral Networks and Locally Connected Networks on Graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st Int. Conf. on Neural Inf. Proc. Sys., NIPS&apos;17</title>
		<meeting>the 31st Int. Conf. on Neural Inf. Proc. Sys., NIPS&apos;17</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">How Powerful are Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Babai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Erdős</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Selkow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Random graph isomorphism. SIaM Journal on computing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="628" to="635" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Describing graphs: A first-order approach to graph canonization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Immerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Complexity theory retrospective</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1990" />
			<biblScope unit="page" from="59" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On the Combinatorial Power of the Weisfeiler-Lehman Algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fürer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="260" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On Weisfeiler-Leman Invariance: Subgraph Counts and Related Graph Properties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Arvind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fuhlbrück</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Köbler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Verbitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fundamentals of Computation Theory</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="111" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An optimal lower bound on the number of variables for graph identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fürer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Immerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Combinatorica</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="389" to="410" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on AI</title>
		<meeting>the AAAI Conference on AI</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4602" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Approximation capabilities of multilayer feedforward networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="251" to="257" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A fair comparison of graph neural networks for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Errica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Podda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bacciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Micheli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Learn. Rep., ICLR</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Glocalized Weisfeiler-Lehman Graph Kernels: Global-Local Feature Maps of Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Data Mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schonauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Protein function prediction via graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="47" to="56" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distinguishing enzyme structures from non-enzymes without alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">D</forename><surname>Dobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Doig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Molecular Biology</title>
		<imprint>
			<biblScope unit="volume">330</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="771" to="783" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Faulkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ballard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Langston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Relational inductive biases, deep learning, and graph networks</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Fast graph representation learning with pytorch geometric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficient gather and scatter operations on graphics processors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">K</forename><surname>Govindaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2007 ACM/IEEE Conf. on Supercomputing</title>
		<meeting>of the 2007 ACM/IEEE Conf. on Supercomputing</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
