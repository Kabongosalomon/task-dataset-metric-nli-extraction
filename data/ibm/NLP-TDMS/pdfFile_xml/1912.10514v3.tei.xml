<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Tag-less Back-Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-02-09">9 Feb 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idris</forename><surname>Abdulmumin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">·</forename><surname>Bashir</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shehu</forename><surname>Galadanci</surname></persName>
							<email>bsgaladanci.se@buk.edu.ng</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliyu</forename><surname>Garba</surname></persName>
							<email>algarba@abu.edu.ng</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bashir</forename><forename type="middle">Shehu</forename><surname>Galadanci</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Idris Abdulmumin Computer Science</orgName>
								<orgName type="institution">Ahmadu Bello University</orgName>
								<address>
									<settlement>Zaria, Kaduna</settlement>
									<country key="NG">Nigeria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Software Engineering</orgName>
								<orgName type="institution">Bayero University</orgName>
								<address>
									<settlement>Kano, Kano</settlement>
									<country key="NG">Nigeria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Aliyu Garba Computer Science</orgName>
								<orgName type="institution">Ahmadu Bello University</orgName>
								<address>
									<settlement>Zaria, Kaduna</settlement>
									<country key="NG">Nigeria</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Tag-less Back-Translation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-02-09">9 Feb 2021</date>
						</imprint>
					</monogr>
					<note type="submission">Received: date / Accepted: date</note>
					<note>Noname manuscript No. (will be inserted by the editor) 2 Idris Abdulmumin et al.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>tagged back-translation · tag-less back-translation · neural machine translation · machine translation · natural language processing</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>An effective method to generate a large number of parallel sentences for training improved neural machine translation (NMT) systems is the use of the back-translations of the target-side monolingual data. The standard back-translation method has been shown to be unable to efficiently utilize the available huge amount of existing monolingual data because of the inability of translation models to differentiate between the authentic and synthetic parallel data during training. Tagging, or using gates, has been used to enable translation models to distinguish between synthetic and authentic data, improving standard back-translation and also enabling the use of iterative back-translation on language pairs that underperformed using standard backtranslation. In this work, we approach back-translation as a domain adaptation problem, eliminating the need for explicit tagging. In the approach -tag-less back-translation -the synthetic and authentic parallel data are treated as out-of-domain and in-domain data respectively and, through pre-training and fine-tuning, the translation model is shown to be able to learn more efficiently from them during training. Experimental results have shown that the approach outperforms the standard and tagged back-translation approaches on low resource English-Vietnamese and English-German neural machine translation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural Machine Translation (NMT) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b47">48]</ref> has been the state-of-the-art approach for machine translation in recent years <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b37">38]</ref>, outperforming Phrase-Based Statistical Machine Translation <ref type="bibr" target="#b32">[33]</ref> when qualitative parallel data between the languages is available in abundance <ref type="bibr" target="#b54">[55]</ref>. This training dataset is usually scarce and expensive to compile for many language pairs. Recently, researchers have proposed methods to exploit the easier-to-get monolingual data of one or both of the languages to augment the available parallel data and improve the performance of the translation models. Such methods include integrating a language model <ref type="bibr" target="#b19">[20]</ref>, back-translation <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b18">19]</ref>, forward translation <ref type="bibr" target="#b52">[53]</ref> and dual learning <ref type="bibr" target="#b20">[21]</ref>. The back-translation approach is simple and has been the most effective technique yet for NMT <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b21">22]</ref>. The method involves training a target-to-source (backward) model on the available authentic bitext. The backward model is then used to translate a large amount of monolingual sentences in the target language into synthetic source sentences, generating the synthetic parallel data. The authentic and synthetic parallel data are then mixed to train a source-to-target (forward) model.</p><p>It has been shown that as the amount of monolingual data used in backtranslation continues to increase, a point is reached when the model stops learning useful representation and, therefore, the performance of the model starts to drop. This is because the usually noise-infested synthetic data starts to overwhelm the authentic data and the model starts to completely unlearn the correct parameters it learns from the authentic training data <ref type="bibr" target="#b16">[17]</ref>. Extensive studies by <ref type="bibr" target="#b15">[16]</ref> have shown that in low resource NMT, noising beam search outputs improve the models more than other generation methods such as sampling. The authors claimed that the method enhances source-side diversity. But the works of <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b51">52]</ref> found that the noising technique is only a form of tagging, indicating to the model that the noised data is back-translated, enabling it to treat the synthetic data as belonging to a different domain. The model then learns different representations, optimally, from the two data. They, instead, introduced the use of explicit tags (and gates) to indicate synthetic inputs. The tagging approach was shown to outperform the standard back-translation.</p><p>In this work, we approach back-translation as a domain adaptation problem, simplifying the works of <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b51">52]</ref> that explicitly differentiate between the two data using noise/tags/gates. Instead of tagging the synthetic data, our approach -the tag-less back-translation -aims to enable the model to learn efficiently from the two data through pre-training and fine-tuning. Instead of relying on the model to differentiate between the data, we used the synthetic data as generic domain (out-of-domain) and pre-train the model on this data. We then used the authentic data as in-domain to fine-tune the pretrained translation model. We hypothesize that although the tagging and noising approaches improve the forward models, our domain-adaptation-tailored approach will provide a flexible method of maximizing the gains in the quantity of the synthetic data and efficiently utilizing the quality in the authentic parallel data. The approach will also enable the use of different training settings on the different data, as obtainable in domain adaptation strategies. It also gets better as more research and more improved ways of domain adaption are proposed.</p><p>In domain adaptation, the generic model is not always expected to perform very well in the domain it is to be deployed, hence the model is fine-tuned with a usually smaller but in-domain data. In many languages, the in-domain data is usually low-resourced or non-existent: having the same issue as in low resource neural machine translation. The in-domain data in itself is not sufficient to create a good model while the more abundant out-of-domain data performs poorly when deployed in the target domain. Mixing the two data results in the in-domain data to be lost in the out-of-domain data and the resulting model is not able to also perform well in the target domain. The larger out-of-domain data is, therefore, used to pre-train a model and the weights of this model are used to initialize the training of the in-domain translation model -a technique referred to as fine-tuning <ref type="bibr" target="#b10">[11]</ref>. When a different language pair is used for pretraining than that used during fine-tuning, the approach is regarded to as transfer learning <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b27">28]</ref>.</p><p>We make the following contributions in this paper:</p><p>• we proposed a novel approach that enables a translation model that is trained on synthetic and authentic parallel data to be able to efficiently learn from the the two data, utilizing the different advantages presented by each. • we successfully applied pre-training and fine-tuning to enable the forward model in back-translation to differentiate between synthetic and authentic data during training, achieving a superior performance to standard and the successful tagged back-translation approaches, • experimental results have shown that the approach is superior to the standard and tagging back-translation approaches in low resource English-Vietnamese and English-German neural machine translation systems.</p><p>The remaining sections are as follows: Section 2 reviews relevant literature on NMT, leveraging monolingual data in NMT and pre-training and finetuning. Section 3 explains the tag-less back-translation approach, Section 4 describes the data and experimental set-up used in training the models, Section 5 discusses the results obtained after the experiment. We discuss further the findings in Section 6. Finally, in Section 7, we concluded the work and suggest future directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>This section presents prior work on NMT, back-translation and pre-training in NMT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Neural Machine Translation (NMT)</head><p>The NMT is based on a sequence-to-sequence encoder-decoder system with attention mechanism <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b33">34]</ref>. The encoders and decoders are made of neural networks that model the conditional probability of a target sentence y given the source sentence x: p(y|x) . The encoder converts the input in the source language into a set of vectors while the decoder converts the set of vectors into the target language through an attention mechanism, one word at a time. The attention mechanism was introduced to keep track of context in longer sentences <ref type="bibr" target="#b1">[2]</ref>.</p><p>The NMT model produces the translation sentence by generating one target word at every time step. Given an input sequence X = (x 1 , ..., x Tx ) and previously translated words (y 1 , ..., y i−1 ), the probability of the next word y i is p(y i |y 1 , ...,</p><formula xml:id="formula_0">y i−1 , X) = g(y i−1 , s i , c i )<label>(1)</label></formula><p>where s i is the decoder hidden state for time step i and is computed as</p><formula xml:id="formula_1">s i = f (s i−1 , y i−1 , c i ).<label>(2)</label></formula><p>Here, f and g are nonlinear transform functions, which can be implemented as long short-term memory (LSTM) network <ref type="bibr" target="#b22">[23]</ref> or gated recurrent units (GRU) <ref type="bibr" target="#b8">[9]</ref> in recurrent neural machine translation (RNMT), and c i is a distinct context vector at time step i, which is calculated as a weighted sum of the input annotations h j Tx j=1 a i,j h j</p><p>where h j is the annotation of x j calculated by a bidirectional Recurrent Neural Network. The weight a i,j for h j is calculated as</p><formula xml:id="formula_3">a i,j = exp e i,j Tx t=1 exp e i,t<label>(4)</label></formula><p>and</p><formula xml:id="formula_4">e i,j = v a tanh(W s i−1 + U h j )<label>(5)</label></formula><p>where v a is the weight vector, W and U are the weight matrices. All of the parameters in the NMT model, represented as θ, are optimized to maximize the following conditional log-likelihood of the M sentence aligned bilingual samples</p><formula xml:id="formula_5">L(θ) = 1 M M m=1 Ty i=1 log(p(y m i |y m &lt;i , X m , θ))<label>(6)</label></formula><p>To remove the recurrence and enable parallelization across multiple GPUs during training, the convolutional neural networks were used to create the convolutional NMT (CNMT) encoder-decoder architecture <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b50">51]</ref>. The CNMT utilizes 1-dimensional convolutional layers followed by gated linear units, GLU <ref type="bibr" target="#b13">[14]</ref>. The decoders compute and apply attention to each of the layers. The model uses positional embeddings along with residual connections <ref type="bibr" target="#b17">[18]</ref>.</p><p>The transformer <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b14">15]</ref> architecture was introduced to remove the recurrence and convolutions of previous architectures. The transformer is based solely on multi-headed self-attention layers. It enables parallelization across multiple GPUs, thereby, reducing training time. The architecture is used in current state-of-the-art translation systems <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b37">38]</ref>.</p><p>In this work, we used a unidirectional LSTM encoder-decoder architecture with Luong attention <ref type="bibr" target="#b33">[34]</ref>. This is a simple recurrent neural network RNMT architecture. Our approach is not architecture-dependent and can be applied to the other architectures or other more enhanced implementations of the RNMT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Leveraging Monolingual Data for NMT</head><p>The use of monolingual data of the target and/or source language has been studied extensively to improve the performance of neural translation models, especially in low resource settings. <ref type="bibr" target="#b19">[20]</ref> explored integrating language models trained on monolingual data into NMT systems, <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b4">5]</ref> proposed augmenting a copy or slightly modified copy respectively of the target data as source, <ref type="bibr" target="#b44">[45]</ref> proposed the back-translation approach, <ref type="bibr" target="#b52">[53]</ref> proposed the forward translation and <ref type="bibr" target="#b20">[21]</ref> used both forward and back-translations to improve the translation models. The back-translation approach has been shown to outperform other approaches in low and high resource languages <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>Various studies have investigated back-translation to improve the backward model, to select the most suitable generation/decoding methods for generating the synthetic data and to reduce the impact of higher ratio of the synthetic to the authentic bitext. The quality of the models trained using back-translation depends on the quality of the backward model <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b51">52]</ref>. To improve the quality of the synthetic parallel data, <ref type="bibr" target="#b21">[22]</ref> used iterative back-translation -iteratively using the back-translated data to improve both the backward and forward models. <ref type="bibr" target="#b28">[29]</ref> and <ref type="bibr" target="#b12">[13]</ref> used high resource languages through transfer learning and <ref type="bibr" target="#b53">[54]</ref> explored the use of both target and source monolingual data to improve both the backward and forward models.</p><p>[37] trained a bilingual system based on <ref type="bibr" target="#b24">[25]</ref> to do both forward and backward translations, eliminating the need for two separate models. <ref type="bibr" target="#b40">[41]</ref> used Transductive data selection methods to select monolingual data that are in the same domain as the test set for back-translation, improving performance.</p><p>The works of <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b41">42]</ref> have found that the ratio of synthetic to authentic data affects the performance of the models most. When the ratio is high, the model tends to learn more from the synthetic data, which contains more mistakes than the authentic data. Investigations have found that the sampling approach of synthetic data generation and adding noise to beam search output outperforms the regular beam decoding technique <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b23">24]</ref>. These approaches were said to improve the models by enhancing source-side diversity. <ref type="bibr" target="#b5">[6]</ref> claimed, instead, that the noise only indicates to the model that the input is either synthetic or authentic, enabling the model to better utilize the two data. <ref type="bibr" target="#b51">[52]</ref> and <ref type="bibr" target="#b5">[6]</ref> used tags (and gates) to enable the model to distinguish between the data and the approach has been shown to efficiently utilize more synthetic data, outperforming standard back-translation and enhancing the efficiency of iterative back-translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Domain Adaptation</head><p>Domain adaptation is the use of a usually few amount of in-domain data to improve the performance of an out-of-domain (general purpose) model before deployment. The amount of the in-domain parallel data is usually not sufficient to train a very good model and the general purpose models usually performs poorly <ref type="bibr" target="#b31">[32]</ref>. There are two categories of domain adaptation -data centric and model centric <ref type="bibr" target="#b10">[11]</ref> with each having several techniques. The techniques in these classifications include using monolingual data <ref type="bibr" target="#b19">[20]</ref>, synthetic data generation <ref type="bibr" target="#b44">[45]</ref>, using data selection <ref type="bibr" target="#b48">[49]</ref> and using tagged out-of-domain parallel data <ref type="bibr" target="#b9">[10]</ref> and fine-tuning <ref type="bibr" target="#b44">[45]</ref> Pre-training has been used successfully in various machine learning tasks to improve performance when the data is not enough to train a good enough model. It was used for training word embeddings <ref type="bibr" target="#b34">[35]</ref>, in computer vision <ref type="bibr" target="#b49">[50]</ref>, fine-tuning NMT models <ref type="bibr" target="#b15">[16]</ref> and as transfer learning in low resource NMT <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b27">28]</ref>. The transfer learning for machine translation approach involves training a model on a high resource language pair and transferring the training on a low resource pair. The works of <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b27">28]</ref> have shown tremendous improvements over models that are trained with the low resource data from scratch.</p><p>In back-translation, <ref type="bibr" target="#b44">[45]</ref> showed that fine-tuning a pre-trained model on indomain data improves the quality of back-translated model. <ref type="bibr" target="#b42">[43]</ref> pre-trained the model on the authentic data and fine-tunes it on the mixed synthetic and authentic data. <ref type="bibr" target="#b28">[29]</ref> and <ref type="bibr" target="#b12">[13]</ref> pre-trained a model on a high resource language and fine-tunes it on a low resource language pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Proposed Method</head><p>The approach is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. As illustrated in Algorithm 1, the authentic parallel data:</p><formula xml:id="formula_6">D P = {(x (u) , y (u) )} U u=1 is used to train a target-to-source model, M x←y . This model -the backward model -is used to translate the monolingual target data, Y = {(y (v) )} V v=1 , to generate the synthetic parallel data: D = {(x (v) , y (v) )} V v=1 .</formula><p>Instead of mixing the two data to train a forward (target) model, we used only the synthetic data to pre-train the forward model, M x→y , until no improvement is observed on the development set. Finally, the forward model is fine-tuned on authentic data.</p><p>It was shown in <ref type="bibr" target="#b28">[29]</ref> that using different vocabulary each during pretraining and fine-tuning leads to drop in performance because, it was said, independent vocabulary use different identifiers even for the same subwords and the network loses benefits of the weights learned during pre-training. The authors proposed learning a joint BPE on a mixture of both the pre-training and fine-tuning data and this has been shown to achieve better results in domain adaptation. In this approach, we have access to both the out-of-domain (synthetic) and the in-domain (authentic) parallel data. This, therefore, enables us to learn a joint BPE and build the training vocabulary for both pre-training and fine-tuning.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Tag-less Back-Translation</head><formula xml:id="formula_7">Input: Parallel data D P = {(x (u) , y (u) )} U u=1 and Monolingual target data Y = {(y (v) )} V v=1 1: procedure BACK-TRANSLATION 2: Train backward model Mx←y on bilingual data D P 3: Use Mx←y to create D = {(x (v) , y (v) )} V v=1 , for y ∈ Y</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Set-up</head><p>We used the TensorFlow <ref type="bibr" target="#b0">[1]</ref> implementation of the OpenNMT <ref type="bibr" target="#b26">[27]</ref> framework to train the models -the NMTSmallV1 configuration. The set-up is based on the NMTSmallV1 configuration. Specifically, the configuration is a 2-layer unidirectional LSTM encoder-decoder model with Luong attention <ref type="bibr" target="#b33">[34]</ref>. It has 512 hidden units and a vocabulary size of 50,000 for both source and target languages. We used Adam <ref type="bibr" target="#b25">[26]</ref> optimizer and a batch size of 64 with a dropout probability of 0.3, a static learning rate of 0.0002 and the models are evaluated on the development set after every 5,000 training steps. The models were evaluated using the bi-lingual evaluation understudy metric, BLEU <ref type="bibr" target="#b38">[39]</ref>, specifically the multi-bleu <ref type="bibr" target="#b30">[31]</ref> implementation. The models are trained until there is no improvement of over 0.2 BLEU after four training steps. As stated in Section 3, the learning of BPE on the training data and the building of training vocabulary for both pre-training and fine-tuning was done on the mixture of the synthetic and authentic parallel data. During fine-tuning, we, therefore, only change the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Data</head><p>For this work, we use the preprocessed low resource English-Vietnamese parallel data <ref type="bibr" target="#b33">[34]</ref> of the IWSLT 2015 Translation task <ref type="bibr" target="#b6">[7]</ref>. We used the 2012 and 2013 test sets for development and testing respectively. We also used the data from the IWSLT 2014 German-English shared translation task <ref type="bibr" target="#b7">[8]</ref> as the second language pair, pre-processed using the data clean-up as well as the train, development and test split in <ref type="bibr" target="#b43">[44]</ref>. For the monolingual data, we used the preprocessed English monolingual data of WMT 2014 English-German translation task <ref type="bibr" target="#b3">[4]</ref>. We shuffled the monolingual data and selected 666,585 monolingual sentences which is five times as much as the En-Vi parallel data. The statistics of the datasets are shown in <ref type="table" target="#tab_0">Table 1</ref>. We learned byte pair encoding (BPE) <ref type="bibr" target="#b45">[46]</ref> with 10,000 merge operations on the training dataset and applied it on the train, development and test datasets. Afterwards, we build the vocabulary on the training dataset. For all the experiments, we used thrice as much of the monolingual as the available parallel data in both of the languages except when we experimented with the ratio of 1:5 (parallel to monolingual data) for the English-Vietnamese NMT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Models</head><p>To compare the performance of our approach with that of the previous works, we implemented the following methods to train translation models on the English-Vietnamese and English-German NMT using the data presented in Section 4.2 above. All models were trained using the same settings stated in Section 4.1</p><p>• We first train baseline models using the available authentic parallel data only. In the models, the baselines have the English language as the target language -Vi-En and De-En. • We then train the backward models also on the authentic parallel data using English language as the source language -En-Vi and En-De. The models are used for the generation of the additional synthetic parallel data for the back-translation approach. • We implemented the various back-translation strategies namely standard back-translation -standard bt, the tagged back-translation -tagged bt and the tag-less back-translation -tag-less bt (joint BPE) using the authentic and synthetic parallel data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>All scores reported are statistically significant with p &lt; 0.05. We used the paired bootstrap resampling of <ref type="bibr" target="#b29">[30]</ref> as implemented in <ref type="bibr" target="#b39">[40]</ref> to estimate the statistical significance confidence scores. See <ref type="table" target="#tab_0">Table 13</ref> in Appendix 1 for confidence scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">English-Vietnamese Low Resource NMT</head><p>The evaluation scores of the best models and the improved models obtained after taking the checkpoint averaging of the last 8 checkpoints are shown in <ref type="table" target="#tab_1">Table 2</ref>. We first created a forward Vietnamese-English (Vi-En) model, baseline, on the available authentic parallel data. The model trained for 75,000 steps before the stopping condition was met. The baseline model was trained further to 110, 000 training steps but the performance continued to flatten without observing any improvement. The model achieved the best single-checkpoint We mixed the two data -synthetic and authentic -without differentiating between the two and used the resulting large dataset to train a forward model. We labelled this model as standard bt -for standard back-translation <ref type="bibr" target="#b44">[45]</ref>. This model was trained for 165,000 before the stopping condition were met. It achieved a single-checkpoint best BLEU score of 24.46 at the 155,000 th training step. We mixed the synthetic and authentic parallel data and learned a joint BPE on the resulting training dataset and build the training vocabulary. We applied the BPE on the synthetic data for pre-training and on the authentic data for fine-tuning. We trained a model, labelled tag-less bt (joint BPE), using this approach. The model achieved a single-checkpoint score of 18.60 BLEU during pre-training and improved to 26.53 BLEU after fine-tuning. The average fine-tuned model was better by about 0.30 BLEU. The average pretrained model performed very low compared to the baseline and the standard back-translation models -18.59 BLEU vs 22.22 and 25.28 BLEUs respectively. This is obviously because the quality of the data used in the training the model -the synthetic data -is lower than that of the other two. The quality of the synthetic data, although generated from a reasonably good backward model, is still not sufficient to train a model whose quality can compare to the other models that are trained in whole or in part with the authentic data. Finetuning the model on the authentic data results in a sharp rise in performance.  The model was fine-tuned until the stopping condition was met. The approach outperformed the baseline and standard back translation models by 5.34 and 2.07 BLEUs respectively. The gap in performance was, however, reduced to 4.61 and 1.55 BLEUs after checkpoint averaging.</p><p>We experimented the other pre-train and fine-tune approach, learning the BPE only on the synthetic data. We build the vocabulary on the synthetic training data after applying the BPE. The synthetic corpus was used to pretrain a forward model for 130,000 steps, achieving a single-checkpoint best score of 17.85 BLEU. The authentic parallel data was then used to fine-tune the model for a further 35,000 training steps. Stopping at each of these steps were based on the stopping condition. The performance of the tag-less bt model improved to 25.16 BLEU after fine-tuning. Although this approach was shown to outperform the baseline and standard back-translation, it underperformed the joint BPE implementation of the tag-less approach by 1.06 BLEU. In <ref type="figure" target="#fig_3">Figures 2a and 2b</ref>, we show how the BLEU scores continue to improve with increase in the training steps. The model trained using the tag-less bt (joint BPE) approach continued to outperform the three others after fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">English-German Low Resource NMT</head><p>We conducted the same set of experiments presented in section 5.1 on the second low resource dataset, the English-German IWSLT'14 parallel dataset. This data, as presented in <ref type="table" target="#tab_0">Table 1</ref>, is made up of a little bit more than 150,000 parallel sentences. We first trained a backward (En-De) model on the available parallel data. This model maxed-out performance on the test set, based on the set-up, at 10.25 BLEU after averaging the last 8 checkpoints. It stopped training at the 80,000 th training steps and achieving the best single model performance at the 65,000 th -10.03 BLEU. We used the average model to generate the synthetic data, translating the available English monolingual data. We trained four separate forward (De-En) models based on the approaches we explained earlier. The first is the baseline trained on the available authentic data, the standard bt on the mixture of the authentic and synthetic data without differentiating, the tag-less bt pre-trained on the synthetic data and fine-tuned on the authentic data having learned the BPE on the synthetic data and updating the vocabulary before fine-tuning and, finally, the tag-less bt (joint BPE) trained also using the tag-less approach but having learned the BPE and built the vocabulary on the mixture of the synthetic and authentic data.</p><p>The results of evaluating the models after training using the various approaches are presented in <ref type="table" target="#tab_1">Table 2</ref> The baseline achieved a modest average performance of 20.95 BLEU after training for 100,000 training steps. The performance on the dataset was improved after applying standard back-translation, achieving a huge +4.92 BLEU improvement over the baseline. The tag-less approach, though better, did not achieve a huge improvement over backtranslation (only +0.16 BLEU) but after applying the improved tag-less (joint BPE), as shown in the previous section, we achieved huge +2.96 BLEU increase in performance. This +2.8 and +7.88 BLEUs over the previous tag-less approach and the baseline respectively.</p><p>For all the subsequent experiments, unless stated otherwise, we used the joint BPE technique to implement the tag-less back-translation approach as it is shown to be the most successful variant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Tagged Vs Tag-Less Back-translation</head><p>We compared the performance of the tag-less bt model -our technique -with that of the successful tagged back-translation of <ref type="bibr" target="#b5">[6]</ref> on the English-Vietnamese data. The synthetic sources were labelled with the &lt;BT&gt; token at the beginning of each sentence and mixed with the authentic sources to generate the mixed tagged parallel corpus. This mixed data is used to train the forward tagged back-translation model -tagged bt. The tagged bt model stopped at  125,000 steps and the training was continued up to 195,000 steps to equal the number of training steps reached by the tag-less bt model. While the tagged approach underperformed the best score of our technique by 1.78 BLEU, it was able to outperform the single-checkpoint standard back-translation by 24.76 to 24.46 BLEUs respectively (+0.3 BLEU) but underperformed the average standard back-translation model by 0.23 BLEU. Finally, we trained a forward model using the tagged back-translation for English-German NMT to compare the performance with our approach on this data. The tagged approach took a further 50,000 training steps to reach a single model best of 27.49 BLEU, but still underperforming the tag-less approach by 0.82 BLEU. The best model obtained after averaging checkpoints was also achieved using our approach, a performance of 28.83 BLEU compared to the tagged 27.75 BLEU, an improvement of 1.08 BLEU. The performances of these models, evaluated on the test set is shown in <ref type="table" target="#tab_2">Table 3</ref>. On this data, the tagged approach performed better than the standard back-translation by +1.88 BLEU on the average models. It can be seen in <ref type="figure" target="#fig_3">Figures 2 and 3</ref> that in both of the experiments conducted on the two data, our tag-less approach out-performed the rest of the back-translation approaches.</p><p>This supports the hypothesis that although the tagged back-translation involves explicit differentiating between the two data using tags, the model trained on the approach may not be able to differentiate between them com- pletely during training as observed in the mixed performance of the models trained on the two different data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Fine-tuning: Synthetic Vs Authentic Data</head><p>Our technique proposed pre-training the forward model on the synthetic parallel data and fine-tuning the model afterwards on the authentic data. This was proposed to enable the model to unlearn the mistakes it learned from the synthetic data using correct sentences in the authentic parallel data. We experiment the other way round to investigate the effects of pre-training on the authentic data and fine-tuning on the synthetic data. We used the baseline as the pre-trained model and fine-tune it on the synthetic data. This approach was labelled as reverse tag-less bt. This approach did not show any benefit to the final forward model, see <ref type="figure" target="#fig_4">Fig. 3</ref>. As expected, the performance of the model decreased and the curve flattens as the number of training steps increases. The best and average scores are shown in <ref type="table" target="#tab_2">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Quantity of Monolingual Data</head><p>As stated earlier, it was found that as the more synthetic data increases, a point is reached where the performance starts to deteriorate <ref type="bibr" target="#b16">[17]</ref>. Instead, our work hypothesizes that the performance of the model will start to decrease only if it is not able to differentiate between the synthetic and authentic training data and, therefore, efficiently learning from the two. We also pointed out that since the data is mixed in both the standard and tagged back-translation approaches, the model may not be able to completely differentiate between the data, although in the latter approach, the model is expected to treat the tagged synthetic sources as a different domain. We, therefore, experiment with different ratios of the authentic to synthetic data to verify this claim. We sample the authentic to synthetic data in the ratios, 1:1, 1:3 and 1:5. The results are shown in <ref type="table" target="#tab_4">Table 5</ref>.</p><p>In the tagged approach, the single-checkpoint best scores continue to rise from using the same amount of monolingual data for back-translation to using three times the authentic data of the monolingual data for back-translation. But, as observed, the performance dropped slightly when we used five times the amount of available parallel data. However, the performance of the tagless back-translation models continues to increase steadily when the ratio of authentic to monolingual data is increased. We observed the performance to improve by about 0.25 BLEU when the amount of synthetic data is tripled and doubled to about 0.5 BLEU after adding another double amount of the synthetic data to the training data. It can also be observed that there was a very low improvement over the performance of baseline and serious underperformance compared to the tag-less approach when we used the same amount of synthetic data with the authentic data to train the models -22.73 BLEU vs 22.22 and 26.15 BLEUs respectively. Overall, we obtained a 3.42, 1.78 and 1.67 BLEU improvements on the average models using the tag-less approach over the tagged approach on the ratios experimented respectively.</p><p>It can be observed also that the performance of the model trained on the 1:1 ratio of monolingual to synthetic data using our approach is very good compared to the model trained using the same amount of data in the tagged approach and subsequent increase in training data leads to steady improvements that at 1:5 ratio, the performance was improved by about 1 BLEU. This steady improvements can show that the model learned useful knowledge on the authentic data but only used the synthetic data for further improvements. Following this trend, we can, therefore, conclude that with more synthetic data compared to the authentic data, the model will only continue to learn and increase its performance if useful representations are learnable on the synthetic data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Fine-tuning Standard And Tagged Back-Translations</head><p>The work of <ref type="bibr" target="#b42">[43]</ref> reported no observable advantage of using the authentic data to train the forward model and fine-tuning it henceforth on the mixed data. Instead, we experimented training the forward model on the mixed data first and then fine-tune it on the authentic data. As shown in <ref type="table" target="#tab_5">Table 6</ref>, this approach reaches the same performance as the old tag-less approach -25.15 BLEUusing the same amount of synthetic sentences albeit after 30,000 more training steps but sill underperforming the joint BPE tag-less back-translation's 26.53 BLEU although training for additional 15,000 training steps. The better joint BPE tag-less approach converges earlier than fine-tuning the standard backtranslation model, at 165,000 training steps. We also explored the use of fine-tuning to determine whether or not the tagged approach will be able to cover the difference in performance with the tag-less approach. After fine-tuning the tagged bt (1:5) model for a further 35,000 training steps, the performance gained was a significant +0.93 BLEU and only 0.17 BLEU over the average after just 20,000 steps of fine-tuning. The performance was still short of the tag-less bt (joint BPE) (1:5) by a significant 1.36 BLEU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>In this work, we proposed an approach for training the forward model in back-translation without using tags or noising the synthetic data. Translation models that are trained on the synthetic and authentic data have been shown to perform better when they are able to differentiate between the two data. Previous approaches have relied on the use of noise in back-translation <ref type="bibr" target="#b15">[16]</ref> especially on low resource languages to improve the performance of models. The authors thought that the approach ensures source-side diversity which has been shown to benefit the models <ref type="bibr" target="#b23">[24]</ref>. The approach was found out to only indicate to the forward model that the noised data is synthetic, enabling it to treat the data differently from the authentic data <ref type="bibr" target="#b5">[6]</ref>. The use of tags has been shown to improve the performance of such models. In this work, we eliminated the need of using of the tags and showed that although it was successful at improving the performance -proving it successful at indicating to the model that a data is synthetic and not authentic -domain adaptation methods are more capable of ensuring the model differentiate between the data. The ability for the model to separate between the data is even more important in low resource languages where the available data is not enough to train standard backward model, thus generating synthetic data with a lot of noises.</p><p>Domain adaptation techniques techniques in machine translation ensures that a better model is trained, leveraging on a larger parallel data of either the same language pair but in a different domain -fine-tuning -or a different language pair -transfer learning. In this technique, the two data are not tagged, mixed and left to the model to differentiate between them. They, rather, are used at the different stages of the training and this ensures the model performs in the target domain as expected. We utilized the synthetic data -which is bigger but more prone to translation noises -as the generic domain and the authentic data -smaller but having more quality -as the in-domain. This selection was not done until the reversed approach was shown not give the desired performance. The superiority of the approach over the successful tagging was shown through experimental results conducted on two low resource language pair: English-Vietnamese and English-German. In each of the languages considered, we obtained an improvement of more than 1 BLEU points over the tagged approach that outperformed the baseline and standard back-translation models.</p><p>We also test the performance of our technique when the amount of monolingual data is increased. We used different ratios of the authentic parallel to monolingual data used. We found that our technique was not only able to handle the increase in the synthetic data, but was able to attain rapid improvement given the smallest amount of synthetic data. We obtained a superior performance by a whopping 3.56 BLEU using the tag-less approach over the tagged approach when the amount of monolingual data is the same as the authentic data. The performance continued to steady increase as the amount of monolingual data is increased. The tagged approach could only handle tripling the amount of synthetic data but the performance started to decrease when the synthetic data was increased further. Using the same amount of synthetic data in ratios 1:1, 1:3 and 1:5, our technique outperformed the tagging technique by 3.42, 1.78 and 1.67 respectively (see average scores in <ref type="table" target="#tab_4">Table 5</ref>).</p><p>Our approach also provides one with the flexibility of using state-of-the-art domain adaptation methods to improve the performance of the already successful back-translation approach. Techniques such as using different dropout and/or learning rate during pre-training and fine-tuning may improve the performance of the forward model. The method may also be applied in highresource languages since both of these settings -low and high resource -can benefit from the ability of the forward model to differentiate between synthetic and authentic data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions and Future Work</head><p>This work has shown that an NMT model pre-trained on synthetic data and fine-tuned on the authentic data outperforms the rather successful method of tagging the synthetic data in low resource NMT by enabling the forward model to differentiate between the authentic and synthetic training data. The approach, however, does not improve the performance when it is reversed and the forward model is pre-trained on the authentic data and then fine-tuned on the synthetic data. As expected, the reverse approach makes the model to unlearn the useful representations learned in favour of the noise in the synthetic data. This justifies our hypothesis that without differentiating between the two data, the synthetic data is most likely to hurt the performance of the forward model.</p><p>It was shown also, in this work, that the more synthetic data used, the better the performance of the forward model, though the most effective ratio was not yet determined through thorough experimentation. This will inform the basis of future works. We experiment fine-tuning the models trained using the standard and tagged back-translation approaches. Experimental results showed the standard back-translation equalling the performance of a variant of the tag-less approach after many more rounds of training. The performance of the tagged approach improved considerably but still trailed the tag-less approach. The most successful of the tag-less approach has been the one that involves learning a joint BPE and building the training vocabulary on the mixture of the synthetic and authentic parallel data. This approach is made possible, unlike in other fine-tuning conditions, because both the generic (synthetic) data and the in-domain (authentic) data are available during the process.</p><p>For future work, the use of different settings -such as increasing or decreasing the learning rate, using dropout and L2 regularization, which may reduce overfitting on the in-domain (authentic) data as shown to be a likely problem in domain adaptation by <ref type="bibr" target="#b2">[3]</ref> -for the pre-training and fine-tuning approaches can be explored to maximize the benefits of the domain adaptation approach in back-translation. The approach can also investigated to improve the forward translation approach -which also leverages on the synthetic data for additional training data. Finally, we intend to investigate the technique in high resource languages in the future.       </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Tag-less Back-Translation: Training the forward model on the synthetic parallel data generated using the backward model. The forward model is then fine-tuned on the authentic data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>; 4 :</head><label>4</label><figDesc>Pre-train forward model Mx→y on parallel data D ; 5: Fine-tune the forward model Mx→y on parallel data D P ; 6: end procedure Output: forward model Mx→y</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>ft tag-less ft (joint BPE) b. German-English</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 :</head><label>2</label><figDesc>Tag-less back-translation: pre-training on synthetic data and fine-tuning on authentic data. Showing how this technique compares to the baseline and the standard back-translation approaches on the test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 :</head><label>3</label><figDesc>Tagged Vs Tag-less back-translation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 :</head><label>4</label><figDesc>Fine-tuning the baseline model on the synthetic data. Evaluation scores on the test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Data Used</figDesc><table><row><cell>data</cell><cell>sentences</cell><cell cols="2">train words (vocab)</cell><cell>dev</cell><cell>test</cell></row><row><cell></cell><cell></cell><cell>En</cell><cell>Vi</cell><cell></cell></row><row><cell>IWSLT'15 En-Vi</cell><cell>133, 317</cell><cell>2,837,240</cell><cell>2,688,387</cell><cell cols="2">1, 553 1, 268</cell></row><row><cell></cell><cell></cell><cell>(50,045)</cell><cell>(103,796)</cell><cell></cell></row><row><cell></cell><cell></cell><cell>En</cell><cell>De</cell><cell></cell></row><row><cell>IWSLT'14 En-De</cell><cell>153, 348</cell><cell>2,706,255</cell><cell>3,311,508</cell><cell cols="2">6, 970 6, 750</cell></row><row><cell></cell><cell></cell><cell>(54,169)</cell><cell>(25,615)</cell><cell></cell></row><row><cell>WMT'14 En-De -Monolingual English</cell><cell>666,585</cell><cell cols="2">16,700,106 (344,138)</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance of the Tag-less Back-translated model compared to the baseline and standard back-translation models for Vietnamese-English and German-English translations. Evaluation scores on the test set. The tag-less approaches show results of pre-training and fine-tuning. After the stoppage condition were met, after 55,000 training steps, the best performing single-checkpoint for the backward model achieved a BLEU score of 24.78 at the 50,000 th training step. Averaging the last 8 checkpoints gave the best performance -25.79 BLEU. This average model was used to back-translate the monolingual English data to generate synthetic parallel data.</figDesc><table><row><cell>data</cell><cell></cell><cell>baseline</cell><cell>standard bt</cell><cell>tag-less bt</cell><cell>tag-less bt (joint BPE)</cell></row><row><cell>Vi-En</cell><cell>best checkpoint BLEU (training step)</cell><cell cols="2">21.19 (65k) 24.46 (155k)</cell><cell>17.85 (90k) 25.16 (145k)</cell><cell>18.60 (105k) 26.53 (165k)</cell></row><row><cell></cell><cell>average</cell><cell>22.22</cell><cell>25.28</cell><cell>17.96 25.77</cell><cell>18.59 26.83</cell></row><row><cell>De-En</cell><cell>best checkpoint BLEU (training step)</cell><cell cols="2">20.30 (75k) 25.11 (150k)</cell><cell>3.01 (75k) 25.16 (120k)</cell><cell>5.43 (60k) 28.31 (155k)</cell></row><row><cell></cell><cell>average</cell><cell>20.95</cell><cell>25.87</cell><cell>3.03 26.03</cell><cell>5.13 28.83</cell></row></table><note>score of 21.19 BLEU at the 65,000 th . We then trained a backward (En-Vi) model, backward.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Performance of Tag-less Back-translation compared to the Tagged back-translation model for Vietnamese-English translation. Evaluation scores on the test set.</figDesc><table><row><cell></cell><cell></cell><cell>tagged bt</cell><cell>tag-less bt (joint BPE)</cell></row><row><cell>En-Vi</cell><cell>(training step) best checkpoint BLEU</cell><cell cols="2">24.76 (165k) 26.53 (165k)</cell></row><row><cell></cell><cell>average</cell><cell>25.05</cell><cell>26.83</cell></row><row><cell>En-De</cell><cell>(training step) best checkpoint BLEU</cell><cell cols="2">27.49 (205k) 28.31 (155k)</cell></row><row><cell></cell><cell>average</cell><cell>27.75</cell><cell>28.83</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Performance of Tag-less Back-translation on the test set: pre-training on synthetic data and fine-tuning on authentic data Vs pre-training on authentic data and fine-tuning on synthetic data for Vietnamese-English.</figDesc><table><row><cell></cell><cell>tag-less bt</cell><cell>reverse tag-less bt</cell></row><row><cell>best checkpoint BLEU (training step)</cell><cell>25.16 (145k)</cell><cell>21.19 (65k) -pre-train 18.95 (100k) -fine-tune</cell></row><row><cell>average</cell><cell>25.77</cell><cell>18.91</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Using different ratios of the authentic to synthetic data for Vietnamese-English translation. Evaluation scores of the models on the test set.</figDesc><table><row><cell></cell><cell></cell><cell>tagged bt</cell><cell></cell><cell cols="3">tag-less bt (joint BPE)</cell></row><row><cell></cell><cell>1:1</cell><cell>1:3</cell><cell>1:5</cell><cell>1:1</cell><cell>1:3</cell><cell>1:5</cell></row><row><cell>best checkpoint BLEU</cell><cell>22.73</cell><cell>24.76</cell><cell>24.62</cell><cell>26.29</cell><cell>26.53</cell><cell>26.91</cell></row><row><cell>(training step)</cell><cell>(85k)</cell><cell>(165k)</cell><cell>(155k)</cell><cell>(130k)</cell><cell>(165k)</cell><cell>(185k)</cell></row><row><cell>average</cell><cell>22.73</cell><cell>25.05</cell><cell>25.47</cell><cell>26.15</cell><cell>26.83</cell><cell>27.14</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Before and after fine-tuning the English-Vietnamese standard and tagged back-translation NMT models on the authentic data. Evaluation scores on the test set.</figDesc><table><row><cell></cell><cell cols="2">tagged bt (1:5)</cell><cell cols="2">standard bt (1:3)</cell></row><row><cell></cell><cell>before</cell><cell>after</cell><cell>before</cell><cell>after</cell></row><row><cell>best checkpoint BLEU</cell><cell>24.62</cell><cell>25.55</cell><cell>24.46</cell><cell>25.15</cell></row><row><cell>(training step)</cell><cell>(155k)</cell><cell>(175k)</cell><cell>(155k)</cell><cell>(180k)</cell></row><row><cell>average</cell><cell>25.47</cell><cell>25.64</cell><cell>25.28</cell><cell>25.32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Performance of Tag-less Back-translation compared to the baseline and standard back-translation models. BLEU Scores for each Checkpoint of the Models for Vietnamese-English NMT (best single-checkpoint and average scores are shown in bold).</figDesc><table><row><cell>training step (thousands)</cell><cell>baseline</cell><cell>standard bt</cell><cell cols="4">tag-less bt pre-train fine-tune pre-train tag-less bt (joint BPE) fine-tune</cell></row><row><cell>5</cell><cell>2.50</cell><cell>1.92</cell><cell>2.88</cell><cell></cell><cell>2.87</cell><cell></cell></row><row><cell>10</cell><cell>4.25</cell><cell>3.50</cell><cell>7.54</cell><cell></cell><cell>9.78</cell><cell></cell></row><row><cell>15</cell><cell>5.54</cell><cell>11.34</cell><cell>10.70</cell><cell></cell><cell>13.06</cell><cell></cell></row><row><cell>20</cell><cell>6.68</cell><cell>15.83</cell><cell>12.86</cell><cell></cell><cell>14.11</cell><cell></cell></row><row><cell>25</cell><cell>10.87</cell><cell>17.20</cell><cell>12.96</cell><cell></cell><cell>15.03</cell><cell></cell></row><row><cell>30</cell><cell>16.89</cell><cell>19.47</cell><cell>14.07</cell><cell></cell><cell>15.92</cell><cell></cell></row><row><cell>35</cell><cell>19.47</cell><cell>18.95</cell><cell>15.10</cell><cell></cell><cell>15.95</cell><cell></cell></row><row><cell>40</cell><cell>20.37</cell><cell>20.81</cell><cell>15.59</cell><cell></cell><cell>16.39</cell><cell></cell></row><row><cell>45</cell><cell>21.09</cell><cell>21.61</cell><cell>15.61</cell><cell></cell><cell>17.08</cell><cell></cell></row><row><cell>50</cell><cell>20.13</cell><cell>22.18</cell><cell>15.99</cell><cell></cell><cell>17.15</cell><cell></cell></row><row><cell>55</cell><cell>21.09</cell><cell>21.98</cell><cell>16.69</cell><cell></cell><cell>17.29</cell><cell></cell></row><row><cell>60</cell><cell>21.11</cell><cell>22.35</cell><cell>16.66</cell><cell></cell><cell>17.15</cell><cell></cell></row><row><cell>65</cell><cell>21.19</cell><cell>22.37</cell><cell>16.73</cell><cell></cell><cell>17.37</cell><cell></cell></row><row><cell>70</cell><cell>20.17</cell><cell>23.43</cell><cell>17.63</cell><cell></cell><cell>17.51</cell><cell></cell></row><row><cell>75</cell><cell>19.41</cell><cell>22.92</cell><cell>17.13</cell><cell></cell><cell>17.92</cell><cell></cell></row><row><cell>80</cell><cell>20.43</cell><cell>23.41</cell><cell>17.33</cell><cell></cell><cell>17.96</cell><cell></cell></row><row><cell>85</cell><cell>20.18</cell><cell>23.62</cell><cell>16.97</cell><cell></cell><cell>17.94</cell><cell></cell></row><row><cell>90</cell><cell>19.80</cell><cell>23.71</cell><cell>17.85</cell><cell></cell><cell>18.58</cell><cell></cell></row><row><cell>95</cell><cell>20.36</cell><cell>23.77</cell><cell>17.30</cell><cell></cell><cell>18.28</cell><cell></cell></row><row><cell>100</cell><cell>19.48</cell><cell>23.73</cell><cell>17.31</cell><cell></cell><cell>18.23</cell><cell></cell></row><row><cell>105</cell><cell>19.63</cell><cell>24.07</cell><cell>17.36</cell><cell></cell><cell>18.60</cell><cell></cell></row><row><cell>110</cell><cell>19.77</cell><cell>23.99</cell><cell>17.34</cell><cell></cell><cell>18.53</cell><cell></cell></row><row><cell>115</cell><cell>-</cell><cell>24.07</cell><cell>16.93</cell><cell></cell><cell></cell><cell>24.31</cell></row><row><cell>120</cell><cell>-</cell><cell>23.98</cell><cell>17.35</cell><cell></cell><cell></cell><cell>24.93</cell></row><row><cell>125</cell><cell>-</cell><cell>24.30</cell><cell>17.64</cell><cell></cell><cell></cell><cell>25.55</cell></row><row><cell>130</cell><cell>-</cell><cell>24.15</cell><cell>17.62</cell><cell></cell><cell></cell><cell>25.79</cell></row><row><cell>135</cell><cell>-</cell><cell>24.18</cell><cell></cell><cell>23.70</cell><cell></cell><cell>26.21</cell></row><row><cell>140</cell><cell>-</cell><cell>24.42</cell><cell></cell><cell>25.12</cell><cell></cell><cell>26.11</cell></row><row><cell>145</cell><cell>-</cell><cell>23.65</cell><cell></cell><cell>25.16</cell><cell></cell><cell>26.33</cell></row><row><cell>150</cell><cell>-</cell><cell>24.39</cell><cell></cell><cell>24.93</cell><cell></cell><cell>26.23</cell></row><row><cell>155</cell><cell>-</cell><cell>24.46</cell><cell></cell><cell>24.68</cell><cell></cell><cell>26.21</cell></row><row><cell>160</cell><cell>-</cell><cell>24.11</cell><cell></cell><cell>24.29</cell><cell></cell><cell>26.45</cell></row><row><cell>165</cell><cell>-</cell><cell>24.12</cell><cell></cell><cell>24.25</cell><cell></cell><cell>26.53</cell></row><row><cell>170</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>26.40</cell></row><row><cell>175</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>26.17</cell></row><row><cell>180</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>26.23</cell></row><row><cell>185</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>26.21</cell></row><row><cell>190</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>26.16</cell></row><row><cell>195</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>26.21</cell></row><row><cell>average</cell><cell>22.22</cell><cell>25.28</cell><cell>17.96</cell><cell>25.77</cell><cell>18.59</cell><cell>26.83</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Performance of Tag-less Back-translation compared to the baseline and standard back-translation models. BLEU Scores for each Checkpoint of the Models for German-English NMT (best single-checkpoint and average scores are shown in bold).</figDesc><table><row><cell>training step (thousands)</cell><cell>baseline</cell><cell>standard bt</cell><cell cols="4">tag-less bt pre-train fine-tune pre-train tag-less bt (joint BPE) fine-tune</cell></row><row><cell>5</cell><cell>2.82</cell><cell>2.13</cell><cell>0.43</cell><cell></cell><cell>0.37</cell><cell></cell></row><row><cell>10</cell><cell>5.00</cell><cell>3.13</cell><cell>0.83</cell><cell></cell><cell>1.11</cell><cell></cell></row><row><cell>15</cell><cell>6.30</cell><cell>4.45</cell><cell>1.09</cell><cell></cell><cell>2.02</cell><cell></cell></row><row><cell>20</cell><cell>7.69</cell><cell>5.48</cell><cell>1.44</cell><cell></cell><cell>2.82</cell><cell></cell></row><row><cell>25</cell><cell>9.13</cell><cell>6.30</cell><cell>1.66</cell><cell></cell><cell>3.35</cell><cell></cell></row><row><cell>30</cell><cell>10.20</cell><cell>6.56</cell><cell>1.84</cell><cell></cell><cell>3.80</cell><cell></cell></row><row><cell>35</cell><cell>11.23</cell><cell>7.76</cell><cell>2.12</cell><cell></cell><cell>4.37</cell><cell></cell></row><row><cell>40</cell><cell>11.45</cell><cell>8.61</cell><cell>2.28</cell><cell></cell><cell>4.81</cell><cell></cell></row><row><cell>45</cell><cell>11.84</cell><cell>9.76</cell><cell>2.38</cell><cell></cell><cell>5.05</cell><cell></cell></row><row><cell>50</cell><cell>13.82</cell><cell>12.93</cell><cell>2.49</cell><cell></cell><cell>5.14</cell><cell></cell></row><row><cell>55</cell><cell>15.73</cell><cell>16.24</cell><cell>2.77</cell><cell></cell><cell>5.41</cell><cell></cell></row><row><cell>60</cell><cell>18.71</cell><cell>18.84</cell><cell>2.87</cell><cell></cell><cell>5.43</cell><cell></cell></row><row><cell>65</cell><cell>19.74</cell><cell>20.21</cell><cell>2.85</cell><cell></cell><cell>5.38</cell><cell></cell></row><row><cell>70</cell><cell>19.95</cell><cell>20.89</cell><cell>2.93</cell><cell></cell><cell></cell><cell>21.32</cell></row><row><cell>75</cell><cell>20.30</cell><cell>21.75</cell><cell>3.01</cell><cell></cell><cell></cell><cell>23.29</cell></row><row><cell>80</cell><cell>20.15</cell><cell>22.54</cell><cell>2.99</cell><cell></cell><cell></cell><cell>24.86</cell></row><row><cell>85</cell><cell>19.96</cell><cell>23.16</cell><cell></cell><cell>9.84</cell><cell></cell><cell>25.54</cell></row><row><cell>90</cell><cell>19.75</cell><cell>23.44</cell><cell></cell><cell>12.74</cell><cell></cell><cell>26.32</cell></row><row><cell>95</cell><cell>19.53</cell><cell>23.64</cell><cell></cell><cell>21.63</cell><cell></cell><cell>26.70</cell></row><row><cell>100</cell><cell>19.51</cell><cell>24.13</cell><cell></cell><cell>23.59</cell><cell></cell><cell>26.85</cell></row><row><cell>105</cell><cell>-</cell><cell>24.07</cell><cell></cell><cell>24.63</cell><cell></cell><cell>27.38</cell></row><row><cell>110</cell><cell>-</cell><cell>23.94</cell><cell></cell><cell>24.84</cell><cell></cell><cell>27.67</cell></row><row><cell>115</cell><cell>-</cell><cell>24.69</cell><cell></cell><cell>25.32</cell><cell></cell><cell>27.82</cell></row><row><cell>120</cell><cell>-</cell><cell>24.31</cell><cell></cell><cell>25.16</cell><cell></cell><cell>28.06</cell></row><row><cell>125</cell><cell>-</cell><cell>24.29</cell><cell></cell><cell>25.00</cell><cell></cell><cell>27.91</cell></row><row><cell>130</cell><cell>-</cell><cell>24.59</cell><cell></cell><cell>24.72</cell><cell></cell><cell>28.01</cell></row><row><cell>135</cell><cell>-</cell><cell>24.93</cell><cell></cell><cell>24.26</cell><cell></cell><cell>28.03</cell></row><row><cell>140</cell><cell>-</cell><cell>24.70</cell><cell>-</cell><cell>-</cell><cell></cell><cell>27.98</cell></row><row><cell>145</cell><cell>-</cell><cell>25.01</cell><cell>-</cell><cell>-</cell><cell></cell><cell>28.28</cell></row><row><cell>150</cell><cell>-</cell><cell>25.11</cell><cell>-</cell><cell>-</cell><cell></cell><cell>28.26</cell></row><row><cell>155</cell><cell>-</cell><cell>25.09</cell><cell>-</cell><cell>-</cell><cell></cell><cell>28.31</cell></row><row><cell>160</cell><cell>-</cell><cell>24.94</cell><cell>-</cell><cell>-</cell><cell></cell><cell>28.24</cell></row><row><cell>165</cell><cell>-</cell><cell>24.90</cell><cell>-</cell><cell>-</cell><cell></cell><cell>28.15</cell></row><row><cell>170</cell><cell>-</cell><cell>24.62</cell><cell>-</cell><cell>-</cell><cell></cell><cell>28.22</cell></row><row><cell>175</cell><cell>-</cell><cell>24.86</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>average</cell><cell>20.95</cell><cell>25.87</cell><cell>3.03</cell><cell>26.03</cell><cell>5.13</cell><cell>28.83</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Tagged Vs Tag-less Back-translation. We used the joint BPE for implementing the tag-less approach. BLEU Scores for each Checkpoint of the Models (best single-checkpoint and average scores are shown in bold).</figDesc><table><row><cell></cell><cell></cell><cell>Vi-En</cell><cell></cell><cell></cell><cell>De-En</cell><cell></cell></row><row><cell>training step (thousands)</cell><cell>tagged bt</cell><cell cols="2">tag-less bt pre-train fine-tune</cell><cell>tagged bt</cell><cell cols="2">tag-less bt pre-train fine-tune</cell></row><row><cell>5</cell><cell>1.78</cell><cell>2.87</cell><cell></cell><cell>2.86</cell><cell>0.37</cell><cell></cell></row><row><cell>10</cell><cell>3.64</cell><cell>9.78</cell><cell></cell><cell>6.52</cell><cell>1.11</cell><cell></cell></row><row><cell>15</cell><cell>9.88</cell><cell>13.06</cell><cell></cell><cell>10.55</cell><cell>2.02</cell><cell></cell></row><row><cell>20</cell><cell>15.29</cell><cell>14.11</cell><cell></cell><cell>14.74</cell><cell>2.82</cell><cell></cell></row><row><cell>25</cell><cell>16.99</cell><cell>15.03</cell><cell></cell><cell>16.48</cell><cell>3.35</cell><cell></cell></row><row><cell>30</cell><cell>19.22</cell><cell>15.92</cell><cell></cell><cell>18.12</cell><cell>3.80</cell><cell></cell></row><row><cell>35</cell><cell>19.62</cell><cell>15.95</cell><cell></cell><cell>19.82</cell><cell>4.37</cell><cell></cell></row><row><cell>40</cell><cell>19.71</cell><cell>16.39</cell><cell></cell><cell>20.49</cell><cell>4.81</cell><cell></cell></row><row><cell>45</cell><cell>20.99</cell><cell>17.08</cell><cell></cell><cell>21.47</cell><cell>5.05</cell><cell></cell></row><row><cell>50</cell><cell>22.03</cell><cell>17.15</cell><cell></cell><cell>21.88</cell><cell>5.14</cell><cell></cell></row><row><cell>55</cell><cell>21.85</cell><cell>17.29</cell><cell></cell><cell>22.29</cell><cell>5.41</cell><cell></cell></row><row><cell>60</cell><cell>22.03</cell><cell>17.15</cell><cell></cell><cell>22.75</cell><cell>5.43</cell><cell></cell></row><row><cell>65</cell><cell>22.33</cell><cell>17.37</cell><cell></cell><cell>23.19</cell><cell>5.38</cell><cell></cell></row><row><cell>70</cell><cell>22.95</cell><cell>17.51</cell><cell></cell><cell>23.67</cell><cell></cell><cell>21.32</cell></row><row><cell>75</cell><cell>23.05</cell><cell>17.92</cell><cell></cell><cell>23.86</cell><cell></cell><cell>23.29</cell></row><row><cell>80</cell><cell>22.90</cell><cell>17.96</cell><cell></cell><cell>24.32</cell><cell></cell><cell>24.86</cell></row><row><cell>85</cell><cell>23.25</cell><cell>17.94</cell><cell></cell><cell>24.60</cell><cell></cell><cell>25.54</cell></row><row><cell>90</cell><cell>23.91</cell><cell>18.58</cell><cell></cell><cell>24.35</cell><cell></cell><cell>26.32</cell></row><row><cell>95</cell><cell>23.87</cell><cell>18.28</cell><cell></cell><cell>24.89</cell><cell></cell><cell>26.70</cell></row><row><cell>100</cell><cell>23.69</cell><cell>18.23</cell><cell></cell><cell>25.12</cell><cell></cell><cell>26.85</cell></row><row><cell>105</cell><cell>24.21</cell><cell>18.60</cell><cell></cell><cell>25.25</cell><cell></cell><cell>27.38</cell></row><row><cell>110</cell><cell>23.91</cell><cell>18.53</cell><cell></cell><cell>25.41</cell><cell></cell><cell>27.67</cell></row><row><cell>115</cell><cell>23.90</cell><cell></cell><cell>24.31</cell><cell>25.48</cell><cell></cell><cell>27.82</cell></row><row><cell>120</cell><cell>24.33</cell><cell></cell><cell>24.93</cell><cell>25.76</cell><cell></cell><cell>28.06</cell></row><row><cell>125</cell><cell>24.08</cell><cell></cell><cell>25.55</cell><cell>25.89</cell><cell></cell><cell>27.91</cell></row><row><cell>130</cell><cell>24.34</cell><cell></cell><cell>25.79</cell><cell>26.04</cell><cell></cell><cell>28.01</cell></row><row><cell>135</cell><cell>24.17</cell><cell></cell><cell>26.21</cell><cell>26.17</cell><cell></cell><cell>28.03</cell></row><row><cell>140</cell><cell>24.27</cell><cell></cell><cell>26.11</cell><cell>26.13</cell><cell></cell><cell>27.98</cell></row><row><cell>145</cell><cell>24.28</cell><cell></cell><cell>26.33</cell><cell>26.02</cell><cell></cell><cell>28.28</cell></row><row><cell>150</cell><cell>24.44</cell><cell></cell><cell>26.23</cell><cell>26.56</cell><cell></cell><cell>28.26</cell></row><row><cell>155</cell><cell>24.15</cell><cell></cell><cell>26.21</cell><cell>26.35</cell><cell></cell><cell>28.31</cell></row><row><cell>160</cell><cell>24.50</cell><cell></cell><cell>26.45</cell><cell>26.30</cell><cell></cell><cell>28.24</cell></row><row><cell>165</cell><cell>24.76</cell><cell></cell><cell>26.53</cell><cell>26.76</cell><cell></cell><cell>28.15</cell></row><row><cell>170</cell><cell>24.56</cell><cell></cell><cell>26.40</cell><cell>26.84</cell><cell></cell><cell>28.22</cell></row><row><cell>175</cell><cell>24.29</cell><cell></cell><cell>26.17</cell><cell>26.65</cell><cell>-</cell><cell>-</cell></row><row><cell>180</cell><cell>24.09</cell><cell></cell><cell>26.23</cell><cell>26.99</cell><cell>-</cell><cell>-</cell></row><row><cell>185</cell><cell>24.34</cell><cell></cell><cell>26.21</cell><cell>27.00</cell><cell>-</cell><cell>-</cell></row><row><cell>190</cell><cell>24.27</cell><cell></cell><cell>26.16</cell><cell>26.90</cell><cell>-</cell><cell>-</cell></row><row><cell>195</cell><cell>24.32</cell><cell></cell><cell>26.21</cell><cell>27.05</cell><cell>-</cell><cell>-</cell></row><row><cell>200</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>27.17</cell><cell>-</cell><cell>-</cell></row><row><cell>205</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>27.49</cell><cell>-</cell><cell>-</cell></row><row><cell>210</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>27.29</cell><cell>-</cell><cell>-</cell></row><row><cell>215</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>27.10</cell><cell>-</cell><cell>-</cell></row><row><cell>220</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>27.18</cell><cell>-</cell><cell>-</cell></row><row><cell>225</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>27.01</cell><cell>-</cell><cell>-</cell></row><row><cell>230</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>27.26</cell><cell>-</cell><cell>-</cell></row><row><cell>235</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>27.44</cell><cell>-</cell><cell>-</cell></row><row><cell>avarage</cell><cell>25.05</cell><cell>18.59</cell><cell>26.83</cell><cell>27.75</cell><cell>5.13</cell><cell>28.83</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>Pre-training on the authentic data and fine-tuning on the synthetic data for Vietnamese-English NMT. BLEU Scores for each Checkpoint of the Models</figDesc><table><row><cell>training step (thousands)</cell><cell>pre-train</cell><cell>fine-tune</cell></row><row><cell>5</cell><cell>2.50</cell><cell>-</cell></row><row><cell>10</cell><cell>4.25</cell><cell>-</cell></row><row><cell>15</cell><cell>5.54</cell><cell>-</cell></row><row><cell>20</cell><cell>6.68</cell><cell>-</cell></row><row><cell>25</cell><cell>10.87</cell><cell>-</cell></row><row><cell>30</cell><cell>16.89</cell><cell>-</cell></row><row><cell>35</cell><cell>19.47</cell><cell>-</cell></row><row><cell>40</cell><cell>20.37</cell><cell>-</cell></row><row><cell>45</cell><cell>21.09</cell><cell>-</cell></row><row><cell>50</cell><cell>20.13</cell><cell>-</cell></row><row><cell>55</cell><cell>21.09</cell><cell>-</cell></row><row><cell>60</cell><cell>21.11</cell><cell>-</cell></row><row><cell>65</cell><cell>21.19</cell><cell>-</cell></row><row><cell>70</cell><cell>20.17</cell><cell>-</cell></row><row><cell>75</cell><cell>19.41</cell><cell>20.00</cell></row><row><cell>80</cell><cell>-</cell><cell>17.60</cell></row><row><cell>85</cell><cell>-</cell><cell>17.57</cell></row><row><cell>90</cell><cell>-</cell><cell>18.16</cell></row><row><cell>95</cell><cell>-</cell><cell>17.62</cell></row><row><cell>100</cell><cell>-</cell><cell>18.95</cell></row><row><cell>105</cell><cell>-</cell><cell>17.98</cell></row><row><cell>110</cell><cell>-</cell><cell>18.04</cell></row><row><cell>115</cell><cell>-</cell><cell>18.63</cell></row><row><cell>120</cell><cell>-</cell><cell>18.36</cell></row><row><cell>125</cell><cell>-</cell><cell>18.20</cell></row><row><cell>130</cell><cell>-</cell><cell>18.25</cell></row><row><cell>135</cell><cell>-</cell><cell>18.46</cell></row><row><cell>140</cell><cell>-</cell><cell>18.09</cell></row><row><cell>145</cell><cell>-</cell><cell>18.32</cell></row><row><cell>150</cell><cell>-</cell><cell>17.94</cell></row><row><cell>155</cell><cell>-</cell><cell>18.21</cell></row><row><cell>160</cell><cell>-</cell><cell>18.25</cell></row><row><cell>165</cell><cell>-</cell><cell>18.38</cell></row><row><cell>170</cell><cell>-</cell><cell>17.69</cell></row><row><cell>average</cell><cell>-</cell><cell>18.91</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 :</head><label>11</label><figDesc>Using different ratios of authentic to synthetic parallel data and its effect on the performance of Vietnamese-English NMT. Evaluation scores (BLEU) on the test set for each checkpoint (tag-less bt colour code: BLACK -pre-train, RED -fine-tune)<ref type="bibr" target="#b22">23</ref>.87 23.62 16.69 18.28 18.52 100 22.89 23.69 23.37 23.04 18.23 18.31 105 22.96 24.21 23.58 24.05 18.60 18.80 110 22.80 23.91 23.49 24.81 18.53 18.42 115 22.92 23.90 23.90 24.87 24.31 19.22 120 22.83 24.33 23.69 25.50 24.93 19.07 125 22.29 24.08 23.79 24.86 25.55 18.47 130 22.53 24.34 24.10 26.29 25.79 19.44 135 22.43 24.17 24.01 25.77 26.21 19.48 140 22.36 24.27 24.52 25.95 26.11 19.16 145 22.34 24.28 24.57 25.94 26.33 19.14 150 22.08 24.44 23.88 25.96 26.23 19.01 155 22.08 24.15 24.62 25.76 26.21</figDesc><table><row><cell>training step</cell><cell></cell><cell>tagged bt</cell><cell></cell><cell cols="3">tag-less bt (joint BPE)</cell></row><row><cell>(thousands)</cell><cell>1:1</cell><cell>1:3</cell><cell>1:5</cell><cell>1:1</cell><cell>1:3</cell><cell>1:5</cell></row><row><cell>5</cell><cell>2.31</cell><cell>1.78</cell><cell>1.87</cell><cell>1.65</cell><cell>2.87</cell><cell>1.82</cell></row><row><cell>10</cell><cell>3.31</cell><cell>3.64</cell><cell>3.23</cell><cell>8.68</cell><cell>9.78</cell><cell>6.16</cell></row><row><cell>15</cell><cell>7.35</cell><cell>9.88</cell><cell>4.05</cell><cell cols="2">11.93 13.06</cell><cell>12.81</cell></row><row><cell>20</cell><cell cols="2">14.45 15.29</cell><cell>5.08</cell><cell cols="2">13.52 14.11</cell><cell>14.20</cell></row><row><cell>25</cell><cell cols="2">17.12 16.99</cell><cell>5.71</cell><cell cols="2">14.36 15.03</cell><cell>13.63</cell></row><row><cell>30</cell><cell cols="5">18.54 19.22 13.36 15.24 15.92</cell><cell>16.08</cell></row><row><cell>35</cell><cell cols="5">20.45 19.62 16.41 15.49 15.95</cell><cell>15.24</cell></row><row><cell>40</cell><cell cols="5">21.76 19.71 17.28 15.90 16.39</cell><cell>16.34</cell></row><row><cell>45</cell><cell cols="5">21.86 20.99 19.20 16.18 17.08</cell><cell>17.32</cell></row><row><cell>50</cell><cell cols="5">21.96 22.03 19.93 16.05 17.15</cell><cell>16.36</cell></row><row><cell>55</cell><cell cols="5">22.87 21.85 20.33 16.52 17.29</cell><cell>17.45</cell></row><row><cell>60</cell><cell cols="5">22.71 22.03 20.85 16.58 17.15</cell><cell>16.99</cell></row><row><cell>65</cell><cell cols="5">23.05 22.33 21.62 16.30 17.37</cell><cell>17.83</cell></row><row><cell>70</cell><cell cols="5">23.33 22.95 21.83 16.82 17.51</cell><cell>16.94</cell></row><row><cell>75</cell><cell cols="5">22.84 23.05 21.84 16.80 17.92</cell><cell>18.36</cell></row><row><cell>80</cell><cell cols="5">23.25 22.90 23.05 16.76 17.96</cell><cell>17.86</cell></row><row><cell>85</cell><cell cols="5">23.51 23.25 22.56 16.54 17.94</cell><cell>18.14</cell></row><row><cell>90</cell><cell cols="5">22.99 23.91 23.15 16.47 18.58</cell><cell>18.44</cell></row><row><cell>95</cell><cell cols="6">23.06 24.82</cell></row><row><cell>160</cell><cell cols="3">21.74 24.50 24.49</cell><cell>-</cell><cell>26.45</cell><cell>25.79</cell></row><row><cell>165</cell><cell cols="3">22.14 24.76 24.12</cell><cell>-</cell><cell>26.53</cell><cell>26.21</cell></row><row><cell>170</cell><cell cols="3">22.73 24.56 25.47</cell><cell>-</cell><cell>26.40</cell><cell>26.27</cell></row><row><cell>175</cell><cell>-</cell><cell>24.29</cell><cell>-</cell><cell>-</cell><cell>26.17</cell><cell>26.51</cell></row><row><cell>180</cell><cell>-</cell><cell>24.09</cell><cell>-</cell><cell>-</cell><cell>26.23</cell><cell>26.78</cell></row><row><cell>185</cell><cell>-</cell><cell>24.34</cell><cell>-</cell><cell>-</cell><cell>26.21</cell><cell>26.91</cell></row><row><cell>190</cell><cell>-</cell><cell>24.27</cell><cell>-</cell><cell>-</cell><cell>26.16</cell><cell>26.60</cell></row><row><cell>195</cell><cell>-</cell><cell>24.32</cell><cell>-</cell><cell>-</cell><cell>26.21</cell><cell>26.57</cell></row><row><cell>200</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>26.58</cell></row><row><cell>205</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>26.64</cell></row><row><cell>210</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>25.70</cell></row><row><cell>215</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>26.29</cell></row><row><cell>average</cell><cell cols="5">22.73 25.05 25.47 26.15 26.83</cell><cell>27.14</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 12 :</head><label>12</label><figDesc>Fine-tuning the tagged and standard back-translations</figDesc><table><row><cell>training step (thousands)</cell><cell>tagged bt (1:5)</cell><cell>standard bt (1:3)</cell></row><row><cell>165</cell><cell>23.38</cell><cell>24.21</cell></row><row><cell>170</cell><cell>25.11</cell><cell>24.61</cell></row><row><cell>175</cell><cell>25.55</cell><cell>24.87</cell></row><row><cell>180</cell><cell>24.99</cell><cell>25.15</cell></row><row><cell>185</cell><cell>24.79</cell><cell>24.11</cell></row><row><cell>190</cell><cell>23.58</cell><cell>23.98</cell></row><row><cell>195</cell><cell>23.37</cell><cell>23.36</cell></row><row><cell>200</cell><cell>23.15</cell><cell>22.97</cell></row><row><cell>average</cell><cell>25.64</cell><cell>25.32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 13 :</head><label>13</label><figDesc>This table showshow often a conclusion with 95% statistical significance is made for comparing the various approaches. We used different sample sizes of 100, 500 and 1000 sentences for each of the approach on English-Vietnamese and English-German low resource NMT.</figDesc><table><row><cell>De-En</cell><cell>Sample size</cell><cell>100 500 1000</cell><cell>100% 100% 100%</cell><cell>100% 100% 100%</cell><cell>100% 100% 100%</cell><cell>100% 100% 100%</cell><cell>87.1% 87.4% 84%</cell><cell>0% 0% 0%</cell><cell>100% 100% 100%</cell><cell></cell><cell>100% 100% 100%</cell><cell></cell><cell>100% 100% 100%</cell></row><row><cell></cell><cell>BLEU</cell><cell>difference</cell><cell>4.92</cell><cell>6.81</cell><cell>1.89</cell><cell>5.09</cell><cell>0.16</cell><cell>-1.72</cell><cell>7.89</cell><cell></cell><cell>2.96</cell><cell></cell><cell>1.08</cell></row><row><cell>Vi-En</cell><cell>Sample size</cell><cell>100 500 1000</cell><cell>100% 100% 100%</cell><cell>100% 100% 100%</cell><cell>27% 28% 30.2%</cell><cell>100% 100% 100%</cell><cell>92.8% 94.6% 94.8%</cell><cell>98.8% 98.2% 98%</cell><cell>100% 100% 100%</cell><cell></cell><cell>100% 100% 100%</cell><cell></cell><cell>100% 100% 100%</cell></row><row><cell></cell><cell>BLEU</cell><cell>difference</cell><cell>3.06</cell><cell>2.83</cell><cell>-0.23</cell><cell>3.55</cell><cell>0.49</cell><cell>0.72</cell><cell>4.61</cell><cell></cell><cell>1.55</cell><cell></cell><cell>1.78</cell></row><row><cell></cell><cell>System Comparison</cell><cell></cell><cell>Standard BT is better than baseline</cell><cell>Tagged BT is better than baseline</cell><cell>Tagged BT is better than Standard BT</cell><cell>Tag-less BT is better than baseline</cell><cell>Tag-less BT better than Standard BT</cell><cell>Tag-less BT is better than Tagged BT</cell><cell>Tag-less BT (joint BPE) is better</cell><cell>than baseline</cell><cell>Tag-less BT (joint BPE) is better</cell><cell>than Standard BT</cell><cell>Tag-less BT (joint BPE) is better</cell><cell>than Tagged BT</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict of interest</head><p>The authors declare that they have no conflict of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>In this section, we provided the complete evaluation scores on the test set for all the models trained.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/.Softwareavailablefromtensorflow.org" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1409.0473" />
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<editor>Y. Bengio, Y. LeCun</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Regularization techniques for fine-tuning in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V M</forename><surname>Barone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Germann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1156</idno>
		<ptr target="https://www.aclweb.org/anthology/D17-1156" />
	</analytic>
	<monogr>
		<title level="m">Proceedings ofthe 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1489" to="1494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leveling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pecina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Saint-Amand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tamchyna</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/w14-3302</idno>
		<title level="m">Proceedings of the Ninth Workshop on Statistical Machine Translation</title>
		<meeting>the Ninth Workshop on Statistical Machine Translation<address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="12" to="58" />
		</imprint>
	</monogr>
	<note>Findings of the 2014 Workshop on Statistical Machine Translation</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Using Monolingual Data in Neural Machine Translation: a Systematic Study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Burlot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yvon</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-6315</idno>
		<ptr target="https://www.aclweb.org/anthology/W18-6315" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Research Papers</title>
		<meeting>the Third Conference on Machine Translation: Research Papers<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="144" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Tagged Back-Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Caswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-5206.URLhttps:/www.aclweb.org/anthology/W19-5206</idno>
		<ptr target="DOI10.18653/v1/W19-5206.URLhttps://www.aclweb.org/anthology/W19-5206" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Conference on Machine Translation</title>
		<meeting>the Fourth Conference on Machine Translation<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="53" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Overview of the IWSLT 2017 Evaluation Campaign</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stüker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sudoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yoshino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Federmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Workshop on Spoken Language Translation</title>
		<meeting>the 14th International Workshop on Spoken Language Translation<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Report on the 11th IWSLT Evaluation Campaign, IWSLT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stüker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Workshop on Spoken Language Translation</title>
		<meeting>the 11th Workshop on Spoken Language Translation<address><addrLine>Lake Tahoe, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ç</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1406.1</idno>
		<ptr target="http://arxiv.org/abs/1406.1078" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An empirical comparison of domain adaptation methods for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dabre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kurohashi</surname></persName>
		</author>
		<idno>10.18653/ v1/P17-2061</idno>
		<ptr target="https://www.aclweb.org/anthology/P17-2061" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="385" to="391" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A survey of domain adaptation for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/C18-1111" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1304" to="1319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Copied Monolingual Data Improves Low-Resource Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Currey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Miceli Barone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Heafield</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-4715</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<meeting>the Second Conference on Machine Translation<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="148" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">NICT&apos;s Supervised Neural Machine Translation Systems for the WMT19 News Translation Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dabre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fujita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Conference on Machine Translation (WMT)</title>
		<meeting>the Fourth Conference on Machine Translation (WMT)<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="168" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Language Modeling with Gated Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="933" to="941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1807.03819" />
		<title level="m">Universal Transformers. ICLR pp</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Understanding Back-Translation at Scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1045</idno>
		<ptr target="https://www.aclweb.org/anthology/D18-1045" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="489" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Back-Translation Sampling by Targeting Difficult Words in Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fadaee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1040</idno>
		<ptr target="https://www.aclweb.org/anthology/D18-1040" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="436" to="446" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Convolutional Sequence to Sequence Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v70/gehring17a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>D. Precup, Y.W. Teh</editor>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1243" to="1252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generalizing Back-Translation in Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Graça</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schamper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khadivi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-5205</idno>
		<ptr target="https://www.aclweb.org/anthology/W19-5205" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Conference on Machine Translation</title>
		<meeting>the Fourth Conference on Machine Translation<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="45" to="52" />
		</imprint>
	</monogr>
	<note>Research Papers)</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On integrating a language model into neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.csl.2017.01.014</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="137" to="148" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dual Learning for Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Ma</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=3157096.3157188" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS&apos;16</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems, NIPS&apos;16<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="820" to="828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Iterative Back-Translation for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">C D</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Neural Machine Translation and Generation</title>
		<meeting>the 2nd Workshop on Neural Machine Translation and Generation<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="18" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Enhancement of Encoder and Attention Using Target Monolingual Corpora in Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Imamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fujita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Neural Machine Translation and Generation</title>
		<meeting>the 2nd Workshop on Neural Machine Translation and Generation<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="55" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Google&apos;s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1611.04558" />
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="339" to="351" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6980" />
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<editor>Y. Bengio, Y. LeCun</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">OpenNMT: Open-Source Toolkit for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Senellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/P17-4012" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of {ACL} 2017, System Demonstrations pp</title>
		<meeting>{ACL} 2017, System Demonstrations pp</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="67" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Trivial Transfer Learning for Low-Resource Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kocmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bojar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-6325</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation (WMT)</title>
		<meeting>the Third Conference on Machine Translation (WMT)<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="244" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">CUNI Submission for Low-Resource Languages in WMT News</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kocmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bojar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Conference on Machine Translation (WMT)</title>
		<meeting>the Fourth Conference on Machine Translation (WMT)<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="234" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Statistical significance tests for machine translation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/W04-3250" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2004 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="388" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Moses: Open Source Toolkit for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Herbst</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=1557769.1557821" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL &apos;07</title>
		<meeting>the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL &apos;07<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Six Challenges for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Knowles</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/w17-3204</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Neural Machine Translation</title>
		<meeting>the First Workshop on Neural Machine Translation<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="28" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Statistical Phrase-Based Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcu</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/N031017" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="127" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Effective Approaches to Attention-based Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1166</idno>
		<ptr target="https://www.aclweb.org/anthology/D15-1166" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1301.3781" />
	</analytic>
	<monogr>
		<title level="m">1st International Conference on Learning Representations, {ICLR}</title>
		<editor>Y. Bengio, Y. LeCun</editor>
		<meeting><address><addrLine>Arizona, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Transfer Learning across Low-Resource, Related Languages for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="296" to="301" />
		</imprint>
	</monogr>
	<note>Asian Federation of Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Bi-Directional Neural Machine Translation with Synthetic Parallel Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carpuat</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-2710</idno>
		<ptr target="https://www.aclweb.org/anthology/W18-2710" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Neural Machine Translation and Generation</title>
		<meeting>the 2nd Workshop on Neural Machine Translation and Generation<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="84" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Scaling Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<idno>10.18653/ v1/W18-6301</idno>
		<ptr target="https://www.aclweb.org/anthology/W18-6301" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Research Papers</title>
		<meeting>the Third Conference on Machine Translation: Research Papers<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Bleu: a Method for Automatic Evaluation of Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073135</idno>
		<ptr target="https://www.aclweb.org/anthology/P02-1040" />
	</analytic>
	<monogr>
		<title level="m">Linguistics, Proceedings of the 40th Annual Meeting of the Association for Computational</title>
		<meeting><address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>De-Vito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<ptr target="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alché-Buc, E. Fox, R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Adaptation of Machine Translation Models with Back-translated Data using Transductive Data Selection Methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Poncelas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Maillette De Buy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Way</surname></persName>
		</author>
		<idno>abs/1906.0</idno>
		<ptr target="http://arxiv.org/abs/1906.07808" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Investigating Backtranslation in Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Poncelas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shterionov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Way</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Maillette De Buy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Passban</surname></persName>
		</author>
		<idno>abs/1804.0</idno>
		<ptr target="http://arxiv.org/abs/1804.06189" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Machine Translation Using Syntactic Analysis. Doctoral</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Popel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
		<respStmt>
			<orgName>Charles University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Sequence Level Training with Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Improving Neural Machine Translation Models with Monolingual Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="86" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Neural Machine Translation of Rare Words with Subword Units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/P16-1162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<title level="m">Sequence to Sequence Learning with Neural Networks</title>
		<imprint>
			<publisher>NIPS</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Attention Is All You Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">31st Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Dynamic data selection for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Der Wees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d17-1147</idno>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2017 -Conference on Empirical Methods in Natural Language Processing, Proceedings</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1400" to="1410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">FixyNN: Efficient Hardware for Mobile Computer Vision via Transfer Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Whatmough</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Venkataramanaiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mattina</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1902.11128" />
		<imprint>
			<date type="published" when="1902-01" />
		</imprint>
	</monogr>
	<note type="report_type">CoRR abs</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Pay Less Attention with Lightweight and Dynamic Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SkVhlh09tX" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, {ICLR}</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Effectively training neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<idno>10.1016/j. neucom.2018.12.032</idno>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">333</biblScope>
			<biblScope unit="page" from="240" to="247" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Exploiting Source-side Monolingual Data in Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zong</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d16-1160</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1535" to="1545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Joint Training for Neural Machine Translation Models with Monolingual Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<idno>abs/1803.00353</idno>
		<ptr target="http://arxiv.org/abs/1803.00353" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Transfer Learning for Low-Resource Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Knight</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1163</idno>
		<ptr target="https://www.aclweb.org/anthology/D16-1163" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1568" to="1575" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
