<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Optimal Transport for structured data with application on graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Titouan</forename><surname>Vayer</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laetitia</forename><surname>Chapel</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Flamary</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Tavenard</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Courty</surname></persName>
						</author>
						<title level="a" type="main">Optimal Transport for structured data with application on graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work considers the problem of computing distances between structured objects such as undirected graphs, seen as probability distributions in a specific metric space. We consider a new transportation distance (i.e. that minimizes a total cost of transporting probability masses) that unveils the geometric nature of the structured objects space. Unlike Wasserstein or Gromov-Wasserstein metrics that focus solely and respectively on features (by considering a metric in the feature space) or structure (by seeing structure as a metric space), our new distance exploits jointly both information, and is consequently called Fused Gromov-Wasserstein (FGW). After discussing its properties and computational aspects, we show results on a graph classification task, where our method outperforms both graph kernels and deep graph convolutional networks. Exploiting further on the metric properties of FGW, interesting geometric objects such as Fréchet means or barycenters of graphs are illustrated and discussed in a clustering context.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>There is a longstanding line of research on learning from structured data, i.e. objects that are a combination of a feature and structural information (see for example <ref type="bibr" target="#b4">(Bakir et al., 2007;</ref><ref type="bibr" target="#b5">Battaglia et al., 2018)</ref>). As immediate instances, graph data are usually ensembles of nodes with attributes (typically R d vectors) linked by some specific relation. Notable examples are found in chemical compounds or molecules modeling , brain connectivity <ref type="bibr" target="#b24">(Ktena et al., 2017)</ref>, or social networks <ref type="bibr">(Yanardag &amp; Vishwanathan, 2015)</ref>. This generic family of objects also encompasses time series <ref type="bibr" target="#b9">(Cuturi &amp; Blondel, 2017)</ref>, trees <ref type="bibr" target="#b11">(Day, 1985)</ref>  even images <ref type="bibr" target="#b3">(Bach &amp; Harchaoui, 2007)</ref>.</p><p>Being able to leverage on both feature and structural information in a learning task is a tedious task, that requires the association in some ways of those two pieces of information in order to capture the similarity between the structured data. Several kernels have been designed to perform this task <ref type="bibr" target="#b35">(Shervashidze et al., 2011;</ref><ref type="bibr" target="#b42">Vishwanathan et al., 2010)</ref>. As a good representative of those methods, the Weisfeiler-Lehman kernel <ref type="bibr" target="#b42">(Vishwanathan et al., 2010)</ref> captures in each node a notion of vicinity by aggregating, in the sense of the topology of the graph, the surrounding features. Recent advances in graph convolutional networks <ref type="bibr" target="#b7">(Bronstein et al., 2017;</ref><ref type="bibr" target="#b21">Kipf &amp; Welling, 2016;</ref><ref type="bibr" target="#b13">Defferrard et al., 2016)</ref> allows learning end-to-end the best combination of features by relying on parametric convolutions on the graph, i.e. learnable linear combinations of features. In the end, and in order to compare two graphs that might have different number of nodes and connections, those two categories of methods build a new representation for every graph that shares the same space, and that is amenable to classification.</p><p>A transportation distance between structured data. Contrasting with those previous methods, we suggest in this paper to see graphs as probability distributions, embedded in a specific metric space. We propose to define a specific notion of distance between those probability distributions, that can be used in most of the classical machine learning approaches. Beyond its mathematical properties, disposing of a distance between structured data, provided it is meaningful, is desirable in many ways: i) it can then be plugged into distance-based machine learning algorithms such as k-nn or t-SNE ii) its quality is not dependent on the learning set size, and iii) it allows considering interesting quantities such as geodesic interpolation or barycenters. To the best of our knowledge, this is one of the first attempts to define such a distance on structured data.</p><p>Yet, defining this distance is not a trivial task. While features can always be compared using a standard metric, such as 2 , comparing structures requires a notion of similarity which can be found via the notion of isometry, since the graph nodes are not ordered (we define later on which cases two graphs are considered identical). We use the notion of transportation distance to compare two graphs represented as probability distributions. Optimal transport (OT) have arXiv:1805.09114v3 [stat.ML] 13 May 2019 inspired a number of recent breakthroughs in machine learning (e.g. <ref type="bibr" target="#b19">(Huang et al., 2016;</ref><ref type="bibr" target="#b8">Courty et al., 2017;</ref><ref type="bibr" target="#b2">Arjovsky et al., 2017)</ref>) because of its capacity to compare empirical distributions, and also the recent advances in solving the underlying problem <ref type="bibr" target="#b32">(Peyré &amp; Cuturi, 2018</ref>). Yet, the natural formulation of OT cannot leverage the structural information of objects since it only relies on a cost function that compares their feature representations. However, some modifications over OT formulation have been proposed in order to compare structural information of objects. Following the pioneering work by Mémoli <ref type="bibr" target="#b27">(Memoli, 2011)</ref>, <ref type="bibr" target="#b33">Peyré et al. (Peyré et al., 2016)</ref> propose a way of comparing two distance matrices that can be seen as representations of some objects' structures. They use an OT metric called Gromov-Wasserstein distance capable of comparing two distributions even if they do not lie in the same ground space and apply it to compute barycenter of molecular shapes. Even though this approach has wide applications, it only encodes the intrinsic structural information in the transportation problem. To the best of our knowledge, the problem of including both structural and feature information in a unified OT formulation remains largely under-addressed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OT distances that include both features and structures.</head><p>Recent approaches tend to incorporate some structure information as a regularization of the OT problem. For example in <ref type="bibr" target="#b1">(Alvarez-Melis et al., 2018)</ref> and , authors constrain transport maps to favor some assignments in certain groups. These approaches require a known and simple structure such as class clusters to work but do not generalize well to more general structural information. In their work <ref type="bibr" target="#b39">(Thorpe et al., 2017)</ref>, propose an OT distance that combines both a Lagrangian formulation of a signal and its temporal structural information. They define a metric, called Transportation L p distance, that can be seen as a distance over the coupled space of time and feature. They apply it for signal analysis and show that combining both structure and feature tends to better capture the signal information. Yet, for their approach to work, the structure and feature information should lie in the same ambiant space, which is not a valid assumption for more general problems such as similarity between graphs. In <ref type="bibr" target="#b30">(Nikolentzos et al., 2017)</ref>, authors propose a graph similarity measure for discrete labeled graph with OT. Using the eigenvector decomposition of the adjency matrix, which captures graph connectivities, nodes of a graph are first embedded in a new space, then a ground metric based on the distance in both this embedding and the labels is used to compute a Wasserstein distance serving as a graph similarity measure.</p><p>Contributions. After defining structured data as probability measures (Section 2), we propose a new framework capable of taking into account both structure and feature information into the optimal transport problem. The framework can compare any usual structured machine learning data even if the feature and structure information dwell in spaces of different dimensions, allowing the comparison of undirected labeled graphs. The framework is based on a distance that embeds a trade-off parameter which allows balancing the importance of the features and the structure. We propose numerical algorithms for computing this distance (Section 3), and we evaluate it (Section 4) on both synthetic and real-world graph datasets. We also illustrate the notion of graph barycenters in a clustering problem.</p><p>Notations. The simplex histogram with n bins will be denoted as Σ n = {h ∈ (R * + ) n , n i=1 h i = 1, }. We note ⊗ the tensor-matrix multiplication, i.e. for a tensor L =</p><formula xml:id="formula_0">(L i,j,k,l ), L ⊗ B is the matrix k,l L i,j,k,l B k,l i,j .</formula><p>.</p><p>is the matrix scalar product associated with the Frobenius norm. For x ∈ Ω, δ x denotes the Dirac measure in x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Structured data as probability measures</head><p>In this paper, we focus on comparing structured data which combine a feature and a structure information. More formally, we consider undirected labeled graphs as tuples of the form G = (V, E, f , s ) where (V, E) are the set of vertices and edges of the graph. f : V → Ω f is a labelling function which associates each vertex v i ∈ V with a feature a i def = f (v i ) in some feature metric space (Ω f , d). We will denote by feature information the set of all the features (a i ) i of the graph. Similarly, s : V → Ω s maps a vertex v i from the graph to its structure representation x i def = s (v i ) in some structure space (Ω s , C) specific to each graph. C : Ω s × Ω s → R + is a symmetric application which aims at measuring the similarity between the nodes in the graph. Unlike the feature space however, Ω s is implicit and in practice, knowing the similarity measure C will be sufficient. With a slight abuse of notation, C will be used in the following to denote both the structure similarity measure and the matrix that encodes this similarity between pairs of nodes in the graph (C(i, k) = C(x i , x k )) i,k . Depending on the context, C can either encode the neighborhood information of the nodes, the edge information of the graph or more generally it can model a distance between the nodes such as the shortest path distance or the harmonic distance <ref type="bibr" target="#b40">(Verma &amp; Zhang, 2017)</ref>. When C is a metric, such as the shortest-path distance, we naturally endow the structure with the metric space (Ω s , C). We will denote by structure information the set of all the structure embeddings (x i ) i of the graph.</p><p>We propose to enrich the previously described graph with a histogram which serves the purpose of signaling the relative importance of the vertices in the graph. To do so, if we } } } <ref type="figure">Figure 1</ref>. (Left) Labeled graph with (ai)i its feature information, (xi)i its structure information and histogram (hi)i that measures the relative importance of the vertices. (Right) Associated structured data which is entirely described by a fully supported probability measure µ over the product space of feature and structure, with marginals µX and µA on the structure and the features respectively. assume that the graph has n vertices, we equip those vertices with weights (h i ) i ∈ Σ n . Through this procedure, we derive the notion of structured data as a tuple S = (G, h G ) where G is a graph as described previously and h G is a function that associates a weight to each vertex. This definition allows the graph to be represented by a fully supported probability measure over the product space feature/structure µ = n i=1 h i δ (xi,ai) which describes the entire structured data (see <ref type="figure">Fig. 1</ref>). When all the weights are equal (i.e. h i = 1 n ), so all vertices have the same relative importance, the structured data holds the exact same information as its graph. However, weights can be used to encode some a priori information. For instance on segmented images, one can construct a graph using the spatial neighborhood of the segmented zones, the features can be taken as the average color in the zone, and the weights as the ratio of image pixels in the zone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Fused Gromov-Wasserstein approach for structured data</head><p>We aim at defining a distance between two graphs G 1 and G 2 , described respectively by their probability measure µ = n i=1 h i δ (xi,ai) and ν = m i=1 g j δ (yj ,bj ) , where h ∈ Σ n and g ∈ Σ m are histograms. Without loss of generality we suppose (x i , a i ) = (x j , a j ) for i = j (same for y j and b j ).</p><p>We introduce Π(h, g) the set of all admissible couplings between h and g, i.e. the set :</p><formula xml:id="formula_1">Π(h, g) = {π ∈ R n×m + s.t. n i=1 π i,j = h j , m j=1 π i,j = g i },</formula><p>where π i,j represents the amount of mass shifted from the bin h i to g j for a coupling π. To that extent, the matrix π describes a probabilistic matching of the nodes of the two graphs. M AB = (d(a i , b j )) i,j is a n × m matrix standing <ref type="figure">Figure 2</ref>. F GW loss Eq for a coupling π depends on both a similarity between each feature of each node of each graph (d(ai, bj))i,j and between all intra-graph structure similarities</p><formula xml:id="formula_2">(|C1(xi, x k ) − C2(xj, x l )|) i,j,k,l .</formula><p>for the distance between the features. The structure matrices are denoted C 1 and C 2 , and µ X and µ A (resp. ν Y and ν B ) are representative of the marginals of µ (resp. ν) w.r.t. the structure and feature respectively (see <ref type="figure">Fig. 1</ref>). We also define the similarity between the structures by measuring the similarity between all pairwise distances within each graph thanks to the 4-dimensional tensor L(C 1 , C 2 ):</p><formula xml:id="formula_3">L i,j,k,l (C 1 , C 2 ) = |C 1 (i, k) − C 2 (j, l)|.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">F GW distance</head><p>We define a novel Optimal Transport discrepancy called the Fused Gromov-Wasserstein distance. It is defined for a trade-off parameter α ∈ [0, 1] as</p><formula xml:id="formula_4">F GW q,α (µ, ν) = min π∈Π(h,g) E q (M AB , C 1 , C 2 , π) (1) where Eq(MAB, C1, C2, π) = (1 − α)M q AB + αL(C1, C2) q ⊗ π, π = i,j,k,l (1 − α)d(ai, bj) q + α|C1(i, k) − C2(j, l)| q πi,jπ k,l</formula><p>The F GW distance looks for the coupling π between the vertices of the graph that minimizes the cost E q which is a linear combinaison of a cost d(a i , b j ) of transporting one feature a i to a feature b j and a cost |C 1 (i, k) − C 2 (j, l)| of transporting pairs of nodes in each structure (see <ref type="figure">Fig. 2</ref>). As such, the optimal coupling tends to associate pairs of feature and structure points with similar distances within each structure pair and with similar features. As an important feature of F GW , by relying on a sum of (inter-and intra-)vertex-to-vertex distances, it can handle structured data with continuous attributed or discrete labeled nodes (thanks to the definition of d) and can also be computed even if the graphs have different number of nodes.</p><p>This new distance is called the F GW distance as it acts as a generalization of the Wasserstein <ref type="bibr" target="#b41">(Villani, 2008)</ref> and Gromov-Wasserstein <ref type="bibr" target="#b27">(Memoli, 2011;</ref><ref type="bibr" target="#b37">Solomon et al., 2016)</ref> distances as stated in the following theorem:</p><p>Theorem 3.1. Interpolation properties.</p><p>As α tends to zero, the F GW distance recovers the Wasserstein distance between the features</p><formula xml:id="formula_5">W q (µ A , ν B ) q lim α→0 F GW q,α (µ, ν) = W q (µ A , ν B ) q = min π∈Π(h,g)</formula><p>π, M q AB and as α tends to one, we recover the Gromov-Wasserstein distance GW q (µ X , ν Y ) q between the structures:</p><formula xml:id="formula_6">lim α→1 F GW q,α (µ, ν) = GW q (µ X , ν Y ) q = min π∈Π(h,g) L(C 1 , C 2 ) q ⊗ π, π</formula><p>Proof of this theorem can be found in the supplementary material.</p><p>Similarly to the Wasserstein and Gromov-Wasserstein distances, F GW enjoys metric properties over the space of structured data as stated in the following theorem: Theorem 3.2. F GW defines a metric for q = 1 and a semi-metric for q &gt; 1.</p><p>If q = 1, and if C 1 , C 2 are distance matrices then F GW defines a metric over the space of structured data quotiented by the measure preserving isometries that are also feature preserving. More precisely, F GW satisfies the triangle inequality and is nul iff n = m and there exists a bijection σ : {1, .., n} → {1, .., n} such that :</p><formula xml:id="formula_7">∀i ∈ {1, .., n}, h i = g σ(i) (2) ∀i ∈ {1, .., n}, a i = b σ(i) (3) ∀i, k ∈ {1, .., n} 2 , C 1 (i, k) = C 2 (σ(i), σ(k)) (4)</formula><p>If q &gt; 1, the triangle inequality is relaxed by a factor 2 q−1 such that F GW defines a semi-metric.</p><p>All proofs can be found in the supplementary material. The resulting application σ preserves the weight of each node (eq. (19)), the features (eq. (20)) and the and the pairwise structure relation between the nodes (eq. (21)). For example, comparing two graphs with uniform weights for the vertices and with shortest path structure matrices, the F GW distance vanishes iff the graphs have the same number of vertices and iff there exists a one-to-one mapping between the vertices of the graphs which respects both the shortest paths and the features. More informally, it means that graphs have vertices with the same labels connected by the same edges.</p><p>The metric F GW is fully unsupervised and can be used in a wide set of applications such as k-nearest-neighbors, distance-substitution kernels, pseudo-Euclidean embeddings, or representative-set methods. Arguably, such a distance also allows for a fine interpretation of the similarity (through the optimal mapping π), contrary to end-to-end learning machines such as neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Fused Gromov-Wasserstein barycenter</head><p>OT barycenters have many desirable properties and applications <ref type="bibr" target="#b0">(Agueh &amp; Carlier, 2011;</ref><ref type="bibr" target="#b33">Peyré et al., 2016</ref>), yet no formulation can leverage both structural and feature information in the barycenter computation. In this section, we consider the F GW distance to define a barycenter of a set of structured data as a Fréchet mean. We look for the structured data µ that minimizes the sum of (weighted) F GW distances within a given set of structured data (µ k ) k associated with structure matrices (C k ) k , features (B k ) k and base histograms (h k ) k . For simplicity, we assume that the histogram h associated to the barycenter is known and fixed; in other words, we set the number of vertices N and the weight associated to each of them.</p><p>In this context, for a fixed N ∈ N and (λ k ) k such that k λ k = 1 , we aim to find the set of features A = (a i ) i and the structure matrix C of the barycenter that minimize the following equation:</p><formula xml:id="formula_8">min µ k λ k F GW q,α (µ, µ k ) (5) = min C∈R N ×N , A∈R N ×n ,(π k ) k k λ k E q (M AB k , C, C k , π k )</formula><p>Note that this problem is jointly convex w.r.t. C and A but not w.r.t. π k . We discuss the proposed algorithm to solve this problem in the next section. Interestingly enough, one can derive several variants of this problem, where the features or the structure matrices of the barycenter can be fixed. Solving the related simpler optimization problem extends straightforwardly. We give examples of such barycenters both in the experimental section where we solve a graph based k-means problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Optimization and algorithmic solution</head><p>In this section we discuss the numerical optimization problem for computing the F GW distance between discrete distributions.</p><p>Solving the Quadratic Optimization problem. Equation 11 is clearly a quadratic problem w.r.t. π. Note that despite the apparent O(m 2 n 2 ) complexity of computing the tensor product, one can simplify the sum to complexity O(mn 2 + m 2 n)  when considering q = 2. In this case, the F GW computation problem can be re-written as finding π * such that:</p><formula xml:id="formula_9">π * = arg min π∈Π(h,g) vec(π) T Q(α)vec(π)+vec(D(α)) T vec(π) (6) where Q = −2αC 2 ⊗ K C 1 and D(α) = (1 − α)M AB .</formula><p>⊗ K denotes the Kronecker product of two matrices, vec the column-stacking operator. With such form, the resulting optimal map can be seen as a quadratic regularized map</p><formula xml:id="formula_10">Algorithm 1 Conditional Gradient (CG) for F GW 1: π (0) ← µ X µ Y 2: for i = 1, . . . , do 3: G ← Gradient from Eq. (7) w.r.t. π (i−1) 4:π (i) ← Solve OT with ground loss G 5: τ (i) ← Line-search for loss (11) with τ ∈ (0, 1) using Alg. 2 6: π (i) ← (1 − τ (i) )π (i−1) + τ (i)π(i) 7: end for Algorithm 2 Line-search for CG (q = 2) 1: c C1,C2 from Eq. (6) in (Peyré et al., 2016) 2: a = −2α C 1π (i) C 2 ,π (i) 3: b= (1−α)M AB +αc C1,C2 ,π (i) −2α C 1π (i) C 2 , π (i−1) + C 1 π (i−1) C 2 ,π (i) 4: c = E 2 (M AB , C 1 , C 2 , π (i−1) ) 5: if a &gt; 0 then 6: τ (i) ← min(1, max(0, −b 2a )) 7: else 8: τ (i) ← 1 if a + b &lt; 0 else τ (i) ← 0 9: end if</formula><p>from initial Wasserstein <ref type="bibr" target="#b15">(Ferradans et al., 2014;</ref><ref type="bibr" target="#b17">Flamary et al., 2014)</ref>. However, unlike these approaches, we have a quadratic but provably non convex term. The gradient G that arises from Eq. (11) can be expressed with the following partial derivative w.r.t. π:</p><formula xml:id="formula_11">G = (1 − α)M q AB + 2αL(C 1 , C 2 ) q ⊗ π<label>(7)</label></formula><p>that can be computed with O(mn 2 +m 2 n) operations when q = 2.</p><p>Solving a large scale QP with a classical solver can be computationally expensive. In <ref type="bibr" target="#b15">(Ferradans et al., 2014)</ref>, authors propose a solver for a graph regularized optimal transport problem whose resulting optimization problem is also a QP. We can then directly use their conditional gradient defined in Alg. 1 to solve our optimization problem. It only needs at each iteration to compute the gradient in Eq. <ref type="formula" target="#formula_11">(7)</ref> and to solve a classical OT problem for instance with a network flow algorithm. The line-search part is a constrained minimization of a second degree polynomial function which is adapted to the non convex loss in Alg. 2. While the problem is non convex, conditional gradient is known to converge to a local stationary point (Lacoste-Julien, 2016).</p><p>Solving the barycenter problem with Block Coordinate Descent (BCD). We propose to minimize eq. (5) using a BCD algorithm, i.e. iteratively minimizing with respect to the couplings π k , to the metric C and the feature vector A.</p><p>The minimization of this problem w.r.t. (π k ) k is equivalent to compute S independent Fused Gromov-Wasserstein distances as discussed above. We suppose that the feature space is Ω f = (R d , 2 2 ) and we consider q = 2. Minimization w.r.t. C in this case has a closed form (see Prop. 4 in ) :</p><formula xml:id="formula_12">W = 0 F GW &gt; 0 GW = 0</formula><formula xml:id="formula_13">C ← 1 hh T k λ k π T k C k π k</formula><p>where h is the histogram of the barycenter as discussed in section 3.2. Minimization w.r.t. A can be computed with (eq. (8) in <ref type="bibr" target="#b10">(Cuturi &amp; Doucet, 2014)</ref>):</p><formula xml:id="formula_14">A ← k λ k B k π T k diag( 1 h )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental results</head><p>We now illustrate the behaviour of F GW on synthetic and real datasets. The algorithms presented in the previous section have been implemented using the Python Optimal Transport toolbox  and will be released upon publication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Illustration of F GW on trees</head><p>We construct two trees as illustrated in <ref type="figure" target="#fig_0">Figure 3</ref>, where the 1D node features are shown with colors (in red, features belong to [0, 1] and in blue in [9, 10]). The structure similarity matrices C 1 and C 2 are the shortest path between nodes. <ref type="figure" target="#fig_0">Figure 3</ref> illustrates the behavior of the F GW distance when the trade-off parameter α changes. The left part recovers the Wasserstein distance (α = 0): red nodes are coupled to red ones and the blue nodes to the blue ones. For a alpha close to 1 (right), we recover the Gromov-Wasserstein distance: all couples of points are coupled to another couple of points, without taking into account the features. Both approaches fail in discriminating the two trees. Finally, for an intermediate α in F GW (center), the bottom and first level structure is preserved as well as the feature matching (red on red and blue on blue), resulting on a positive distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Graph-structured data classification</head><p>Datasets We consider 12 widely used benchmark datasets divided into 3 groups. BZR, COX2 <ref type="bibr" target="#b38">(Sutherland et al., 2003)</ref>, PROTEINS, ENZYMES <ref type="bibr" target="#b6">(Borgwardt &amp; Kriegel, 2005)</ref>, CUNEIFORM <ref type="bibr" target="#b22">(Kriege et al., 2018)</ref> and SYNTHETIC <ref type="bibr" target="#b14">(Feragen et al., 2013)</ref> are vector attributed graphs. MUTAG <ref type="bibr" target="#b12">(Debnath et al., 1991)</ref>, PTC-MR  and <ref type="bibr">NCI1 (Wale et al., 2008)</ref> contain graphs with discrete attributes derived from small molecules. IMDB-B, IMDB-M (Yanardag &amp; Vishwanathan, 2015) contain unlabeled graphs derived from social networks. All datas are available in .</p><p>Experimental setup Regarding the feature distance matrix M AB between node features, when dealing with real valued vector attributed graphs, we consider the 2 distance between the labels of the vertices. In the case of graphs with discrete attributes, we consider two settings: in the first one, we keep the original labels (denoted as RAW); we also consider a Weisfeiler-Lehman labeling (denoted as WL) by concatenating the labels of the neighbors. A vector of size H is created by repeating this procedure H times <ref type="bibr" target="#b42">(Vishwanathan et al., 2010;</ref><ref type="bibr" target="#b23">Kriege et al., 2016)</ref>. In both cases, we compute the feature distance matrix by us-</p><formula xml:id="formula_15">ing d(a i , b j ) = H k=0 δ(τ (a k i ), τ (b k j ))</formula><p>where δ(x, y) = 1 if x = y else δ(x, y) = 0 and τ (a k i ) denotes the concatenated label at iteration k (for k = 0 original labels are used). Regarding the structure distances C, they are computed by considering a shortest path distance between the vertices.</p><p>For the classification task, we run a SVM using the indefinite kernel matrix e −γF GW which is seen as a noisy observation of the true positive semidefinite kernel <ref type="bibr" target="#b26">(Luss &amp; d'Aspremont, 2007)</ref>. We compare classification accuracies with the following state-of-the-art graph kernel methods: (SPK) denotes the shortest path kernel <ref type="bibr" target="#b6">(Borgwardt &amp; Kriegel, 2005)</ref>, (RWK) the random walk kernel <ref type="bibr" target="#b18">(Gärtner et al., 2003)</ref>, (WLK) the Weisfeler Lehman kernel <ref type="bibr" target="#b42">(Vishwanathan et al., 2010)</ref>, (GK) the graphlet count kernel <ref type="bibr" target="#b34">(Shervashidze et al., 2009)</ref>. For real valued vector attributes, we consider the HOPPER kernel (HOPPERK) <ref type="bibr" target="#b14">(Feragen et al., 2013)</ref> and the propagation kernel (PROPAK) . We build upon the GraKel library <ref type="bibr" target="#b36">(Siglidis et al., 2018)</ref> to construct the kernels and C-SVM to perform the classification. We also compare F GW with the PATCHY-SAN framework for CNN on graphs <ref type="bibr" target="#b29">(Niepert et al., 2016)</ref>(PSCN) building on our own implementation of the method.</p><p>To provide compare between the methods, most papers about graph classification usually perform a nested cross validation (using 9 folds for training, 1 for testing, and reporting the average accuracy of this experiment repeated 10 times) and report accuracies of the other methods taken from the original papers. However, these comparisons are not fair because of the high variance on most datasets w.r.t. the folds chosen for training and testing. This is why, in our experiments, the nested cross validation is performed on the same folds for training and testing for all methods. In the result tables 1,2 and 3 we add a (*) when the best score does not yield to a significative improvement (based on a Wilcoxon signed rank test on the test scores) compared to the second best one. Note that, because of their small sizes, we repeat the experiments 50 times for MUTAG and PTC-MR datasets. For all methods using SVM, we cross validate the parameter C ∈ {10 −7 , 10 −6 , ..., 10 7 }. The range of the WL parameter H is {0, 1..., 10}, and we also compute this kernel with H fixed at 2, 4. The decay factor λ for RWK {10 −6 , 10 −5 ..., 10 −2 }, for the GK kernel we set the graphlet size κ = 3 and cross validate the precision level and the confidence δ as in the original paper <ref type="bibr" target="#b34">(Shervashidze et al., 2009</ref>). The t max parameter for PROPAK is chosen within {1, 3, 5, 8, 10, 15, 20}. For PSCN, we choose the normalized betweenness centrality as labeling procedure and cross validate the batch size in {10, 15, ..., 35} and number of epochs in {10, 20, ..., 100}. Finally for F GW , γ is cross validated within {2 −10 , 2 −9 , ..., 2 10 } and α is cross validated via a logspace search in [0, 0.5] and symmetrically [0.5, 1] (15 values are drawn).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and discussion</head><p>Vector attributed graphs. The average accuracies reported in <ref type="table" target="#tab_1">Table 1</ref> show that FGW is a clear state-of-the-art method and performs best on 4 out of 6 datasets with performances in the error bars of the best methods on the other two datasets. Results for CUNEIFORM are significantly below those from the original paper <ref type="bibr" target="#b22">(Kriege et al., 2018)</ref> which can be explained by the fact that the method in this paper uses a graph convolutional approach specially designed for this dataset and that experiment settings are different. In comparison, the other competitive methods are less consistent as they exhibit some good performances on some datasets only.</p><p>Discrete labeled graphs. We first note in <ref type="table">Table 2</ref> that F GW using WL attributes outperforms all competitive methods, including F GW with raw features. Indeed, the WL attributes allow encoding more finely the neighborood of the vertices by stacking their attributes, whereas FGW with raw features only consider the shortest path distance between vertices, not their sequence of labels. This result calls for using meaningful feature and/or structure matrices in the FGW definition, that can be dataset-dependant, in order to enhance the performances. We also note that F GW with WL attributes outperforms the WL kernel method, highlighting the benefit of an optimal transport-based distance over a kernel-based similarity. Surprisingly results of PSCN are significantly lower than those from the original paper. We believe that it comes from the difference between the Non-attributed graphs. The particular case of the GW distance for graph classification is also illustrated on social datasets, that contain no labels on the vertices. Accuracies reported in <ref type="table">Table 3</ref> show that it greatly outperforms SPK and GK graph kernel methods. This is, to the best of our knowledge, the first application of GW for social graph classification.</p><p>Comparison between F GW , W and GW During the validation step, the optimal value of α was consistently selected inside the ]0, 1[ interval, excluding 0 and 1, suggesting that both structure and feature pieces of information are necessary (details are given in the supplementary material).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Unsupervised learning: graphs clustering</head><p>In the last experiment, we evaluate the ability of F GW to perform a clustering of multiple graphs and to retrieve meaningful barycenters of such clusters. To do so, we generate a dataset of 4 groups of community graphs. Each graph follows a simple Stochastic Block Model <ref type="bibr">(Wang &amp; Wong, 1987;</ref><ref type="bibr" target="#b31">Nowicki &amp; Snijders, 2001</ref>) and the groups are defined w.r.t. the number of communities inside each graph and the distribution of their labels. The dataset is composed of 40 graphs (10 graphs per group) and the number of nodes of each graph is drawn randomly from {20, 30, ..., 50} as illustrated in <ref type="figure" target="#fig_2">Fig. 4</ref>. We perform a k-means clustering using the F GW barycenter defined in eq. (5) as the centroid of Optimal Transport for structured data with application on graphs Centroids iter <ref type="figure">Figure 5</ref>. Evolution of the centroids of each cluster in the k-means clustering, from (Left) the random initialization (Right) until convergence to the final centroid.</p><p>the groups and the F GW distance for the cluster assignment. We fix the number of nodes of each centroid to 30. We perform a thresholding on the pairwise similarity matrix C of the centroid at the end in order to obtain an adjacency matrix for visualization purposes. The threshold value is empirically chosen so as to minimize the distance induced by the frobenius norm between the original matrix C and the shortest path matrix obtained from the adjency matrix. The evolution of the barycenters along the iterations is reported in <ref type="figure">Figure 5</ref>. We can see that these centroids recover community structures and feature distributions that are representative of their cluster content. On this example, note that the clustering recovers perfectly the known groups in the dataset. To the best of our knowledge, there exists no other method able to perform a clustering of graphs and to retrieve the average graph in each cluster without having to solve a pre-image problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion and conclusion</head><p>Countless problems in machine learning involve structured data, usually stressed in light of the graph formalism. We consider here labeled graphs enriched by an histogram, which naturally leads to represent structured data as probability measures in the joint space of their features and structures. Widely known for their ability to meaningfully compare probability measures, transportation distances are generalized in this paper so as to be suited in the context of structured data, motivating the so-called Fused Gromov-Wasserstein distance. We theoretically prove that it defines indeed a distance on structured data, and consequently on graphs of arbitrary sizes. F GW provides a natural framework for analysis of labeled graphs as we demonstrate on classification, where it reaches and surpasses most of the time the state-of-the-art performances, and in graph-based k-means where we develop a novel approach to represent the clusters centroids using a barycentric formulation of F GW . We believe that this metric can have a significant impact on challenging graph signal analysis problems.</p><p>While we considered a unique measure of distance between nodes in the graph structure (shortest path), other choices could be made with respect to the problem at hand, or eventually learned in an end-to-end manner. The same applies to the distance between features. We also envision a potential use of this distance in deep learning applications where a distance between graph is needed (such as graph autoencoders). Another line of work will also try to lower the computational complexity of the underlying optimization problem to ensure better scalability to very large graphs. 6. Supplementary Material 6.1. Proofs</p><p>First we recall the notations from the paper :</p><p>Let two graphs G 1 and G 2 described respectively by their probability measure µ = n i=1 h i δ (xi,ai) and ν = m i=1 g j δ (yj ,bj ) , where h ∈ Σ n and g ∈ Σ m are histograms with Σ n = {h ∈ (R * + ) n , n i=1 h i = 1, }. We introduce Π(h, g) the set of all admissible couplings between h and g, i.e. the set</p><formula xml:id="formula_16">Π(h, g) = {π ∈ R n×m + s.t. n i=1 π i,j = h j , m j=1 π i,j = g i },</formula><p>where π i,j represents the amount of mass shifted from the bin h i to g j for a coupling π.</p><p>Let (Ω f , d) be a compact measurable space acting as the feature space. We denote the distance between the features as M AB = (d(a i , b j )) i,j , a n × m matrix.</p><p>The structure matrices are denoted C 1 and C 2 , and µ X and µ A (resp. ν Y and ν B ) the marginals of µ (resp. ν) w.r.t. the structure and feature respectively. We also define the similarity between the structures by measuring the similarity between all pairwise distances within each graph thanks to the 4-dimensional tensor L(C 1 , C 2 ):</p><formula xml:id="formula_17">L i,j,k,l (C 1 , C 2 ) = |C 1 (i, k) − C 2 (j, l)|.</formula><p>We also consider the following notations :</p><formula xml:id="formula_18">J q (C 1 , C 2 , π) = i,j,k,l L i,j,k,l (C 1 , C 2 ) q π i,j π k,l<label>(8)</label></formula><formula xml:id="formula_19">H q (M AB , π) = i,j d(a i , b j ) q π i,j (9) Eq(MAB, C1, C2, π) = (1 − α)M q AB + αL(C1, C2) q ⊗ π, π = i,j,k,l (1 − α)d(ai, bj) q + αL i,j,k,l (C1, C2) q πi,jπ k,l<label>(10)</label></formula><p>Respectively J q , H q and E q designate the Gromov-Wasserstein (GW ) loss, the Wasserstein (W ) loss and the F GW loss so that :</p><formula xml:id="formula_20">F GW q,α (µ, ν) = min π∈Π(h,g) E q (M AB , C 1 , C 2 , π) (11) W q (µ A , ν B ) q = min π∈Π(h,g) H q (M AB , π) (12) GW q (µ X , ν Y ) q = min π∈Π(h,g) J q (C 1 , C 2 , π)<label>(13)</label></formula><p>Please note that the minimum exists since we minimize a continuous function over a compact subset of R n×m and hence the F GW distance is well defined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1.">BOUNDS</head><p>We first introduce the following lemma: Lemma 6.1. F GW q,α (µ, ν) is lower-bounded by the straight-forward interpolation between W q (µ A , ν B ) q and GW q (µ X , ν Y ) q :</p><formula xml:id="formula_21">F GW q,α (µ, ν) ≥ (1−α)W q (µ A , ν B ) q +αGW q (µ X , ν Y ) q<label>(14)</label></formula><p>Proof. Let π α be the coupling that minimizes E q (M AB , C 1 , C 2 , ·). Then we have:</p><formula xml:id="formula_22">F GW q,α (µ, ν) = E q (M AB , C 1 , C 2 , π α ) = (1 − α)H q (M AB , π α ) + αJ q (C 1 , C 2 , π α )</formula><p>But also:</p><formula xml:id="formula_23">W q (µ A , ν B ) q ≤ H q (M AB , π α ) GW q (µ X , ν Y ) q ≤ J q (C 1 , C 2 , π α )</formula><p>The provided inequality is then derived.</p><p>We also have two other straight-forward lower bounds for F GW :</p><formula xml:id="formula_24">F GW q,α (µ, ν) ≥ (1 − α) W q (µ A , ν B ) q (15) F GW q,α (µ, ν) ≥ α GW q (µ X , ν Y ) q<label>(16)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2.">INTERPOLATION PROPERTIES</head><p>We now claim the following theorem: Theorem 6.2. Interpolation properties.</p><p>As α tends to zero, the FGW distance recovers W q (µ A , ν B ) q between the features, and as α tends to one, we recover GW q (µ X , ν Y ) q between the structures:</p><formula xml:id="formula_25">lim α→0 F GW q,α (µ, ν) = W q (µ A , ν B ) q lim α→1 F GW q,α (µ, ν) = GW q (µ X , ν Y ) q</formula><p>Proof. Let π W ∈ Π(h, g) be the optimal coupling for the Wasserstein distance W q (µ A , ν B ) between µ A and ν B and let π α ∈ Π(h, g) be the optimal coupling for the F GW distance F GW q,α (µ, ν). We consider :</p><formula xml:id="formula_26">F GW q,α (µ, ν) − (1 − α)W q (µ A , ν B ) q = E q (M AB , C 1 , C 2 , π α ) − (1 − α)H q (M AB , π W ) * ≤ E q (M AB , C 1 , C 2 , π W ) − (1 − α)H q (M AB , π W ) = i,j,k,l α|C 1 (i, k) − C 2 (j, l)| q π W i,j π W k,l = αJ q (C 1 , C 2 , π W )</formula><p>In (*) we used the suboptimality of the coupling π W w.r.t the F GW distance. In this way we have proven :</p><formula xml:id="formula_27">F GW q,α (µ, ν) ≤ (1−α)W q (µ A , ν B ) q +αJ q (C 1 , C 2 , π W )<label>(17)</label></formula><p>Now let π GW ∈ Π(h, g) the optimal coupling for the Gromov-Wasserstein distance GW q (µ X , ν Y ) between µ X and ν Y . Then :</p><formula xml:id="formula_28">F GW q,α (µ, ν) − αGW q (µ X , ν Y ) q = E q (M AB , C 1 , C 2 , π α ) − αJ q (C 1 , C 2 , π GW ) * ≤ E q (M AB , C 1 , C 2 , π GW ) − αJ q (C 1 , C 2 , π GW ) = (1 − α) i,j,k,l (1 − α)d(a i , b j ) q π GW i,j = (1 − α)H q (M AB , π GW )</formula><p>where in (*) we used the suboptimality of the coupling π GW w.r.t the F GW distance so that :</p><formula xml:id="formula_29">F GW q,α (µ, ν) ≤ αGW q (µ X , ν Y ) q +(1−α)H q (M AB , π GW )<label>(18)</label></formula><p>As α goes to zero Eq. <ref type="formula" target="#formula_11">(17)</ref>  </p><formula xml:id="formula_30">F GW q,α (µ, ν) = GW q (µ X , ν Y ) q 6.1.3. F GW IS A DISTANCE</formula><p>For the following proofs we suppose that C 1 and C 2 are distance matrices, n ≥ m and α ∈]0, ..., 1[. We claim the following theorem :</p><p>Theorem 6.3. F GW defines a metric for q = 1 and a semi-metric for q &gt; 1.</p><p>F GW defines a metric over the space of structured data quotiented by the measure preserving isometries that are also feature preserving. More precisely, F GW satisfies the triangle inequality and is nul iff n = m and there exists a bijection σ : {1, .., n} → {1, .., n} such that :</p><formula xml:id="formula_31">∀i ∈ {1, .., n}, h i = g σ(i) (19) ∀i ∈ {1, .., n}, a i = b σ(i) (20) ∀i, k ∈ {1, .., n} 2 , C 1 (i, k) = C 2 (σ(i), σ(k))<label>(21)</label></formula><p>If q &gt; 1, the triangle inequality is relaxed by a factor 2 q−1 such that F GW defines a semi-metric</p><p>We first prove the equality relation for any q ≥ 1 and we discuss the triangle inequality in the next section.</p><p>Equality relation Theorem 6.4. For all q ≥ 1, F GW q,α (µ, ν) = 0 iff there exists an application σ : {1, .., n} → {1, .., m} which verifies <ref type="formula">(19)</ref>, <ref type="formula">(20)</ref> and <ref type="formula" target="#formula_31">(21)</ref> Proof. First, let us suppose that n = m and that such a bijection exists. Then if we consider the transport map π * associated with i → i and j → σ(i) i.e the map π * = (I d × σ) with I d the identity map.</p><p>By eq (19), π * ∈ Π(h, g) and clearly using <ref type="formula">(20)</ref> and <ref type="formula" target="#formula_31">(21)</ref>:</p><formula xml:id="formula_32">E q (C 1 , C 2 , π * ) = (1 − α) i,k d(a i , b σ(i) ) q h i g σ(i) h k g σ(k) + α i,k |C 1 (i, k) − C 2 (σ(i), σ(k))| q h i g σ(i) h k g σ(k) = 0<label>(22)</label></formula><p>We can conclude that F GW q,α (µ, ν) = 0.</p><p>Conversely, suppose that F GW q,α (µ, ν) = 0 and q ≥ 1. We define :</p><formula xml:id="formula_33">∀i, k ∈ {1, ..., n} 2 ,Ĉ 1 (i, k) = 1 2 C 1 (i, k) + 1 2 d(a i , a k ) (23) ∀j, l ∈ {1, ..., m} 2 ,Ĉ 2 (j, l) = 1 2 C 2 (j, l) + 1 2 d(b j , b l )<label>(24)</label></formula><p>To prove the existence of a bijection σ satisfying the theorem properties we will prove that the Gromov-Wasserstein distance GW q (Ĉ 1 ,Ĉ 2 , µ, ν) vanishes.</p><p>Let π ∈ Π(h, g) be any admissible transportation plan. Then for n ≥ 1, :</p><formula xml:id="formula_34">J n (Ĉ 1 ,Ĉ 2 , π) = i,j,k,l L(Ĉ 1 (i, k),Ĉ 2 (j, l)) n π i,j π k,l = i,j,k,l 1 2 (C 1 (i, k) − C 2 (j, l)) + 1 2 (d(a i , a k ) − d(b j , b l )) n π i,j π k,l * ≤ i,j,k,l 1 2 |C 1 (i, k) − C 2 (j, l)| n π i,j π k,l + i,j,k,l 1 2 |d(a i , a k ) − d(b j , b l )| n π i,j π k,l</formula><p>In (*) we used the convexity of t → t n and Jensen inequality. We denote the first term (I) and (II) the second term. Com-</p><formula xml:id="formula_35">bining triangle inequalities d(a i , a k ) ≤ d(a i , b j )+d(b j , a k ) and d(b j , a k ) ≤ d(b j , b l ) + d(b l , a k ) we have : d(a i , a k ) ≤ d(a i , b j ) + d(a k , b l ) + d(b j , b l )<label>(25)</label></formula><p>We split (II) in two parts</p><formula xml:id="formula_36">S 1 = {i, j, k, l ; d(a i , a k ) − d(b j , b l ) ≥ 0} and S 2 = {i, j, k, l ; d(a i , a k ) − d(b j , b l ) ≤ 0} such that (II) = i,j,k,l∈S1 (d(a i , a k ) − d(b j , b l )) n π i,j π k,l + i,j,k,l∈S2 (d(b j , b l )) − d(a i , a k )) n π i,j π k,l</formula><p>In the same way as Eq. (25) we have :</p><formula xml:id="formula_37">d(b j , b l ) ≤ d(a i , a k ) + d(a i , b j ) + d(a k , b l )<label>(26)</label></formula><p>So Eq. <ref type="formula" target="#formula_35">(25)</ref> and <ref type="formula" target="#formula_37">(26)</ref> give :</p><formula xml:id="formula_38">(II) ≤ i,j,k,l 1 2 |d(a i , b j ) + d(a k , b l )| n π i,j , π k,l def = M n (π)<label>(27)</label></formula><p>Finally we have shown that :</p><p>∀π ∈ Π(h, g), ∀n ≥ 1, Jn(Ĉ1,Ĉ2, π) ≤ 1 2 Jn(C1, C2, π)+Mn(π)</p><p>Now let π * be the optimal coupling for F GW q,α (µ, ν).</p><p>If F GW q,α (µ, ν) = 0 then since E q (C 1 , C 2 , π * ) ≥ αJ q (C 1 , C 2 , π * ) and E q (C 1 , C 2 , π * ) ≥ (1 − α)H q (M AB , π * ), we have:</p><formula xml:id="formula_40">J q (C 1 , C 2 , π * ) = 0<label>(29)</label></formula><p>and H q (M AB , π * ) = 0</p><formula xml:id="formula_41">Then i,j d(a i , b j ) q π * i,j = 0.</formula><p>Since all terms are positive we can conclude that ∀m ∈ N * , i,j d(a i , b j ) m π * i,j = 0. In this way :</p><formula xml:id="formula_42">Mq(π * ) = 1 2 h q p i,j d(ai, bj) p π * i,j k,l d(a k , b l ) q−p π * k,l = 0<label>(30)</label></formula><p>Using equations <ref type="formula" target="#formula_18">(28)</ref> and <ref type="formula" target="#formula_40">(29)</ref> we have shown :</p><formula xml:id="formula_43">J q (Ĉ 1 ,Ĉ 2 , π * ) = 0</formula><p>So π * is the optimal coupling for GW q (Ĉ 1 ,Ĉ 2 , µ, ν) and GW q (Ĉ 1 ,Ĉ 2 , µ, ν) = 0. By virtue to Gromov-Wasserstein properties (see <ref type="bibr" target="#b27">(Memoli, 2011)</ref>), there exists an isomorphism between the metric spaces associated with µ and ν.</p><p>In the discrete case this results in the existence of a function σ : {1, .., m} → {1, .., n} which is a weight preserving isometry and thus bijective. In this way, we have m = n and σ verifiying Eq (19). The isometry property leads also to :</p><formula xml:id="formula_44">∀i, k ∈ {1, .., n} 2 ,Ĉ 1 (i, k) =Ĉ 2 (σ(i), σ(k))<label>(31)</label></formula><p>Moreover, since π * is the optimal coupling for GW q (Ĉ 1 ,Ĉ 2 , µ, ν) leading to a zero cost, then π * is supported by σ, in particular π * = (I d × σ)</p><p>So H q (M AB , π * ) = i d(a i , b σ(i) ) q h i g σ(i) . Since H q (M AB , π * ) = 0 and all the weights are strictly positive we can conclude that a i = b σ(i) .</p><p>In this way, d(a i , a k ) = d(b σ(i) , b σ(k) ), so using the equality (31) and the definition ofĈ 1 andĈ 2 in <ref type="formula">(23)</ref> and <ref type="formula" target="#formula_33">(24)</ref> we can conclude that :</p><formula xml:id="formula_45">∀i, k ∈ {1, .., n} × {1, .., n}, C 1 (i, k) = C 2 (σ(i), σ(k))</formula><p>which concludes the proof.</p><p>Triangle Inequality Theorem 6.5. For all q = 1, F GW verifies the triangle inequality.</p><p>Proof. To prove the triangle inequality of F GW distance for arbitrary measures we will use the gluing lemma (see <ref type="bibr" target="#b41">(Villani, 2008)</ref>) which stresses the existence of couplings with a prescribed structure. Let h, g, f ∈ Σ n × Σ m × Σ k . Let also µ = n i=1 h i δ ai,xi , ν = m j=1 g j δ bj ,yj and γ = k p=1 f p δ cp,zp be three structured data as described in the paper. We note C 1 (i, k) the distance between vertices x i and x k , C 2 (i, k) the distance between vertices y i and y k and C 3 (i, k) the distance between vertices z i and z k .</p><p>Let P and Q be two optimal solutions of the F GW transportation problem between µ and ν and ν and γ respectively.</p><p>We define :</p><formula xml:id="formula_46">S = P diag( 1 g )Q</formula><p>(note that S is well defined since g j = 0 for all j). Then by definition S ∈ Π(h, f ) because : S1 m = P diag( 1 g )Q1 m = P diag( g g ) = P 1 m = h (same reasoning for f ).</p><p>We first prove the triangle inequality for the case q = 1.</p><p>By suboptimality of S : Since P and Q are the optimal plans we have : <ref type="bibr">C3, ν, γ)</ref> which prove the triangle inequality for q = 1.</p><formula xml:id="formula_47">F GW1,α(C1, C3, µ, γ) ≤ i,j,k,l (1 − α)d(ai, cj) + αL(C1(i, k), C3(j, l))Si,jS k,l = i,j,k,l ((1 − α)d(ai, cj) + αL(C1(i, k), C3(j, l)) × e Pi,eQe,j ge o P k,o Q o,l go = i,j,k,l (1 − α)d(ai, cj) + α|C1(i, k) − C3(j, l)| × e Pi,</formula><formula xml:id="formula_48">F GW1,α(C1, C3, µ, γ) ≤ F GW1,α(C1, C2, µ, ν) + F GW1,α(C2,</formula><p>Theorem 6.6. For all q &gt; 1, F GW verifies the relaxed triangle inequality :</p><p>F GWq,α(C1, C3, µ, γ) ≤ 2 q−1 F GWq,α(C1, C2, µ, ν)</p><formula xml:id="formula_49">+ F GWq,α(C2, C3, ν, γ)</formula><p>Proof. Let q &gt; 1, We have :</p><formula xml:id="formula_50">∀x, y ∈ R + , (x + y) q ≤ 2 q−1 x q + y q<label>(32)</label></formula><p>Indeed,</p><formula xml:id="formula_51">(x + y) q = ( 1 2 q−1 ) 1 q x ( 1 2 q−1 ) 1 q + ( 1 2 q−1 ) 1 q y ( 1 2 q−1 ) 1 q q ≤ ( 1 2 q−1 ) 1 q−1 + ( 1 2 q−1 ) 1 q−1 q−1 x q 1 2 q−1 + y q 1 2 q−1 = x q 1 2 q−1 + y q 1 2 q−1</formula><p>Last inequality is a consequence of Hlder inequality. Then using same notations : where in (*) we use the triangle inequality of d and in (**) the triangle inequality of |.| and (32).</p><p>Since P and Q are the optimal plans we have :</p><p>F GWq,α(C1, C3, µ, γ) ≤ 2 q−1 F GWq,α(C1, C2, µ, ν) + F GWq,α(C2, C3, ν, γ)</p><p>Optimal Transport for structured data with application on graphs  Which prove that F GW q,α defines a semi metric for q &gt; 1 with coefficient 2 q−1 for the triangle inequality relaxation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Comparaison with W and GW</head><p>Cross validation results During the nested cross validation, we divided the dataset into 10 and use 9 folds for training, where α is chosen within [0, 1] via a 10-CV crossvalidation, 1 fold for testing, with the best value of α (with the best average accuracy on the 10-CV) previously selected. The experiment is repeated 10 times for each dataset except for MUTAG and PTC where it is repeated 50 times. Table 6.2 and 6.2 report the average number of time α was chose within ]0, ...1[ without 0 and 1 corresponding to the Wasserstein and Gromov-Wasserstein distances respectively. Results suggests that both structure and feature pieces of information are necessary as α is consistently selected inside ]0, ...1[ except for PTC and COX2.</p><p>Nested CV results We report in tables 8 and 6 the average classification accuracies of the nested classification procedure by taking W and GW instead of F GW (i.e by  taking α = 0, 1). Best result for each dataset is in bold. A (*) is added when best score does not yield to a significative improvement compared to the second best score. The significance is based on a Wilcoxon signed rank test between the best method and the second one.</p><p>Results illustrates that F GW encompasses the two cases of W and GW , as scores of F GW are usually greater or equal on every dataset than scores of both W and GW and when it is not the case the difference is not statistically significant.</p><p>Timings In this paragraph we provide some timings for the discrete attributed datasets. <ref type="table" target="#tab_8">Table 7</ref> displays the average timing for computing F GW between two pair of graphs. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Example of F GW , GW and W on synthetic trees. Dark grey color represents a non null πi,j value between two nodes i and j. (Left) the W distance between the features with α = 0, (Middle) F GW (Right) the GW between the structures α = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Examples from the clustering dataset, color indicates the labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>F</head><label></label><figDesc>GWq,α(C1, C3, µ, γ)≤ i,j k,l (1 − α)d(ai, cj) q + αL(C1(i, k), C3(j, l)) q Si,jS k,l = i,j k,l ((1 − α)d(ai, cj) q + αL(C1(i, k), C3(j, l)) q α)d(ai, cj) q + α|C1(i, k) − C3(j, l)| q α)(d(ai, be) + d(be, cj)) q + α|C1(i, k) − C2(e, o) + C2(e, o) − C3(j, l)| q × Pi,eQe,j ge P k,o Q o,l go * * ≤ 2 q−1 i,j,k,l,e,o (1 − α)d(ai, be) q + αL(C1(i, k), C2(e, o)) α)d(be, cj) q + αL(C2(e, o), C3(j, l)) q × Pi,eQe,j ge P k,o Q o,l go</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>or 1 Univ. Bretagne-Sud, CNRS, IRISA, F-56000 Vannes 2 Univ. Côte d'Azur, CNRS, OCA Lagrange, F-06000 Nice 3 Univ. Rennes, CNRS, LETG, F-35000 Rennes. Correspondence to: Titouan Vayer &lt;titouan.vayer@irisa.fr&gt;. Proceedings of the 36 th International Conference on Machine Learning, Long Beach, California, PMLR 97, 2019. Copyright 2019 by the author(s).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Average classification accuracy on the graph datasets with vector attributes. 12±4.15* 77.23±4.86 76.67±7.04 71.00±6.76 74.55±2.74 100.00±0.00</figDesc><table><row><cell cols="2">VECTOR ATTRIBUTES BZR</cell><cell></cell><cell>COX2</cell><cell cols="2">CUNEIFORM ENZYMES</cell><cell>PROTEIN</cell><cell>SYNTHETIC</cell></row><row><cell cols="5">FGW SP 85.HOPPERK 84.15±5.26 79.57±3.46 32.59±8.73</cell><cell>45.33±4.00 71.96±3.22 90.67±4.67</cell></row><row><cell>PROPAK</cell><cell cols="4">79.51±5.02 77.66±3.95 12.59±6.67</cell><cell>71.67±5.63* 61.34±4.38 64.67±6.70</cell></row><row><cell>PSCN K=10</cell><cell cols="4">80.00±4.47 71.70±3.57 25.19±7.73</cell><cell>26.67±4.77 67.95±11.28 100.00±0.00</cell></row><row><cell>PSCN K=5</cell><cell cols="4">82.20±4.23 71.91±3.40 24.81±7.23</cell><cell>27.33±4.16 71.79±3.39 100.00±0.00</cell></row><row><cell cols="5">Table 2. Average classification accuracy on the graph datasets with</cell></row><row><cell>discrete attributes.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">DISCRETE ATTR. MUTAG</cell><cell>NCI1</cell><cell>PTC-MR</cell><cell></cell></row><row><cell>FGW RAW SP</cell><cell cols="3">83.26±10.30 72.82±1.46 55.71±6.74</cell><cell></cell></row><row><cell cols="4">FGW WL H=2 SP 86.42±7.81 85.82±1.16 63.20±7.68</cell><cell></cell></row><row><cell cols="4">FGW WL H=4 SP 88.42±5.67 86.42±1.63 65.31±7.90</cell><cell></cell></row><row><cell>GK K=3</cell><cell cols="3">82.42±8.40 60.78±2.48 56.46±8.03</cell><cell></cell></row><row><cell>RWK</cell><cell cols="3">79.47±8.17 58.63±2.44 55.09±7.34</cell><cell></cell></row><row><cell>SPK</cell><cell cols="3">82.95±8.19 74.26±1.53 60.05±7.39</cell><cell></cell></row><row><cell>WLK</cell><cell cols="3">86.21±8.48 85.77±1.07 62.86±7.23</cell><cell></cell></row><row><cell>WLK H=2</cell><cell cols="3">86.21±8.15 81.85±2.28 61.60±8.14</cell><cell></cell></row><row><cell>WLK H=4</cell><cell cols="3">83.68±9.13 85.13±1.61 62.17±7.80</cell><cell></cell></row><row><cell>PSCN K=10</cell><cell cols="3">83.47±10.26 70.65±2.58 58.34±7.71</cell><cell></cell></row><row><cell>PSCN K=5</cell><cell cols="3">83.05±10.80 69.85±1.79 55.37±8.28</cell><cell></cell></row><row><cell cols="5">Table 3. Average classification accuracy on the graph datasets with</cell></row><row><cell>no attributes.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">WITHOUT ATTRIBUTE IMDB-B</cell><cell>IMDB-M</cell><cell></cell></row><row><cell>GW SP</cell><cell cols="3">63.80±3.49 48.00±3.22</cell><cell></cell></row><row><cell>GK K=3</cell><cell cols="3">56.00±3.61 41.13±4.68</cell><cell></cell></row><row><cell>SPK</cell><cell cols="3">55.80±2.93 38.93±5.12</cell><cell></cell></row><row><cell cols="5">folds assignment for training and testing, which suggests</cell></row><row><cell cols="2">that PSCN is difficult to tune.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Wale, N., Watson, I. A., and Karypis, G. Comparison of descriptor spaces for chemical compound retrieval and classification.</figDesc><table><row><cell>Knowledge and Information Systems, 14</cell></row><row><cell>(3):347-375, Mar 2008.</cell></row><row><cell>Wang, Y. J. and Wong, G. Y. Stochastic blockmodels for</cell></row><row><cell>directed graphs. Journal of the American Statistical As-</cell></row><row><cell>sociation, 82(397):8-19, 1987.</cell></row><row><cell>Yanardag, P. and Vishwanathan, S. Deep graph kernels.</cell></row><row><cell>In Proceedings of the 21th ACM SIGKDD International</cell></row><row><cell>Conference on Knowledge Discovery and Data Mining,</cell></row><row><cell>pp. 1365-1374, 2015.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>and Eq. (15) give lim α→0 F GW q,α (µ, ν) = W q (µ A , ν B ) q and as α goes to one Eq. (18) and Eq. (16) give lim</figDesc><table><row><cell>α→1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Percentage of α chosen in ]0, ..., 1[ compared to {0, 1} for discrete labeled graphs DISCRETE ATTR. MUTAG NCI1 PTC</figDesc><table><row><cell>FGW RAW SP</cell><cell>100%</cell><cell>100% 98%</cell></row><row><cell cols="2">FGW WL H=2 SP 100%</cell><cell>100% 88%</cell></row><row><cell cols="2">FGW WL H=4 SP 100%</cell><cell>100% 88%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Percentage of α chosen in ]0, ..., 1[ compared to {0, 1} for vector attributed graphs</figDesc><table><row><cell cols="2">VECTOR ATTRIBUTES BZR</cell><cell cols="5">COX2 CUNEIFORM ENZYMES PROTEIN SYNTHETIC</cell></row><row><cell>FGW SP</cell><cell cols="2">100 % 90%</cell><cell>100%</cell><cell>100%</cell><cell>100%</cell><cell>100%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>Average classification accuracy on the graph datasets with discrete attributes. 26±10.30 72.82±1.46 55.71±6.74 FGW WL H=2 SP 86.42±7.81 85.82±1.16 63.20±7.68 FGW WL H=4 SP 88.42±5.67 86.42±1.63* 65.31±7.90 W RAW SP 79.36±3.49 70.5±4.63 54.79±5.76 W WL H=2 SP 87.78±8.64 85.83±1.75 63.90±7.66 W WL H=4 SP 87.15±8.23 86.42±1.64 66.28±6.95*</figDesc><table><row><cell>DISCRETE ATTR. MUTAG</cell><cell>NCI1</cell><cell>PTC-MR</cell></row><row><cell cols="2">FGW RAW SP 83.GW SP 82.73±9.59 73.40±2.80</cell><cell>54.45± 6.89</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 .</head><label>7</label><figDesc>Average timings for the computation of F GW between two pairs of graph DISCRETE ATTR. MUTAG NCI1 PTC-MR FGW 2.5 MS 7.3 MS 3.7 MS</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 .</head><label>8</label><figDesc>Average classification accuracy on the graph datasets with vector attributes. 12±4.15 77.23±4.86* 76.67±7.04 71.00±6.76 74.55±2.74 100.00±0.00 W 85.36±4.87* 77.23±3.16 61.48±10.23 71.16±6.32* 75.98± 1.97* 34.07±11.33 GW SP 82.92±6.72 77.65±5.88 50.66±8.91 23.66±3.63 71.96± 2.40 41.66±4.28</figDesc><table><row><cell cols="2">VECTOR ATTRIBUTES BZR</cell><cell>COX2</cell><cell>CUNEIFORM ENZYMES</cell><cell>PROTEIN</cell><cell>SYNTHETIC</cell></row><row><cell>FGW SP</cell><cell>85.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work benefited from the support from OATMIL ANR-17-CE23-0012 project of the French National Research Agency (ANR). We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan X GPU used for this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Barycenters in the wasserstein space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Agueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carlier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Mathematical Analysis</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="904" to="924" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Structured Optimal Transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Alvarez-Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AISTATS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Image classification with segmentation graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007-06" />
			<biblScope unit="volume">00</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Predicting Structured Data (Neural Information Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Bakir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V</forename><surname>Vishwanathan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
	<note>ISBN 0262026171</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Faulkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ballard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Langston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
	<note>Relational inductive biases, deep learning, and graph networks. ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Shortest-path kernels on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM, ICDM &apos;05</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Optimal transport for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Courty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Flamary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rakotomamonjy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEETPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1853" to="1865" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Soft-DTW: a differentiable loss function for time-series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICML</title>
		<meeting>the ICML<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>International Convention Centre</publisher>
			<date type="published" when="2017-08" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast computation of wasserstein barycenters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<editor>Xing, E. P. and Jebara, T.</editor>
		<meeting><address><addrLine>Bejing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="22" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Optimal algorithms for comparing trees with labeled leaves</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Day</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of classification</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="28" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Debnath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Lopez De Compadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Debnath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Shusterman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hansch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Medicinal Chemistry</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="786" to="797" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Scalable kernels for graphs with continuous attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Feragen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kasenburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="216" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Regularized discrete optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ferradans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papadakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Peyré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Aujol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1853" to="1882" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Pot python optimal transport library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Flamary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Courty</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Optimal transport with laplacian regularization: Applications to domain adaptation and shape matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Flamary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Courty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rakotomamonjy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On graph kernels: Hardness results and efficient alternatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gärtner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Flach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wrobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="129" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Supervised word mover&apos;s distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4862" to="4870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Benchmark data sets for graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>abs/1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recognizing cuneiform signs using graph based methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fisseler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Weichert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Cost-Sensitive Learning (COST)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">On valid optimal assignment kernels and applications to graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Giscard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Wilson</surname></persName>
		</author>
		<idno>abs/1606.01141</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Distance metric learning using graph convolutional networks: Application to functional brain networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Ktena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Parisot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ferrante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rajchl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="469" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.00345</idno>
		<title level="m">Convergence rate of frank-wolfe for non-convex objectives</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Support vector machine classification with indefinite kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspremont</surname></persName>
		</author>
		<idno>978-1-60560-352-0</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Neural Information Processing Systems, NIPS&apos;07</title>
		<meeting>the 20th International Conference on Neural Information Processing Systems, NIPS&apos;07</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="953" to="960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Gromov wasserstein distances and the metric approach to object matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Memoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations of Computational Mathematics</title>
		<imprint>
			<biblScope unit="page" from="1" to="71" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Propagation kernels: efficient graph kernels from propagated information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bauckhage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2016-02" />
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="209" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">of Proceedings of Machine Learning Research</title>
		<meeting><address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Matching node embeddings for graph similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nikolentzos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meladianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2429" to="2435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Estimation and prediction for stochastic blockstructures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nowicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Snijders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American statistical association</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">455</biblScope>
			<biblScope unit="page" from="1077" to="1087" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Peyré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00567</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Computational optimal transport. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Gromov-wasserstein averaging of kernel and distance matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Peyré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2664" to="2672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Efficient graphlet kernels for large graph comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Petri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Siglidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nikolentzos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Limnios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Giatsidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Skianis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vazirgianis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grakel</surname></persName>
		</author>
		<title level="m">Graph Kernel Library in Python. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Entropic metric alignment for correspondence problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Peyré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<idno type="DOI">http:/doi.acm.org/10.1145/2897824.2925903</idno>
		<ptr target="http://doi.acm.org/10.1145/2897824.2925903" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2016-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Splinefitting with a genetic algorithm: A method for developing classification structure-activity relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>O&amp;apos;brien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Weaver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and computer sciences</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1906" to="1915" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A transportation l p distance for signal analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thorpe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Rohde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Slepčev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="187" to="210" />
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Hunt for the unique, stable, sparse and fast feature learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<editor>Vishwanathan, S., and Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="88" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Optimal Transport: Old and New. Grundlehren der mathematischen Wissenschaften</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Villani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1201" to="1242" />
			<date type="published" when="2010-08" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
