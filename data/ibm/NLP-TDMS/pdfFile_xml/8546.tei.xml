<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T09:10+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Knowledge Graph Embedding by Translating on Hyperplanes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Science and Technology</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Science and Technology</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
							<email>zhengc@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Knowledge Graph Embedding by Translating on Hyperplanes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We deal with embedding a large scale knowledge graph composed of entities and relations into a continuous vector space. TransE is a promising method proposed recently, which is very efficient while achieving state-of-the-art predictive performance. We discuss some mapping properties of relations which should be considered in embedding, such as reflexive , one-to-many, many-to-one, and many-to-many. We note that TransE does not do well in dealing with these properties. Some complex models are capable of preserving these mapping properties but sacrifice efficiency in the process. To make a good trade-off between model capacity and efficiency, in this paper we propose TransH which models a relation as a hyperplane together with a translation operation on it. In this way, we can well preserve the above mapping properties of relations with almost the same model complexity of TransE. Additionally, as a practical knowledge graph is often far from completed, how to construct negative examples to reduce false negative labels in training is very important. Utilizing the one-to-many/many-to-one mapping property of a relation, we propose a simple trick to reduce the possibility of false negative labeling. We conduct extensive experiments on link prediction, triplet classification and fact extraction on benchmark datasets like WordNet and Freebase. Experiments show TransH delivers significant improvements over TransE on predictive accuracy with comparable capability to scale up.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Knowledge graphs such as Freebase ( <ref type="bibr" target="#b1">Bollacker et al. 2008</ref>), WordNet (Miller 1995) and GeneOntology <ref type="bibr" target="#b0">(Ashburner et al. 2000</ref>) have become very important resources to support many AI related applications, such as web/mobile search, Q&amp;A, etc. A knowledge graph is a multi-relational graph composed of entities as nodes and relations as different types of edges. An instance of edge is a triplet of fact (head entity, relation, tail entity) (denoted as (h, r, t)). In the past decade, there have been great achievements in building large scale knowledge graphs, however, the general paradigm to support computing is still not clear. Two major difficulties are: (1) A knowledge graph is a symbolic and logical system while applications often involve numerical computing in continuous spaces; (2) It is difficult to aggregate global knowledge over a graph. The traditional method of reasoning by formal logic is neither tractable nor robust when dealing with long range reasoning over a real large scale knowledge graph. Recently a new approach has been proposed to deal with the problem, which attempts to embed a knowledge graph into a continuous vector space while preserving certain properties of the original graph ( <ref type="bibr" target="#b17">Socher et al. 2013;</ref><ref type="bibr" target="#b4">Bordes et al. 2013a;</ref><ref type="bibr" target="#b20">Weston et al. 2013;</ref><ref type="bibr" target="#b2">Bordes et al. 2011;</ref><ref type="bibr" target="#b5">2013b;</ref><ref type="bibr" target="#b6">Chang, Yih, and Meek 2013)</ref>. For example, each entity h (or t) is represented as a point h (or t) in the vector space while each relation r is modeled as an operation in the space which is characterized by an a vector r, such as translation, projection, etc. The representations of entities and relations are obtained by minimizing a global loss function involving all entities and relations. As a result, even the embedding representation of a single entity/relation encodes global information from the whole knowledge graph. Then the embedding representations can be used to serve all kinds of applications. A straightforward one is to complete missing edges in a knowledge graph. For any candidate triplet (h, r, t), we can confirm the correctness simply by checking the compatibility of the representations h and t under the operation characterized by r.</p><p>Generally, knowledge graph embedding represents an entity as a k-dimensional vector h (or t) and defines a scoring function f r (h, t) to measure the plausibility of the triplet (h, r, t) in the embedding space. The score function implies a transformation r on the pair of entities which characterizes the relation r. For example, in translation based method (TransE) ( <ref type="bibr" target="#b5">Bordes et al. 2013b</ref>), f r (h, t) 񮽙 񮽙h+r−t񮽙 񮽙 1/2 , i.e., relation r is characterized by the translating (vector) r. With different scoring functions, the implied transformations vary between simple difference ( <ref type="bibr" target="#b3">Bordes et al. 2012</ref>), translation ( <ref type="bibr" target="#b5">Bordes et al. 2013b</ref>), affine <ref type="bibr" target="#b6">(Chang, Yih, and Meek 2013)</ref>, general linear ( <ref type="bibr" target="#b2">Bordes et al. 2011</ref>), bilinear ( <ref type="bibr">Jenatton et al. 2012;</ref><ref type="bibr" target="#b19">Sutskever, Tenenbaum, and Salakhutdinov 2009)</ref>, and nonlinear transformations ( <ref type="bibr" target="#b17">Socher et al. 2013</ref>). Accordingly the model complexities (in terms of number of parameters) vary significantly. (Please refer to <ref type="table">Table 1</ref> and Section "Related Works" for details.)</p><p>Among previous methods, <ref type="bibr">TransE (Bordes et al. 2013b</ref>) is a promising one as it is simple and efficient while achieving state-of-the-art predictive performance. However, we find that there are flaws in TransE when dealing with relations with mapping properties of reflexive/one-to-many/manyto-one/many-to-many. Few previous work discuss the role of these mapping properties in embedding. Some advanced models with more free parameters are capable of preserving these mapping properties, however, the model complexity and running time is significantly increased accordingly. Moreover, the overall predictive performances of the advanced models are even worse than TransE ( <ref type="bibr" target="#b5">Bordes et al. 2013b</ref>). This motivates us to propose a method which makes a good trad-off between model complexity and efficiency so that it can overcome the flaws of TransE while inheriting the efficiency.</p><p>In this paper, we start by analyzing the problems of TransE on reflexive/one-to-many/many-to-one/many-tomany relations. Accordingly we propose a method named translation on hyperplanes (TransH) which interprets a relation as a translating operation on a hyperplane. In TransH, each relation is characterized by two vectors, the norm vector (w r ) of the hyperplane, and the translation vector (d r ) on the hyperplane. For a golden triplet (h, r, t), that it is correct in terms of worldly facts, the projections of h and t on the hyperplane are expected to be connected by the translation vector d r with low error. This simple method overcomes the flaws of TransE in dealing with reflexive/one-to-many/many-to-one/many-to-many relations while keeping the model complexity almost the same as that of TransE. Regarding model training, we point out that carefully constructing negative labels is important in knowledge embedding. By utilizing the mapping properties of relations in turn, we propose a simple trick to reduce the chance of false negative labeling. We conduct extensive experiments on the tasks of link prediction, triplet classification and fact extraction on benchmark datasets like WordNet and Freebase, showing impressive improvements on different metrics of predictive accuracy. We also show that the running time of TransH is comparable to TransE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>The most related work is briefly summarized in <ref type="table">Table 1</ref>. All these methods embed entities into a vector space and enforce the embedding compatible under a scoring function. Different models differ in the definition of scoring functions f r (h, r) which imply some transformations on h and t.</p><p>TransE ( <ref type="bibr" target="#b5">Bordes et al. 2013b</ref>) represents a relation by a translation vector r so that the pair of embedded entities in a triplet (h, r, t) can be connected by r with low error. TransE is very efficient while achieving state-of-theart predictive performance. However, it has flaws in dealing with reflexive/one-to-many/many-to-one/many-to-many relations.</p><p>Unstructured is a simplified case of TransE, which considers the graph as mono-relational and sets all translations r = 0, i.e., the scoring function is 񮽙h − t񮽙. It is used as a naive baseline in ( <ref type="bibr" target="#b3">Bordes et al. 2012;</ref><ref type="bibr" target="#b5">2013b</ref>). Obviously it cannot distinguish different relations.</p><p>Distant Model ( <ref type="bibr" target="#b2">Bordes et al. 2011</ref>) introduces two independent projections to the entities in a relation. It represents a relation by a left matrix W rh and a right matrix W rt . Dissimilarity is measured by L 1 distance between W rh h and W rt t. As pointed out by <ref type="bibr" target="#b17">(Socher et al. 2013</ref>), this model is weak in capturing correlations between entities and relations as it uses two separate matrices.</p><p>Bilinear Model ( <ref type="bibr">Jenatton et al. 2012;</ref><ref type="bibr">Sutskever, Tenen- baum, and Salakhutdinov 2009</ref>) models second-order correlations between entity embeddings by a quadratic form: h 񮽙 W r t. Thus, each component of an entity interacts with each component of the other entity.</p><p>Single Layer Model ( <ref type="bibr" target="#b17">Socher et al. 2013</ref>) introduces nonlinear transformations by neural networks. It concatenates h and t as an input layer to a non-linear hidden layer then the linear output layer gives the resulting score:</p><formula xml:id="formula_0">u 񮽙 r f (W rh h + W rt t + b r )</formula><p>. A similar structure is proposed in <ref type="bibr" target="#b7">(Collobert and Weston 2008)</ref>. <ref type="bibr">NTN (Socher et al. 2013</ref>) is the most expressive model so far. It extends the Single Layer Model by considering the second-order correlations into nonlinear transformation (neural networks). The score function is</p><formula xml:id="formula_1">u 񮽙 r f (h 񮽙 W r t + W rh h + W rt t + b r ).</formula><p>As analyzed by the authors, even when the tensor W r degenerates to a matrix, it covers all the above models. However, the model complexity is much higher, making it difficult to handle large scale graphs.</p><p>Beyond these works directly targeting the same problem of embedding knowledge graphs, there are extensive related works in the wider area of multi-relational data modeling, matrix factorization, and recommendations. Please refer to the Introduction part of ( <ref type="bibr" target="#b5">Bordes et al. 2013b</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Embedding by Translating on Hyperplanes</head><p>We first describe common notations. h denotes a head entity, r denotes a relation and t denotes a tail entity. The bold letters h, r, t denote the corresponding embedding representations. ∆ denotes the set of golden triplets, and ∆ 񮽙 denotes the set of incorrect triplets. Hence we use (h, r, t) ∈ ∆ to state "(h, r, t) is correct". E is the set of entities. R is the set of relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relations' Mapping Properties in Embedding</head><p>As introduced in Introduction &amp; Related Work <ref type="table">(Table 1)</ref>, TransE models a relation r as a translation vector r ∈ R k and assumes the error 񮽙h + r − t񮽙 񮽙1//2 is low if (h, r, t) is a golden triplet. It applies well to irreflexive and one-to-one relations but has problems when dealing with reflexive or many-to-one/one-to-many/many-to-many relations.</p><p>Considering the ideal case of no-error embedding where h + r − t = 0 if (h, r, t) ∈ ∆, we can get the following consequences directly from TransE model.</p><p>• If (h, r, t) ∈ ∆ and (t, r, h) ∈ ∆, i.e., r is a reflexive map, then r = 0 and h = t.</p><p>• If ∀i ∈ {0, . . . , m}, (h i , r, t) ∈ ∆, i.e., r is a many-to-one map, then h 0 = . . . = h m . Similarly, if ∀i, (h, r, t i ) ∈ ∆, i.e., r is a one-to-many map, then t 0 = . . . = t m .</p><p>The reason leading to the above consequences is, in TransE, the representation of an entity is the same when involved in any relations, ignoring distributed representations <ref type="table">Table 1</ref>: Different embedding models: the scoring functions f r (h, t) and the model complexity (the number of parameters). n e and n r are the number of unique entities and relations, respectively. It is the often case that n r 񮽙 n e . k is the dimension of embedding space. s is the number of hidden nodes of a neural network or the number of slices of a tensor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Score function fr(h, t) # Parameters</p><formula xml:id="formula_2">TransE (Bordes et al. 2013b) 񮽙h + r − t񮽙 񮽙 1/2 , r ∈ R k O(nek + nrk) Unstructured (Bordes et al. 2012) 񮽙h − t񮽙 2 2 O(nek) Distant (Bordes et al. 2011) 񮽙W rh h − Wrtt񮽙1, W rh , Wrt ∈ R k×k O(nek + 2nrk 2 ) Bilinear (Jenatton et al. 2012) h 񮽙 Wrt, Wr ∈ R k×k O(nek + nrk 2 )</formula><p>Single Layer of entities when involved in different relations. Although TransE does not enforce h + r − t = 0 for golden triplets, it uses a ranking loss to encourage lower error for golden triplets and higher error for incorrect triplets ( <ref type="bibr" target="#b5">Bordes et al. 2013b</ref>), the tendency in the above propositions still exists.</p><formula xml:id="formula_3">u 񮽙 r f (W rh h + Wrtt + br) O(nek + nr(sk + s)) ur, br ∈ R s , W rh , Wrt ∈ R s×k NTN (Socher et al. 2013) u 񮽙 r f (h 񮽙 Wrt + W rh h + Wrtt + br) O(nek + nr(sk 2 + 2sk + 2s)) ur, br ∈ R s , Wr ∈ R k×k×s , W rh , Wrt ∈ R s×k TransH (this paper) 񮽙(h − w 񮽙 r hwr) + dr − (t − w 񮽙 r twr)񮽙 2 2 O(nek + 2nrk) wr, dr ∈ R k h t r (a) TransE h t d r t ⊥ h ⊥ (b) TransH</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Translating on Hyperplanes (TransH)</head><p>To overcome the problems of TransE in modeling reflexive/one-to-many/many-to-one/many-to-many relations, we propose a model which enables an entity to have distributed representations when involved in different relations. As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, for a relation r, we position the relation-specific translation vector d r in the relation-specific hyperplane w r (the normal vector) rather than in the same space of entity embeddings. Specifically, for a triplet (h, r, t), the embedding h and t are first projected to the hyperplane w r . The projections are denoted as h ⊥ and t ⊥ , respectively. We expect h ⊥ and t ⊥ can be connected by a translation vector d r on the hyperplane with low error if (h, r, t) is a golden triplet. Thus we define a scoring function 񮽙h ⊥ + d r − t ⊥ 񮽙 2 2 to measure the plausibility that the triplet is incorrect. By restricting 񮽙w r 񮽙 2 = 1, it is easy to get</p><formula xml:id="formula_4">h ⊥ = h − w 񮽙 r hw r , t ⊥ = t − w 񮽙 r tw r . Then the score function is f r (h, t) = 񮽙(h − w 񮽙 r hw r ) + d r − (t − w 񮽙 r tw r )񮽙 2 2 .</formula><p>The score is expected to be lower for a golden triplet and higher for an incorrect triplet. We name this model TransH. The model parameters are, all the entities' embeddings,</p><formula xml:id="formula_5">{e i } |E| i=1</formula><p>, all the relations' hyperplanes and translation vectors, {(w r , d r )} |R| r=1 . In TransH, by introducing the mechanism of projecting to the relation-specific hyperplane, it enables different roles of an entity in different relations/triplets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training</head><p>To encourage discrimination between golden triplets and incorrect triplets, we use the following margin-based ranking loss:</p><formula xml:id="formula_6">L = 񮽙 (h,r,t)∈∆ 񮽙 (h 񮽙 ,r 񮽙 ,t 񮽙 )∈∆ 񮽙 (h,r,t) [f r (h, t) + γ − f r 񮽙 (h 񮽙 , t 񮽙 )] + ,</formula><p>where [x] + 񮽙 max(0, x), ∆ is the set of positive (golden) triplets, ∆ 񮽙 (h,r,t) denotes the set of negative triplets constructed by corrupting (h, r, t), γ is the margin separating positive and negative triplets. The next subsection will introduce the details of constructing ∆ 񮽙 (h,r,t) . The following constraints are considered when we minimize the loss L:</p><formula xml:id="formula_7">∀e ∈ E, 񮽙e񮽙 2 ≤ 1, //scale (1) ∀r ∈ R, |w 񮽙 r d r |/񮽙d r 񮽙 2 ≤ 񮽙, //orthogonal (2) ∀r ∈ R, 񮽙w r 񮽙 2 = 1, //unit normal vector (3)</formula><p>where the constraint (2) guarantees the translation vector d r is in the hyperplane. Instead of directly optimizing the loss function with constraints, we convert it to the following unconstrained loss by means of soft constraints:</p><formula xml:id="formula_8">L = 񮽙 (h,r,t)∈∆ 񮽙 (h 񮽙 ,r 񮽙 ,t 񮽙 )∈∆ 񮽙 (h,r,t) 񮽙 fr(h, t) + γ − f r 񮽙 (h 񮽙 , t 񮽙 ) 񮽙 + + C 񮽙 񮽙 e∈E 񮽙 񮽙e񮽙 2 2 − 1 񮽙 + + 񮽙 r∈R 񮽙 (w 񮽙 r dr) 2 񮽙dr񮽙 2 2 − 񮽙 2 񮽙 + 񮽙 ,<label>(4)</label></formula><p>where C is a hyper-parameter weighting the importance of soft constraints. We adopt stochastic gradient descent (SGD) to minimize the above loss function. The set of golden triplets (the triplets from the knowledge graph) are randomly traversed multiple times. When a golden triplet is visited, a negative triplet is randomly constructed (according to the next section). After a mini-batch, the gradient is computed and the model parameters are updated. Notice that the constraint <ref type="formula">(3)</ref> is missed in <ref type="figure">Eq. (4)</ref>. Instead, to satisfy constraint (3), we project each w r to unit 񮽙 2 -ball before visiting each mini-batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reducing False Negative Labels</head><p>As described in the previous section, training involves constructing negative triplets for a golden triplet. Previous methods simply get negative triplets by randomly corrupting the golden triplet. For example, in TransE, for a golden triplet (h, r, t), a negative triplet (h 񮽙 , r, t 񮽙 ) is obtained by randomly sampling a pair of entities (h 񮽙 , t 񮽙 ) from E. However, as a real knowledge graph is often far from completed, this way of randomly sampling may introduce many false negative labels into training.</p><p>We adopt a different approach for TransH. Basically, we set different probabilities for replacing the head or tail entity when corrupting the triplet, which depends on the mapping property of the relation, i.e., one-to-many, many-to-one or many-to-many. We tend to give more chance to replacing the head entity if the relation is one-to-many and give more chance to replacing the tail entity if the relation is manyto-one. In this way, the chance of generating false negative labels is reduced. Specifically, among all the triplets of a relation r, we first get the following two statistics: (1) the average number of tail entities per head entity, denoted as tph; (2) the average number of head entities per tail entity, denoted as hpt. Then we define a Bernoulli distribution with parameter tph tph+hpt for sampling: given a golden triplet (h, r, t) of the relation r, with probability tph tph+hpt we corrupt the triplet by replacing the head, and with probability hpt tph+hpt we corrupt the triplet by replacing the tail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>We empirically study and evaluate related methods on three tasks: link prediction ( <ref type="bibr" target="#b5">Bordes et al. 2013b</ref>), triplets classification ( <ref type="bibr" target="#b17">Socher et al. 2013</ref>), and relational fact extraction ( <ref type="bibr" target="#b20">Weston et al. 2013</ref>). All three tasks evaluate the accuracy of predicting unseen triplets, from different viewpoints and application context.  <ref type="table" target="#tab_0">Test)   WN18  18  40,943  141,442  5,000  5,000  FB15k 1,345  14,951  483,142  50,000 59,071  WN11  11  38,696  112,581  2,609 10,544  FB13  13  75,043  316,232</ref> 5,908 23,733 <ref type="bibr">FB5M 1,192 5,385,322 19,193</ref>,556 50,000 59,071</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Link Prediction</head><p>Used in ( <ref type="bibr" target="#b2">Bordes et al. 2011;</ref><ref type="bibr" target="#b5">2013b)</ref>, this task is to complete a triplet (h, r, t) with h or t missing, i.e., predict t given (h, r) or predict h given (r, t). Rather than requiring one best answer, this task emphasizes more on ranking a set of candidate entities from the knowledge graph.</p><p>We use the same two data sets which are used in TransE ( <ref type="bibr" target="#b2">Bordes et al. 2011;</ref><ref type="bibr" target="#b5">2013b</ref>): WN18, a subset of Wordnet; FB15k, a relatively dense subgraph of Freebase where all entities are present in Wikilinks database <ref type="bibr">1</ref> . Both are released in ( <ref type="bibr" target="#b5">Bordes et al. 2013b</ref>). Please see <ref type="table" target="#tab_0">Table 2</ref> for more details.</p><p>Evaluation protocol. We follow the same protocol in TransE ( <ref type="bibr" target="#b5">Bordes et al. 2013b</ref>): For each testing triplet (h, r, t), we replace the tail t by every entity e in the knowledge graph and calculate a dissimilarity score (according to the scoring function f r ) on the corrupted triplet (h, r, e) . Ranking the scores in ascending order, we then get the rank of the original correct triplet. Similarly, we can get another rank for (h, r, t) by corrupting the head h. Aggregated over all the testing triplets, two metrics are reported: the averaged rank (denoted as Mean), and the proportion of ranks not larger than 10 (denoted as Hits@10). This is called the "raw" setting. Notice that if a corrupted triplet exists in the knowledge graph, as it is also correct, ranking it before the original triplet is not wrong. To eliminate this factor, we remove those corrupted triplets which exist in either training, valid, or testing set before getting the rank of each testing triplet. This setting is called "filt". In both settings, a lower Mean is better while a higher Hits@10 is better.</p><p>Implementation. As the data sets are the same, we directly copy experimental results of several baselines from <ref type="bibr" target="#b5">(Bordes et al. 2013b</ref>). In training TransH, we use learning rate α for SGD among {0.001, 0.005, 0.01}, the margin γ among {0.25, 0.5, 1, 2}, the embedding dimension k among {50, 75, 100}, the weight C among {0.015625, 0.0625, 0.25, 1.0}, and batch size B among {20, 75, 300, 1200, 4800}. The optimal parameters are determined by the validation set. Regarding the strategy of constructing negative labels, we use "unif" to denote the traditional way of replacing head or tail with equal probability, and use "bern." to denote reducing false negative labels by replacing head or tail with different probabilities. Under the "unif" setting, the optimal configurations are: α = 0.01, γ = 1, k = 50, C = 0.25, and B = 75 on WN18; α = 0.005, γ = 0.5, k = 50, C = 0.015625, and B = 1200 on FB15k. Under "bern" setting, the optimal configurations are: α = 0.01, γ = 1, k = 50, C = 0.25, and B = 1200 on WN18; α = 0.005, γ = 0.25, k = 100, C = 1.0, and B = 4800 on FB15k. For both datasets, we traverse all the training triplets for 500 rounds.</p><p>Results. The results are reported in <ref type="table" target="#tab_2">Table 3</ref>. The simple models TransE, TransH, and even the naive baseline Unstructured (i.e., TransE without translation) outperform other approaches on WN18 in terms of the Mean metric. This may be because the number of relations in WN18 is quite small so that it is acceptable to ignore the different types of relations. On FB15k, TransH consistently outperforms the counterparts. We hypothesize that the improvements are due to the relaxed geometric assumption compared with TransE so that the reflexive/one-to-many/manyto-one/many-to-many relations can be better handled. To confirm the point, we dig into the detailed results of different mapping categories of relations, as reported in <ref type="table" target="#tab_3">Ta- ble 4</ref>. Within the 1,345 relations, 24% are one-to-one, 23% are one-to-many, 29% are many-to-one, and 24% are manyto-many 2 . Overall, TransE is the runner up on FB15k. However, its relative superiorities on one-to-many and many-toone relations are not as good as those on one-to-one relations. TransH brings promising improvements to TransE on one-to-many, many-to-one, and many-to-many relations. Outstripping our expectations, the performance on one-toone is also significantly improved (&gt; 60%). This may be due to the "graph" property: entities are connected with relations so that better embeddings of some parts lead to better results on the whole. <ref type="table">Table 5</ref> reports the results of Hits@10 on some typical one-to-many/many-to-one/manyto-many/reflexive relations. The imrovement of TransH over TransE on these relations are very promising.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Triplets Classification</head><p>This task is to confirm whether a given triplet (h, r, t) is correct or not, i.e., binary classification on a triplet. It is used in <ref type="bibr" target="#b17">(Socher et al. 2013</ref>) to evaluate NTN model. Three data sets are used in this task. Two of them are the same as in <ref type="bibr">NTN (Socher et al. 2013</ref>): WN11, a subset of WordNet; FB13, a subset of Freebase. As WN11 and FB13 contain very small number of relations, we also use the FB15k data set which contains much more relations. See <ref type="table" target="#tab_0">Table 2</ref> for details.</p><p>Evaluation protocol. We follow the same protocol in NTN ( <ref type="bibr" target="#b17">Socher et al. 2013</ref>). Evaluation of classification needs negative labels. The released sets of WN11 and FB13 already contain negative triplets which are constructed by <ref type="bibr" target="#b17">(Socher et al. 2013</ref>), where each golden triplet is corrupted to get one negative triplet. For FB15k, we construct the negative triplets following the same procedure used for FB13 in <ref type="bibr" target="#b17">(Socher et al. 2013</ref>).</p><p>The decision rule for classification is simple: for a triplet (h, r, t), if the dissimilarity score (by the score function <ref type="table">Table 5</ref>: Hits@10 of TransE and TransH on some examples of one-to-many * , many-to-one † , many-to-many ‡ , and reflexive § relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation</head><p>Hits@10 f r ) is below a relation-specific threshold σ r , then predict positive. Otherwise predict negative. The relation-specific threshold σ r is determined according to (maximizing) the classification accuracy on the validation set. Implementation. For WN11 and FB13, as we use the same data sets, directly copying the results of different methods from <ref type="bibr" target="#b17">(Socher et al. 2013</ref>). For FB15k not used in <ref type="bibr" target="#b17">(Socher et al. 2013</ref>), we implement TransE and TransH by ourselves, and use the released code for NTN.</p><p>For TransE, we search learning rate α in {0.001, 0.005, 0.01, 0.1}, margin γ in {1.0, 2.0}, embedding dimension k in {20, 50, 100}, and batch size B in {30, 120, 480, 1920}. We also apply the trick of reducing false negative labels to TransE. The optimal configurations of TransE (bern.) are: α = 0.01, k = 20, γ = 2.0, B = 120, and L 1 as dissimilarity on WN11; α = 0.001, k = 100, γ = 2.0, B = 30, and L 1 as dissimilarity on FB13; α = 0.005, k = 100, γ = 2.0, B = 480, and L 1 as dissimilarity on FB15k. For TransH, the search space of hyperparameters is identical to link prediction. The optimal hyperparameters of TransH (bern.) are: α = 0.01, k = 100, γ = 2.0, C = 0.25, and B = 4800 on WN11; α = 0.001, k = 100, γ = 0.25, C = 0.0625, and B = 4800 on FB13; α = 0.01, k = 100, γ = 0.25, C = 0.0625, and B = 4800 on FB15k. We didn't change the configuration of NTN code on FB113 where dimension k = 100, number of slices equals 3. Since FB15k is relatively large, we limit the number of epochs to 500.</p><p>Results. Accuracies are reported in <ref type="table">Table 6</ref>. On WN11, TransH outperforms all the other methods. On FB13, the powerful model NTN is indeed the best one. However, on the larger set FB15k, TransE and TransH are much better than NTN. Notice that the number <ref type="bibr">(1,</ref><ref type="bibr">345)</ref> of relations of FB15k is much larger than that (13) of FB13 while the number of entities are close (see <ref type="table" target="#tab_0">Table 2</ref>). This means FB13 is a very dense subgraph where strong correlations exist between entities. In this case, modeling the complex correlations between entities by tensor and nonlinear  Relation Category 1-to-1 1-to-n n-to-1 n-to-n 1-to-1 1-to-n n-to-1 n-to-n transformation helps with embedding. However, on the much sparser subgraph of FB15k, it seems the simple assumption of translation or translation on hyperplanes is enough while the complex model of NTN is not necessary. Concerning running time, the cost of NTN is much higher than TransE/TransH. In addition, on all the three data sets, the trick of reducing false negative labeling (the results with "bern.") helps both <ref type="bibr">TransE and TransH. In NTN (Socher et al. 2013</ref>), the results of combining it with word embedding ( <ref type="bibr" target="#b12">Mikolov et al. 2013</ref>) are also reported. However, how best to combine word embedding is model dependent and also an open problem that goes beyond the scope of this paper. For a clear and fair comparison, all the results in <ref type="table">Table 6</ref> are without combination with word embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relational Fact Extraction from Text</head><p>Extracting relational facts from text is an important channel for enriching a knowledge graph. Most existing extracting methods ( <ref type="bibr" target="#b14">Mintz et al. 2009;</ref><ref type="bibr" target="#b16">Riedel, Yao, and McCallum 2010;</ref><ref type="bibr" target="#b9">Hoffmann et al. 2011;</ref><ref type="bibr" target="#b18">Surdeanu et al. 2012</ref>) distantly collect evidences from an external text corpus for a candidate fact, ignoring the capability of the knowledge graph itself to reason the new fact. Actually, knowledge graph embedding is able to score a candidate fact, without observing any evidence from external text corpus. Recently ( <ref type="bibr" target="#b20">Weston et al. 2013</ref>) combined the score from TransE (evidence from knowledge graphs) with the score from a text side extraction model (evidence <ref type="table">Table 6</ref>: Triplet classification: accuracies (%). "40h", "5m" and "30m" in the brackets are the running (wall clock) time.  including the negative class-"NA". Then the data set is split into two parts: one for training, another for testing. As to the text side extraction method, both TransE and TransH can be used to provide prior scores for any text side methods. For a clear and fair comparison with TransE reported in ( <ref type="bibr" target="#b20">Weston et al. 2013</ref>), we implement the same text side method Wsabie M2R in ( <ref type="bibr" target="#b20">Weston et al. 2013</ref>), which is denoted as Sm2r in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>For knowledge graph embedding, ( <ref type="bibr" target="#b20">Weston et al. 2013</ref>) used a subset of Freebase consisting of the most popular 4M entities and all the 23k Freebase relations. As they have not released the subset used in their experiment, we follow a similar procedure to produce a subset FB5M <ref type="table" target="#tab_0">(Table 2)</ref> from Freebase. What is important is, we remove all the entity pairs that appear in the testing set from FB5M so that the generalization testing is not fake. We choose parameters for TransE/TransH without a comprehensive search due to the scale of FB5M. For simplicity, in both TransE and TransH, we set the embedding dimension k to be 50, the learning rate for SGD α to 0.01, the margin γ to 1.0, and dissimilarity of TransE to L 2 .</p><p>Following the same rule of combining the score from knowledge graph embedding with the score from the text side model, we can obtain the precision-recall curves for TransE and TransH, as shown in <ref type="figure" target="#fig_1">Figure 2 (a)</ref>. From the figure we can see TransH consistently outperforms TransE as a "prior" model on improving the text side extraction method Sm2r.</p><p>The results in <ref type="figure" target="#fig_1">Figure 2</ref> (a) depend on the specific rule of combining the score from knowledge graph embedding with the score from text side model. Actually the combining rule in ( <ref type="bibr" target="#b20">Weston et al. 2013</ref>) is quite ad-hoc, which may not be the best way. Thus <ref type="figure" target="#fig_1">Figure 2</ref> (a) does not clearly demonstrate the separate capability of TransE/TransH as a stand-alone model for relational fact prediction. To clearly demonstrate the stand-alone capability of TransE/TransH, we first use the text side model Sm2r to assign each entity pair to the relation with the highest confidence score, then keep those facts where the assigned relation is not "NA". On these accepted candidate facts, we only use the score of TransE/TransH to predict. The results are illustrated in <ref type="figure" target="#fig_1">Figure 2 (b)</ref>. Both TransE and TransH perform better than the text side model Sm2r on this subset of candidates. TransH performs much better than TransE when recall is higher than 0.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we have introduced TransH, a new model to embed a knowledge graph in a continuous vector space. TransH overcomes the flaws of TransE concerning the reflexive/one-to-many/many-to-one/many-to-many relations while inheriting its efficiency. Extensive experiments on the tasks of link prediction, triplet classification, and relational fact extraction show that TransH brings promising improvements to TransE. The trick of reducing false negative labels proposed in this paper is also proven to be effective.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Simple illustration of TransE and TransH.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Precision-recall curves of TransE/TransH for fact extraction. (a) Combining the score from TransE/TransH and the score from Sm2r using the same rule in (Weston et al. 2013). (b) On the candidate facts accepted by Sm2r, we only use the score from TransE/TransH for prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 : Data sets used in the experiments.</head><label>2</label><figDesc></figDesc><table>Dataset 
#R 
#E 
#Trip. (Train / Valid / </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 3 : Link prediction results</head><label>3</label><figDesc></figDesc><table>Dataset 
WN18 
FB15k 

Metric 
MEAN 
HITS@10 
MEAN 
HITS@10 
Raw 
Filt. 
Raw Filt. 
Raw 
Filt. Raw Filt. 

Unstructured (Bordes et al. 2012) 
315 
304 
35.3 38.2 1,074 979 
4.5 
6.3 
RESCAL (Nickel, Tresp, and Kriegel 2011) 1,180 1,163 37.2 52.8 
828 
683 28.4 44.1 
SE (Bordes et al. 2011) 
1,011 
985 
68.5 80.5 
273 
162 28.8 39.8 
SME (Linear) (Bordes et al. 2012) 
545 
533 
65.1 74.1 
274 
154 30.7 40.8 
SME (Bilinear) (Bordes et al. 2012) 
526 
509 
54.7 61.3 
284 
158 31.3 41.3 
LFM (Jenatton et al. 2012) 
469 
456 
71.4 81.6 
283 
164 26.0 33.1 
TransE (Bordes et al. 2013b) 
263 
251 
75.4 89.2 
243 
125 34.9 47.1 

TransH (unif.) 
318 
303 
75.4 86.7 
211 
84 
42.5 58.5 
TransH (bern.) 
400.8 
388 
73.0 82.3 
212 
87 
45.7 64.4 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 : Results on FB15k by relation category</head><label>4</label><figDesc></figDesc><table>Task 
Predicting left (HITS@10) 
Predicting right (HITS@10) 

</table></figure>

			<note place="foot" n="1"> http://code.google.com/p/wiki-links/</note>

			<note place="foot" n="2"> For each relation r, we compute averaged number of tails per head (tphr), averaged number of head per tail (hptr). If tphr &lt; 1.5 and hptr &lt; 1.5, r is treated as one-to-one. If tphr ≥ 1.5 and hptr ≥ 1.5, r is treated as a many-to-many. If hptr &lt; 1.5 and tphr ≥ 1.5, r is treated as one-to-many. If hptr ≥ 1.5 and tphr &lt; 1.5, r is treated as many-to-one.</note>

			<note place="foot" n="3"> http://iesl.cs.umass.edu/riedel/data-univSchema/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Gene ontology: Tool for the unification of biology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ashburner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Botstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dolinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Dwight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Eppig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature genetics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="29" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Freebase: A collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data</title>
		<meeting>the 2008 ACM SIGMOD International Conference on Management of Data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning structured embeddings of knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 25th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A semantic matching energy function for learning with multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yakhnenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1304.7158</idno>
		<title level="m">Irreflexive and hierarchical relations as translations</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multirelational latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">;</forename><surname>-T</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1602" to="1612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weston</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th Annual International Conference on Machine Learning</title>
		<meeting>the 25th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Incorporating non-local information into information extraction systems by gibbs sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Grenager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Knowledge-based weak supervision for information extraction of overlapping relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 49th Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="541" to="550" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Obozinski</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A latent factor model for highly multi-relational data</title>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 25</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<biblScope unit="page" from="3167" to="3175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Wordnet: A lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A threeway model for collective learning on multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11), ICML &apos;11</title>
		<meeting>the 28th International Conference on Machine Learning (ICML-11), ICML &apos;11<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-instance multi-label learning for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Modelling relational data using bayesian clustered tensor factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1821" to="1828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Connecting language and knowledge bases with embedding models for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yakhnenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1366" to="1371" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
