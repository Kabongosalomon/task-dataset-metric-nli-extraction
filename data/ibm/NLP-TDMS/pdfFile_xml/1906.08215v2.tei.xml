<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bayesian Learning from Sequential Data using Gaussian Processes with Signature Covariances</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Csaba</forename><surname>Toth</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><surname>Oberhauser</surname></persName>
						</author>
						<title level="a" type="main">Bayesian Learning from Sequential Data using Gaussian Processes with Signature Covariances</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We develop a Bayesian approach to learning from sequential data by using Gaussian processes (GPs) with so-called signature kernels as covariance functions. This allows to make sequences of different length comparable and to rely on strong theoretical results from stochastic analysis. Signatures capture sequential structure with tensors that can scale unfavourably in sequence length and state space dimension. To deal with this, we introduce a sparse variational approach with inducing tensors. We then combine the resulting GP with LSTMs and GRUs to build larger models that leverage the strengths of each of these approaches and benchmark the resulting GPs on multivariate time series (TS) classification datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The evolution of some state variable, parameter or object gives naturally gives rise to sequential data, which is defined by having a notion of order on the incoming information. The ordering relation, or index set does not have to represent physical time, but for simplicity we will call it as such. For example, besides time series, sources of sequential data are text <ref type="bibr" target="#b55">(Pennington et al., 2014)</ref>, DNA <ref type="bibr" target="#b30">(Heather &amp; Chain, 2016)</ref>, or even topological data analysis . This ubiquity of sequential data has received special attention by the machine learning community in recent years. This paper is motivated by the following three approaches:</p><p>Deep learning approaches. Deep learning approaches, such as the celebrated LSTM network <ref type="bibr" target="#b34">(Hochreiter &amp; Schmidhuber, 1997)</ref>, other forms of RNNs <ref type="bibr" target="#b14">(Cho et al., 2014)</ref> and convolutional networks have successfully been applied to a variety of tasks involving sequential data <ref type="bibr" target="#b63">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b54">Oord et al., 2016)</ref>. Deep learning models can approximate any continuous function, but the cost is a large number of parameters, high variance and poor interpretability. This leaves the door open for alternative approaches not only as competitors, but as complementary building blocks in a larger model.</p><p>Bayesian approaches. Often not only point predictions, but estimates of the associated uncertainties are required <ref type="bibr" target="#b27">(Ghahramani, 2013)</ref>. GPs <ref type="bibr" target="#b57">(Rasmussen &amp; Williams, 2006)</ref> provide flexible priors over functions of the data in nonparametric Bayesian models. In the context of sequential data, two prominent ways to use GPs are: (1) using as covariance functions kernels specifically designed for sequences <ref type="bibr" target="#b46">(Lodhi et al., 2002;</ref><ref type="bibr" target="#b16">Cuturi, 2011;</ref><ref type="bibr" target="#b17">Cuturi &amp; Doucet, 2011;</ref><ref type="bibr" target="#b2">Al-Shedivat et al., 2017)</ref>, (2) modelling the evolution in a latent space, that emits the observations, as a discrete dynamical system with a GP prior on the transition function, a model called the Gaussian Process State Space Model (GPSSM) <ref type="bibr" target="#b25">(Frigola et al., 2013;</ref><ref type="bibr" target="#b51">Mattos et al., 2016;</ref><ref type="bibr" target="#b24">Eleftheriadis et al., 2017;</ref><ref type="bibr" target="#b20">Doerr et al., 2018;</ref><ref type="bibr" target="#b35">Ialongo et al., 2019)</ref>. These two approaches are not mutually exclusive; if one models the latent system as a higher order Markov process, then sequence kernels can incorporate the effect of past states.</p><p>Signature approaches. The signature feature map is a well-developed tool from stochastic analysis that represents a sequence as an element in a linear space of tensors, <ref type="bibr" target="#b10">(Chen, 1958;</ref><ref type="bibr" target="#b48">Lyons et al., 2007)</ref>. While not a mainstream machine learning approach, it is gaining attention since it can represent non-linear functions of sequences as linear functions of signature features, and can be made invariant to parametrization similar to dynamic time warping (DTW). For example, <ref type="bibr" target="#b40">(Kidger et al., 2019)</ref> use them as layer in a deep learning architecture; <ref type="bibr" target="#b42">(Kiraly &amp; Oberhauser, 2019)</ref> introduce kernels for sequences by taking inner products of signature features;  use them for maximum mean discrepancies between laws of stochastic processes. In particular, if κ : R d × R d → R is a kernel for vector-valued data, then <ref type="bibr" target="#b42">(Kiraly &amp; Oberhauser, 2019)</ref>  ∆ i l ,j l κ(x i l , y j l ), for two sequences x = (x i ) lx i=1 and y = (y j ) ly j=1 , with the double difference operator defined as ∆ i,j κ(x i , y j ) := κ(x i+1 , y j+1 )−κ(x i , y j+1 )−κ(x i+1 , y j )+κ(x i , y j ), and the sums are taken over multi-indices i m = (i 1 , . . . , i m ) with 1 ≤ i 1 ≤ · · · ≤ i m &lt; l x (and analogous for j m ) and some explicitly computable coefficients c(i) ∈ [0, 1].</p><p>Our contribution. In principle, one can just use the signature kernel and algorithms from <ref type="bibr" target="#b42">(Kiraly &amp; Oberhauser, 2019)</ref> as covariance to define a GP for sequential data. However, the computational complexity becomes quickly prohibitive and the low-rank approximation too crude, which ultimately does not lead to competitive results on many TS benchmarks. We therefore develop a different approach to signature covariances that builds on two recent advances in GP inference, namely variational inference <ref type="bibr" target="#b65">(Titsias, 2009;</ref><ref type="bibr" target="#b32">Hensman et al., 2015;</ref><ref type="bibr" target="#b50">Matthews et al., 2016)</ref> and interdomain inducing points <ref type="bibr" target="#b43">(Lázaro-Gredilla &amp; Figueiras-Vidal, 2009</ref>) to alleviate the computational burden. In particular, we show that one can use sparse tensors as inter-domain inducing points by optimizing a variational bound. Moreover, we use this GP as a building block in combination with RNNs to build models that combine the strenghts of these different tools. This results in scalable inference algorithms and we use this to benchmark on standard TS datasets (i) against popular non-Bayesian time series classifiers purely in terms of accuracy, (ii) against alternative Bayesian models by comparing the calibration of uncertainties for predictions. Code and benchmarks are publically available at http://github.com/tgcsaba/GPSig.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background and Notation</head><p>Given data (X, Y ) consisting of n X inputs X = (x 1 , . . . , x n X ) ⊂ X with labels Y = (y 1 , . . . , y n X ) ∈ R, a common Bayesian approach is to put a prior on a set of functions {f |f : X → R}, update this prior by conditioning on (X, Y ), and then use the resulting posterior to make inference about the label y of an unseen point x . When this is done with Gaussian priors, the central object is a GP f = (f x ) x∈X which is specified by mean and covariance function. Throughout we interchangibly use the notation f x and f (x). Below we recall how covariances can be constructed from feature maps and discuss the case when X is a space of sequences of arbitrary length.</p><p>The feature space view. Given a map ϕ : X → V that injects X into a linear space V , a natural way to put a prior on a function class X → R is to consider linear functions of ϕ as model, that is f (x) := , ϕ(x) to model f (x i ) ≈ y i for some "weights" ∈ V . Uncertainty about f is then specified by uncertainty about (and the hyperparameters of ϕ). We refer to ϕ as a feature map and to V as feature space. Throughout we assume that f = (f x ) x∈X is a centered GP and predictions about unseen points can then be made by Gaussian conditioning. If the task is classification where the labels Y are discrete, such an approach can be still applied by using a GP f = (f x ) x∈X as nuiscance function to put a prior on the class membership probability by specifying p(y = 1|x) = σ(f (x)) where σ is for example a sigmoid.</p><p>Polynomial features. The classical example is X = R d and ϕ(x) := (1, x, x ⊗2 , x ⊗3 , . . . ,</p><formula xml:id="formula_0">x ⊗M )<label>(1)</label></formula><p>where x ⊗m ∈ (R d ) ⊗m is a tensor. We recall background on tensors in Appendix A. If we set f (x) = , ϕ(x) and put a centered Gaussian prior on = ( 1 , . . . , M ), then by linearity of the tensor product and expectation it follows that <ref type="bibr">2m)</ref> . Taking Σ 2 m to be an isotropic diagonal matrix σ 2 m · I ⊗m recovers the polynomial kernel,</p><formula xml:id="formula_1">E[f x f y ] = 1 + M m=1 Σ 2 m , x ⊗m ⊗ y ⊗m where Σ 2 m := E[ m ⊗ m ] ∈ (R d ) ⊗(</formula><formula xml:id="formula_2">E[f x f y ] = M m=0 σ 2 m x, y m</formula><p>where we use the convention x, y 0 = σ 0 0 = 1. Many variations exist, for example other classes of polynomials, such as Hermite polynomials (the eigenfunctions of the classic RBF kernel), can increase the effectiveness, since they allow to make the associated feature expansion infinite dimensional. However, what makes any such class of polynomials a sensible choice for ϕ is that by the Stone-Weierstrass theorem, any continuous compactly supported function X → R can be arbitrary well approximated as linear functions of ϕ(x). This approximation property often runs under the name universality <ref type="bibr" target="#b53">(Micchelli et al., 2006;</ref><ref type="bibr" target="#b62">Sriperumbudur et al., 2011)</ref>.</p><p>Sequences as paths. We study the case when one observation x is a sequence of l x tuples, x = (x i , t i ) i=1,...,lx , and each tuple (x i , t i ) specifies that at "time" t i a vector x i ∈ R d was measured. We denote with X seq the set of all such sequences. and emphasize that the length l x ≥ 1 is not fixed, which is a common case in real-world data. We now introduce an even larger set than X seq : sequential data evolves in discrete time but often arises by sampling a quantity that evolves in continuous time. Thus above the set X seq of sequences lurks the larger set of finite horizon paths</p><formula xml:id="formula_3">X paths = {x ∈ C([0, t x ], R d ) : t x ∈ R + , x(0) = 0, x bv &lt; ∞},</formula><p>which are simply continuous R d -valued functions on some bounded time-interval.</p><p>Here</p><formula xml:id="formula_4">x bv := sup 0≤t1&lt;···&lt;tn≤tx n i=1 |x(t i+1 ) − x(t i )|</formula><p>denotes the usual bounded variation norm 1 . The set X seq naturally embeds into X paths by mapping a sequence x ∈ X seq to the path that is given by linear interpolation between the points (0, 0), (t 1 , x 1 ), . . . , (t lx , x lx ); formally x ∈ X seq maps to the element of X paths defined as</p><formula xml:id="formula_5">t → (t i+1 − t i ) −1 (x i (t i+1 − t) + x i+1 (t − t i )) (2) for t ∈ [t i , t i+1 )</formula><p>. Henceforth, we implicitly use this embedding, i.e. with slight abuse of notation, given x ∈ X seq we also write x ∈ X paths . Key to our approach, and what makes it different to classic approaches, is to define a GP indexed by the larger set X paths rather than just X seq . At first, this looks wasteful since X paths is much bigger than X seq and in practice one only has access to discrete time data (already for storage reasons). But on a theoretical side, going from discrete time to continuous time has two big advantages: (i) by construction, such a GP is consistent in the high-frequency limit (that is when we sample an object evolving in continuous time at higher and higher frequencies), (ii) we can make use of well-developed theory from stochastic analysis; in particular we use so-called signature features for paths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">From signature features to covariances</head><p>The signature feature map Φ can be seen as a generalization of the polynomial feature map ϕ as defined in (1) from the domain X = R d of vectors to the domain of paths X paths . It is defined as</p><formula xml:id="formula_6">Φ(x) = (1, tx 0 dx, tx 0 dx ⊗2 , . . . , tx 0 dx ⊗m ) (3) where t 0 dx ⊗(m+1) := tx 0 s 0 dx ⊗m ⊗ dx(s) ∈ (R d ) ⊗m and t 0 dx ⊗1 := x(t).</formula><p>Signatures are classic objects in stochastic analysis, but probably unfamiliar to researchers in ML and we provide background in Appendix B. Additionally, we recommend <ref type="bibr" target="#b11">(Chevyrev &amp; Kormilitzin, 2016)</ref> for a hands-on introduction to signature features that provides a good complement to our presentation, motivating signatures as a generalization of polynomial features to sequences.</p><p>For what follows, only three facts will be used about x → Φ(x): (1) it maps paths of different length to the same space, thus makes paths of different length comparable, (2) functions of paths can be arbitrary well approximated by linear functions of Φ(x), (3) it distinguishes paths that follow different trajectories, but not paths that only differ by parametrization). These points explain why signature features Φ(x) are a natural generalization of polynomial 1 Our approach generalizes to much rougher paths, such as Brownian trajectories and we give details in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vectors</head><p>Paths  <ref type="formula">(4)</ref> but also about the subset of it that consist of parametrization invariant functions. To make this precise, we call x = (x t ) a reparametrization of y = (y t ) if there exists a a smooth increasing function ρ : [0, t x ] → [0, t y ] (the "time change") such that x t = y ρ(t) for all t. We call an element f of (4) parametrization invariant if f (x) = f (y) when x and y are reparametrizations. Often the function we want to learn is invariant to a bit of reparametrization but not extreme reparametrization, so we need a more nuanced way to quantify parametrization (in)variance. Hence, what we really want is a hyperparameter τ ≥ 0 that signifies the degree of parametrization invariance: for τ = 0 all the mass of our prior should concentrate on the "extreme case" that is the subset of (4) consisting of functions that are parametrization invariant; and as τ gets increased the probability mass should spread out to functions that are sensitive to parametrization. This would allow to infer the degree of parametrization invariance by automatic relevance discovery (ARD). To accomplish this, we parametrize signature features with a parameter τ ≥ 0 as follows GP regularity. One expects that the GP with covariance (5) has nice regularity properties, however, the index set X paths is a very large space so some care is needed. In Appendix C, we compute covering numbers that yield explicit bounds on the modulus of continuity in terms of the path path length x bv .</p><formula xml:id="formula_7">Domain x ∈ R d x = (xt) t∈[0,tx] ∈ X paths Features ϕ(x) = (x ⊗m )m Φ(x) = ( dx ⊗m )m Feature space m (R d ) ⊗m m (R d ) ⊗m Functions f : R d → R f : X paths → R Covariance m σ 2 m x, y m m σ 2 m dxs, dyt</formula><formula xml:id="formula_8">Φ τ (x) := (1, tx 0 dx τ , tx 0 dx ⊗2 τ , . . . , tx 0 dx ⊗m τ ) where x τ (t) := (τ · t, x(t)) ∈ R 1+d .</formula><p>Theorem 1. Let L &gt; 0 and X L paths := {x ∈ X paths : x bv ≤ L}. There exists a centered GP f = (f x ) x∈X L paths with k(x, y) as defined in (5) as covariance function and that has continuous sample paths x → f x . Further, an explicit bound on its modulus of continuity in terms of L is given in equation <ref type="formula" target="#formula_0">(14)</ref>.</p><p>We now have a well-defined GP for Bayesian inference for sequences at hand that inherits many of the attractive properties of signature features. To turn this into useful models for large TS benchmarks we develop efficient inference algorithms in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Sparse variational inducing tensors</head><p>To reiterate, we are given data (X, Y ) consisting of n X sequences X = (x 1 , . . . , x n X ) of maximal length l X := max x∈X l x ⊂ X seq that evolve in R d with labels Y = (y 1 , . . . , y n X ), and the task is to predict labels y of unseen points x . For sequential data, the sample size n X and associated covariance matrix inversion is not the only compational bottleneck but also the maximal length of sequences l X , and the dimension d of the state space matter: n X , l X and d can be simultaneously large.</p><p>In this section, we introduce a sparse inference scheme to approximate the posterior of our GP, that locates the inducing points in a space other than the data-domain; an approach that is usually coined the term inter-domain sparse variational inference <ref type="bibr" target="#b43">(Lázaro-Gredilla &amp; Figueiras-Vidal, 2009;</ref><ref type="bibr" target="#b50">Matthews et al., 2016)</ref>. This allows for more efficient datarepresentation and faster inference. Key to our approach is that signature features take values in a well-understood subset of the feature space m≥0 (R d ) ⊗m . This allows as us to augment the index set with structured tensors, and locate inducing points in this larger index set.</p><p>Variational inference. As is well-known, inference for GPs scales as O(n 3 X ), see Section 3.3. in <ref type="bibr" target="#b57">(Rasmussen &amp; Williams, 2006)</ref>. This first led to sparse models, <ref type="bibr" target="#b56">(Quiñonero-Candela &amp; Rasmussen, 2005)</ref>, that select a subset Z = {z 1 , . . . , z n Z } of X consisting of n Z n X points, and subsequently to pseudo-inputs, <ref type="bibr" target="#b61">(Snelson &amp; Ghahramani, 2006)</ref>, that select points Z that are not necessarily in X. This was a big step towards complexity reduction, but pseudo-inputs are prone to overfitting, <ref type="bibr" target="#b49">(Matthews, 2017)</ref>. A different idea is to treat Z as parameters of a variational approximation <ref type="bibr" target="#b65">(Titsias, 2009)</ref> and not as model parameters; that is the points Z are choosen simultaneously with the hyperparameters of the GP by maximising a lower bound on the logmarginal likelhood log p(Y ), the so-called evidence lower bound (ELBO), given as</p><formula xml:id="formula_9">log p(Y ) ≥ E q(f X ) [log p(Y |f X )] − D KL [q(f Z ) p(f Z )] ,<label>(7)</label></formula><p>where f X and f Z denotes the GP evaluated at the data-points and the inducing locations. Typically, q(f Z ) is given a freeform multivariate Gaussian to be learnt from the data, and then extended to other indices of the GP by prior conditional matching, i.e. q(f X |f Z ) = p(f X |f Z ). Initially applied to regression, this was extended to classification <ref type="bibr" target="#b9">(Chai, 2012;</ref><ref type="bibr" target="#b32">Hensman et al., 2015)</ref>. Among its advantages are that it gives a nonparametric approximation to the true posterior, adding inducing points only improves the approximation, and any optimization method can be used to maximize the ELBO, most importantly, stochastic optimization; see <ref type="bibr" target="#b31">(Hensman et al., 2013;</ref><ref type="bibr" target="#b3">Bauer et al., 2016;</ref><ref type="bibr" target="#b8">Bui et al., 2017)</ref>.</p><p>Inter-domain approaches. Another idea is to go beyond the original index set and place inducing points Z in a different space X , that is, given a centered GP g = (g x ) x∈X one augments the original index set X by a set X to define a new GP (g x ) x∈X∪X and then locates the inducing points in this bigger model. This was suggested in <ref type="bibr" target="#b43">(Lázaro-Gredilla &amp; Figueiras-Vidal, 2009</ref>) in the context of integral transforms, which was extended in , and studied in more generality in <ref type="bibr" target="#b50">(Matthews et al., 2016)</ref>. In general, it is not obvious how to find a useful augmentation set X and define the covariance enlarged to X ∪ X .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">A sparse variational tensor augmentation.</head><p>Given any GP with a covariance function k(x, y) := Φ(x), Φ(y) where Φ is explicitly known 2 , we propose that a natural augmentation candidate is the "feature space" X := span{Φ(x) : x ∈ X} itself. The covariance function k of g can be simply extended to X ∪ X by linearity,</p><formula xml:id="formula_10">k(x, z) := k(z, x) := α k(x, x ) + β k(x, x ) (8) for x ∈ X, z = αΦ(x ) + βΦ(x ) ∈ X , α, β ∈ R; analo- gous for k(z, z ) with z, z ∈ X . For our GP, X = span{Φ τ (x) : x ∈ X paths } ⊂ m≥0 V ⊗m</formula><p>where we denote V := R d . We can thus extend our signature covariance (5) to X seq ∪ m≥0 V ⊗m by (8). This provides a flexible class of inducing point locations Z by optimizing over elements of the tensor algebra Z ⊂ m≥0 V ⊗m . We coin these inducing point locations as inducing tensors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consistency of augmentation.</head><p>A subtle point about augmenting the index set is that maximizing the ELBO in <ref type="formula" target="#formula_9">(7)</ref> is not necessarily equivalent anymore to minimizing a rigorously defined KL divergence between the true posterior process and its approximation over the unaugmented index set. In <ref type="bibr" target="#b50">(Matthews et al., 2016)</ref>, a sufficient condition given for this to hold is that the prior GP evaluated at the newly added indices is deterministic conditioned on the original GP. In the case of (8), this is easily seen to be true, since the augmented indices arise as linear combinations of elements in the original index set. Therefore, the corresponding GP evaluations arise as linear combinations of evaluations of the original process by the fact that the feature space X is a representation of the Hilbert space generated by the process <ref type="bibr" target="#b7">(Berlinet &amp; Thomas-Agnan, 2003)</ref>.</p><p>Representation of inducing tensors. We define our sparse inducing tensors as</p><formula xml:id="formula_11">z = (z m ) m=0,...,M ∈ M m=0 V ⊗m , where z 0 ∈ R and z m = v m,1 ⊗ v m,2 ⊗ · · · ⊗ v m,m for m ≥ 1.</formula><p>We remark that this construction does not generally give tensors that can be signatures of paths. However, they can be represented as linear combinations of signatures, hence the previous argument about the augmentation carries over. Also, informally, what gives the data-efficiency of inducing tensors is exactly that they are not represented in a basis of signatures, but as sparse tensors.</p><p>By linearity of integration and the inner product, the induc-</p><formula xml:id="formula_12">ing point covariance E[f z f z ] equals the inner product M m=0 σ 2 m z m , z m = M m=0 σ 2 m m k=1 v m,k , v m,k , (9) the cross-covariance E[f x f z ] = M m=0 σ 2 m Φ m (x), z m , Φ m (x), z m = dx t1 , v m,1 · · · dx tm , v m,m</formula><p>where the integration is over the simplex 0 &lt; t 1 &lt; · · · &lt; t lx .</p><p>Finally, note that we just need to evaluate the above for piecewise linear paths since these are the only paths that arise via the embedding (2), X seq → X paths . For such paths, the above integrals reduce to iterated sums, hence</p><formula xml:id="formula_13">Φ m (x), z m equals i c(i) x i1+1 − x i1 , v m,1 · · · x im+1 − x im , v m,m ,<label>(10)</label></formula><p>where the sum is taken over all m-tuples i = (i 1 , . . . , i m ) of the form 1 ≤ i 1 ≤ · · · ≤ i m ≤ l x and c(i) ≤ 1 is given by an explicit calculation. Similarly to (6), replacing c(i 1 , . . . , i m ) with 1 if there are no repeating indices in (i 1 , . . . , i m ) and otherwise with 0 gives a good approximation 3 . Below we use this approximation to (10) since it makes the recursive algorithms simpler but note that a simple modification exactly computes (10) for a marginal computational overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Algorithms.</head><p>We need to compute the three covariance matrices: (1) K ZZ of inducing tensors Z and inducing tensors Z, (2) K ZX of inducing tensors Z and sequences X, (3) K XX of sequences X and sequences X. Using the above tensor representations allows to give vectorized algorithms for <ref type="formula" target="#formula_0">(1)</ref> and <ref type="formula">(2)</ref> in Algorithms 1 and 2, respectively. For <ref type="formula">(3)</ref> we use a modification of Algorithm 3 from <ref type="bibr" target="#b42">(Kiraly &amp; Oberhauser, 2019)</ref> which we recall in Appendix D.2. We use notation defined in Appendix D.1, which can be briefly summarized as: Σ denotes the slice-wise sum operator, the (forward) cumulative sum operator, +1 the shift operator, and denotes the element-wise product of arrays. Additionally, we set</p><formula xml:id="formula_14">∆x i := x i+1 − x i for i ∈ {1, . . . , l x − 1}. For v, v ∈ V , d denotes the time to compute v, v , c the memory requirement of v. Proposition 1. Algorithm 1 computes the covariance ma- trix K ZZ of n Z inducing points in O(M 2 · n 2 Z · d) steps. Algorithm 2 computes the cross-covariance matrix K ZX in O(M 2 · n X · n Z · l X · d) steps. Additionally to storing the inducing tensors Z, Algorithm 1 requires O(M 2 · n 2 Z ) memory, Algorithm 2 requires O(M 2 · n X · n Z · l X ) memory.</formula><p>Proposition 1 follows by inspection of the algorithms and we emphasize the following points: (i) Both algorithms are linear in the maximal sequence length l X . (ii) M is a hyperparameter, and in all our experiments we learnt from the data M ≤ 5, thus the quadratic complexity in M is negligible. (iii) The memory cost of inducing tensors Z is much less than for the data X, which is stored in O(n X · l X · d) memory, which is important because the inducing tensors are variational parameters, and not amenable to subsampling, while the learning inputs can be subsampled as noted by <ref type="bibr" target="#b31">(Hensman et al., 2013)</ref>. Especially for GPUs memory cost is decisive and such savings are very important.</p><p>The computation of K XX detailed in Appendix D.2 has time complexity O((M + d) · n 2 X · l 2 X ) and memory of O(d · n X · l X + n 2 X · l 2 X ). However, given a factorizing likelihood, one only requires K X := [k(x, x)] x∈X , which eliminates the quadratic cost in n X . It turns out that this is enough to train on GPUs with reasonable minibatch sizes (e.g. n X = 50) on several real world datasets. We remark that the low-rank algorithms in <ref type="bibr" target="#b42">(Kiraly &amp; Oberhauser, 2019)</ref> allow to trade off accuracy for linear cost in l X , but we found that using the full-rank algorithm performs much better, and the above will allow us to apply it to several datasets with great results. Finally, note that the ELBO <ref type="formula" target="#formula_9">(7)</ref> requires an additional matrix inversion and multiplication in O(n 2 Z · n X + n 3 Z ) time, which is not significant in our case.</p><p>Variations. The following variations produce a more flexible covariance function: (i) given a nonlinear function</p><formula xml:id="formula_15">ϕ : R d → V into a linear space V , lift a sequence x = (t i , x i )</formula><p>to a path by taking the linear interpolation of (0, 0), (t 1 , ϕ(x 1 )), . . . , (t lx , ϕ(x lx )); with ϕ the identity on R d this recovers the original embedding (2), (ii) adding lags is a classic time series pre-processing technique, justified by Takens' theorem, <ref type="bibr" target="#b64">(Takens, 1981)</ref>, that guarantees that attractors in a high-dimensional dynamical system can be reconstructed from low-dimensional observations. Both</p><p>Algorithm 1 Computing the inducing covariances K ZZ 1: Input:</p><formula xml:id="formula_16">Tensors Z = (z i ) i=1,...,n Z ⊂ m n=0 V ⊗n , scalars (σ 2 0 , σ 2 1 , . . . , σ 2 m ), depth m ∈ N 2: Compute K[i, j, n, k] ← v i n,k , v j n,k for i, j ∈ {1, . . . , n Z }, n ∈ {1, . . . , m} and k ∈ {1, . . . , n} 3: Initialize R[i, j] ← σ 2 0 for i, j ∈ {1, . . . , n Z } 4: for n = 1 to m do 5: Assign A ← K[:, :, n, 1] 6:</formula><p>for k = 2 to n do 7: Update R ← R + σ 2 m · A[:, :, Σ] 10: end for 11: Output: Matrix of cross-covariances K ZX ← R points add non-linearities to the feature space which can make the learning more efficient. If the original sequence evolves in R d , this preprocessing results in a sequence (and then path) that evolves in a general high-dimensional space V . However, formulas (8), (9), and (10) show that only inner product evaluations on V are used and these can be computationally cheap even if V is high or even infinite dimensional. For example, following <ref type="bibr" target="#b42">(Kiraly &amp; Oberhauser, 2019)</ref> we may take a kernel κ : R d ×R d → R and use ϕ(x) := κ(x, ·), to build a sequence κ x ∈ V seq in the RKHS of κ. The only change in complexity is to replace d in the big O-bounds by the cost of the kernel evaluation. Such extensions also increase the number of hyperparameters which can have adversarial effects, but in our experiments both extensions led generically to better results.  <ref type="bibr" target="#b4">(Baydogan, 2015)</ref> with the 1 st and 2 nd best in bold and italicized for each row x <ref type="bibr" target="#b49">Matthews et al., 2017)</ref>, Keras <ref type="bibr" target="#b15">(Chollet et al., 2015)</ref>, we implemented three models: GP-Sig, GP-Sig-LSTM, and GP-Sig-GRU. All three use the signature covariance with the sparse inducing tensors of Section 4. GP-Sig is a plain vanilla variational GP classifier. Previous applications of neural nets to covariance constructions, in particular <ref type="bibr">(Wilson et al., 2016;</ref><ref type="bibr" target="#b2">Al-Shedivat et al., 2017)</ref>, inspired GP-Sig-LSTM and GP-Sig-GRU that include an RNN as a sequence-to-sequence transformation with h hidden units; see Figures 1 and 2 wherex denotes augmentation with lags and κx a static kernel as in above variation paragraph. We benchmarked these GP models on 16 multivariate TS classification datasets, a collection introduced in <ref type="bibr" target="#b4">(Baydogan, 2015)</ref> that has become a semistandard archive in TS classification, e.g. we cite 7 papers in Appendix E.4 that use these datasets. The same datasets are also used in <ref type="bibr" target="#b36">(Ismail Fawaz et al., 2019)</ref> to compare several deep learning architectures for TSC.</p><formula xml:id="formula_17">Iterate A ← K[:, :, n, k] A 8: end for 9: Update R ← R + σ 2 n · A 10: end for 11: Output: Matrix of inducing covariances K ZZ ← R Algorithm 2 Computing the cross-covariances K ZX 1: Input: Tensors Z = (z i ) i=1,...,n Z ⊂ m n=0 V ⊗n , sequences X = (x i ) i=1,...,n X ⊂ X seq , scalars (σ 2 0 , σ 2 1 , . . . , σ 2 m ), depth M ∈ N 2: Compute K[i, j, l, m, k] ← v i m,k , ∆x j,t l for i ∈ {1, . . . , n Z }, j ∈ {1, . . . , n X }, l ∈ {1, . . . , l X − 1}, m ∈ {1, . . . , M } and k ∈ {1, . . . , m} 3: Initialize R[i, j] ← σ 2 0 for i ∈ {1, . . . , n Z }, j ∈ {1, . . . , n X } 4: for m = 1 to M do</formula><formula xml:id="formula_18">GP-SIG-LSTM GP-SIG-GRU GP-SIG GP-LSTM GP-GRU GP-KCONV1D MEAN RANK (NLPP, n X &lt; 300) 2 .</formula><formula xml:id="formula_19">(R d ) seq κx V seq Φ(κx) M m=0 V ⊗m f x GP Figure 1: The GP-Sig model x (R d ) seq φ θ (x) (R h ) seq κ φ θ (x) V seq Φ(κ φ θ (x) ) M m=0 V ⊗m f x GP Figure 2: The GP-Sig-RNN model 5. Experiments TS classification. Using GPFlow (de G.</formula><p>As Bayesian baselines we used three GP models: (i) GP-L-STM and (ii) GP-GRU consist of an LSTM and a GRU network with an RBF kernel on top, in which case the RNNs are used as a sequence-to-vector transformation from R d seq to R h ; (iii) GP-KConv1D uses the convolutional kernel introduced in (van der Wilk et al., 2017) in 1-dimension (time). Throughout we used sparse variational inference: for GP-Sig-LSTM, GP-Sig-GRU, GP-Sig, the inducing tensors detailed in Section 4 are used; for GP-LSTM and GP-GRU the inducing points are located in the output space of the RNN layer, R h ; for GP-KConv1D, the inducing patches of (van der Wilk et al., 2017) are used.</p><p>We used n Z = 500 for all models 4 ; further all use a static kernel in one form or another, which we fixed to be the RBF kernel. The signature kernel was truncated 5 at M = 4, and for GP-Sig p = 1 lags were used; the GP-Sig-RNNs did not use lags, as the sequence of hidden states already incorporate lagged information about past observations. The window size in GP-KConv-1D was set to w = 10. The RNN-architectures were selected independently for all models by grid-search among 6 variants, that is, the number of hidden units from <ref type="bibr">[8,</ref><ref type="bibr">32,</ref><ref type="bibr">128]</ref> and with or without dropout. For training, early stopping was used with n = 500 epochs patience; a learning rate of α = 1 × 10 −3 ; a minibatch size of 50; as optimizer Adam <ref type="bibr" target="#b41">(Kingma &amp; Ba, 2014)</ref> and Nadam <ref type="bibr" target="#b21">(Dozat, 2015)</ref> were employed. Implementations are detailed in Appendix E.1, the datasets in Appendix E.2, the training and grid-search methodology in Appendix E.3.</p><p>Discussion of results. For GPs, we report accuracies and negative log-predictive probabilities (nlpp), the latter take not only accuracies, but the calibration of probabilities into account as well. <ref type="table" target="#tab_3">Table 2</ref> shows the average ranks among the GPs. The full table of nlpps and accuracies with mean and standard deviation over 5 model trains are reported in Appendix E.4 in <ref type="table">Table 5</ref> and <ref type="table" target="#tab_10">Table 6</ref>. As non-Bayesian baselines, we report accuracies of eight frequentist TS classifiers in <ref type="table" target="#tab_12">Table 7</ref>. On <ref type="figure" target="#fig_8">Figure 5</ref>, we visualize the box-plot distributions of (i) negative log-predictive probabilities of GPs, (ii) accuracies of both GPs and frequentist methods.</p><p>The signature models perform consistently the best in terms of average rankings of both nlpp and accuracy among the GPs. Particularly, they achieve stronger mean performance and a smaller variance across datasets. To explain this, inspecting the results in Tables 5, 6, we observe that all other GP baselines perform very poorly on some datasets, while the signature based models perform at least moderately well on all datasets. We believe this ties in to the universality property of signatures, see Appendix B.5. The convolutional GP, GP-KConv1D, which also has a very small parameter set, performed rather competitively with the deep kernel baselines, even on larger datasets. Comparison among variants of GP-Sig can be summarized as follows: for smaller datasets (n X &lt; 300), GP-Sig outperforms other variants as it has a very small parameter set; for larger datasets (n X ≥ 300), GP-Sig-LSTM performs best which conforms with the intuition that RNNs suffer from small sample sizes. A related observation is that GP-LSTM and GP-GRU perform about on par, while GP-Sig-LSTM does much better than GP-Sig-GRU, which suggests that the signature makes explicit use of the additional gate in the LSTM network.</p><p>Compared only in terms of accuracy, GP-Sig competes with frequentist classifiers: it outperforms the usual DTW baseline and competes with state-of-the art classifiers such as MUSE and MLSTMFCN. Purely based on accuracy, these win overall, but the difference is usually small, hence the extra Bayesian advantages come at a small cost. Furthermore, since the MLSTMFCN is also a deep learning baseline, it would be interesting to see how it performs incorporated into a deep kernel, possibly used as a sequence-to-sequence transformation with the signature kernel on top. Obviously TS classification is a vast field and many other models could be considered; e.g. we did not use recurrent GPs or GPSSMs since (1) they have so-far not been used for TS classification, possibly because there is no sequential nature in the output space, (2) we did not find a GPflow implementation that would allow to use sequence kernels in the GP transition function. (Implementation of <ref type="bibr" target="#b35">(Ialongo et al., 2019)</ref> does currently not allow taking subsequences of past states. An implementation would require much further work, but an interesting project would be to combine our models.)</p><p>Inducing tensors vs inducing sequences. Our results rely on the inter-domain approach using tensors to locate inducing points from Section 4. An alternative is to use sequences for the inducing points, Z ⊂ X seq , and controlling their maximal length l Z := max z∈Z l z to be of order m, i.e. l Z ∼ m. We coin this approach inducing sequences. Intuitively, one expects the inducing tensors to be more efficient than inducing sequences, since they make full use of the structure of the signature feature space/covariance. To test this intuition, we compared the performance of the inducing tensors and inducing sequences subject to both having the same computational complexitiy. For this experiment, we took the AUSLAN dataset <ref type="bibr" target="#b22">(Dua &amp; Graff, 2017)</ref>, which consists of n c = 95 classes for n X = 1140 training examples. This is a challenging dataset as the inducing variables need to characterize the abundance of classification boundaries.</p><p>We used GP-Sig with the same settings as in the previous experiments. The hyperparameters of the kernel were a-priori  learnt with n Z = 500 inducing tensors, and we purely investigated how the quality of the approximation changes for both approaches by varying the number of inducing points n Z . For each number of inducing variables, both approaches were trained independently 5 times for 300 epochs with random initialization of the inducing variables, for details on which see Appendix E.3. We plot on <ref type="figure" target="#fig_2">Figure 3</ref> three metrics:</p><p>(1) the achieved ELBO on the training set, (2) the achieved accuracy, and (3) nlpp on the testing set. At n Z = 500 both approaches are close to saturation, but the inducing tensors consistently perform better. We remark that in practice, an important aspect is also how well the kernel hyperparameters can be recovered, that we did not consider here, and is a tricky question for sparse variational inference in general <ref type="bibr" target="#b3">(Bauer et al., 2016)</ref>. Although, intuition suggests that the closer the model to saturation is with respect to the inducing points, the more consistent should the optimization be with un-sparsified variational inference. Visualizing inducing tensors. To gain more intuition, we visualized the feature space for one of the trained models on AUSLAN with n Z = 500 inducing tensors. We used UMAP <ref type="bibr" target="#b52">(McInnes et al., 2018)</ref> with the semimetric (x, y) → (k(x, x) + k(y, y) − 2 k(x, y)) 0.5 for x, y ∈ X ∪ X , see <ref type="figure" target="#fig_3">Figure 4</ref>. There are two imminent observations: (i) in the point cloud corresponding to the data, the classes hardly look linearly separable; (ii) the tensors, however, seem to live on a completely separate subspace than the data. The algorithm achieves 92% accuracy on this set, therefore, point (i) is likely due to information being lost in the projection. However, point (ii) challenges the intuition about classical sparse variational inference, that the inducing points are located mixed-in with the datapoints, concentrating close to the classification boundaries <ref type="bibr" target="#b32">(Hensman et al., 2015)</ref>. In general, the mechanism of how inter-domain inducing points represent the information in the data seems to be more complicated than classically.</p><p>To explain point (ii), we remark that this phenomenon is not surprising at all: signature features live in a manifold that is embedded in the linear tensor space M m=0 V ⊗m . In general, sparse tensors of the form (4.1) will not be signatures of paths. We believe variational inference works because of an interplay of two factors: Firstly, signatures of finite sequences can be written as finite linear combinations of such sparse tensors. Secondly, the prior conditional term used to define q(f x |f Z )) = p(f x |f Z ). The feature space is congruent to the prior GP <ref type="bibr" target="#b7">(Berlinet &amp; Thomas-Agnan, 2003)</ref>, which means that for x ∈ X seq , the value of f x given f Z is not only almost deterministic when x is close to any of z ∈ Z, but when it is close to any linear combinations of elements in Z. By the first remark this can always be achieved given a large enough n Z . To sum up, the inducing tensors do not represent signature features individually, but form atomic building blocks such that their linear combinations induce the actual variational posterior at the data-examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We used a classical object from stochastic analysis -signatures -to define a GP for sequential data. The GP inherits many of the theoretical guarantees that are known for signature features such as universality and parametrization invariance. To make it scalable, we develop "inducing tensors" that exploit the structure of the feature space, inter-domain inducing points, and variational inference. Applied in a plain vanilla variational framework, this yields a classifier, GP-Sig, that is not only competitive in terms of nlpp with other GP models, but also with state-of-the-art frequentist TS classifiers in terms of accuracy alone. As one of our reviewers remarked, several datasets we consider have a strong signal-to-noise ratio, which makes it worthwhile to point out that even for such datasets, the alternative GP baselines suffer on at least some of them, while the proposed models are consistently able to learn on all datasets. This observation ties in to the universality property, and it suggests that GPs with signatures can be a good starting point when building Bayesian models on time series datasets.</p><p>We also demonstrate that signatures can be used as a building block in deep kernels to build larger GP models that leverage the benefits of both, RNNs and signatures. Interestingly, we find that the vanilla GP-Sig model outperforms the GP-Sig-RNNs for smaller datasets, conforming to the intuition that smaller sample sizes are detrimental for recurrent neural nets. To really get the best of both worlds, one could insert an additional model selection step, that specifies whether a parametric transformation is layer used before feeding the input into the kernel or not. Alternatively, it could also be possible to increase the flexibility of the sequential GP model while staying within a purely nonparametric framework using deep GPs <ref type="bibr" target="#b18">(Damianou &amp; Lawrence, 2013</ref>) by e.g. applying a GP layer as observationwise state-space embedding before the kernel computation. The inference framework of <ref type="bibr" target="#b58">(Salimbeni et al., 2019)</ref> for deep GPs could also come in handy when moving to datasets with lower signal-to-noise ratios, which can require GP models capable of handling not only epistemic (reducible) uncertainty, but aleatoric (irreducible) uncertainty in the data. It would also be interesting to see if such sequence kernels can be used to improve recurrent GP models <ref type="bibr" target="#b51">(Mattos et al., 2016;</ref><ref type="bibr" target="#b35">Ialongo et al., 2019)</ref> by incorporating sequential information into the GP transition function, that could potentially allow for a more efficient latent state representation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Tensors</head><p>We recall classical constructions with tensors.</p><p>A.1. Tensor products of vector spaces</p><formula xml:id="formula_20">If u = (u 1 , . . . , u d ) ∈ R d and v = (v 1 , . . . , v e ) ∈ R e then u ⊗ v ∈ R d ⊗ R e is the (d × e)-matrix with indices i ∈ {1, . . . , d}, j ∈ {1, .</formula><p>. . , e} and the (i, j)-th entry given</p><formula xml:id="formula_21">as (u ⊗ v) i,j = u i v j . Similarly, for u ∈ R d ,v ∈ R e , w ∈ R f , the tensor u ⊗ v ⊗ w ∈ R d ⊗ R e ⊗ R f has indices i ∈ {1, . . . , d}, j ∈ {1, . . . , e}, j ∈ {1, .</formula><p>. . , f } and its (i, j, k)-th entry is given as</p><formula xml:id="formula_22">(u ⊗ v ⊗ w) i,j,k = u i v j w k , etc.</formula><p>In the paragraph about variations in Section 4, we mention that one can also lift the sequence to a path evolving in an infinite-dimensional space V rather than R d before computing its signatures. Since dx ⊗m ∈ V ⊗m this requires to take a tensor product of an infinite-dimensional space V . Since this might be less known in ML, let us briefly recall a coordinate-free definition of the tensor product: If U and V are vector spaces (not necessarily finite dimensional) then there exists a linear space U ⊗ V and a bilinear map</p><formula xml:id="formula_23">ι : U × V → U ⊗ V such that any other bilinear map on U × V factors through U ⊗ V , that is given any bilinear map B : U × V → Z into a vector space Z, there exists a linear mapB : U ⊗ V → Z such thatB • ι = B.</formula><p>Further, the vector space U ⊗ V is unique up to isomorphism. If U, V are finite dimensional it is easy to verify that one recovers the coordinate-wise definition we recalled at the beginning of this section. If U = V then we write V ⊗2 instead of V ⊗ V ; further by convention we define V ⊗0 := {1}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Sequences of tensors</head><formula xml:id="formula_24">M m=0 V ⊗m The direct product m≥0 V m of vector spaces V 1 , V 2 , . . . is the set of sequences m≥0 V m := {(t 0 , t 1 , t 2 , . . . , ) : t m ∈ V m }.</formula><p>In our setting, we apply this when V is a vector space and V m := V ⊗m , the get the space</p><formula xml:id="formula_25">m≥0 V ⊗m .</formula><p>That is, an element t = (t m ) m≥0 ∈ m≥0 V ⊗m is a sequence of tensors of increasing depth, that is t 0 = 1 since by convention V ⊗0 = {1}, t 1 ∈ V is a vector, t 2 ∈ V ⊗2 , etc.</p><p>The space m≥0 V ⊗m is itself a vector space if one defines addition and scalar multiplication coordinate-wise: for s = (s m ) m≥0 ,t = (t m ) m≥0 ∈ m≥0 V ⊗m s + t = (s 0 + t 0 , s 1 + t 1 , s 2 + t 2 , . . .) λ · s = (λs 0 , λs 1 , λs 2 , . . .)</p><p>That is, if V = R d we add vectors to vectors, matrices to matrices, etc. We note that the space m≥0 V ⊗m is not just a vector space but has also a natural algebra structure and the space m≥0 V ⊗m is often referred to as the tensor algebra over V .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Inner products of tensors</head><p>We have seen how to build out of a linear space V another linear space V ⊗m of tensors. If V also carries an inner product, ·, · V this extends canonically to an inner product on subset of m≥0 V ⊗m ; set</p><formula xml:id="formula_26">v 1 ⊗ · · · ⊗ v m , w 1 ⊗ · · · ⊗ w m := m i=1 v i , w i V</formula><p>and extend linearly to {t ∈ m≥0 V ⊗m : t, t &lt; ∞}. In particular, we can take linear functionals of t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Example: the classic polynomial features</head><p>Take R d with the standard Euclidean inner product. In Section 2 we recalled the classic "polynomial feature map" that takes a point in R d to monomials in coordinates in x,</p><formula xml:id="formula_27">ϕ : x → (x ⊗m ) m≥0 ∈ m≥0 (R d ) ⊗m .<label>(11)</label></formula><p>We can build a functions f : V → R by taking linear functionals of ϕ, that is for ∈</p><formula xml:id="formula_28">M m=0 (R d ) ⊗m define f : x → , ϕ(x) .</formula><p>It might be helpful for readers less familiar with tensor products to spell out the definition of f in coordinates: by definition of the inner product we have</p><formula xml:id="formula_29">, Φ(x) = M m=1 m , x ⊗m .</formula><p>Spelled out in coordinates, x = (x 1 , . . . , x d ) and m = ( i1,...,im m ) i1,...,im∈{1,...,d} , the terms in the sum read as</p><formula xml:id="formula_30">m , x ⊗ = i1,...,im∈{1,...,d} i1,...,im m x i1 · · · x im .</formula><p>Thus formulated in coordinates one has f (x) = 0 + 1 1 x 1 + · · · 1 1 x d + 1,1 2 x 2 1 + 1,2 2 x 1 x 2 + · · · + 2,2 2 x 2 d + . . .</p><formula xml:id="formula_31">+ 1,...,1 m x m 1 + · · · + d,...,d m x m d</formula><p>which is how the polynomial feature map is often represented, see <ref type="bibr" target="#b57">(Rasmussen &amp; Williams, 2006</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Signature features</head><p>In this Section we give background on signature features. Signature features can be seen as a natural generalization of the polynomial feature map, but instead of mapping a point in R d to a sequence of tensors, they map paths X path to a sequence of tensors. They generalize many of the nice properties of polynomial features such as universality and simulatenuously give the option to ignore the timeparametrization without an explicit search over all possible time changes (like in DTW approaches).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Definition</head><p>By Definition <ref type="formula">(3)</ref>, the signature features are given as iterated integrals</p><formula xml:id="formula_32">Φ(x) = (1, tx 0 dx, tx 0 dx ⊗2 , . . . , tx 0 dx ⊗m τ ) where t 0 dx ⊗(m+1) := tx 0 s 0 dx ⊗m ⊗ dx(s) ∈ (R d ) ⊗m and by convention t 0 dx ⊗1 := x(t). Hence, Φ(x) ∈ M m=0 (R d ) ⊗m .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Examples</head><p>Coordinate-wise. For a path x : t → (x 1 (t), . . . , x d (t)) that evolves in R d , one can spell this out in coordinates: the m-th signature feature dx ⊗m ∈ (R d ) ⊗m is the tensor that has as its (i 1 , . . . , i m ) ∈ {1, . . . , d} m -th coordinate entry the real number computed by a Riemann-Stieltjes integral dx i1 (t 1 ) · · · dx im (t m ) = ẋ i1 (t 1 ) · · ·ẋ im (t m )dt 1 · · · dt m where the integration is taken over 0 ≤ t 1 &lt; · · · &lt; t m ≤ t x andẋ i (t) := dxi(t) dt .</p><p>Linear paths. Consider the path x : [0, 1] → R d that just runs along a straight line</p><formula xml:id="formula_33">x(t) = t → tv<label>(12)</label></formula><p>where v ∈ R d is a given vector. Plugging (12) into the definition of the iterated integrals, we get by a direct calculation that</p><formula xml:id="formula_34">dx ⊗m = v ⊗m m! ∈ (R d ) ⊗m .</formula><p>We see that for this special case of a path x that is fully described by its increment x(t x ) − x(0) = v, the signature features Φ(x) equal the polynomial features ϕ(x(t x )−x(0)) of the total increment v = x(t x ) − x(0) up to a rescaling by a constant 1 m! . (This is one of the many reasons why signature features are regarded as "polynomials of paths").</p><p>Piecewise linear paths. In general, these integrals need to be computed by standard integration techniques but for a piecewise linear path x, that is</p><formula xml:id="formula_35">[0, t] is partitioned into L dis- joint intervals, [0, t x ] = L−1 i=0 [t i , t i+1 ]</formula><p>, and x is piecewise linear on each of these pieces,</p><formula xml:id="formula_36">x(t) = t · v i for t ∈ [t i , t i+1 ]</formula><p>for a vector v i ∈ R d , then these iterated integrals just reduce to iterated sums, dx ⊗m equals</p><formula xml:id="formula_37">i c(i)v i1 ⊗ · · · ⊗ v im · (t i1+1 − t t1 ) · · · (t ti m +1 − t ti m )</formula><p>where the sum is taken over all tuples i = (i 1 , . . . , i m ) ∈ {1, . . . , L} m and c(i) is the inverse of the natural number |{p : {1, . . . , L} → {1, . . . , m}, p(i + 1) ≥ p(i), p(i) = i}|!.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Parametrization invariance.</head><p>A classic result going back to Chen <ref type="bibr" target="#b10">(Chen, 1958)</ref> shows that the map x → Φ 0 (x) is injective up to tree-like equivalence. Loosely speaking, tree-like equivalence is from a purely analytic point of view more natural to work with than reparametrization since tree-like equivalence between paths is analogous to Lebesgue almost sure equivalence between sets. Howevever, we emphasize that from a practical point of view, the difference between paths that are tree-like equivalent and paths that differ by a reparametrization is negligible and we invite the reader to use them as synonyms throughout this article. Nevertheless, we give the precise definition below and refer the interested reader to <ref type="bibr" target="#b29">(Hambly &amp; Lyons, 2010)</ref> for a detailed discussion. In particular this implies that for any function of the form</p><formula xml:id="formula_38">f (x) = , Φ 0 f (x)</formula><p>f (x) = f (y) if and only if x and y differ by parametrization (strictly speaking, by a tree-like equivalence). This ability to factor out time-invariance can be very powerful since the space of all possible time reparametrization is huge and we never make an explicit search over all possible time changes like in the calculation of DTW distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Parametrization variance.</head><p>Often the functions of sequences f (x) one is interested in, are invariant up to a certain degree of reparametrization but not invariant to extreme reparametrizations. For a stylized example consider TS that arise as blood pressure measurements from patients responding to medication: some patients respond slower, some faster, depending on metabolism and many other factors. Up to a certain degree of time-reparameterisation one should observe a similar shaped TS if the medication works. However, the feature map should allow to distinguish extreme cases, e.g. where the blood pressure is rapidly falling.</p><p>To address, we added an extra coordinate to the path x before computing the signature features of this enhanced path x τ (t) = (τ · t, x(t)) ∈ R d+1 , for τ &gt; 0. The enhanced path x τ is never tree-like since the first coordinate t → t · τ is strictly increasing. Formulated, differently: this "trick'' makes the parametrization part of the trajectory. Hence, the map</p><formula xml:id="formula_39">X paths x → Φ τ (x) := Φ(x τ ) ∈ M m=0 (R 1+d ) ⊗m is injective for τ &gt; 0.</formula><p>B.5. Universality.</p><p>One of the most attractive properties of the classical polynomial feature map x → ϕ(x) for vectors x ∈ X = R d , (11), is that any continuous function f : X = R d → R can be uniformly approximated on compact sets as a linear functional of ϕ, that is f (x) ≈ , ϕ(x) for some . The reason is that linear combinations of monomials (polynomials) form an algebra and the Stone-Weierstrass theorem applies. Such approximation properties of feature maps are usually referred to as "universality" in the ML literature.</p><p>One of the most attractive properties of the signature feature map x → Φ τ (x) for paths x ∈ X paths is that a universality result holds. For every continuous f :</p><formula xml:id="formula_40">X paths → R, K ⊂ X paths compact, &gt; 0 there exists a M ≥ 1, ∈ M m=0 V ⊗m such that sup x∈K |f (x) − , Φ τ (x) | &lt; .</formula><p>The analogous result holds for τ = 0 when we replace the domain X paths by equivalence classes of paths (under reparameterisation/tree-like equivalence). For a proof and many extensions, see .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.6. High-frequency sampling</head><p>One way to think about the embedding of X seq → X paths is that X paths represents the "real-world" where quantities evolve in continuous time but due to pratical reasons such as storage cost we only have access to their preimage in X seq . A natural question is what happens when the sampling gets finer and finer. We believe such consistency in the high-frequency sampling limit is important for the same reason, consistency in the number of samples n X is important: although in practice we only deal with finite numbers (finite number of samples, sequences rather than paths), we want that our method makes sense as we get more and more information. In the context of learning with sequences this does not only require to study n X → ∞ but also the limit as the mesh size of that sampling grid converges to 0.</p><p>Consistency. More formally, given x ∈ X paths consider a sequence (π k ) of partitions</p><formula xml:id="formula_41">π k = {(t k 1 , . . . , t k n ) : 0 ≤ t k 1 &lt; · · · &lt; t k n ≤ t x } with vanishing mesh mesh(π k ) := max |t k i+1 − t k i | → 0 as k → ∞.</formula><p>Each partition π k gives rise to sequence x k by sampling γ along the time points in π k . Following our convention we identify x k as a piecewise linear path in X paths and it is easy to verify that x − x k → 0 as k → ∞. Informally, as k → ∞ we go from discrete to continuous time. One of the nice properties of our GP covariance, is that it is consistent under such limits: given x, y ∈ X paths , k(x k , y k ) → k(x, y) as k → ∞. Having a well-defined GP on paths that is consistent under such approximations from discrete to continuous time guarantee that no constants blow up as the sequences gets longer (sampling gets high frequent).</p><p>Rough paths. So far we assumed that X seq consists of bounded variation paths but in the "real-world", the evolution of quantities is often subject to noise, e.g. a classical model in physics and engineering is</p><formula xml:id="formula_42">x(t) := a(t) + B(t)</formula><p>where a is a bounded variation path but B is a Brownian sample path. Since Brownian sample paths are not of bounded variation, x is not of bounded variation. However, the same consistency arguments as above go through but one has to replace the iterated Riemann-Stieltjes integrals by Ito-Stratonovich integrals in the definition of Φ(x).</p><p>Even rougher trajectories such as fractional Brownian motion and non-Markovian processes can be handled that way with so-called rough path integrals. This is well-beyond the scope of the present article but we refer the interest reader to  for such results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. GPs with Signature Covariances</head><p>We specified a covariance function k on the set X paths as inner product of the signature map. This guarantees that (x, y) → k(x, y) = Φ(x), Φ(y) is a positive definite function and from the general theory of stochastic processes the existence of a centered GP (f x ) x∈X paths such that E[f x f y ] = k(x, y) follows. However, this does not guarantee that the sample paths x → f x are continuous. Seminal work of Dudley <ref type="bibr" target="#b23">(Dudley, 2010)</ref> showed that such regularity estimates can be derived by bounding the growth of the covering number of the index set of the GP f under the semi-metric</p><formula xml:id="formula_43">d k (x, y) = E[|f x − f y | 2 ] = k(x, x) − 2k(x, y) + k(y, y).</formula><p>Already when when the index set is finite dimensional "nice" covariance functions can lead to discontinuous GPs, see e.g. Section 1.4. in <ref type="bibr">(Adler &amp; Taylor, 2009</ref>). Our GP has as index set the space of bounded variation paths X paths which is infinite-dimensional so some caution is needed. However, as we show below we can cover this space by lattice paths and derive covering number estimates that imply continuity. </p><formula xml:id="formula_44">d k (x, y) ≡ Φ(x) − Φ(y), Φ(x) − Φ(y) = Φ(x) − Φ(y) .</formula><p>By definition Φ and of the norm · on M m=0 (R d ) ⊗m this reads</p><formula xml:id="formula_45">d 2 k (x, y) = M m=1 dx ⊗m − dy ⊗m 2 ≤ M max m=1,...,M ∆ 2 m (x, y)</formula><p>where we denote ∆ m (x, y) := dx ⊗m − dy ⊗m . Let X s,L lattice ⊂ X L paths be the set of lattice paths starting at 0 ∈ R d that take steps of size s and that are of total length at most L. By the results in Section 4 of <ref type="bibr" target="#b47">(Lyons &amp; Xu, 2011)</ref>, for every x ∈ X L paths and every n ≥ 1 there exists a y ∈ X L2 −n ,L lattice such that for every m ≥ 1,</p><formula xml:id="formula_46">∆ m (x, y) ≤ d 2 n−1 4L m−1 (m − 1)! .<label>(13)</label></formula><p>Since L m−1 (m−1)! ≤ e L −1 we can apply (13) with n = n( ) := 1 − log 2 d √ M 4(e L −1) to get ∆ m (x, y) ≤ . Hence, there exists a lattice path y ∈ X L2 −n( ) ,L lattice such that</p><formula xml:id="formula_47">d k (x, y) ≤ .</formula><p>Further, the set X paths is finite and we can bound it by</p><formula xml:id="formula_48">|X L2 −n( ) ,L lattice | ≤ (2 d + 1) L2 n( ) ≤ 2 (d+1)L2 n( ) = 2 2(d+1)L2 n( )−1 = 2 2(d+1)L √ M</formula><p>where the first inequality follows since a lattice path has at every step 2 d directions to choose from and in addition can choose not to make a step. The last equality follows from the definition of n( ). Since x ∈ X L paths was chosen arbitrary it follows that X L paths can be covered by 2 2(d+1)L √ M balls of radius centered at lattice paths.</p><p>Theorem 3 combined with Dudley's celebrated entropy estimates gives regularity results for samples of our GP. In fact, this even yields a modulus of continuity for our GP. Dudley's results immediately yield a modulus of continuity in probability. By standard arguments this can be strengthened to give an almost sure modulus of continuity. Concretely, we use the formulation given in Theorem 2.7.1 in Chapter 5 of <ref type="bibr" target="#b39">(Khoshnevisan, 2002)</ref> which guarantees that</p><formula xml:id="formula_49">lim sup δ→0 ω(δ) δ 0 N ( 2 , L)d + cδ ln ln 1 δ ≤ 24.</formula><p>The bound <ref type="formula" target="#formula_0">(14)</ref> follows immediately since first term in the denominator equals</p><formula xml:id="formula_50">δ 0 log 2 N 2 , L d = 2(d + 1)L √ M 2 √ 2 √ δ = √ δ4 (d + 1)L √ M .</formula><p>D. Further algorithms D.1. Notation for computations.</p><p>We define notation based on <ref type="bibr" target="#b42">(Kiraly &amp; Oberhauser, 2019)</ref> for concisely describing vectorized computations. We use 1-based indexing for arrays to keep in line with the notation of the main text. Let A and B be k-fold arrays of size (n 1 × · · · × n k ), indexed by i j ∈ {1, . . . , n j } for j ∈ {1, . . . , k}. We define the following operations. (iii) The shift along axis j by +m for m ∈ N as:</p><p>A[:, . . . , :, +m, :, . . . , :][i 1 , . . . , i j , . . . , i k ]</p><formula xml:id="formula_51">:= A[i 1 , . . . , i j − m, . . . i k ], if i j &gt; m, 0, if i j ≤ m.</formula><p>(iv) The element-wise product of arrays A and B as:</p><formula xml:id="formula_52">A B[i 1 , . . . , i k ] := A[i 1 , . . . , i k ] · B[i 1 , . . . , i k ].</formula><p>Additionally, note that the use of the cumulative sum, , in conjunction with the shift by 1 operator, +1, along the same axis is equivalent to an exclusive cumulative sum, where in the new array the i j th index contains the sum of the original array's elements from 1 to i j − 1.</p><p>Algorithm 3 Computing covariances at sequences, K XX 1: Input: Sequences X = (x i ) i=1,...,n X ⊂ X seq , scalars (σ 2 0 , σ 2 1 , . . . , σ 2 M ), depth M ∈ N 2: Compute K[i, j, l, k] ← ∆x i,t l , ∆x j,t k for i, j ∈ {1, . . . , n X }, l, k ∈ {1, . . . , l X } 3: Initialize R[i, j] ← σ 2 0 for i, j ∈ {1, . . . , n X } 4: Update R ← R + σ 2 1 · K[:, :, Σ, Σ] 5: Assign A ← K 6: for m = 2 to M do 7:</p><p>Iterate A ← K A[:, :, + 1, + 1] 8:</p><p>Update R ← R + σ 2 n · A[:, :, Σ, Σ] 9: end for 10: Output: Matrix of covariances K XX ← R</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Covariances between sequences and sequences</head><p>We describe in Algorithm 3 the computation of the covariance matrix K XX of n X for sequences X = (x i ) i=1,...,n X ⊂ X seq , which is a modification of Algorithm 3 from <ref type="bibr" target="#b42">(Kiraly &amp; Oberhauser, 2019)</ref>. The observant reader will notice that for the vectorization a requirement is that all sequences in X have the same length, l X := sup x∈X l x . In practice, this is only a computational restriction and can be circumvented by tabulating each sequence to be the same length, e.g. by repeating the last observation as required. The convenience of the parametrization invariance of signatures is that the results remain unchanged.</p><p>Simple inspection says that the complexity of Algorithm 3 is of O((M + d) · n 2 X · l 2 X ) in time and O(d · n X · l X + n 2 X · l 2 X ) in memory. Although, note that for factorizing likelihoods the computation of the ELBO and making inference about unseen examples x * ∈ X seq with credible intervals only requires the diagonals of K XX , i.e. K X := [k(x, x)] x∈X . Hence, for convenience, we give vectorized pseudo-code in Algorithm 4 for computing K X , which has complexities O((M + c) · n X · l 2 X ) in time and O(d · n X · l X + n X · l 2 X ).</p><p>Algorithm 4 Computing variances at sequences, K X 1: Input: The implementation of all considered GP models are available at GITHUBAUTHOR. Here, we detail the technicalities related to the implementation of each model. GP-Sig. This is the standard GP model with the signature kernel over sequences. This is built on top of GPflow (de G. <ref type="bibr" target="#b49">Matthews et al., 2017)</ref>, and other than a few tweaks, they interface with GPflow models in a straightforward manner. Particularly for the kernel, there are several variants available with different state space embeddings, including RBF and Matérn static kernels. The hyperparameters of the kernel which are learnt from the data are: (1) the lengthscales corresponding to each state space dimension, (2) the scaling parameters that multiply each signature level, allowing to strengthen or weaken its effect, (3) the lag values by which the additional lagged versions of each coordinate are shifted, that is a continuous parameter and is applied using linear interpolation and flat extrapolation (i.e. when the queried time-point is negative then the value at time 0 is used). In Section 5, we denoted the augmented sequence with a time coordinate and p lags bŷ x := (t i , x ti , x ti−s1 , . . . , x ti−sp ) i=1,...,lx . The lagged coordinates use the same lengthscales as the original ones, which in many cases leads to better generalization compared to not using lags (e.g. Takens' theorem <ref type="bibr" target="#b64">(Takens, 1981)</ref>). The signature kernel is also normalized using the standard kernel normalizationk(x, y) := k(x, y)/ k(x, x) k(y, y), which we apply individually to each signature level. The supported inducing variables are InducingTensors and InducingSequences corresponding to the two variants described in the main text.</p><formula xml:id="formula_53">Sequences X = (x i ) i=1,...,n X ⊂ X seq , scalars (σ 2 0 , σ 2 1 , . . . , σ 2 M ), depth M ∈ N 2: Compute K[i, l, k] ← ∆x i,t l , ∆x i,t k for i ∈ {1, . . . , n X }, l, k ∈ {1, . . . , l X } 3: Initialize R[i] ← σ 2 0 for i ∈ {1,</formula><p>GP-Sig-LR. As previously mentioned, there exists a lowrank variant of the signature kernel as introduced in <ref type="bibr" target="#b42">(Kiraly &amp; Oberhauser, 2019)</ref>, which aims to approximate the feature map using a low-rank approximation, rather than computing inner product of signature features directly. Our implementation first uses the Nyström approximation to find a lowdimensional approximation of the state-space embedding, and then uses the primal formulation of the signature algorithms (see Algorithm 5 in <ref type="bibr" target="#b42">(Kiraly &amp; Oberhauser, 2019)</ref>) to compute the signature kernel, while keeping the size of the low-rank factors manageable with sparse randomized projections <ref type="bibr" target="#b45">(Li et al., 2006)</ref>. Its advantage is that it extends to very long time series due to linear complexity in the time series length l X ∈ N, while the quadratic complexity of the full-rank kernel needs to be addressed another way. We did not include this variant among the experiments because overall it performed much worse than the full-rank variant. There were two main issues: (i) on several datasets it failed to fit the dataset due to being less flexible and noise, (ii) even when the predictive means are good, it can still give severely miscalibrated uncertainties similarly to classic kernel approximation techniques (Nyström, RFF), since an LR covariance matrix results in a degenerate GP prior.</p><p>GP(-Sig)-LSTM/GRU. The RNN based models with a GP layer placed on top use the Keras implementation of the RNN architectures <ref type="bibr" target="#b15">(Chollet et al., 2015)</ref>, while the GP parts use the GPflow API, which is possible as both packages can define the computational graph using the Tensorflow backend. However, since none of the packages supports the other, the resulting models have to be trained somewhat manually using the slightly more primitive Tensorflow API, and therefore are not very user friendly. It is up to future work to build a more user friendly API that makes it possible to deploy models that combine neural networks and sparse variational GPs in a convenient manner.</p><p>GP-KConv1D. The 1-dimensional convolutional kernel essentially uses the same code as <ref type="bibr" target="#b19">(van der Wilk et al., 2017)</ref> included in the GPflow package, with some tweaks that allow different length time series to be compared by padding each sequence with nans and masking the nan entries during the computation. We also normalize the features corresponding to this kernel to unit length in the feature space using the standard kernel normalization. In the experiments, we set the window size to w = 10, but a few datasets have min x∈X l x &lt; 10, and in those cases we set w = min (10, min x∈X l x ). Also, as the sequence length l x , and hence, the number of windows can vary from instance to instance, the weighted version of the convolutional kernel from <ref type="bibr" target="#b19">(van der Wilk et al., 2017)</ref> is not applicable in this case, and the translation invariant version is used. <ref type="table" target="#tab_8">Table 3</ref> details the datasets from <ref type="bibr" target="#b4">(Baydogan, 2015)</ref> that we used for benchmarking. Here c denotes the number of classes, d the dimension of the sequence state space, l x the range of sequence lengths, n X and n X respectively denote the number of examples in the pre-specified training and testing sets. In the experiments, all state space dimensions were normalized to zero mean and unit variance. For the models GP-Sig(-LSTM/GRU), GP-KConv1D, we subsampled very long time series to l X = 500, in order to deal with the quadratic complexity of kernel evaluations and be able to fit within GPU memory limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. Datasets details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3. Training details</head><p>Initialization. For all models considered in the main text in Section 5, the RBF kernel was used as static kernel, which has lengthscale parameters (l 1 , . . . , l d ), i.e. the RBF kernel over R d is up to rescaling given by</p><formula xml:id="formula_54">κ(x, x ) := exp − 1 2 (x − x ) Σ −1 (x − x )</formula><p>with Σ ii := l 2 i a diagonal matrix. We used the initialization l</p><formula xml:id="formula_55">(0) i := E[(x i − x i ) 2 ] · d</formula><p>, where x i , x i are two independent copies of the i-th input space coordinate, and we used a stochastic estimator of this with typically n = 1000 observation samples from the data.</p><p>All considered models in Section 5 used some form of inducing variables. For the signature models, they were placed in the feature space of the signature map in the form of inducing tensors. These inducing tensors given in (4.1) are tensor products of elements in V . As detailed at the end of Section 4, although the state space of a sequence is R d , we can embed this sequence into a path that evolves in a linear space V that does not have to be R d . One way to do this is to use an observation-wise state space embedding given by a kernel κ : R d × R d → R and map a sequence x = (t i , x i ) to a sequence κ x = (t i , κ xi ) that evolves in the RKHS V of κ; here κ x := κ(x, ·) ∈ V . Therefore signatures of depth M now live in the space M m=0 V ⊗m , which is for most kernels κ a genuine infinite-dimensional space. However, all computations from Sections 3 and 4 carry on mutatis mutandis, with the difference being that we do not have the flexibility to represent the inducing tensors as tensor products of arbitrary elements in V , which are generally infinite dimensional. In this case, we take</p><formula xml:id="formula_56">z = (z m ) m=0,...,M ∈ M m=0 V ⊗m 0<label>(15)</label></formula><p>with z 0 ∈ R and z m = κ(x m,1 , ·) ⊗ · · · ⊗ κ(x m,m , ·) with</p><formula xml:id="formula_57">x i,j ∈ R d for 1 ≤ j ≤ i, 1 ≤ i ≤ m, 1 ≤ m ≤ M , where V 0 := {κ(x, ·) : x ∈ R d }.</formula><p>Put differently, the inducing tensors are also constrained to being tensor products of only such elements in V which arise as reproducing kernels 6 associated to vectors in R d . Hence, the complexity of evaluating κ(x, ·), κ(x , ·) is the same as in R d , and storing an element κ(x, ·) ∈ V 0 is the same memory. Now, the initialization of the inducing tensors is simply done by sampling random observations from the input sequences in a two step manner: (1) a random input sequence is selected, (2) from the sequence a time-increasing subset of its observations are selected and plugged into the tensor products given in (15), and this procedure is repeated n Z times.</p><p>Other forms of inducing variables used by the models in Section 5 are inducing points for the GP-RNNs and inducing patches for GP-KConv1D. The inducing points are initialized randomly by selecting a x ∈ X and computing its RNN-image φ θ (x), which is then used as an inducing point, and repeated for all n Z . The inducing patches are also initialized in two steps: (1) select a random input sequence x ∈ X, (2) select a random window from x, (x i , x i+1 , . . . , x i+w−1 ), where 1 ≤ w ≤ min x∈X l x denotes the window length in the convolutional kernel.</p><p>For the alternative sparse inference scheme for signatures described in Section 5, denoted the method of inducing sequences, we use the same initialization as for the inducing patches: select a random sequence, and select a random window, and repeat for all n Z .</p><p>The means and covariances of the inducing points used the usual whitening transformation, that is, reparametrization in terms of the Cholesky factor L of K ZZ , K ZZ = LL , and parameters initialized from zeros and identity.</p><p>The RNNs use the usual initializations, that is, Glorot initialization for the weights <ref type="bibr" target="#b28">(Glorot &amp; Bengio, 2010)</ref>, orthogonal initialization for the recurrent weights <ref type="bibr" target="#b59">(Saxe et al., 2014)</ref>, and zeros for the bias.</p><p>Optimization details. The training for the benchmarking experiment in Section 4 was performed on 11 GPUs overall: 4 Tesla K40Ms, 5 Geforce 2080 TIs and 2 Quadro GP100 graphics cards. All models were trained 5 times for the benchmarking and the RNN based models an additional 6 times for the grid-search. Thus, the training of overall 480 models required extensive computational resources.</p><p>In all experiments in Section 5, we used similar optimization details, that is, optimization with early stopping and checkpointing by optimizing on 80% of the training data and monitoring the nlpp 7 on a 20% validation set. We used <ref type="bibr">6</ref> The reproducing kernel associated to a point x ∈ X is simply the kernel function evaluated in one of its arguments at x, i.e. κ(x, ·) ∈ H0 ⊂ H for a kernel κ : X × X → R. <ref type="bibr">7</ref> We found that monitoring the validation nlpp rather than the a minibatch size of 50, fixed learning rate α = 1 × 10 −3 , and a patience value of n = 500 epochs. As optimizer, GP-Sig and GP-KConv1D used Nadam <ref type="bibr" target="#b21">(Dozat, 2015)</ref>, while the RNN based models used Adam <ref type="bibr" target="#b41">(Kingma &amp; Ba, 2014)</ref>. Additionally, as is well-known for SVGPs <ref type="bibr" target="#b3">(Bauer et al., 2016)</ref>, first fixing the hyperparameters and only optimizing over the variational approximation for a fixed number of epochs is beneficial which we follow. Furthermore, after the main training phase of the hyperparameters has finished, to learn the rest of the validation data that was excluded from the optimization, we re-merge the validation set into the training set, fix the hyperparameters, and optimize only over the variational parameters again to assimilate the remaining information into the variational approximation.</p><p>Hence, the training for all models is split into the following phases (1) partition the data in an 80 − 20 ratio for optimization and monitoring, (2) with fixed kernel hyperparameters initialized as described previously, train the variational parameters for fixed n epochs to tighten the ELBO bound;</p><p>(3) unfix the hyperparameters and train by monitoring the nlpp on the validation set, stopping after no improvement for n epochs, and restoring and best model; (4) re-merge the validation set into the training data and train the variational distribution again only for a fixed n epochs with the kernel hyperparameters fixed. In all scenarios, we used n = 500.</p><p>For GP-Sig, the insertion of an additional optimization phase was found to be beneficial. Particularly, we reparametrize the scaling parameters for the signature levels σ = (σ 0 , . . . , σ M ) as σ = (β · σ 0 , . . . , β · σ M ), where β ∈ R + . Then, phase (3) is split into two steps: first, train with unfixing all kernel hyperparameters except (σ 0 , . . . σ M ), which are a-priori all set as 1; secondly, now unfixing all parameters, continue training with early stopping. This trick allows to calibrate the overall variance of the GP using β in the first step, while fixing σ 0 = · · · = σ M . The intution why this works is that the signature levels in general contain complementary information about a given sequence, and fixing them to be equal first enforces the model to find a fit of the data for all signature levels jointly, i.e. in some sense this is an implicit regularization step. The second step allows to slightly adjust the contribution of each level without relying too heavily on any one of them. On the RNN-based signature models this trick did not give substantial improvements, possibly because the variance of the RNN layer generally outweights the variance of the signature layer.</p><p>In our experience, when using GP-Sig on datasets with a larger n X , it can yield a further improvement to gradually increase the learning to rate to α = 1 × 10 −2 to allow the optimizer to explore the space in more depth, and then decrease it back to α = 1 × 10 −3 to drive it to the closest local optima. However, on the smaller datasets this was found to validation accuracy leads to better generalization behaviour.</p><p>be counterproductive, and in the experiments we chose to stick with a unified scheme that worked consistently on all datasets. However, we also remark that without applying any of the previously described techniques, and training from front to back all parameters jointly with a small learning rate (e.g. α = 1 × 10 −3 ) gives good results already, but a few percents of test set accuracy can be gained on some datasets by using them.</p><p>Architecture search. <ref type="table">Table 4</ref> details each of the architectures used for the models containing an RNN layer, where H denotes the number of hidden units used, and D is a boolean trigger, that specifies whether dropout was used for the given experiment or not. In the case D = 1, we used the settings dropout = 0.25 and recurrent_dropout = 0.05, otherwise both were set to 0. To find the best performing architecture, we conducted a grid-search among 6 considered architectures, that is, H ∈ [8, 32, 128] and D ∈ [0, 1]. For the grid-search, only the training data was used, and the data was split in a 60 − 20 − 20 fashion, using 60% for training, 20% for early stopping and checkpointing, and the last 20% was used to evaluate the performance. The training itself was carried out using the same initialization and schedule as described, and was performed only once for each method and setting pair, due to the large number of datasets that we considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4. Benchmark results</head><p>We report in <ref type="table">Table 5</ref> and <ref type="table" target="#tab_10">Table 6</ref> the negative log-predictive probabilities and accuracies of the GP models considered in Section 5. For each method-dataset pair, 5 models were trained with the initialization described in Appendix E.3. The variance of the results is therefore due to random initialization of some parameters, and the minibatch randomness while training. The RNN based models used the architectures detailed in <ref type="table">Table 4</ref>. As non-Bayesian baselines, we report the results of recent frequentist TS classification methods from the respective publications, that is, <ref type="bibr" target="#b17">(Cuturi &amp; Doucet, 2011;</ref><ref type="bibr" target="#b5">Baydogan &amp; Runger, 2015a;</ref><ref type="bibr" target="#b38">Karlsson et al., 2016;</ref><ref type="bibr">Tuncel &amp; Baydogan, 2018;</ref><ref type="bibr" target="#b60">Schäfer &amp; Leser, 2017;</ref><ref type="bibr" target="#b37">Karim et al., 2019)</ref>. Particularly for MLSTMFCN, we report the same results as in <ref type="bibr" target="#b60">(Schäfer &amp; Leser, 2017)</ref>. In <ref type="figure" target="#fig_8">Figure 5</ref>, we visualize the box-plot distributions of (1) negative log-predictive probabilities of the GPs, (2) classification accuracies of both the GPs and the frequentist baselines. <ref type="table">Table 5</ref>: Mean and standard deviation of negative predictive log-probabilities (nlpp) on test sets over 5 independent runs DATASET GP-SIG-LSTM GP-SIG-GRU GP-SIG GP-LSTM GP-GRU GP-KCONV1D ARABIC DIGITS 0.047 ± 0.030 0.023 ± 0.006 0.071 ± 0.021 0.082 ± 0.022 0.066 ± 0.010 0.050 ± 0.003 AUSLAN 0.106 ± 0.007 0.123 ± 0.045 0.550 ± 0.114 0.650 ± 0.071 0.248 ± 0.063 1.900 ± 0.139 CHARACTER TRAJ.</p><p>0.031 ± 0.007 0.258 ± 0.265 0.108 ± 0.005 2.506 ± 1.007 3.523 ± 0.635 0.409 ± 0.141 CMUSUBJECT16 0.088 ± 0.020 0.040 ± 0.009 0.089 ± 0.027 0.270 ± 0.080 0.089 ± 0.039 0.255 ± 0.002 DIGITSHAPES 0.008 ± 0.001 0.035 ± 0.051 0.021 ± 0.001 0.013 ± 0.002 0.727 ± 0.569 0.035 ± 0.003 ECG 0.402 ± 0.023 0.431 ± 0.037 0.356 ± 0.008 0.496 ± 0.018 0.601 ± 0.137 0.543 ± 0.019 JAP. VOWELS 0.080 ± 0.031 0.053 ± 0.009 0.069 ± 0.003 0.061 ± 0.029 0.052 ± 0.005 0.067 ± 0.001 KICK VS PUNCH 0.301 ± 0.109 0.493 ± 0.128 0.224 ± 0.014 0.696 ± 0.046 0.674 ± 0.037 0.662 ± 0.017 LIBRAS 0.320 ± 0.045 0.346 ± 0.091 0.259 ± 0.021 0.911 ± 0.056 1.110 ± 0.248 1.608 ± 0.311 NETFLOW 0.218 ± 0.009 0.259 ± 0.078 0.189 ± 0.014 0.251 ± 0.041 0.194 ± 0.011 0.168 ± 0.081 PEMS 0.704 ± 0.130 1.100 ± 0.064 0.520 ± 0.058 1.194 ± 0.308 0.784 ± 0.111 0.537 ± 0.010 PENDIGITS 0.289 ± 0.127 0.399 ± 0.206 0.146 ± 0.007 0.185 ± 0.027 0.187 ± 0.043 0.181 ± 0.005 SHAPES 0.014 ± 0.004 0.012 ± 0.004 0.011 ± 0.002 0.016 ± 0.008 0.168 ± 0.142 0.012 ± 0.001 UWAVE 0.113 ± 0.011 0.121 ± 0.017 0.140 ± 0.004 0.745 ± 0.151 1.168 ± 1.063 0.189 ± 0.008 WAFER 0.048 ± 0.021 0.081 ± 0.011 0.105 ± 0.010 0.105 ± 0.086 0.029 ± 0.011 0.085 ± 0.002 WALK VS RUN 0.030 ± 0.008 0.030 ± 0.008 0.023 ± 0.007 0.048 ± 0.040 0.028 ± 0.000 0.066 ± 0.001   <ref type="table" target="#tab_1">DATASET  GP-SIG-LSTM GP-SIG-GRU  GP-SIG  GP-LSTM  GP-GRU  GP-KCONV1D</ref> ARABIC DIGITS 0.992 ± 0.003 0.994 ± 0.002 0.979 ± 0.004 0.985 ± 0.004 0.986 ± 0.005 0.984 ± 0.001 AUSLAN 0.983 ± 0.003 0.978 ± 0.006 0.925 ± 0.014 0.880 ± 0.012 0.949 ± 0.014 0.784 ± 0.012 CHARACTER TRAJ.</p><formula xml:id="formula_58">MEAN</formula><p>0.991 ± 0.003 0.925 ± 0.078 0.979 ± 0.002 0.233 ± 0.331 0.114 ± 0.050 0.941 ± 0.013 CMUSUBJECT16</p><p>1.000 ± 0.000 1.000 ± 0.000 0.979 ± 0.017 0.924 ± 0.051 0.993 ± 0.014 0.897 ± 0.000 DIGITSHAPES 1.000 ± 0.000 0.988 ± 0.025 1.000 ± 0.000 1.000 ± 0.000 0.812 ± 0.153 1.000 ± 0.000 ECG 0.816 ± 0.029 0.832 ± 0.012 0.848 ± 0.010 0.782 ± 0.032 0.734 ± 0.033 0.760 ± 0.018 JAP. VOWELS 0.981 ± 0.005 0.985 ± 0.004 0.982 ± 0.005 0.982 ± 0.004 0.986 ± 0.005 0.986 ± 0.002 KICK VS PUNCH 0.900 ± 0.063 0.820 ± 0.098 0.900 ± 0.000 0.620 ± 0.075 0.600 ± 0.110 0.700 ± 0.089 LIBRAS 0.921 ± 0.013 0.899 ± 0.031 0.923 ± 0.004 0.776 ± 0.019 0.742 ± 0.050 0.698 ± 0.026 NETFLOW 0.931 ± 0.002 0.921 ± 0.012 0.937 ± 0.003 0.928 ± 0.011 0.926 ± 0.012 0.945 ± 0.027 PEMS 0.763 ± 0.016 0.775 ± 0.019 0.820 ± 0.014 0.745 ± 0.044 0.769 ± 0.020 0.794 ± 0.008 PENDIGITS 0.928 ± 0.030 0.902 ± 0.048 0.955 ± 0.002 0.953 ± 0.008 0.951 ± 0.008 0.946 ± 0.001 SHAPES 1.000 ± 0.000 1.000 ± 0.000 1.000 ± 0.000 1.000 ± 0.000 0.867 ± 0.163 1.000 ± 0.000 UWAVE 0.970 ± 0.004 0.968 ± 0.006 0.964 ± 0.001 0.870 ± 0.029 0.763 ± 0.225 0.947 ± 0.002 WAFER 0.988 ± 0.005 0.978 ± 0.005 0.965 ± 0.004 0.966 ± 0.037 0.994 ± 0.002 0.984 ± 0.001 WALK VS RUN 1.000 ± 0.000 1.000 ± 0.000 1.000 ± 0.000 1.000 ± 0.000 1.000 ± 0.000 1.000 ± 0.000   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>uses signatures to derive the following kernel k(x, y) =</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Achieved ELBO (top), accuracy (middle), mean nlpp (bottom) after 300 epochs of training the variational parameters with random initialization and pre-learnt kernel hyperparameters, that were treated as fixed. Solid is the mean over 5 independent runs, dashed is the 1-std region.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>A UMAP visualization of the allocation of feature representations of data-points (coloured), and inducing tensors (black) in the feature space on the AUSLAN dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Definition 1 .</head><label>1</label><figDesc>A bounded variation path x : [0, t x ] → V is tree-like if there exists a continuous function h : [0, t x ] → [0, ∞) such that h(0) = h(T ) = 0 and such that for all s &lt; t |x(t) − x(s)| ≤ h(s) + h(t) − 2 inf u∈[s,t] h(u). Theorem 2. Let x : [0, t x ] → V and y : [0, t y ] → V be two paths of bounded variation. Then Φ(x) = Φ(y) if and only if x ← − y is tree-like where denotes path concatenation and ← − y (t) := y(t y − t) denotes time-reversal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Theorem 3 .</head><label>3</label><figDesc>For L &gt; 0 and &gt; 0 denote with N ( , L) the covering number of the set X L paths := {x ∈ X paths : x bv ≤ L} of bounded variation paths of length less or equal than L under the d k pseudo-metric. Then log 2 N ( , L) ≤ 2(d + 1)L √ M Proof. By definition of the metric d k</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Theorem 4 .</head><label>4</label><figDesc>There exists a centered GP (f x ) x∈X L paths that has a covariance E[f x f y ] the signature covariance function k(x, y) = Φ(x), Φ(y) . Moreover, if we denote its modulus of continuity on X L paths with where c &gt; 0 denotes a universal constant. Proof. The existence of a centered GP (f x ) x with covariance k follows from general results about Gaussian processes. The existence of a continuous modification (f x ) x∈X paths,L of follows from Dudley's theorem if</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(i) The cumulative sum along axis j as:A[:, . . . , :, , :, . . . , :][i 1 , . . . , i j−1 , i j , i j+1 , . . . i k ] 1 , . . . , i j−1 , κ, i j+1 , . . . , i k ].(ii) The slice-wise sum along axis j as:A[:, . . . , :, Σ, :, . . . , :][i 1 , . . . , i j−1 , i j+1 , . . . , i k ] 1 , . . . , i j−1 , κ, i j+1 , . . . i k ].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Box-plots of negative log-predictive probabilities (left) and classification accuracies (right) on 16 TSC datasets</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Mathematical Institute, University of Oxford, Oxford, United Kingdom. Correspondence to: Csaba Toth &lt;csaba.toth@maths.ox.ac.uk&gt;, Harald Oberhauser &lt;harald.oberhauser@maths.ox.ac.uk&gt;. Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s).</figDesc><table /><note>1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>: Comparison of polynomial and signature features</cell></row><row><cell>features ϕ(x): not only do they use the same feature space</cell></row><row><cell>(sequences of tensors), they also have the same attractive</cell></row><row><cell>properties such as being able to approximate continuous</cell></row><row><cell>functions. Below we discuss how to make x → Φ(x) dis-</cell></row><row><cell>tinguish paths with different time-parametrizations.</cell></row></table><note>Parametrization (in)variance. A classic empirical find- ing that led to DTW is that functions of sequences are to a certain degree invariant to the time parametrization: for example, different speakers pronounce words at different speeds. However, sometimes the parametrization matters, e.g. for financial data. Thus we do not only care about the set of functions {f : X paths → R | cont. and compactly supported}</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Average ranks of GPs on datasets</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Tuncel, K. S. and Baydogan, M. G. Autoregressive forests for multivariate time series modeling. Pattern Recognition, 73:202-215, 2018. van der Wilk, M., Rasmussen, C. E., and Hensman, J. Convolutional gaussian processes, 2017. Wilson, A. G., Hu, Z., Salakhutdinov, R., and Xing, E. P. Deep kernel learning. In Gretton, A. and Robert, C. C. (eds.), Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, volume 51 of Proceedings of Machine Learning Research, pp. 370-378, Cadiz, Spain, 09-11 May 2016. PMLR.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3 :</head><label>3</label><figDesc>Specification of datasets used for benchmarking</figDesc><table><row><cell>DATASET</cell><cell>c</cell><cell>d</cell><cell>lx</cell><cell>n X</cell><cell>n X</cell></row><row><cell>ARABIC DIGITS</cell><cell>10</cell><cell>13</cell><cell cols="2">4-93 6600</cell><cell>2200</cell></row><row><cell>AUSLAN</cell><cell>95</cell><cell>22</cell><cell cols="2">45-136 1140</cell><cell>1425</cell></row><row><cell>CHAR. TRAJ.</cell><cell>20</cell><cell>3</cell><cell>109-205</cell><cell>300</cell><cell>2558</cell></row><row><cell>CMUSUBJECT16</cell><cell>2</cell><cell>62</cell><cell>127-580</cell><cell>29</cell><cell>29</cell></row><row><cell>DIGITSHAPES</cell><cell>4</cell><cell>2</cell><cell>30-98</cell><cell>24</cell><cell>16</cell></row><row><cell>ECG</cell><cell>2</cell><cell>2</cell><cell>39-152</cell><cell>100</cell><cell>100</cell></row><row><cell>JAP. VOWELS</cell><cell>9</cell><cell>12</cell><cell>7-29</cell><cell>270</cell><cell>370</cell></row><row><cell>KICK VS PUNCH</cell><cell>2</cell><cell>62</cell><cell>274-841</cell><cell>16</cell><cell>10</cell></row><row><cell>LIBRAS</cell><cell>15</cell><cell>2</cell><cell>45</cell><cell>180</cell><cell>180</cell></row><row><cell>NETFLOW</cell><cell>2</cell><cell>4</cell><cell>50-997</cell><cell>803</cell><cell>534</cell></row><row><cell>PEMS</cell><cell cols="2">7 963</cell><cell>144</cell><cell>267</cell><cell>173</cell></row><row><cell>PENDIGITS</cell><cell>10</cell><cell>2</cell><cell>8</cell><cell cols="2">300 10692</cell></row><row><cell>SHAPES</cell><cell>3</cell><cell>2</cell><cell>52-98</cell><cell>18</cell><cell>12</cell></row><row><cell>UWAVE</cell><cell>8</cell><cell>3</cell><cell>315</cell><cell>896</cell><cell>3582</cell></row><row><cell>WAFER</cell><cell>2</cell><cell>6</cell><cell>104-198</cell><cell>298</cell><cell>896</cell></row><row><cell>WALK VS RUN</cell><cell>2</cell><cell cols="2">62 128-1918</cell><cell>28</cell><cell>16</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Mean and standard deviation of accuracies on test sets over 5 independent runs</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>Accuracies of frequentist time series classification methods</figDesc><table><row><cell>DATASET</cell><cell>SMTS</cell><cell cols="6">LPS MVARF DTW ARKERNEL GRSF MLSTMFCN MUSE</cell></row><row><cell>ARABIC DIGITS</cell><cell cols="2">0.964 0.971</cell><cell>0.952 0.908</cell><cell>0.988</cell><cell>0.975</cell><cell>0.990</cell><cell>0.992</cell></row><row><cell>AUSLAN</cell><cell cols="2">0.947 0.754</cell><cell>0.934 0.727</cell><cell>0.918</cell><cell>0.955</cell><cell>0.950</cell><cell>0.970</cell></row><row><cell>CHARACTER TRAJ.</cell><cell cols="2">0.992 0.965</cell><cell>0.928 0.948</cell><cell>0.900</cell><cell>0.994</cell><cell>0.990</cell><cell>0.937</cell></row><row><cell>CMUSUBJECT16</cell><cell cols="2">0.997 1.000</cell><cell>1.000 0.930</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell></row><row><cell>DIGITSHAPES</cell><cell cols="2">1.000 1.000</cell><cell>1.000 1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell></row><row><cell>ECG</cell><cell cols="2">0.818 0.820</cell><cell>0.785 0.790</cell><cell>0.820</cell><cell>0.880</cell><cell>0.870</cell><cell>0.880</cell></row><row><cell>JAP. VOWELS</cell><cell cols="2">0.969 0.951</cell><cell>0.959 0.962</cell><cell>0.984</cell><cell>0.800</cell><cell>1.000</cell><cell>0.976</cell></row><row><cell>KICK VS PUNCH</cell><cell cols="2">0.820 0.900</cell><cell>0.976 0.600</cell><cell>0.927</cell><cell>1.000</cell><cell>0.900</cell><cell>1.000</cell></row><row><cell>LIBRAS</cell><cell cols="2">0.909 0.903</cell><cell>0.945 0.888</cell><cell>0.952</cell><cell>0.911</cell><cell>0.970</cell><cell>0.894</cell></row><row><cell>NETFLOW</cell><cell cols="2">0.977 0.968</cell><cell>NA 0.976</cell><cell>NA</cell><cell>0.914</cell><cell>0.950</cell><cell>0.961</cell></row><row><cell>PEMS</cell><cell cols="2">0.896 0.844</cell><cell>NA 0.832</cell><cell>0.750</cell><cell>1.000</cell><cell>NA</cell><cell>NA</cell></row><row><cell>PENDIGITS</cell><cell cols="2">0.917 0.908</cell><cell>0.923 0.927</cell><cell>0.952</cell><cell>0.932</cell><cell>0.970</cell><cell>0.912</cell></row><row><cell>SHAPES</cell><cell cols="2">1.000 1.000</cell><cell>1.000 1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell></row><row><cell>UWAVE</cell><cell cols="2">0.941 0.980</cell><cell>0.952 0.916</cell><cell>0.904</cell><cell>0.929</cell><cell>0.970</cell><cell>0.916</cell></row><row><cell>WAFER</cell><cell cols="2">0.965 0.962</cell><cell>0.931 0.974</cell><cell>0.968</cell><cell>0.992</cell><cell>0.990</cell><cell>0.997</cell></row><row><cell>WALK VS RUN</cell><cell cols="2">1.000 1.000</cell><cell>1.000 1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell></row><row><cell>MEAN ACC.</cell><cell cols="2">0.945 0.933</cell><cell>0.949 0.899</cell><cell>0.938</cell><cell>0.955</cell><cell>0.970</cell><cell>0.962</cell></row><row><cell>MED. ACC.</cell><cell cols="2">0.964 0.964</cell><cell>0.952 0.929</cell><cell>0.952</cell><cell>0.984</cell><cell>0.990</cell><cell>0.976</cell></row><row><cell>SD. ACC.</cell><cell cols="2">0.059 0.073</cell><cell>0.055 0.111</cell><cell>0.073</cell><cell>0.058</cell><cell>0.039</cell><cell>0.043</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Mercer's Theorem guarantees the existence of Φ, but not in a sufficiently explicit form.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">It converges to Φ(x), z when the grid gets finer, |ti+1 −ti| ↓ 0, see to (10).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Using n Z = 500 is clearly superfluous for small datasets, which is fixed for the sake of consistent settings across datasets.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">For these experiments, the M = 4 value seemed to give an optimal trade-off between computational complexity and expressiveness of the kernel, see Appendix E.1 for more details.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>CT is supported by the "Mathematical Institute Award" by the University of Oxford, HO is supported by the EPSRC grant "Datasig" [EP/S026347/1], the Alan Turing Institute, and the Oxford-Man Institute. CT and HO would like to thank the reviewers for helpful and constructive comments.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr">DATASET</ref> <p>GP-SIG-LSTM GP-SIG-GRU GP-LSTM GP-GRU</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Random Fields and Geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
		<imprint>
			<publisher>Springer Monographs in Mathematics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">York</forename><surname>Springer New</surname></persName>
		</author>
		<ptr target="https://books.google.co.uk/books?id=R5BGvQ3ejloC" />
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning scalable deep kernels with recurrent structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Al-Shedivat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Saatchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno>1532-4435</idno>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2850" to="2886" />
			<date type="published" when="2017-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Understanding probabilistic sparse Gaussian process approximations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Der Wilk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1533" to="1541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Multivariate Time Series Classification Datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baydogan</surname></persName>
		</author>
		<idno>2020-02-05</idno>
		<ptr target="http://mustafabaydogan.com" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning a symbolic representation for multivariate time series classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Baydogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Runger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="400" to="422" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Time series representation and similarity based on local autopatterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Baydogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">C</forename><surname>Runger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="476" to="509" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Reproducing Kernel Hilbert Spaces in Probability and Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berlinet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Thomas-Agnan</surname></persName>
		</author>
		<ptr target="https://books.google.co.uk/books?id=v79sBNG34coC" />
		<imprint>
			<date type="published" when="2003" />
			<publisher>Springer US</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A Unifying Framework for Gaussian Process Pseudo-Point Approximations using Power Expectation Propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v18/16-603.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">104</biblScope>
			<biblScope unit="page" from="1" to="72" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Variational multinomial logit Gaussian process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M A</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1745" to="1808" />
			<date type="published" when="2012-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Integration of paths-a faithful representation of paths by non-commutative formal power series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Amer. Math. Soc</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="395" to="407" />
			<date type="published" when="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A Primer on the Signature Method in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Chevyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kormilitzin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-03" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Signature moments to characterize laws of stochastic processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Chevyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Oberhauser</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1810.10971" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint 1810.10971</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Persistence Paths and Signature Features in Topological Data Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Chevyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Oberhauser</surname></persName>
		</author>
		<idno type="DOI">10.1109/tpami.2018.2885516</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Fast global alignment kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011-01" />
			<biblScope unit="page" from="929" to="936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1101.0673</idno>
		<title level="m">Autoregressive Kernels For Time Series. arXiv e-prints, art</title>
		<imprint>
			<date type="published" when="2011-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Damianou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="207" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A gaussian process library using tensorflow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Der Wilk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nickson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fujii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boukouvalas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>León-Villagrá</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hensman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gpflow</surname></persName>
		</author>
		<idno>40:1-40:6</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Probabilistic recurrent state-space models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Doerr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schiegg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-T</forename><surname>Duy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Toussaint</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sebastian</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>Dy, J. and Krause, A.</editor>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsmässan, Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Incorporating Nesterov Momentum into Adam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dozat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Graff</surname></persName>
		</author>
		<ptr target="http://archive.ics.uci.edu/ml" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sample functions of the gaussian process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Dudley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Selected Works of RM Dudley</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="187" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Identification of gaussian process state space models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Eleftheriadis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nicholson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Deisenroth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">;</forename><surname>Hensman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garnett</forename></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7115-identification-of-gaussian-process-state-space-models.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30</title>
		<editor>R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5309" to="5319" />
		</imprint>
	</monogr>
	<note>Guyon, I</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bayesian inference and learning in gaussian process statespace models with particle mcmc</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Frigola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lindsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Schön</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E. ; C J C</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinberger</forename></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5085-bayesian-inference-and-learning-in-gaussian-process-state-space-models-with-particle-mcmc.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>K. Q.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
	<note>Burges</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Variational gaussian process state-space models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Frigola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="3680" to="3688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Bayesian non-parametrics and the probabilistic approach to modelling. Philosophical transactions. Series A, Mathematical, physical, and engineering sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno type="DOI">10.1098/rsta.2011.0553</idno>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">371</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<editor>Teh, Y. W. and Titterington, M.</editor>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics<address><addrLine>Sardinia, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-05" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="13" to="15" />
		</imprint>
		<respStmt>
			<orgName>Chia Laguna Resort</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Uniqueness for the Signature of a path of bounded variation and the reduced path group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hambly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lyons</surname></persName>
		</author>
		<idno type="DOI">10.4007/annals.2010.171.109</idno>
		<ptr target="http://dx.doi.org/10.4007/annals.2010.171.109" />
	</analytic>
	<monogr>
		<title level="j">Ann. of Math</title>
		<imprint>
			<biblScope unit="volume">171</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="109" to="167" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The sequence of sequencers: The history of sequencing dna</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Heather</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chain</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ygeno.2015.11.003.URLhttp:/www.sciencedirect.com/science/article/pii/S0888754315300410</idno>
		<idno>0888- 7543. doi</idno>
		<ptr target="https://doi.org/10.1016/j.ygeno.2015.11.003.URLhttp://www.sciencedirect.com/science/article/pii/S0888754315300410" />
	</analytic>
	<monogr>
		<title level="j">Genomics</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Gaussian processes for big data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hensman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Fusi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<idno>abs/1309.6835</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Scalable variational gaussian process classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hensman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<ptr target="http://dblp.uni-trier.de/db/conf/aistats/aistats2015.html#HensmanMG15" />
	</analytic>
	<monogr>
		<title level="m">AISTATS, volume 38 of JMLR Workshop and Conference Proceedings. JMLR.org</title>
		<editor>Lebanon, G. and Vishwanathan, S. V. N.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Variational fourier features for gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hensman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Durrande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Solin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Overcoming mean-field approximations in recurrent Gaussian process models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Ialongo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Der Wilk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hensman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v97/ialongo19a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<editor>Chaudhuri, K. and Salakhutdinov, R.</editor>
		<meeting>the 36th International Conference on Machine Learning<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="9" to="15" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep learning for time series classification: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ismail Fawaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Forestier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Idoumghar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Muller</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10618-019-00619-1</idno>
		<ptr target="https://doi.org/10.1007/s10618-019-00619-1" />
	</analytic>
	<monogr>
		<title level="j">Data Min. Knowl. Discov</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="917" to="963" />
			<date type="published" when="2019-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multivariate lstm-fcns for time series classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Karim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Darabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harford</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neunet.2019.04.014.URLhttp:/www.sciencedirect.com/science/article/pii/S0893608019301200</idno>
		<idno>0893- 6080. doi</idno>
		<ptr target="https://doi.org/10.1016/j.neunet.2019.04.014.URLhttp://www.sciencedirect.com/science/article/pii/S0893608019301200" />
	</analytic>
	<monogr>
		<title level="m">Neural Networks</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="237" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Generalized random shapelet forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Karlsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Papapetrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Boström</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10618-016-0473-y</idno>
		<idno>1384-5810. doi: 10. 1007/s10618-016-0473-y</idno>
		<ptr target="https://doi.org/10.1007/s10618-016-0473-y" />
	</analytic>
	<monogr>
		<title level="j">Data Min. Knowl. Discov</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1053" to="1085" />
			<date type="published" when="2016-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Multiparameter processes: an introduction to random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Khoshnevisan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep signature transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kidger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Perez Arribas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Salvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lyons</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/8574-deep-signature-transforms.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32</title>
		<editor>Wallach, H., Larochelle, H., Beygelzimer, A., d&apos;Alché-Buc, F., Fox, E., and Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3099" to="3109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Kernels for sequentially ordered data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Kiraly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Oberhauser</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v20/16-314.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">31</biblScope>
			<biblScope unit="page" from="1" to="45" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Inter-domain Gaussian processes for sparse inference using inducing features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lázaro-Gredilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Figueiras-Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1087" to="1095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<title level="m">Gaussian Processes with Signature Covariances</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Very sparse random projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Church</surname></persName>
		</author>
		<idno type="DOI">10.1145/1150402.1150436</idno>
		<imprint>
			<date type="published" when="2006-01" />
			<biblScope unit="page" from="287" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Text classification using string kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lodhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Watkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="419" to="444" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Inversion of signature for paths of bounded variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Lectures from the 34th Summer School on Probability Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lévy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<pubPlace>Saint-Flour</pubPlace>
		</imprint>
	</monogr>
	<note>Differential equations driven by rough paths. With an introduction concerning the Summer School by Jean Picard</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Scalable Gaussian process inference using variational methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G D</forename><surname>Matthews</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
		<respStmt>
			<orgName>Cambridge University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">On sparse variational methods and the Kullback-Leibler divergence between stochastic processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G D G</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hensman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="231" to="239" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Recurrent Gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L C</forename><surname>Mattos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Damianou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Forth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Barreto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<ptr target="http://inverseprobability.com/publications/mattos-recurrent16.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<editor>Larochelle, H., Kingsbury, B., and Bengio, S.</editor>
		<meeting>the International Conference on Learning Representations<address><addrLine>San Juan, PR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Umap: Uniform manifold approximation and projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Saul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Grossberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Open Source Software</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">29</biblScope>
			<biblScope unit="page">861</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Universal kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Micchelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2651" to="2667" />
			<date type="published" when="2006-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wavenet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<title level="m">A generative model for raw audio</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1162</idno>
		<ptr target="https://www.aclweb.org/anthology/D14-1162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A unifying view of sparse approximate Gaussian process regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quiñonero-Candela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1939" to="1959" />
			<date type="published" when="2005-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Gaussian processes for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>The MIT Press</publisher>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="715" to="719" />
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deep Gaussian processes with importance-weighted variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Salimbeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dutordoir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hensman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Deisenroth</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<editor>Chaudhuri, K. and Salakhutdinov, R.</editor>
		<meeting>the 36th International Conference on Machine Learning<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="9" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Exact solutions to the nonlinear dynamics of learning in deep linear neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Multivariate time series classification with weasel+muse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Leser</surname></persName>
		</author>
		<idno>abs/1711.11343</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Sparse Gaussian processes using pseudo-inputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Snelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1257" to="1264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Universality, characteristic kernels and rkhs embedding of measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Sriperumbudur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Lanckriet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2389" to="2410" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V. ; Z</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinberger</forename></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>K. Q.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
	<note>Ghahramani,</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Detecting strange attractors in turbulence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Takens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Dynamical systems and turbulence</title>
		<meeting><address><addrLine>Warwick</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1980" />
			<biblScope unit="page" from="366" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Variational learning of inducing variables in sparse gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Titsias</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v5/titsias09a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics</title>
		<editor>van Dyk, D. and Welling, M.</editor>
		<meeting>the Twelth International Conference on Artificial Intelligence and Statistics<address><addrLine>Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-04" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="16" to="18" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
