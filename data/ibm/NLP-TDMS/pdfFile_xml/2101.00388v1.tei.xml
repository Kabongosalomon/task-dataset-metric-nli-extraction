<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Robust and Domain-Adaptive Approach for Low-Resource Named Entity Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houjin</forename><surname>Yu</surname></persName>
							<email>yuhoujin@outlook.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Ling</forename><surname>Mao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zewen</forename><surname>Chi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wei</surname></persName>
							<email>weiw@hust.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<addrLine>Hu&apos;bei</addrLine>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heyan</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Robust and Domain-Adaptive Approach for Low-Resource Named Entity Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>named entity recognition</term>
					<term>low resource</term>
					<term>domain- adaptive</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, it has attracted much attention to build reliable named entity recognition (NER) systems using limited annotated data. Nearly all existing works heavily rely on domain-specific resources, such as external lexicons and knowledge bases. However, such domain-specific resources are often not available, meanwhile it's difficult and expensive to construct the resources, which has become a key obstacle to wider adoption. To tackle the problem, in this work, we propose a novel robust and domain-adaptive approach RDANER for low-resource NER, which only uses cheap and easily obtainable resources. Extensive experiments on three benchmark datasets demonstrate that our approach achieves the best performance when only using cheap and easily obtainable resources, and delivers competitive results against stateof-the-art methods which use difficultly obtainable domainspecific resources. All our code and corpora can be found on https://github.com/houking-can/RDANER.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Named entity recognition (NER) is a fundamental task which is often used as a first step in numerous natural language processing (NLP) tasks, including relation extraction <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> and knowledge graph construction <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. Most existing NER approaches such as neural network-based methods <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b6">[7]</ref>, often require a large amount of training data (annotated entity spans and types) to achieve satisfactory performance. It is clearly expensive, and sometimes impossible, to obtain a large amount of annotated data in a new domain for the NER task. Under such circumstance, low-resource NER has attracted much deserved attention recently, which aims to build reliable NER systems using limited annotated data.</p><p>Many approaches have been proposed to address lowresource NER. Early works are mainly based on handcrafted rules <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b9">[10]</ref>, but they suffer from limited performance in practice. More recently, researches on lowresource NER focus on learning information and knowledge from extra domain-specific resources to improve the NER performance. According to the required resources, they can be divided into two types: learning-based methods <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref> and domain-specific pre-training methods <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. Learning-based methods belong to supervised learning in some sense, such as transfer learning, multi-task learning and distantly-supervised learning, which leverage information and knowledge provided by external lexicons and knowledge bases. In fact, it needs extensive amount of experts effort to construct such resources. Unlike learning-based methods, domain-specific pre-training methods adopt transfer-based pre-training (unsupervised learning) on large amount of in-domain corpora to enable knowledge transfer. Domainspecific pre-training methods need less manual effort, but GPU clusters or TPUs (quite expensive) are required to speed up the training process. Both kinds of methods utilize extra knowledge, either from experts or in-domain corpora, which have been shown to be effective for low-resource NER.</p><p>Most existing methods for low-resource NER are summarized in <ref type="table" target="#tab_0">Table I</ref>. From the table, we observe that these methods highly dependent on the availability of domainspecific resources. However, these resources are often not available, meanwhile it's difficult and expensive to construct them, which has become a key obstacle to wider adoption. For example, it's easy to obtain a general domain knowledge base (like Wikipedia), but we could hardly find a publicly financial knowledge base. In fact, it requires large amounts of experts effort and money to build a domain-specific knowledge base.</p><p>To tackle the problem, we propose a novel robust and domain-adaptive approach RDANER for low-resource NER only using cheap and easily obtainable resources. Specifically, the proposed approach consists of two steps: transformer-based language model fine-tuning (LM finetuning) and bootstrapping. Here, LM refers specifically to the transformer-based language model. Firstly, we fine-tune a general domain pre-trained LM on in-domain corpora to make it fit on the target domain. Fortunately, it's easy to obtain a general domain pre-trained LM and a large amount of unannotated in-domain corpora. Then we perform a bootstrapping process, starting from an initial NER model trained on the small fully-annotated seed data, and then we use it to predict on an unannotated corpus which is further used to train the model iteratively until convergence. Our proposed approach alleviates the requirements of difficultly obtainable domain-specific resources, and builds reliable </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Required resources Limitations Transfer learning <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b14">[15]</ref> Parallel corpora, dictionary Difficult to obtain and expensive to construct Multi-task learning <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> Annotations for other tasks Distantly supervised learning <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> Knowledge bases, lexicons Domain-specific pre-training <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> Pre-trained language models Time-consuming and expensive to pre-train NER systems under low-resource conditions, which is a trade-off between effectiveness and efficiency.</p><p>To evaluate our proposed approach, we conduct lowresource experiments on three benchmark datasets in two challenging domains: computer science and biomedical. Extensive experiments demonstrate that our proposed approach is not only effective but also efficient. When only using cheap and easily obtainable resources, our approach outperforms baselines with an average improvement of 3.5 F1. Beside, the proposed approach achieves competitive performance against the state-of-the-art methods which utilize difficultly obtainable domain-specific resources.</p><p>II. RELATED WORK Named entity recognition (NER) has been studied widely for decades. Traditionally, NER is concerned with identifying general named entities, such as person, location, and organization names in unstructured text. Nowadays, researches have been extended to many specific domains, including biomedical, financial and academic. Most early NER works are based on hand-crafted rules designed by experts <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b9">[10]</ref>. Recently, neural network-based NER models <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b6">[7]</ref> have yielded great improvement over the early features-based models, meanwhile, requiring little feature engineering and domain knowledge <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b8">[9]</ref>. The biggest limitation of such neural models is that they highly dependent on large amounts of annotated data. As a result, the performance of these models degrades dramatically in low-resource settings. Lowresource NER, which aims to build reliable NER systems, has attracted much attention in recent times. Researches on low-resource NER mainly focus on utilizing extra domainspecific resources to improve the performance. There are mainly two types methods: learning-based methods and domain-specific pre-training methods. We introduce the related works of them respectively as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Learning-based Methods</head><p>Learning-based methods for low-resource NER assume some domain-specific resources are available, such as lexicons, parallel corpora and knowledge bases. These methods can be divided into three types: transfer learning (TL), multi-task learning (MTL) and distantly-supervised learning (DSL). TL has been extensively used for improving lowresource NER <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>. Most of them focus on transferring cross-domain knowledge into NER, which rely on annotation projection methods where annotations in high-resource domains are projected to the low-resource domains leveraging parallel corpora <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref> and shared representation <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b23">[24]</ref>. In fact, many TL methods are designed for general domain NER tasks, because it is easier to obtain parallel corpora or bilingual dictionaries from the general domain than from a specific domain.</p><p>Similarly, MTL utilizes knowledge from extra annotations provided by the dataset, and adopts jointly training on multiple tasks to help improve NER performance <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>. Different from TL, MTL aims at improving the performance of all the tasks instead of low-resource task only. The requirements of MTL methods for low-resource NER are manual-labeled annotations for other tasks.</p><p>Another trend for better low-resource NER performance is DSL, which has attracted many attentions to alleviate human efforts. DSL methods use domain-specific dictionaries and knowledge bases to generate large mount of weaklyannotated data. SwellShark <ref type="bibr" target="#b18">[19]</ref> and AutoNER <ref type="bibr" target="#b11">[12]</ref> use dictionary matching for named entity span detection. Reference <ref type="bibr" target="#b24">[25]</ref> combines bootstrapping and weakly-annotated data augmentation by using a reference set. Their approaches work well only when domain-specific resources are available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Domain-specific Pre-training Methods</head><p>Transformer-based pre-training have been shown to be powerful for NLP tasks <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, including low-resource NER. But most publicly pre-trained LMs (like GPT, BERT) are trained on general domain corpora, they often yields unsatisfactory results in many specific domains. A solution for this problem is domain-specific pre-training, which trains LMs on in-domain corpora. SCIBERT <ref type="bibr" target="#b12">[13]</ref> and BIOBERT <ref type="bibr" target="#b13">[14]</ref> are two domain-specific BERT variants for scientific text and biomedical text respectively, showing powerful performance in corresponding domains. Different from learningbased methods, pre-training doesn't dependent on the resources required experts effort to construct. Unfortunately, it is quite expensive to pre-train LMs from scratch, needing GPU clusters or TPUs to speed up the training processes.</p><p>Most existing works heavily rely on difficultly obtainable domain-specific resources, requiring either experts effort or high-performance hardware. However, they are often not available or cost lots of money to construct them. Without corresponding domain-specific resources, it's hard for these methods to be applied in a new domain. To solve this problem, we propose a novel robust and domain-adaptive approach RDANER for low-resource NER, which only uses cheap and easily obtainable researches. The most related works to ours are semi-supervised methods, which have been explored to further improve the accuracy by either augmenting labeled datasets or bootstrapping techniques <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b27">[28]</ref>. Reference <ref type="bibr" target="#b14">[15]</ref> uses a combination of cross-lingual transfer learning and active learning for bootstrapping low-resource entity recognizers. Reference <ref type="bibr" target="#b24">[25]</ref> combines bootstrapping and weaklyannotated data augmentation by using an external lexicon to improve NER performance. Different from <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b24">[25]</ref>, our proposed approach assumes no parallel corpora or lexicons in the target domain.</p><p>We describe our approach RDANER in detail as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. APPROACH: RDANER</head><p>For a specific low-resource NER task, we assume to have (1) a small fully-annotated seed dataset D s that has every token tagged by entity type, (2) a small in-domain corpus D c , which is used to generate weakly-annotated data, (3) a general domain pre-trained language model LM and (4) a large-scale in-domain corpus C T of the target domain T .</p><p>As noted in the introduction, RDANER consists of two processes: LM fine-tuning and the bootstrapping. <ref type="figure" target="#fig_0">Fig. 1</ref> shows the architecture of the proposed approach. We will describe each of them in follow sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. LM Fine-tuning</head><p>Given a general domain pre-trained language model LM and a domain-specific corpus C T , our goal is to make LM fit on the target domain T . In this work, LM is BERT, and we follow the work of pre-training BERT <ref type="bibr" target="#b25">[26]</ref>. Before feeding word sequences into BERT, 15% of the words in each sequence are replaced with a [MASK] token at random. BERT attempts to predict the original value of the masked words, based on the context provided by the other non-masked words in the sequence. The objective is masked language modeling (MLM) cross-entropy <ref type="bibr" target="#b25">[26]</ref>, which measures the likelihood of predictions for masked words. When finetuning has completed, we get a BERT variant, BERT T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Bootstrapping</head><p>Bootstrapping is proposed to further improve the accuracy. First, we train an initial NER model M 0 using the small seed dataset D s . We use BERT T as the word-level encoder, and a linear-chain CRF after a softmax layer. For an input sequence X = (x 1 , x 2 , ..., x n ) and a sequence of tag predictions y = (y 1 , y 2 , ..., y n ), BERT T converts x i into a fixedlength vector w i , and outputs the probability distributions h on</p><formula xml:id="formula_0">R K : h = softmax(w)<label>(1)</label></formula><p>where w = (w 1 , w 2 , ..., w n ), K is the number of tags and depends on the the number of classes and on the tagging scheme. The linear-chain CRF model defines the posterior probability of y given X to be:</p><formula xml:id="formula_1">p(y|X; A) = 1 Z(X) exp n k=0 h k (y k ; X) + n−1 k=1 A y k ,y k+1</formula><p>(2) where Z(X) is a normalization factor over all possible tags of X, h k (y k ; X) indicates the probability of taking the y k tag at position k, y 0 and y n+1 are start and end tags. A is the transfer matrix, and A y k ,y k+1 means the probability of a transition from tag states y k to y k+1 . The most likely tag sequence of X is represented as follows:</p><formula xml:id="formula_2">y * = arg max y∈R K p(y|X; A)<label>(3)</label></formula><p>A is learnt through the maximum log-likelihood estimation, which maximizes the log-likelihood function L of training set sequences in seed dataset D s :</p><formula xml:id="formula_3">L(D s , A) = M m=1 log p(y (m) |X (m) ; A)<label>(4)</label></formula><p>where M is the size of the fully-annotated seed dataset D s . Then, we use the initial NER model M 0 to assign labels on D c , and get a weakly-annotated dataset D * c . Moreover, we combine D s with D * c and get an augmented dataset. Different from training, we set thresholds θ to filter tags with low probabilities outputted by the softmax layer. Finally, we iteratively train the NER model with D * c , until the model has achieved an acceptable level of accuracy, or until the maximum number of iterations. Algorithm1 show the overall process of assigning weakly labels, where O stands for none tag.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>We conduct experiments on three benchmark datasets in two challenging domains to evaluate and compare our proposed approach with state-of-the art methods. We further investigate the effectiveness of LM fine-tuning and the bootstrapping process respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>• SciERC <ref type="bibr" target="#b3">[4]</ref> annotates entities, their relations, and coreference clusters. Four relations are annotated.</p><p>Algorithm 1: Weakly labels assignment Input: Annotated seed data (D s )  </p><formula xml:id="formula_4">Input: Unannotated corpus (D c ) Output: NER model (M K ) Train NER model M 0 on D s for i in 1 . . . K do D (i−1) * c ← Predict using M i−1 D (i−1) c ← Relabel D (i−1) * c s.t. if p(y * k ) &lt; θ then y * k ← O end Train model M i on D s + D (i−1) c end return M K</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation Metric</head><p>Following previous works <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, the microaveraged F1 score is used as the evaluation metric. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Baselines</head><p>We evaluate our proposed approach RDANER with BERT BASE , which is the strongest baseline in our work. Then, we compare RDANER against two domain-specific BERT variants to show the effectiveness of LM fine-tuning. Furthermore, we show the performance of bootstrapping process and compare RDANER with state-of-the art learning-based methods which use difficultly obtainable domain-specific resources. More details are as follows.</p><p>• BERT BASE <ref type="bibr" target="#b25">[26]</ref>   <ref type="bibr" target="#b11">[12]</ref> circumvents the requirements of extra human effort, however, it needs large high-quality dictionaries to achieve satisfactory performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Implementation Details</head><p>Our model is implemented with AllenNLP 2 . For all pretrained BERT variants, we use PyTorch version of them and the original code released publicly. All experiments are conducted on a single GTX 1080Ti GPU (12GB).</p><p>We fine-tune BERT BASE on two large-scale in-domain corpora: CS and BIO using transformers library 3 , and get two BERT variants BERT CS and BERT BIO respectively. We initialize BERT CS and BERT BIO with weights from BERT BASE , and we use the same vocabulary as BERT BASE . Different from original BERT code, we set a maximum sentence length of 120 tokens for CS corpus and 80 tokens for BIO corpus, which are in line with most sentence length in the corresponding corpus. Both of them are fine-tuned for 1 epoch, and we don't continue train the model allowing longer sentence length as BERT does.</p><p>To simulate a limited annotated data setting, we randomly select subsets of training data as seed training datasets with varying data ratios at 10%, 20%, 30%, 50% and 100%. The remaining training data simulate small in-domain corpora. Numbers of sentences of each ratio are shown in <ref type="table" target="#tab_0">Table II</ref>. The maximum number of iterations is set to 10, and we take the average of last 5 iterations as the result of each model. In order to reduce training time, we set different epochs for different seed training datasets, epochs decrease as seed increase.</p><p>To investigate the effectiveness of the proposed approach, all parameters are fine-tuned on the dev set and we do not perform extensive hyperparameter search. For all compared methods, we use the code published in their papers, and follow the same experimental settings. We add a CRF layer after the softmax layer for all BERT variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RESULTS AND DISCUSSION</head><p>In this section, we evaluate our proposed approach from the following aspects. First, we evaluate the proposed approach against BERT BASE to investigate the effectiveness of LM fine-tuning process. Second, we show the performance of the bootstrapping process and investigate the impact of the threshold θ. Furthermore, we compare our proposed approach with domain-specific BERT variants and state-of-theart learning-based methods which use difficultly obtainable domain-specific resources. <ref type="table" target="#tab_0">Table III</ref> shows the test set evaluation results of LM finetuning on the three datasets. The reported results are the mean across five different runs with different random seeds. As introduced before, we use BERT CS for Sci-ERC dataset, BERT BIO for BC5CDR and Disease datasets. We observe that LM fine-tuning consistently outperform BERT BASE on all three datasets. More specifically, LM fine-tuning gains an average improvement of 3.35 F1 on Sci-ERC, 2.02 F1 on BC5CDR and 1.24 F1 on NCBI-Disease. It indicates that LM fine-tuning on in-domain corpora is effective and domain-adaptive. We also note that there is performance gap degradation when training data increase for each dataset, which indicates that LM fine-tuning works better on less training data. This is due to the text representation has greater impact on NER models when less training data are provided. <ref type="table" target="#tab_0">Table III</ref> shows the performance of the RDANER on the three datasets. RDANER consists two processes: LM finetuning and bootstrapping. We observe that the bootstrapping process can always further improve the accuracy of NER models, which indicates the bootstrapping process is reliable. Based on LM fine-tuning, the bootstrapping process gains an average improvement of 1.29 F1. Using only 50% of training data, RDANER achieves reasonable performance, which BERT BASE needs 100% of training data to achieve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Effectiveness of LM Fine-tuning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Performance of Bootstrapping</head><p>In order to get a further understanding of the bootstrapping process, the iteratively training process with 10% of the training data is shown in <ref type="table" target="#tab_0">Table IV</ref>. Iteration 0 is the initial model trained on seed dataset, and we use the model to predict labels for unknown tokens repeatedly, which yields a jump in performance in the first iteration (Iter 1), since the predicted labels are informative. We observe that the bootstrapping process gains an average improvement of 1.86 F1 on initial models across the three datasets. The improvement achieved on Sci-ERC and BC5CDR is mainly due to the gain in recall. On the contrary, the improvement achieved on NCBI-Disease is mainly due to the gain in precision. Because there is only 1 type of entity needed to be recognized on NCBI-Disease dataset, NER on NCBI-Disease is more likely to achieve a high precision.</p><p>Impact of θ: <ref type="figure" target="#fig_2">Fig. 2</ref> shows the F1 scores curves on development sets with different thresholds of label assignment probability. This experiment is to find the optimal thresholds for different ratios of training data. Theoretically, increasing the threshold does result in a lower number of false positives, leading to higher precision. Instead, lower thresholds lead to higher recalls. For Sci-ERC, slowly increasing the threshold results in a slightly more balanced F1 measure. But the F1 scores start decreasing when precision is higher than a threshold. We observe that the less training data, the smaller the optimal thresholds, indicating recall has a greater impact on F1 under lower resources. Different from Sci-ERC, curves on BC5CDR and NCBI-Disease are more stable, increasing the threshold does little impact on F1 scores except using 10% of the training data. This demonstrates that entity annotated by models have a very high accuracy. We attribute this to the fact that BC5CDR and NCBI-Disease have fewer entity types, resulting in high accuracy of classification. <ref type="table" target="#tab_6">Table V</ref> shows the performance of domain-specific BERT variants on the three datasets. We observe that SCIBERT and BIOBERT perform well on their corresponding domains. SCIBERT achieves satisfactory F1 scores on Sci-    ERC (computer science), and BIOBERT performs best on BC5CDR and NCBI-Disease (biomedical). Unfortunately, it is very expensive to obtain such domain-specific BERT variants because the training processes are computationally expensive, which require high-performance hardware. As reported in previous works <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, it takes 1 week to train SCIBERT from scratch on a single TPU v3 with 8 cores, and 23 days to fine-tune BIOBERT on eight NVIDIA V100 (32GB) GPUs. Resources required by domain-specific BERT variants and RDANER are summarized in <ref type="table" target="#tab_0">Table VI</ref>. To facilitate comparison, GPU time is converted roughly to the time to train on a single GTX 1080Ti GPU (12GB). We note that it is very time-consuming to train a domainspecific BERT variant from scratch on a single GPU, about 4 months or more. However, RDANER costs only several hours and achieves satisfactory performance. It shows that our proposed approach obtains a trade-off between effectiveness and efficiency. Compared with domain-specific BERT variants, there is only a performance drop of 1.87 F1 scores on average for RDANER. That is totally acceptable as it is much cheaper and more efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison with Domain-specific Pre-training Methods</head><p>Surprisingly, RDANER outperforms SCIBERT on Sci-ERC when using 10%, 20%, 100% of the training data. Note that SCIBERT is the best BERT variant in computer Furthermore, we observe that SCIBERT and BIOBERT perform similarly on the three datasets. The reason is that SCIBERT is trained on a corpus 82% from biomedical domain and 18% from computer science. It indicates that introducing large-scale in-domain unannotated text for pretraining and fine-tuning can significantly improve performance. In some sense, BIOBERT is an enhanced version of BERT BIO , and the difference between them is that BIOBERT uses a much larger corpus than BERT BIO (18B tokens vs. 37.7M tokens). LM fine-tuning gains more improvement with larger in-domain corpora, and we can choose the size of in-domain corpora for LM fine-tuning on needs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison with State-of-the-art Learning-based Methods</head><p>In this section, we exploit all training data and use the perfect thresholds of label assignment probability to improve the performance of RDANER. The averaged results over 5 repetitive runs are summarized in <ref type="table" target="#tab_0">Table VII</ref>. We observe that RDANER performs reasonably well when compared to various state-of-the-art methods that use difficultly obtainable domain-specific resources, including distantly supervise learning-base (DSL) methods and multi-task learningbased (MTL) methods. Because we can't find any parallel resources in computer science and biomedical domains, we don't compare RDANER with transfer learning methods.</p><p>AutoNER and SwellShark are two DSL methods, and we observe that RDANER consistently outperforms them. We can't apply AutoNER and SwellShark on Sci-ERC dataset, because the domain-specific lexicons are unavailable. This also shows DSL methods are highly dependent on the availability of domain-specific resources. Although AutoNER and SwellShark claim that they don't use any human annotated data, they actually leverage the information of large domain-specific lexicons. For example, AutoNER uses a lexicon contains 322,882 chemical and disease entity surfaces, and SwellShark uses ontologies for generating weaklyannotated data. Such resources are often not available, leading DSL methods less adaptable. Interesting, we notice that our approach achieves close results to AutoNER and SwellShark using 20% of the training data (960 sentences) of BC5CDR, and 10% of the training data (626 sentence) of NCBI-Disease. Since it is much more challenging to construct big domain-specific lexicons, we suggest using RDANER to build reliable NER systems when domainspecific lexicons are unavailable.</p><p>DyGIE++ and SpERT are two latest state-of-the-art MTL methods, and both of them are built on top of BERT encodings and utilize extra annotations of other tasks. DyGIE++ achieves best F1 score on Sci-ERC, indicating that the extra annotations for other tasks, such as relation, event and coreference labels are helpful to improve performance. However, DyGIE++ perform mediocrely on BC5CDR and NCBI-Disease dataset, due to the lack of extra annotations of the two datasets. We note that SpERT performs better than DyGIE++ on BC5CDR and NCBI-Disease. This is partly due to SpERT only uses relation labels, lacking of extra annotations has less impact on SpERT than DyGIE++. Surprisingly, without extra annotations, RDANER achieves the second best F1 score on Sci-ERC. Besides, RDANER outperforms DyGIE++ and SpERT on BC5CDR and NCBI datasets. It demonstrates our proposed approach is not only effective but also domain-adaptive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>In this paper, we propose a novel robust and domainadaptive approach RDANER for low-resource NER only using cheap and easily obtainable resources. We conduct low-resource experiments in two challenging domains and find that: 1) RDANER is effective and efficient for lowresource NER, and it achieves competitive performance against the state-of-the-art methods which utilize difficultly obtainable domain-specific resources. 2) Beside, RDANER is domain-adaptive, which can be easily applied to a new domain.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Architecture of RDANER: LM fine-tuning + Bootstrapping.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>1 https://github.com/google-research/bert</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>F1 scores curves on development set vs. thresholds of label assignment probability.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table I :</head><label>I</label><figDesc>Summarization of the existing low-resource NER methods.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table II :</head><label>II</label><figDesc>Datasets overview.</figDesc><table><row><cell>Seed</cell><cell>Sci-ERC</cell><cell>BC5CDR</cell><cell>NCBI-Disease</cell></row><row><cell>10%</cell><cell>189</cell><cell>485</cell><cell>626</cell></row><row><cell>20%</cell><cell>359</cell><cell>960</cell><cell>1,171</cell></row><row><cell>30%</cell><cell>540</cell><cell>1,348</cell><cell>1,737</cell></row><row><cell>50%</cell><cell>895</cell><cell>2,319</cell><cell>2,943</cell></row><row><cell>100%</cell><cell>1,857</cell><cell>4,611</cell><cell>5,825</cell></row><row><cell>Domain</cell><cell>CS</cell><cell>Biomedical</cell><cell>Biomedical</cell></row><row><cell>Entity types</cell><cell>6</cell><cell>2</cell><cell>1</cell></row></table><note>• BC5CDR [1] is from the most recent BioCreative V Chemical and Disease Mention Recognition task. Two relations are annotated.• NCBI-Disease [29] focuses on Disease Name Recog- nition. Only 1 relation is annotated.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table II</head><label>II</label><figDesc>In-domain Corpora: We construct two in-domain corpora, CS and BIO, to fine-tine BERT BASE . The CS consists 40k papers from AI conference proceedings and 87k papers in AI community from arXiv. The BIO consists 200k abstracts which are randomly sampled from PubMed.</figDesc><table><row><cell>BASE</cell><cell>1 ,</cell></row><row><cell cols="2">which trained on a general domain corpora including</cell></row><row><cell>English Wikipedia and BooksCorpus.</cell><cell></cell></row></table><note>gives the statistics of the datasets used in this work. To be directly comparable with previous works, we used the official train/dev/test set splits on all datasets.B. Cheap and Easily Obtainable Resources• General Domain Pre-trained LM: We use BERT•</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table III :</head><label>III</label><figDesc>NER F1 scores of LM fine-tuning on Sci-ERC, BC5CDR and NCBI-Disease with varying training data ratios.</figDesc><table><row><cell>Dataset</cell><cell>Seed</cell><cell>BERT BASE</cell><cell>LM fine-tuning</cell><cell>RDANER</cell></row><row><cell></cell><cell>10%</cell><cell>54.07</cell><cell>57.39</cell><cell>58.83</cell></row><row><cell></cell><cell>20%</cell><cell>57.15</cell><cell>61.64</cell><cell>62.28</cell></row><row><cell>Sci-ERC</cell><cell>30%</cell><cell>60.09</cell><cell>63.33</cell><cell>64.61</cell></row><row><cell></cell><cell>50%</cell><cell>62.25</cell><cell>64.73</cell><cell>65.48</cell></row><row><cell></cell><cell>100%</cell><cell>65.24</cell><cell>68.46</cell><cell>68.96</cell></row><row><cell></cell><cell>10%</cell><cell>74.21</cell><cell>76.61</cell><cell>78.25</cell></row><row><cell></cell><cell>20%</cell><cell>78.43</cell><cell>80.51</cell><cell>82.35</cell></row><row><cell>BC5CDR</cell><cell>30%</cell><cell>79.39</cell><cell>81.70</cell><cell>83.55</cell></row><row><cell></cell><cell>50%</cell><cell>82.19</cell><cell>84.25</cell><cell>85.26</cell></row><row><cell></cell><cell>100%</cell><cell>85.61</cell><cell>86.87</cell><cell>87.38</cell></row><row><cell></cell><cell>10%</cell><cell>73.03</cell><cell>76.09</cell><cell>78.14</cell></row><row><cell></cell><cell>20%</cell><cell>79.56</cell><cell>80.20</cell><cell>83.46</cell></row><row><cell>NCBI-Disease</cell><cell>30%</cell><cell>83.79</cell><cell>84.37</cell><cell>85.46</cell></row><row><cell></cell><cell>50%</cell><cell>84.88</cell><cell>85.70</cell><cell>86.80</cell></row><row><cell></cell><cell>100%</cell><cell>86.37</cell><cell>87.49</cell><cell>87.89</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table IV :</head><label>IV</label><figDesc>Performance of iterative bootstrapping process using 10% training data. (P: Precision, R: Recall)</figDesc><table><row><cell></cell><cell></cell><cell>Sci-ERC</cell><cell></cell><cell></cell><cell>BC5CDR</cell><cell></cell><cell cols="2">NCBI-Disease</cell><cell></cell></row><row><cell>Iter</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>0</cell><cell>53.84</cell><cell>61.44</cell><cell>57.39</cell><cell>77.42</cell><cell>75.82</cell><cell>76.61</cell><cell>75.62</cell><cell>76.56</cell><cell>76.09</cell></row><row><cell>1</cell><cell>55.26</cell><cell>61.84</cell><cell>58.36</cell><cell>76.87</cell><cell>77.40</cell><cell>77.13</cell><cell>76.72</cell><cell>77.56</cell><cell>77.14</cell></row><row><cell>2</cell><cell>55.34</cell><cell>61.62</cell><cell>58.31</cell><cell>77.13</cell><cell>78.65</cell><cell>77.88</cell><cell>77.23</cell><cell>77.71</cell><cell>77.47</cell></row><row><cell>3</cell><cell>55.00</cell><cell>62.76</cell><cell>58.63</cell><cell>78.65</cell><cell>76.88</cell><cell>77.75</cell><cell>76.43</cell><cell>77.71</cell><cell>77.07</cell></row><row><cell>4</cell><cell>55.18</cell><cell>62.10</cell><cell>58.43</cell><cell>78.26</cell><cell>77.28</cell><cell>77.76</cell><cell>75.22</cell><cell>80.31</cell><cell>77.68</cell></row><row><cell>5</cell><cell>55.14</cell><cell>62.22</cell><cell>58.47</cell><cell>78.41</cell><cell>77.14</cell><cell>77.77</cell><cell>77.04</cell><cell>77.60</cell><cell>77.32</cell></row><row><cell>6</cell><cell>55.69</cell><cell>62.64</cell><cell>58.96</cell><cell>77.14</cell><cell>77.77</cell><cell>77.45</cell><cell>78.12</cell><cell>77.25</cell><cell>77.68</cell></row><row><cell>7</cell><cell>55.30</cell><cell>63.00</cell><cell>58.90</cell><cell>77.40</cell><cell>77.13</cell><cell>77.26</cell><cell>78.59</cell><cell>76.88</cell><cell>77.73</cell></row><row><cell>8</cell><cell>55.23</cell><cell>62.16</cell><cell>58.49</cell><cell>77.77</cell><cell>78.26</cell><cell>78.01</cell><cell>78.16</cell><cell>77.19</cell><cell>77.67</cell></row><row><cell>9</cell><cell>56.24</cell><cell>62.28</cell><cell>59.11</cell><cell>76.88</cell><cell>77.75</cell><cell>77.31</cell><cell>78.06</cell><cell>77.81</cell><cell>77.93</cell></row><row><cell>10</cell><cell>55.69</cell><cell>63.18</cell><cell>59.20</cell><cell>77.75</cell><cell>78.41</cell><cell>78.08</cell><cell>80.33</cell><cell>76.56</cell><cell>78.40</cell></row><row><cell></cell><cell>(a) Sci-ERC</cell><cell></cell><cell></cell><cell cols="2">(b) BC5CDR</cell><cell></cell><cell></cell><cell cols="2">(c) NCBI-Disease</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table V :</head><label>V</label><figDesc>NER F1 scores of domain-specific BERT variants on Sci-ERC, BC5CDR and NCBI-Disease with varying training data ratios.</figDesc><table><row><cell>Dataset</cell><cell>Seed</cell><cell>SCIBERT</cell><cell>BIOBERT</cell><cell>RDANER</cell></row><row><cell></cell><cell>10%</cell><cell>57.59</cell><cell>57.68</cell><cell>58.83</cell></row><row><cell></cell><cell>20%</cell><cell>62.05</cell><cell>61.84</cell><cell>62.28</cell></row><row><cell>Sci-ERC</cell><cell>30%</cell><cell>65.63</cell><cell>63.35</cell><cell>64.61</cell></row><row><cell></cell><cell>50%</cell><cell>66.95</cell><cell>64.66</cell><cell>65.48</cell></row><row><cell></cell><cell>100%</cell><cell>68.55</cell><cell>67.89</cell><cell>68.96</cell></row><row><cell></cell><cell>10%</cell><cell>81.92</cell><cell>83.56</cell><cell>78.25</cell></row><row><cell></cell><cell>20%</cell><cell>84.85</cell><cell>85.57</cell><cell>82.35</cell></row><row><cell>BC5CDR</cell><cell>30%</cell><cell>86.14</cell><cell>86.87</cell><cell>83.55</cell></row><row><cell></cell><cell>50%</cell><cell>87.50</cell><cell>87.94</cell><cell>85.26</cell></row><row><cell></cell><cell>100%</cell><cell>90.01</cell><cell>89.11</cell><cell>87.38</cell></row><row><cell></cell><cell>10%</cell><cell>80.82</cell><cell>81.08</cell><cell>78.14</cell></row><row><cell></cell><cell>20%</cell><cell>83.69</cell><cell>85.82</cell><cell>83.46</cell></row><row><cell>NCBI-Disease</cell><cell>30%</cell><cell>86.35</cell><cell>87.02</cell><cell>85.46</cell></row><row><cell></cell><cell>50%</cell><cell>87.37</cell><cell>88.53</cell><cell>86.80</cell></row><row><cell></cell><cell>100%</cell><cell>88.57</cell><cell>89.36</cell><cell>87.89</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table VI</head><label>VI</label><figDesc></figDesc><table><row><cell cols="5">: Resources required by domain-specific BERT variants</cell></row><row><cell cols="5">and RDANER. RDANERCS use BERTCS and RDANERBIO use</cell></row><row><cell cols="5">BERTBIO as the text encoder. (B: billion, M: million; d: day, h:</cell></row><row><cell>hour)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Variants</cell><cell cols="4">SCIBERT BIOBERT RDANER CS RDANER BIO</cell></row><row><cell>Tokens</cell><cell>3.2B</cell><cell>18.0B</cell><cell>17.2M</cell><cell>37.7M</cell></row><row><cell>GPU Time</cell><cell>42d</cell><cell>254d</cell><cell>2.5h</cell><cell>3.5h</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table VII :</head><label>VII</label><figDesc>NER F1 scores of state-of-the-art learning-based methods using full training data. AutoNER and SwellShark require domain-specific lexicons that are unavailable for Sci-ERC.</figDesc><table><row><cell>Models</cell><cell>Sci-ERC</cell><cell cols="2">BC5CDR NCBI-Disease</cell></row><row><cell>AutoNER [12]</cell><cell>-</cell><cell>84.80</cell><cell>75.52</cell></row><row><cell>SwellShark [19]</cell><cell>-</cell><cell>84.23</cell><cell>80.80</cell></row><row><cell>SpERT [17]</cell><cell>67.62</cell><cell>86.65</cell><cell>86.12</cell></row><row><cell>DyGIE++ [16]</cell><cell>69.80</cell><cell>85.44</cell><cell>84.11</cell></row><row><cell>RDANER</cell><cell>68.96</cell><cell>87.38</cell><cell>87.71</cell></row><row><cell cols="4">science domain currently. We attribute this to the fact</cell></row><row><cell cols="4">that Sci-ERC is a small dataset with only 1,857 annotated</cell></row><row><cell cols="4">sentences. Unfortunately, bootstrapping is prone to over-</cell></row><row><cell cols="4">fitting on small dataset. What's more, NER on Sci-ERC</cell></row><row><cell cols="4">is very challenging because it consists 6 entity types and</cell></row><row><cell cols="4">the definitions of entities are ambiguous, such as Other</cell></row><row><cell cols="4">Scientific Term and Method. Unlike Sci-ERC, BC5CDR and</cell></row><row><cell cols="4">NCBI-Disease are two larger datasets with no more than</cell></row><row><cell cols="4">2 entity types, thus NER on the two biomedical datasets</cell></row><row><cell cols="4">is much easier. BIOBERT, as the best BERT variant in</cell></row><row><cell cols="4">biomedical domain currently, almost beats all other BERT</cell></row><row><cell cols="4">variants. Although there is still a significant gap between</cell></row><row><cell cols="4">RDANER and BIOBERT, RDANER obtain satisfactory F1</cell></row><row><cell cols="3">scores costing less money and time.</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/allenai/allennlp 3 https://github.com/huggingface/transformers</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The work is supported by National Key R&amp;D Plan (No. 2018YFB1005100), NSFC (No. 61772076, 61751201 and 61602197), NSFB (No. Z181100008918002) and the funds of Beijing Advanced Innovation Center for Language Resources (No. TYZ19005). Xian-Ling Mao is the corresponding author.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Biocreative v cdr task corpus: a resource for chemical disease relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sciaky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Mattingly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Wiegers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Database</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cotype: Joint extraction of typed entities and relations with knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Abdelzaher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web</title>
		<meeting>the 26th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1015" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-ninth AAAI conference</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multitask identification of entities, relations, and coreference for scientific knowledge graph construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on EMNLP</title>
		<meeting>the 2018 Conference on EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3219" to="3232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the NAACL</title>
		<meeting>the 2016 Conference of the NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">End-to-end sequence labeling via bidirectional LSTM-CNNs-CRF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the ACL</title>
		<meeting>the 54th Annual Meeting of the ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1064" to="1074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Empower sequence labeling with task-aware neural language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Extracting opinion expressions with semi-markov conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference on EMNLP</title>
		<meeting>the 2012 Conference on EMNLP</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1335" to="1345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Joint entity recognition and disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y.</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on EMNLP</title>
		<meeting>the 2015 Conference on EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="879" to="888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dual adversarial neural transfer for lowresource named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S M</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the ACL</title>
		<meeting>the 57th Annual Meeting of the ACL</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3461" to="3471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning named entity tagger using domain-specific dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on EMNLP</title>
		<meeting>the 2018 Conference on EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2054" to="2064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">SciBERT: A pretrained language model for scientific text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on EMNLP-IJCNLP</title>
		<meeting>the 2019 Conference on EMNLP-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3615" to="3620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Biobert: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A little annotation does a lot of good: A study in bootstrapping low-resource named entity recognizers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on EMNLP-IJCNLP</title>
		<meeting>the 2019 Conference on EMNLP-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5164" to="5174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Entity, relation, and event extraction with contextualized span representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Wennberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on EMNLP-IJCNLP</title>
		<meeting>the 2019 Conference on EMNLP-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5784" to="5789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Span-based joint entity and relation extraction with transformer pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ulges</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.07755</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Knowledge-augmented language model and its application to unsupervised namedentity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the NAACL</title>
		<meeting>the 2019 Conference of the NAACL</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1142" to="1150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Swellshark: A generative model for biomedical named entity recognition without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ratner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06360</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Joint bilingual name tagging for parallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM international CIKM</title>
		<meeting>the 21st ACM international CIKM</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1727" to="1731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Transfer learning for sequence tagging with hierarchical recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06345</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On jointly recognizing and aligning bilingual named entities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-Y.</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the ACL</title>
		<meeting>the 48th Annual Meeting of the ACL</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="631" to="639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Weakly supervised crosslingual named entity recognition via effective annotation and representation projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Florian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the ACL</title>
		<meeting>the 55th Annual Meeting of the ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1470" to="1480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Low-resource name tagging learned with weakly labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on EMNLP-IJCNLP</title>
		<meeting>the 2019 Conference on EMNLP-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="261" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Biomedical named entity recognition via reference-set augmented bootstrapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mathew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fakhraei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ambite</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00282</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5754" to="5764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A unified model for cross-domain and semi-supervised named entity recognition in chinese social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ncbi disease corpus: a resource for disease name recognition and concept normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">I</forename><surname>Dogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of biomedical informatics</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
