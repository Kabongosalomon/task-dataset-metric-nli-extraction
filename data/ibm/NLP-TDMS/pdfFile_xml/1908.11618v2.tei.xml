<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">WANG: MULTI-GRAINED SPATIO-TEMPORAL MODELING FOR LIP-READING Multi-Grained Spatio-temporal Modeling for Lip-reading</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhao</forename><surname>Wang</surname></persName>
							<email>wangchenhao17@mails.ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">WANG: MULTI-GRAINED SPATIO-TEMPORAL MODELING FOR LIP-READING Multi-Grained Spatio-temporal Modeling for Lip-reading</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Lip-reading aims to recognize speech content from videos via visual analysis of speakers' lip movements. This is a challenging task due to the existence of homophemes -words which involve identical or highly similar lip movements, as well as diverse lip appearances and motion patterns among the speakers. To address these challenges, we propose a novel lip-reading model which captures not only the nuance between words but also styles of different speakers, by a multi-grained spatio-temporal modeling of the speaking process. Specifically, we first extract both frame-level fine-grained features and short-term medium-grained features by the visual front-end, which are then combined to obtain discriminative representations for words with similar phonemes. Next, a bidirectional ConvLSTM augmented with temporal attention aggregates spatio-temporal information in the entire input sequence, which is expected to be able to capture the coarse-gained patterns of each word and robust to various conditions in speaker identity, lighting conditions, and so on. By making full use of the information from different levels in a unified framework, the model is not only able to distinguish words with similar pronunciations, but also becomes robust to appearance changes. We evaluate our method on two challenging word-level lip-reading benchmarks and show the effectiveness of the proposed method, which also demonstrate the above claims.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Lip-reading, the ability to understand speech using only visual information, is an attractive but highly challenging skill. It plays a crucial role in human communication and speech understanding, as highlighted by the McGurk effect. There are several valuable applications, such as aids for hearing-impaired or speech-impaired persons, analysis of silent movies, and liveness verification in video authentication systems. It is also an important complement to the acoustic speech recognition systems, especially in noisy environments. For such reasons and also the development of deep learning which enables efficient feature learning and extraction, lip-reading has been receiving more and more attention in recent years.</p><p>A typical lip-reading framework consists of two steps: analyzing the motion information in the image sequence, and converting that information into words or sentences. One common challenge in this process is various imaging conditions, such as poor lighting, strong shadows, motion blur, low resolution, foreshortening, etc. More importantly, there is a fundamental limitation on performance due to homophemes. These are many words or phrases that sound different, but involve the same or very similar movements of the speaker's lips. c 2019. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms. arXiv:1908.11618v2 [cs.CV] 2 Sep 2019 For example, the phonemes "p", "b" in English are visually identical; while the words "pack" and "back", are homophemes that can hardly be distinguished through lip-reading when there is no more context information.</p><p>Motivated by these problems, we hope to build a model which utilizes both fine-grained and coarse-grained spatio-temporal features to enhance the model's discriminative power and robustness. Specifically , we propose a multi-grained spatio-temporal network for lipreading. The front-end network uses a spatio-temporal ConvNet and a spatial-only ConvNet in parallel, which extract medium-grained short-term and fine-grained, per-time-step features respectively. In order to fuse these features more effectively, we introduce a spatial attention mask to learn an adaptive, position-wise feature fusion strategy. A two-layer bidirectional ConvLSTM augmented with (forward) input attention is used as the back-end to generate the coarse-grained long-term spatio-temporal features.</p><p>In summary, we make three contributions. Firstly, we propose a novel multi-grained spatio-temporal network to solve the lip-reading problem. Secondly, instead of simple concatenation, we fuse the information of different granularity with a learnable spatial attention mechanism. Finally, we apply ConvLSTM to the lip-reading task for the first time. We report the word classification results on two challenging lip-reading datasets, LRW and LRW-1000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we briefly summarize previous related work about lip-reading and ConvL-STMs. Lip reading. Research on lip-reading has a long history. Most early methods are based on carefully hand-engineered features. A classical type of methods is to use Hidden Markov Models (HMM) to model the temporal structure within the extracted frame-wise features <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b2">4,</ref><ref type="bibr" target="#b12">14]</ref>. Other well-known features include the Discrete Cosine Transform (DCT) <ref type="bibr" target="#b11">[13]</ref>, Active Appearance Model (AAM), Motion History Image (MHI) <ref type="bibr" target="#b6">[8]</ref>, Local Binary Pattern (LBP) <ref type="bibr" target="#b24">[26]</ref>, and vertical optical flow based features <ref type="bibr" target="#b13">[15]</ref>. With the rapid development of deep learning technologies and the appearance of large-scale lip-reading databases <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b21">23]</ref>, researchers have started to use convolutional neural networks to extract the features of each frame and also use recurrent units for holistic temporal modeling <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">11,</ref><ref type="bibr" target="#b18">20]</ref>. In 2016, <ref type="bibr" target="#b3">[5]</ref> proposed the first large-scale word-level lip-reading database together with several end-toend lip-reading models. Since then, more and more work perform end-to-end recognition with the help of deep neural networks (DNN).</p><p>According to the design of the front-end network, these modern lip-reading methods can be roughly divided into three categories: (a) fully 2D CNN based, which build on the success of 2D ConvNets in image representation learning; (b) fully 3D CNN based, which is inspired by the success of 3D ConvNets in action recognition, among which LipNet[2] is a representative work that yields good results on the GRID audiovisual corpus; and (c) mixture of 2D and 3D convolutions, which inherit the merits of both (a) and (b) by capturing the temporal dynamics in a sequence and extracting discriminative features in the spatial domain simultaneously. Recently, methods of type (c) have become dominant in lip-reading due to its excellent performance. For example, in 2018, <ref type="bibr" target="#b10">[12]</ref> attained 83.0% word accuracy on the LRW dataset based on the type (c) architecture, achieving a new state-of-the-art result. However, the above method simply stacks 3D and 2D convolutional layers, which may not fully unleash the power of the two components. Our model proposes a new approach to take the respective advantages of 3D and 2D ConvNets, by using them as two separate branches  <ref type="figure">Figure 1</ref>: The architecture of the proposed framework, which consists of a spatio-temporal convolution module followed by a two-branch structure and a two-layer Bi-ConvLSTM with forward input attention. Finally, a fully-connected layer is used to obtain the prediction results.</p><p>and fusing the features adaptively, similar to the popular two-stream architecture for action recognition <ref type="bibr" target="#b15">[17]</ref>. LSTM and ConvLSTM. For general-purpose sequence modeling, LSTM <ref type="bibr" target="#b7">[9]</ref> as a special RNN structure has been proven stable and powerful in modeling long-range dependencies. LSTMs often lead to better performance where temporal modeling capacity is required, and are thus widely used in NLP, video prediction, lip-reading, and so on. A common practice of using LSTMs in video recognition is to employ a fully-connected layer before the LSTM. Although this FC-LSTM layer has been proven powerful for temporal modeling, it loses too much information about the spatial correlation in the data. To address this, Shi et al. proposed ConvLSTM <ref type="bibr" target="#b14">[16]</ref>, which is based on the LSTM design but considers both temporal and spatial correlation in a video sequence with additional convolution operations, effectively fusing temporal and spatial features. It has been successfully applied to action recognition <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b19">21]</ref>, gesture recognition <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b25">27]</ref> and other fields <ref type="bibr" target="#b17">[19]</ref>. Additionally, a new spatio-temporal LSTM unit <ref type="bibr" target="#b20">[22]</ref> is recently designed to memorize both temporal and spatial representations, obtaining better performance than the conventional LSTM.</p><p>In this paper, we introduce ConvLSTM to the lip-reading task for the first time. When aggregating information from the whole lip sequence, its ability to capture both long and short term temporal dependencies while considering the spatial relationships in feature maps makes it ideal for accommodating to differences across speakers. We also augment the Con-vLSTM with an attention mechanism on the inputs, which will be described in detail in Sec. 3.3.1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Multi-Grained Spatio-temporal Modeling For</head><p>Lip-reading Given a sequence of the mouth region corresponding to an utterance, our goal is to capture both the fine-grained patterns that can distinguish one word from another, and the coarsegrained patterns describing mouth shapes and motion information that are ideally invariant to the varied styles of different speakers. As mentioned earlier, simply cascading 2D and 3D convolution may not be optimal for lip-reading, since some movements may be very weak and thereby lost during pooling. Therefore, we split the learning process into three sub-networks that complement each other. In this section, we present the proposed multi-grained spatio-temporal framework which learns the latent spatio-temporal patterns of different words from three different spatio-temporal scales for the lip-reading task. As shown in <ref type="figure">Fig. 1</ref>, the network consists of a 2D ResNet-34 based fine-grained module, a 52-layer DenseNet-3D medium-grained module, and a coarse-grained module that adaptively fuses and aggregates the features from these two modules. By jointly learning the latent patterns at multiple spatio-temporal scales and efficiently fusing these information, we achieve much better performance. We now give a detailed description of the architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Fine-grained Module</head><p>Words with similar mouth movements are fairly ubiquitous. However, when we compare the sequences side by side and examine each time-step, very often we can still observe slight differences in appearance. This observation leads to the idea that enhancing spatial representations alone to some extent may improve the discriminative power of the model. As an effective tool to capture the salient features in images, 2D convolutional operations have been proven successful in several related tasks, such as image recognition, object detection, segmentation, and so on. We introduce cascaded 2D convolutional operations here to extract the salient features in each frame. Different from the traditional role of 2D convolutional operation in other methods, the 2D convolutions introduced here should not merely function as a feature extractor, but highlight salient appearance cues in each frame, which will eventually help enhance the fine-grained patterns for subtle differences among words. In our model, the 2D ConvNet is a 34-layer ResNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Medium-grained Module</head><p>3D convolution have become widely adopted in video recognition and proven capable of capturing short-term spatio-temporal patterns. They are expected to be more robust than using 2D convolutions which produce frame-wise features because they account for motion information. Moreover, while there are words with subtle differences that require finegrained information, most words are still able to be distinguished through the ways they are pronounced, albeit somewhat speaker-dependent. This requires the model to be capable of modeling medium-grained, short-term dynamics, which is a job suitable for 3D convolutions. In our model, the medium-grained 3D ConvNet is a 52-layer 3D-DenseNet <ref type="bibr" target="#b21">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Coarse-grained Module</head><p>The coarse-grained module begins by fusing the features from the previous two modules. Different from most previous methods which directly cascade 2D and 3D convolutions, we introduce an attention mechanism to combine the fine-grained features and the mediumgrained features into a primary representation. As shown in <ref type="figure">Fig. 1</ref>, the attention mask is implemented with an 1 × 1 × 1 convolution, which adaptively adjusts the fusion weights at each spatial location. This spatial attention mask and the final fused features F are obtained by</p><formula xml:id="formula_0">S = 2DCNN(X), T = 3DCNN(X), mask = σ (WT), F = T mask + S (1 − mask).<label>(1)</label></formula><p>where X are the input feature maps, S, T are the respective outputs of the two branches, W is a learned parameter, σ is the sigmoid function, and denotes element-wise multiplication.</p><p>Every person has his or her own speaking style and habits, such as nodding or turning his or her head while speaking. Meanwhile, owing to the appearance factors such as lighting conditions, speaker's pose, make-up, accent, age and so on, the image sequences of even the same word would have several different styles. Considering the diversity of the appearance factors, a robust lip-reading model has to model the global latent patterns in the sequence in a high-level to highlight the representative patterns and cover the slight style-variations in the sequence. FC-LSTMs are capable of modeling long-range temporal dependencies and have a powerful gating mechanism. But the major drawback of FC-LSTM in handling spatiotemporal data is its usage of full connections in input-to-state and state-to-state transitions in which no spatial correlation is encoded. To overcome this problem, we use a two-layer bidirectional ConvLSTM module augmented with forward input attention which proceeds to model the global latent patterns in the whole sequence based on the fused initial representations. It is able to cover the various styles and speech modes in the speaking process, which will be demonstrated in the experiment section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Bi-ConvLSTM with Forward Input Attention</head><p>Compared with the conventional LSTM, ConvLSTM proposed in <ref type="bibr" target="#b14">[16]</ref>, as a convolutional counterpart of conventional fully connected LSTM, introduces the convolution operation into input-to-state and state-to-state transitions. ConvLSTM is capable of modeling 2D spatiotemporal image sequences by explicitly encoding their 2D spatial structures into the temporal domain. ConvLSTM models temporal dependency while preserving spatial information.</p><p>Thus it has been widely applied for many spatio-temporal tasks. Similar to FC-LSTM, a ConvLSTM unit consists of a memory cell c t , an input gate i t , an output gate o t and a forget gate f t . The main equations of ConvLSTM are as follows:</p><formula xml:id="formula_1">i t = σ (W xi * X t + W hi * H t−1 + W ci • C t−1 + b i ) f t = σ W x f * X t + W h f * H t−1 + W c f • C t−1 + b f C t = f t • C t−1 + i t • tanh (W xc * X t + W hc * H t−1 + b c ) o t = σ (W xo * X t + W ho * H t−1 + W co • C t + b o ) H t = o t • tanh (C t )<label>(2)</label></formula><p>where ' * ' denotes the convolution operator and '•' denotes the Hadamard product.</p><p>However, the structures of existing RNN neurons mainly focus on controlling the contributions of current and historical information but do not explore the difference in importance among different time-steps <ref type="bibr" target="#b23">[25]</ref>. So we introduce an attention mechanism to the forward direction of the bidirectional ConvLSTM, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. The input attention can determine the relative importance of different frames and assign a suitable weight to each timestep. This augmented Bi-ConvLSTM can not only learn spatial temporal features but also select important frames. We only use attention on the inputs to Bi-ConvLSTM's forward direction:</p><formula xml:id="formula_2">a t = σ (W Xa X f;t + W ha h f;t−1 )<label>(3)</label></formula><p>where the current (forward) input X f;t and the previous hidden state h f;t−1 are used to determine the levels of importance of each frame of the forward input X f;t . The attention response modulates the forward input and computes</p><formula xml:id="formula_3">X f;t = a t • X f;t<label>(4)</label></formula><p>The recursive computations of activations of the other units in the RNN block are then based on the attention-weighted input X f;t , instead of the original input X f;t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we present the results of our experiments on the word-level LRW and LRW-1000 datasets. We give a brief description to the two datasets and our implementation, and finally a detailed analysis of our experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Lip Reading in the Wild (LRW) <ref type="bibr" target="#b3">[5]</ref>. The LRW database consists of short segments (1.16 seconds) from BBC programs, mainly news and talk shows. It is a very challenging dataset since it contains more than 1000 speakers and large variations in head pose and illumination. For each target word, it has a training set of 1000 segments, a validation and an evaluation set of 50 segments each. The total duration of this corpus is 173 hours. The corpus with 500 words is also much larger than previous lip-reading databases used for word recognition.</p><p>LRW-1000 <ref type="bibr" target="#b21">[23]</ref>. LRW-1000 is a challenging Mandarin lip-reading dataset due to its large variations in scale, resolution, background clutter, and speaker attributes. The speakers are mostly interviewers, broadcasters, program guests, and so on. The dataset consists of 1000 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>Our models are implemented with PyTorch and trained on servers with three NVIDIA Titan X GPUs, each with 12GB memory. In our experiments, for the LRW dataset, the mouth region of interests (ROIs) are already centered, and a fixed bounding box of 96 by 96 is used for all videos. All images are converted to grayscale, and then cropped to 88 × 88.</p><p>As an effective data augmentation step, we also randomly flip all the frames in the same sequence horizontally. For the two-branch models, we first train each individual branch to convergence, and then fine-tune the model end-to-end. We use the Adam optimizer with an initial learning rate of 0.0001 and a momentum of 0.9. During the fine-tuning with RGB LRW-1000, the maximum number of frames is set to 30. The first convolutional layer has kernel of size 64 × 5 × 7 × 7 (channels / time / height / width), while max pooling has a kernel of size 1 × 3 × 3. We then reshape the feature map to 24 × 24. In our model, the two branches are constructed by a 34-layer ResNet and a 52-layer 3D-DenseNet <ref type="bibr" target="#b21">[23]</ref> respectively. We use a 1 × 1 × 1 3D convolution to reduce the dimensionality. Then 512 × 29 × 3 × 3 fusion feature is fed to a two-layer Bi-ConvLSTM with forward input attention. The Bi-ConvLSTM has kernel size 3 × 3. The output layer is a fully connection layer to obtain prediction results. We average the framewise prediction for the final results. The two blocks of layers transform the tensors as 88 × 88  <ref type="bibr" target="#b10">[12]</ref> (the results are our reproduction). 'DenseNet-3D' uses a 52-layer DenseNet-3D front-end proposed in <ref type="bibr" target="#b21">[23]</ref>. </p><formula xml:id="formula_4">→ 22 × 22 −−−−→ upsample 24 × 24 → 12 × 12 → 6 × 6 → 3 × 3.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>Performance estimates are expressed in terms of word-level error rate on LRW dataset and LRW-1000 dataset, respectively. We set up a few control experiments including only 2D CNN branch, only 3DCNN branch, two-branch / Bi-GRU, two-branch / Bi-ConvLSTM and our model. Results on two datasets are provided in <ref type="table" target="#tab_0">Table 1</ref>. On the LRW dataset, our model shows marginally better results which we believe is because the model can learn the multigrained spatio-temporal features.</p><p>From <ref type="table" target="#tab_0">Table 1</ref> we can find that the ResNet-34 model and the DenseNet-3D model perform equally well on the LRW dataset, achieving an accuracy of 81.70%. However, the recognition results of these two structures are different. In LRW-1000, the ResNet-34 / Bi-GRU is better than 3D-DenseNet / Bi-GRU. The possible reason for this is that 2D CNN can better capture the fine-grained features in each time-step to discriminate words. Compared with the baseline two-branch models, we introduce the soft attention based fusion mechanism to learn an adaptive weight to keep the most discriminative information from the two branches and indeed to lead to more powerful spatio-temporal features. On LRW dataset, compared with the results of our ResNet-34 + Bi-GRU baseline, there is an increase of 1.28%. But the two-branch performance is higher than the DenseNet-3D / Bi-GRU results. The attention mask is shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. From these figures, we can find that the attention mask can learn the weights well. It can pay close attention to the lip area to make the learning process automatically modify the fusion weights to generate the early-stage representation. Therefore the two-branch / Bi-GRU architecture can obtain more robust results.</p><p>For the LRW database, compared with two-branch / Bi-GRU and two-branch / Bi-ConvLSTM, it is clear from the results that bidirectional ConvLSTM modules are able to significantly improve the performance over two-branch / Bi-GRU. This structure not only indicates that temporal information has been learned but also highlights the importance of spatial information for the lip-reading task.</p><p>Clips from the LRW dataset include context and may introduce redundant information to the network. From <ref type="table" target="#tab_2">Table 2</ref> we can find that the Bi-ConvLSTM with forward input attention works better, likely because it can focus on controlling the contributions of current and historical different importance levels on different frames and identify the most important ones. <ref type="table" target="#tab_2">Table 2</ref> shows the effectiveness of our forward input attention Bi-ConvLSTM. Therefore our model outperforms the two-branch / Bi-ConvLSTM. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Accuracy Chung18 <ref type="bibr" target="#b4">[6]</ref> 71.50% Chung17 <ref type="bibr" target="#b5">[7]</ref> 76.20% Petridis18 (end-to-end) <ref type="bibr" target="#b10">[12]</ref> 82.00% Petridis18 (reproduced) 81.70% Stafylakis17 <ref type="bibr" target="#b16">[18]</ref> 83.00% Proposed Model 83.34%</p><p>(b) the state-of-the-art on LRW-1000</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Accuracy LSTM-5 <ref type="bibr" target="#b21">[23]</ref> 25.76% D3D <ref type="bibr" target="#b21">[23]</ref> 34.76% 3D+2D <ref type="bibr" target="#b21">[23]</ref> 38.19% 3D+2D (reproduced) 33.78% Proposed Model 36.91% <ref type="table" target="#tab_2">Table 2</ref> summarizes the performance of state-of-the-art networks on LRW and LRW-1000. Our network has an absolute increase of 1.6% over our reproduction of the baseline ResNet-34 model in <ref type="bibr" target="#b10">[12]</ref> on LRW database. From the above results we see that the mixed 3D-2D architecture still shows very strong performance. However the results also shows the importance of fine-grained spatio-temporal features in the lip-reading task. The results also confirm that it is reasonable to use the attention mask to merge the fine-grained and mediumgrained features, and replace FC-LSTM with ConvLSTM. Our model takes full advantage of the 3D ConvNet, the 2D ConvNet and the ConvLSTM. The proposed attention-augmented variant of ConvLSTM further enhances its ability for spatio-temporal feature fusion. The forward input attention in Bi-ConvLSTM not only learns spatial and temporal features but also explore the different importance levels of different frames. But we reproduced the 3D+2D model in the database of the accuracy is lower in the <ref type="bibr" target="#b21">[23]</ref>. This reason may be that we do not use the fully-connected layers in the model, and we also do not use three-stage training. Therefore, the best recognition results can be obtained by taking full use of the intrinsic advantages of the different networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with the state-of-the-art</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have proposed a novel two-branch model with forward input attention augmented Bi-ConvLSTM for lip-reading. The model utilizes both 2D and 3D ConvNets to extract both frame-wise spatial features and short-term spatio-temporal features, and then fuses the features with an adaptive mask to obtain strong, multi-grained features. Finally, we use a Bi-ConvLSTM augmented with forward input attention to model long-term spatio-temporal information of the sequence. Using this architecture, we demonstrate state-of-the-art performance on two challenging lip-reading datasets. We believe the model has great potential beyond visual speech recognition. How to better utilize spatial information in temporal sequence modeling to obtain more fine-grained spatio-temporal features is also a worthwhile research. In the future, we will continue to simplify the front-end and extract multi-grained features with a more lightweight structure.</p><p>IEEE International Conference on Acoustics, 2016.</p><p>[2] Yannis M Assael, Brendan Shillingford, Shimon Whiteson, and Nando de Freitas. Lipnet: End-to-end sentence-level lipreading. arXiv preprint arXiv:1611.01599, 2016.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>(a) The forward input attention augmented Bi-ConvLSTM. The attentionaugmented ConvLSTM layer CF-ATT processes the inputs in the forward direction attentively, while the plain ConvLSTM layer CLSTMb processes information in reversed order. (b) The ConvLSTM forward input attention unit, where , as before, denotes element-wise multiplication.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The attention mask automatically adjusts the position-specific fusion weights and generates an initial representation. For clarity, only the frames corresponding to the target word are shown. word classes and has 718, 018 samples, totaling 57 hours. The minimum and maximum length of the samples are about 0.01 seconds and 2.25 seconds respectively, with an average of about 0.3 seconds for each sample.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Classification accuracy of the two-branch network on the LRW database and LRW-1000 database. 'ResNet-34' uses the 34-layer ResNet frontend proposed in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison with the state-of-the-art on the test set of LRW and LRW-1000. '(reproduced)' denotes the result of our reproduction.(a) the state-of-the-art on the LRW</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Improved speaker independent lipreading using speaker adaptive training and deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibrahim</forename><surname>Almajai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Lan</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The natural statistics of audiovisual speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandramouli</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Trubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Stillittano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Caplier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asif A</forename><surname>Ghazanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">1000436</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Lipreading from color video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Greg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenq-Neng</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1192" to="1195" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Lip reading in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Son</forename><surname>Joon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="87" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learning to lip read words by watching videos. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Son</forename><surname>Joon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Lip reading sentences in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3444" to="3453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Toward movement-invariant automatic lip-reading and speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Duchnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Hunke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietrich</forename><surname>Busching</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1995" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="109" to="112" />
		</imprint>
	</monogr>
	<note>International Conference on</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Videolstm convolves, attends and flows for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirill</forename><surname>Gavrilyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihir</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">166</biblScope>
			<biblScope unit="page" from="41" to="50" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Audio-visual speech recognition using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Noda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuhiro</forename><surname>Nakadai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hiroshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tetsuya</forename><surname>Okuno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ogata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="722" to="737" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">End-to-end audiovisual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stavros</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Themos</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingehuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feipeng</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6548" to="6552" />
		</imprint>
	</monogr>
	<note>Georgios Tzimiropoulos, and Maja Pantic</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An image transform approach for hmm based automatic lipreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerasimos</forename><surname>Potamianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><forename type="middle">Peter</forename><surname>Graf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Cosatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP 98. Proceedings. 1998 International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="173" to="177" />
		</imprint>
	</monogr>
	<note>Image Processing</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ashutosh Garg, and Andrew W Senior. Recent advances in the automatic recognition of audiovisual speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerasimos</forename><surname>Potamianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chalapathy</forename><surname>Neti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Gravier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1306" to="1326" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Lip reading using optical flow and support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ayaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shaikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dinesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayavardhana</forename><surname>Mz Che Azemin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gubbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image and Signal Processing (CISP), 2010 3rd International Congress on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="327" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhourong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit</forename><forename type="middle">Yan</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><forename type="middle">Kin</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><forename type="middle">Chun</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<idno type="arXiv">arXiv:1703.04105</idno>
		<title level="m">Themos Stafylakis and Georgios Tzimiropoulos. Combining residual networks with lstms for lipreading</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Convolutional long short-term memory networks for recognizing first person interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swathikiran</forename><surname>Sudhakaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oswald</forename><surname>Lanz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2339" to="2346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improving lip-reading performance for robust audiovisual speech recognition using dnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwanchiva</forename><surname>Thangthai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry-John</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Theobald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AVSP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="127" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Human action recognition by learning spatio-temporal features with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiying</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqin</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaji</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="17913" to="17922" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Predrnn: Recurrent neural networks for predictive learning using spatiotemporal lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S Yu</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="879" to="888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Lrw-1000: A naturally-distributed large-scale benchmark for lip reading in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingmin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyu</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG 2019)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features using 3dcnn and convolutional lstm for gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiyi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afaq</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bennamoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3120" to="3128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adding attentiveness to the neurons in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianru</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanning</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="135" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Lipreading with local spatiotemporal descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoying</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matti</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1254" to="1265" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multimodal gesture recognition using 3-d convolution and convolutional lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiyi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="4517" to="4524" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
