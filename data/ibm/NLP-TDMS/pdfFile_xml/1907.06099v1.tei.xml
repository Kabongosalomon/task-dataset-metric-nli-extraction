<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Task Recurrent Convolutional Network with Correlation Loss for Surgical Video Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="201913-04-30">April 30, 2019 13 Jul 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueming</forename><surname>Jin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Imsight Medical Technology</orgName>
								<orgName type="institution" key="instit2">Co, Ltd</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Qin</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Centre for Smart Health</orgName>
								<orgName type="department" key="dep2">School of Nursing</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Task Recurrent Convolutional Network with Correlation Loss for Surgical Video Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="201913-04-30">April 30, 2019 13 Jul 2019</date>
						</imprint>
					</monogr>
					<note type="submission">Preprint submitted to Medical Image Analysis</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T15:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Surgical video analysis</term>
					<term>multi-task learning</term>
					<term>correlation loss</term>
					<term>deep learning * Corresponding author: qdou@csecuhkeduhk (Qi Dou)</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Surgical tool presence detection and surgical phase recognition are two fundamental yet challenging tasks in surgical video analysis and also very essential components in various applications in modern operating rooms. While these two analysis tasks are highly correlated in clinical practice as the surgical process is well-defined, most previous methods tackled them separately, without making full use of their relatedness. In this paper, we present a novel method by developing a multi-task recurrent convolutional network with correlation loss (MTRCNet-CL) to exploit their relatedness to simultaneously boost the performance of both tasks. Specifically, our proposed MTRCNet-CL model has an end-to-end architecture with two branches, which share earlier feature encoders to extract general visual features while holding respective higher layers targeting for specific tasks. Given that temporal information is crucial for phase recognition, long-short term memory (LSTM) is explored to model the sequential dependencies in the phase recognition branch. More importantly, a novel and effective correlation loss is designed to model the relatedness between tool presence and phase identification of each video frame, by minimizing the divergence of predictions from the two branches. Mutually leveraging both lowlevel feature sharing and high-level prediction correlating, our MTRCNet-CL method can encourage the interactions between the two tasks to a large extent, and hence can bring about benefits to each other. Extensive experiments on a large surgical video dataset (Cholec80) demonstrate outstanding performance of our proposed method, consistently exceeding the state-of-the-art methods by a large margin (e.g., 89.1% v.s. 81.0% for the mAP in tool presence detection and 87.4% v.s. 84.5% for F1 score in phase recognition). The code can be found on our project website.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Surgical tool presence detection and surgical phase recognition are two fundamental yet challenging tasks in surgical video analysis and also very essential components in various applications in modern operating rooms. While these two analysis tasks are highly correlated in clinical practice as the surgical process is well-defined, most previous methods tackled them separately, without making full use of their relatedness. In this paper, we present a novel method by developing a multi-task recurrent convolutional network with correlation loss (MTRCNet-CL) to exploit their relatedness to simultaneously boost the performance of both tasks. Specifically, our proposed MTRCNet-CL model has an end-to-end architecture with two branches, which share earlier feature encoders to extract general visual features while holding respective higher layers targeting for specific tasks. Given that temporal information is crucial for phase recognition, long-short term memory (LSTM) is explored to model the sequential dependencies in the phase recognition branch. More importantly, a novel and effective correlation loss is designed to model the relatedness between tool presence and phase identification of each video frame, by minimizing the divergence of predictions from the two branches. Mutually leveraging both lowlevel feature sharing and high-level prediction correlating, our MTRCNet-CL method can encourage the interactions between the two tasks to a large extent, and hence can bring about benefits to each other. Extensive experiments on a large surgical video dataset (Cholec80) demonstrate outstanding performance of our proposed method, consistently exceeding the state-of-the-art methods by a large margin (e.g., 89.1% v.s. 81.0% for the mAP in tool presence detection and 87.4% v.s. 84.5% for F1 score in phase recognition). The code can be found on our project website.</p><p>Keywords: Surgical video analysis, multi-task learning, correlation loss, deep learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the advancements in medicine and information technologies, the operating room (OR) has undergone tremendous transformations evolving into a highly complicated and technologically rich environment <ref type="bibr" target="#b12">(Cleary &amp; Kinsella (2005)</ref>; <ref type="bibr" target="#b22">James et al. (2007)</ref>; <ref type="bibr" target="#b28">Lalys &amp; Jannin (2014)</ref>; <ref type="bibr" target="#b6">Bouget et al. (2017)</ref>). These transformations allow the execution of more complex procedures and also increase the amount of information available in modern OR. To better tackle this new OR scenario, the context-aware systems (CAS) have gradually been developed to provide detailed comprehension of rich information and contextual support to the clinicians <ref type="bibr" target="#b9">(Bricon-Souf &amp; Newman (2007)</ref>; <ref type="bibr" target="#b13">Dergachyova et al. (2016)</ref>). With interpreting the operation procedure and tool usage, automated surgical phase recognition and tool presence detection serve as the primary functions in CAS and such accurate systems are expected to be highly demanded <ref type="bibr" target="#b35">(Padoy et al. (2012)</ref>; <ref type="bibr" target="#b28">Lalys &amp; Jannin (2014)</ref>; <ref type="bibr" target="#b47">Wesierski &amp; Jezierska (2018)</ref>).</p><p>Specifically, automatically recognizing the surgical phase and tool enables CAS to assist clinicians during two periods: intra-operation and post-operation. The intra-operative recognition is able to generate real-time warning for clinicians by detecting rare cases and unexpected variations <ref type="bibr" target="#b7">(Bouget et al. (2015)</ref>), and to support decision making of junior surgeons through timely communication within surgical team <ref type="bibr" target="#b37">(Quellec et al. (2014</ref><ref type="bibr" target="#b38">(Quellec et al. ( , 2015</ref>; <ref type="bibr" target="#b18">Forestier et al. (2015)</ref>). The online recognition can also help to improve OR resource management. By knowing which surgical workflow is currently occurring and which tool is utilized, the completion time of surgery can be estimated. Therefore, it can facilitate relevant clinical staff to prepare the following patient in advance, resulting in minimal patient waiting time and maximal OR throughput ; <ref type="bibr" target="#b6">Bouget et al. (2017)</ref>). In addition, the post-operative recognition can enhance the efficiency of surgical report documentation and video database indexing, which are currently tedious and time-consuming manual jobs. The indexed record of surgical procedure can further facilitate the surgeon training, review and skill assessment <ref type="bibr" target="#b51">(Zappella et al. (2013)</ref>; <ref type="bibr" target="#b0">Ahmidi et al. (2017)</ref>; <ref type="bibr" target="#b41">Sarikaya et al. (2017)</ref>). The fully annotated database can also be utilized to generate the statistical information, which is beneficial for the surgical workflow optimization <ref type="bibr" target="#b4">(Bhatia et al. (2007)</ref>; <ref type="bibr" target="#b47">Wesierski &amp; Jezierska (2018)</ref>).</p><p>However, developing automated methods to recognize tool presence and surgical phase from surgical videos is very challenging. First, there is a large variety of surgical tools with some abnormal cases, such as partial appearances and overlap of multiple tools. Second, complicated surgical scenes lead to limited inter-phase variance while high intra-phase variance. Lastly, observed surgical scenes are often blurred due to the camera motion and gas generated by tools, and even completely occlude when blood stains the camera lens. Extra noise and artifacts introduced by consequent lens cleaning process make the recognition tasks even harder. To meet these challenges, early methods relied on handcrafted features such as gradient magnitude <ref type="bibr" target="#b5">(Blum et al. (2010)</ref>), combinative descriptors <ref type="bibr" target="#b29">(Lalys et al. (2012)</ref>), and intensity values <ref type="bibr" target="#b51">(Zappella et al. (2013)</ref>). However, the empirical design of these low-level features heavily de-  <ref type="formula" target="#formula_0">(2018)</ref>). Unfortunately, most existing deep learning based methods addressed the tool and phase recognition tasks independently, without considering the intrinsic association between them. According to the regulation of surgery procedure, surgeons are requested to perform specified operations with corresponding sets of instruments for different surgery phases. Therefore, there exists a high correlation between the surgical phase and tool usage. Taking the cholecystectomy procedure as an example (see <ref type="figure" target="#fig_0">Figure 1</ref>), hooks are often used to perform the dissection operations; clipper and scissors are required in clipping and cutting stage. In fact, many previous works directly employed binary instrument usage signals to perform phase recognition, which manifested the benefit of tool information to phase recognition <ref type="bibr" target="#b35">(Padoy et al. (2012);</ref><ref type="bibr" target="#b18">Forestier et al. (2015)</ref>). Recently, <ref type="bibr" target="#b45">Twinanda et al. (2017)</ref> implemented a multi-task framework with shared early layers and incorporated tool information in the feature learning process, which firstly achieved joint tool and phase recognitions. The promising performance demonstrates that effectively leveraging such relatedness plays an essential role in improving both tasks, i.e., tool presence detection and phase recognition.</p><p>The correlation between the multiple tasks is often quite complicated. For example, in surgical videos, the same tool may present in different phases, while an operation phase may involve a variety of instrument combinations. To this end, the shortcomings existing in the aforementioned approaches may fail to precisely capture the correlation. For example, the method proposed by <ref type="bibr" target="#b45">Twinanda et al. (2017)</ref> uses hidden Markov model (HMM) to enforce the temporal constraints on the phase prediction, instead of introducing sequential information in the network training procedure, which plays a key factor in tackling videobased tasks. Therefore, how to precisely capture and sufficiently leverage the close yet subtle correlations between these two tasks remains a problem to be further investigated.</p><p>In this paper, we present a novel method, i.e., multi-task recurrent convolutional network with correlation loss (MTRCNet-CL), to simultaneously tackle tool presence detection and phase recognition tasks. The proposed end-to-end framework is capable of comprehensively alleviating the shortcomings of other surgical video analysis approaches and improve the ability of correlation capture. The source codes and relevant supporting documents can be found on our project website 1 . Our main contributions are summarized as follows:</p><p>• We develop a novel multi-task recurrent convolutional network for surgical video analysis. It consists of two branches, which share early convolutional layers, and are well designed for solving particular tasks at respective higher layers. Given that temporal information is crucial for phase recognition, we employ a recurrent unit, i.e. long-short term memory (LSTM), in this branch to encode sequential dependencies. In this way, we form a multi-task learning model composing of CNN and RNN modules.</p><p>• We propose a correlation loss to provide additional regularization inspired from domain knowledge, by minimizing the divergence of probability predictions for the two tasks. This mechanism is to penalize the model, when the correlated tasks resulting in conflicting predictions. The features which are discriminative for each task can have dynamic interactions at hierarchy during the training process, presenting a novel style of multi-task learning.</p><p>• We extensively validate our proposed MTRCNet-CL on a large endoscopy surgical video dataset (Cholec80). Our method outperforms existing stateof-the-art approaches significantly and consistently (e.g., 89.1% v.s. 81.0% for the mAP in tool presence detection and 87.4% v.s. 84.5% for F1 score in phase recognition), demonstrating outstanding efficacy of our developed multi-task learning strategy for surgical video analysis. The source code and relevant supporting documents will be released and publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Surgical Video Analysis</head><p>Most previous works treated tool presence detection and phase recognition from surgical video as two independent tasks. The literature presenting to solve automatic tool presence detection problem in the computer assisted intervention community is relatively limited. The early approaches were based on the low-level handcrafted features, such as the combination of shape, color and texture features <ref type="bibr" target="#b29">(Lalys et al. (2012)</ref>). Recently, researchers have been dedicated to employing CNNs to learn more discriminative visual features. Some methods proposed to recognize each kind of tools independently. For example, <ref type="bibr" target="#b32">Luo et al. (2016)</ref> utilized multiple CNNs to extract the visual feature, but the performance is not satisfactory since such methods ignored the intrinsic association among different tools. Others formulated this task as a multi-label classification problem and leveraged the underlying relationship of tools. <ref type="bibr" target="#b46">Wang et al. (2017)</ref> integrated VGG and GoogleNet to take advantage of the deep CNN model ensemble. <ref type="bibr" target="#b40">Sahu et al. (2017)</ref> paid attention to analyzing the imbalance on tool co-occurrences and exploited stratification techniques during the network training process. <ref type="bibr" target="#b11">Choi et al. (2017)</ref> developed a real-time detection CNN model based on YOLO. Al <ref type="bibr" target="#b1">Hajj et al. (2017)</ref> proposed to leverage sequential information to detect surgical tools in cataract surgery videos, which used optical flow to fuse the multi-image during the network training.</p><p>The methods which proposed to recognize phase were mainly divided into three categories according to what types of data to be utilized, including manually annotated signal, sensor signal and the combination of them. In addition, video data as the main sensor signals can be further separated into external OR video and endoscope video used in minimally invasive surgery. First, many studies leveraged various manually annotated data to recognize surgical phases. For example, <ref type="bibr" target="#b35">Padoy et al. (2012)</ref> exploited binary instrument usage signal and utilized statistical modeling based on dynamic time warping (DTW) and HMMs to analyze the data. Forestier et al. used tool usage information, the anatomical structure, and the surgical motion which are collectively known as surgical triplets to represent frame information. Decision tree and DTW combined with a clustering algorithm were employed to process the data <ref type="bibr" target="#b17">(Forestier et al. (2013</ref><ref type="bibr" target="#b18">(Forestier et al. ( , 2015</ref>). These manually annotated signals can represent some typical features of phases, therefore methods based on them can achieve quite good performances. However, this kind of signals needs additional workload which is time-consuming and tedious for surgeons. Moreover, it cannot be obtained in real time, therefore aforementioned methods are invalid when doing a real online surgery. In this regard, some previous works were dedicated to presenting methods which are solely based on the live sensor signal. <ref type="bibr" target="#b25">Klank et al. (2008)</ref> presented a feature extraction mechanism based on genetic programming to automatically extract visual features from surgical video. Support vector machines were then used to classify the phases of cholecystectomy surgery from the extracted feature vectors. However, the average accuracy is only around 50% in some cases due to the low level extracted features. Considering that the methods solely based on visual features cannot reach the satisfying performance, some researchers presented approaches to leverage the live sensor signal and manually annotated signal simultaneously. <ref type="bibr" target="#b36">Padoy et al. (2008)</ref> proposed to combine the tool usage signals and visual cues computed from two videos, including OR video to record the surgery environment and endoscope video. A left-right HMM was constructed from these signals. <ref type="bibr" target="#b5">Blum et al. (2010)</ref> found a projection function from visual features of video frames to tool usage signals. HMM and DTW were then utilized to model sequential dependencies. This method can be used in test time because the tool signals are not needed anymore as long as the projection function is obtained. However, the tool annotations are still needed in the learning process. How to effectively and efficiently recognize surgical phase still remains an open problem.</p><p>With the advancements of high-level feature learning, many studies tended to leverage deep CNNs and RNNs to extract feature solely from online sensor signal. Much more discriminative features contribute to the compelling performance of phase recognition and meanwhile, alleviate the challenge of high annotation workload. <ref type="bibr" target="#b14">DiPietro et al. (2016)</ref> used RNN to model the robot kinematics and achieved an accurate phase recognition for robotic surgery. <ref type="bibr" target="#b30">Lea et al. (2016)</ref> employed temporal filters to convolve the sequential stacked spatial features extracted from CNN. DTW is then used as a classifier to recognize phase based on the spatio-temporal feature. <ref type="bibr" target="#b23">Jin et al. (2018)</ref> proposed a unified framework called SV-RCNet, where CNN is utilized to extract visual features from encoscopy videos, and RNN is seamlessly integrated to model the temporal information. The network jointly optimized the visual representations and temporal dynamics, and achieved promising performance. <ref type="bibr" target="#b49">Yengera et al. (2018)</ref> presented a self-supervised pre-training approach based on remaining surgery duration prediction for addressing surgical phase recognition with less annotated data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Multi-task Learning</head><p>Aforementioned methods tackled tool presence detection and phase recognition tasks separately, which cannot take the advantage of complementary information of these two tasks to benefit each other. In addition, from some works of phase recognition <ref type="bibr" target="#b5">(Blum et al. (2010)</ref>; <ref type="bibr" target="#b35">Padoy et al. (2012)</ref>; <ref type="bibr" target="#b27">Lalys et al. (2013)</ref>; <ref type="bibr" target="#b50">Yu et al. (2019)</ref>), it is observed that tool usage information is beneficial for recognizing phase as the input signal. Therefore, with joint learning the phase recognition and tool presence detection, tool usage information can be indirectly used for the improvement of phase recognition through the shared features.</p><p>Recently, effectively leveraging the close correlations between multiple tasks have achieved great success in natural data analysis <ref type="bibr" target="#b33">(Mahmud et al. (2017)</ref>; <ref type="bibr" target="#b21">Hinami et al. (2017)</ref>; <ref type="bibr" target="#b19">Gebru et al. (2017)</ref>; <ref type="bibr" target="#b31">Liu et al. (2017)</ref>). For example, <ref type="bibr" target="#b33">Mahmud et al. (2017)</ref> presented a multi-task network with three streams. The extracted features were concatenated for jointly inferring the activity labels and starting time. <ref type="bibr" target="#b21">Hinami et al. (2017)</ref> proposed to learn a multi-task Fast R-CNN for object detection, action and attribute classification. The network shared features in earlier layers and employed a fully connected layer as a classifier in each branch. Although achieving outstanding performance, the former method lacks shared weights to enable dynamic interaction. While in the latter one, the branches are not well designed based on the task characteristics, and hence the intrinsic relatedness is not sufficiently exploited.</p><p>Many studies in medical image analysis domain have also corroborated the importance of harnessing the relatedness to simultaneously improve performance of both tasks. Multi-task learning has been demonstrating state-of-the-art results on many challenging tasks, such as cardiac left ventricle full quantification <ref type="bibr" target="#b48">(Xue et al. (2017)</ref>), pulmonary nodule classification and localization regression ), nuclei detection and fine-grained classification <ref type="bibr" target="#b52">(Zhou et al. (2017)</ref>), surgical instrument segmentation and localization <ref type="bibr" target="#b26">(Laina et al. (2017)</ref>), synthetic CT generation and organs-at-risk segmentation <ref type="bibr" target="#b8">(Bragman et al. (2018)</ref>), pancreas localization and segmentation <ref type="bibr" target="#b39">(Roth et al. (2018)</ref>). For surgical video analysis, <ref type="bibr" target="#b45">Twinanda et al. (2017)</ref> recently presented a multi-task network called EndoNet to simultaneously carry out the two tasks of tool detection and phase recognition. The network consisted of two branches that shared the early layers to extract the visual features. Hierarchical HMM was subsequently applied to enforce the temporal constraints to refine the phase recognition results. Although this work has achieved outstanding performance, temporal dependencies, which are crucial for phase analysis, are detached from the unified framework. <ref type="bibr" target="#b53">Zisimopoulos et al. (2018)</ref> proposed to first train a ResNet to recognize tool presence and then combine the tool binary predictions and tool features from the last layer to train a RNN for phase recognition, which achieved promising results in cataract video analysis. Very recently, <ref type="bibr" target="#b34">Nakawala et al. (2019)</ref> present a Deep-Onto network which integrates deep models with ontology and production rules. The method can recognize different types of surgical contexts, including phase, tool and action. However, it is lacking in careful design of task-specific branches, which uses the multilayer perceptron for each task. Therefore, there is still room for further investigation and improvement in terms of correlation modeling and temporal information involving.</p><p>In addition, many works have attempted to learn the relationship through a matrix space or utilize additional regularization to increase the model learning capability. For example, <ref type="bibr" target="#b2">Augenstein et al. (2018)</ref> propose to leverage unlabeled or auxiliary data for better text classification. They first designed a label embedding layer to learn a relationship space between disparate labels. Based on it, a label transfer network is employed to leverage the predictions of the auxiliary tasks to estimate a label for the target task. <ref type="bibr" target="#b3">Bachman et al. (2014)</ref> present an agreement regularizer to minimize variation of pseudo-ensemble models for improving sentiment analysis. They first obtain several pseudo-ensemble child models by perturbing the parent model through some noise process. Then they examine the relationship of pseudo-ensembles by the agreement regularizer and penalize the whole model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>Aiming to sufficiently take advantage of the natural relatedness of tool presence detection and surgical phase recognition tasks, we present a novel framework with two branches which share the early feature encoders and respectively hold higher layers for specific tasks. The LSTM unit is embedded in the phase branch, which introduces the sequential dynamic into the unified framework. In addition, we propose a correlation loss to minimize the divergence of the distributions of predicted probabilities, thus enforcing the consistency of outputs for the two correlated tasks. The overview of our proposed MTRCNet-CL is shown in <ref type="figure">Figure 2</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mapping Matrix</head><p>Tool Label <ref type="figure">Figure 2</ref>: An overview of the proposed MTRCNet-CL for joint tool presence detection and phase recognition from surgical videos in a unified end-to-end framework. LSTM networks are illustrated by diagrams to indicate how temporal information is modeled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Multi-task Learning Network Architecture</head><p>To meet the challenges of surgical video recognition, in the shared backbone part, we employ a 50-layer residual convolutional network to extract representative high-level features <ref type="bibr" target="#b20">(He et al. (2016)</ref>). In each residual unit, we stack three convolutional layers, each followed by a batch normalization layer and a ReLU non-linearity layer. After constructing the residual unit, we gradually stack 16 blocks to improve the network depth and finally form the deep residual network. The backbone part ends with an average pooling layer and outputs a 2048-dimension feature vector.</p><p>For conducting multi-tasks, the network construction splits into two branches, respectively targeting for tool presence detection and surgical phase recognition tasks. Considering that tool presence is defined solely based on visual information in the single frame, a fully-connected layer is directly connected to the backbone network with a sigmoid layer followed to produce predictions for the tools. As for phase recognition which relies on temporal information, we connect the shared backbone layers with LSTM units in this branch. There are several gates to modulate the interactions between the memory cell c t and its environment. Hidden state h t retains the past information and supplies it to the memory cell through the gates. The details are instantiated in the diagrams in <ref type="figure">Figure 2</ref>. Different from the traditional linear models, such as HMM, our employed LSTM takes full advantage of long-term temporal information <ref type="bibr" target="#b15">(Donahue et al. (2015)</ref>). Moreover, to capture richer dynamics in surgical videos, we implement a distributed system enabling multiple GPUs computations, which allows us to extend the length of input sequences easily.</p><p>The tool branch (with a fully-connected CNN layer) and phase branch (with a RNN layer) are both seamlessly connected with the shared backbone convolutional layers. Overall, we get a recurrent convolutional network to process multi-tasks on surgical videos (MTRCNet). The entire framework is trained end-to-end supervised by both tool and phase annotations via joint learning. By introducing temporal information in the whole training process, the framework can make use of the complementary information of visual and temporal space simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Objective Functions for Joint Learning of CNN and RNN</head><p>We jointly learn the two tasks by training the framework with extracted video clips. We denote each video clip input to the network by x = {x t , . . . , x t−1 , x t }, where x t is the first frame and x t is the last frame in this clip sample. The number of frames in each video clip is represented as N f . Meanwhile, we denote the shared layers by U with weights β and its obtained feature vector for frame x t is represented by g t . The stacked higher layers in two branches are respectively represented by V T for tool and V P for phase with weights θ T and θ P , respectively.</p><p>We treat the tool presence detection as a multi-label classification problem, given that different categories of tools may appear in the same frame. To this end, we utilize multi-label logistic loss to calculate the classification error for the tool branch. Denoting the C as the set of tool categories, the tool branch loss function is defined as follows:</p><formula xml:id="formula_0">L T (x t ; β, θ T ) = − c∈C (y T t,c log(p t,c ) + (1 − y T t,c ) log(1 −p t,c )),<label>(1)</label></formula><p>where y T t,c ∈ {0, 1} is the ground truth of tool presence for frame x t , which equals to 1 when the c-th tool presents in the t-th frame;p t,c represents the prediction of the c-th tool presenting in frame x t . In the phase recognition task, we use softmax cross-entropy function to calculate the loss of this multi-class classification task:</p><formula xml:id="formula_1">L P (x t ; β, θ P ) = − logp z=y P t t (x t :t , h t :t−1 ),<label>(2)</label></formula><p>wherep z t represents the predicted probability of frame x t belonging to the phase class z; y P t is denoted as the phase ground truth label of frame x t ; h t indicates the updated hidden state calculated by LSTM with input frame x t and previous hidden state h t−1 . With such recurrent module in the unified framework, sequential dynamics in the video are jointly learned with visual representations.</p><p>Training the entire framework in an end-to-end manner enables to simultaneously and interactively recognize the tool and phase. With shared visual features extracted by the earlier layers and joint optimization of two branches, the learning of both tasks can benefit from each other. Specifically, according to Eq.1 and Eq.2, the shared weights β in the earlier convolutional layers U are optimized by both tool presence detection loss L T and phase recognition loss L P . The gradients derived from tool branch can flow to the layers in phase branch, and vice versa. More importantly, the recurrent module LSTM in phase branch brings the temporal information to the unified network, which can be jointly learned with the shared convolutional module. To this end, temporal information not only has a positive effect on the phase recognition, but also implicitly benefits the tool detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Correlation Loss Modeling Relatedness between Tasks</head><p>In surgery, surgeons are requested to perform some specified operations with corresponding tools in a specific surgical phase. Therefore, the tool presence and surgical phase have well-defined prior correlations with clear domain knowledge. In this regard, the classical multi-task learning network which learns the shared features at low-level layers while uses task-specific high-level predictors, such as our afore-defined MTRCNet, is suboptimal for this problem.</p><p>Previous multi-task method ) for surgical video analysis improves the traditional architecture by directly concatenating tool predictions with visual features to conduct phase recognition. Instead, we design more carefully about how to more effectively model the relatedness between the two tasks. Specifically, we argue that we can also make reliable predictions for tool presence, by only using the features from the phase recognition branch, as there exist underlying mapping patterns between the two label spaces. Moreover, this obtained probability distribution of tool presence inferred via the phase features, can serve as a referenceable prior to regularize the predictions of the tool branch. With this analysis, we construct a correlation cell, i.e., a mapping matrix with 128 × 7 dimensions, to translate the underlying correlation between these two tasks. Practically, this mapping matrix is to linearly cast the high-dimensional spatial-temporal features to a compact semantic space with the meanings of surgical tools. Furthermore, the divergence of the probability distributions of the tool usage is minimized via a derived correlation loss, penalizing the inconsistency between the tool branch and the inferred prior. The <ref type="figure" target="#fig_2">Figure 3</ref> illustrates the concepts of this process.</p><p>We choose the Kullback-Leibler (KL) divergence to establish this additional regularization, with considerations on particular characteristics of our problem. Usually, it is common to use mean square error (i.e., L2 Norm) to measure the Euclidean distance of two vectors, especially in scenarios of regression problems. However, measuring L2 Norm is inappropriate in our problem setting, as we are not enforcing the equality of absolute values of probability predictions. We are instead expecting to measure the distance of two probability distributions. The cross-entropy loss is also not optimal, given that the two distributions are both unfixed, which would result in failure in measuring the absolute difference if using cross-entropy loss. Meanwhile, the Earth-Mover distance (i.e., Wasserstein-1 distance), which is widely used and brings in stability for generative adversarial networks, is not suitable for our task. It is good at tackling the situation where the distribution's support does not have non-negligible intersection, and when KL-divergence is just infinite. In our setting, the two predictions are highly correlated, such that KL-divergence can be smoothly used to measure the difference between two distributions. In these regards, we derive the correlation loss for our multi-task learning based on KL-divergence. We regard the two distributions obtained from both branches as equally important, and therefore, we compute the KL-divergence bi-directionally. We choose not to use the Jensen-Shannon (JS) divergence (symmetrical distance), with consideration that the distance is calculated towards the average of two distributions; such mixture has no intuitive meaning in the real-world application. Instead, the bidirectional KL-divergence directly computes on the tool and phase distributions, which is a more straight and stable implementation. Formally, with r t denoting the features output from LSTM in phase branch for frame x t , the tool prior probabilityp t,c can be inferred by forwarding r t to the mapping matrix M with parameters θ M , followed by a Sigmoid activation:</p><formula xml:id="formula_2">p t,c = Sigmoid(M(r t ; θ M )).</formula><p>( <ref type="formula">3)</ref> We denote the predicted probability distribution of tool c obtained in the tool branch byp t,c = [p t,c , 1 −p t,c ], and the prior inferred from the phase branch byp t,c = [p t,c , 1 −p t,c ]. The correlation loss of each tool is calculated bidirectionally with KL divergence as D KL (p t,c p t,c ), and D KL (p t,c p t,c ). Overall, the correlation loss is the sum of those for all categories of tools:</p><formula xml:id="formula_3">L co (x t ; β, θ T , θ P , θ M ) = c∈C ( 1 2 D KL (p t,c p t,c ) + 1 2 D KL (p t,c p t,c )), D KL (p t,c p t,c ) =p t,c logp t,c p t,c + (1 −p t,c ) log 1 −p t,c 1 −p t,c .<label>(4)</label></formula><p>By enhancing the consistency between the two predictions, the weights for each task are not only optimized by corresponding ground truth, but also influenced by the information from the other related task. In other words, the correlation loss forces the phase branch to encode tool presence information into feature vectors r t , and meanwhile it constrains tool branch to take into account phase representation by enforcing the tool branch to learn from the perspective of phase. In addition, the correlation loss provides additional regularization and supervision to improve the interactions when updating the weights θ T and θ P in two branches. By encouraging the joint optimization of tool and phase branch, the correlation between the two tasks can be further captured and modeled in the correlation cell. The updated correlation cell provides more accurate tool prediction inference from phase branch, which forms a beneficial circulation for the entire network training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Overall Loss Function, Training Procedure and Implementations</head><p>We denote N c as the number of clip samples in the whole training database and each sample contains N f frames. The parameters in the entire framework is represented by W = {β, θ P , θ T , θ M }. The overall joint loss function then can be formulated as:</p><formula xml:id="formula_4">L = 1 N c N f Nc i=1 N f t=1 (L T (x t,i ) + λ 1 L P (x t,i ) + λ 2 L co (x t,i )) + λ 3 W 2 2 ,<label>(5)</label></formula><p>where x t,i is the t-th input frame in the i-th video clip sample; the first three terms represent tool detection loss, phase recognition loss and correlation loss, respectively; the last term corresponds to the weight decay regularization. The λ 1 , λ 2 and λ 3 are hyper-parameters to balance the loss. We employ stochastic gradient descent method to jointly update weights of the entire framework. With shared low-level feature extracted in earlier convolutional layers and high-level constraint using correlation loss and mapping matrix, the dynamic interaction between two branches can be facilitated during the joint training procedure.</p><p>To this end, our MTRCNet-CL can sufficiently leverage the close relatedness between the two tasks, and hence improve the performance of both tasks.</p><p>To sufficiently take advantages of relatedness between the two tasks, it is necessary to carefully design the training procedure. In practice, we exploit a three-step strategy to train our framework. In step-1, given that the parameter scale of backbone shared layers is much larger than that of two branches, we initialize the weights of backbone shared layers with a pretrained model on ImageNet <ref type="bibr" target="#b20">(He et al. (2016)</ref>). The branch-specific weights [θ T , θ P ] are randomly initialized with Xavier uniform initializer. Then the MTRCNet with two branches is jointly trained with L T and L P . In step-2, we freeze [β, θ T , θ P ] and solely train the mapping matrix θ M from phase feature towards tool labels. After obtaining a reliable mapping matrix which is able to construct the close relatedness of two tasks, in step-3, we jointly optimize the entire parameters of MTRCNet [β, θ T , θ P ] and the weights of mapping matrix θ M towards Eq. 5, i.e., the overall loss function L with correlation loss added. Note that when the two types of annotations are unequal, we can divide the jointly training in step-1 into two independently training process of tool and phase branch with corresponding subsets, which can assist to make full use of the data annotations. During the testing inference, the tool and phase predictions are output by the two network branches. The tool predictions from the mapping matrix are not utilized or averaged with ones from the tool branches, as the performances have no obvious improvement when being evaluated on the validation dataset. As shown in the experimental results, the designed training and testing strategy delivers an outstanding performance.</p><p>In implementation, we first down-sample the original videos to enrich the temporal information in one input video clip. We choose to down-sample the video from 25fps to 1fps considering that tool presence is annotated in 1fps. We resize the frames from the original resolution of 1920×1080 and 854×480 into 250 × 250 to dramatically save memory and reduce network parameters.</p><p>The data augmentations with 224 × 224 cropping and mirroring is performed to enlarge the training dataset. We train the model using back-propagation with stochastic gradient descent, with the momentum of 0.9 and weighted decay of 5e−4. We initialize learning rate of shared convolutional layers as 5e−5 and two sub-branch layers as 5e−4, and are divided by a factor of 10 when the validation loss plateaus. Our framework is implemented based on PyTorch with 4 NVIDIA Titan Xp GPUs for training. Such implementation of multiple GPUs enables the input length of each video clip to reach 10 seconds and enables the batch size to reach 400. It takes around 8 hours for training the entire framework. We use one GPU configuration in test inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset and Evaluation Metrics</head><p>We extensively validate the proposed MTRCNet-CL method on a large public surgical dataset, i.e., Cholec80 <ref type="bibr" target="#b44">(Twinanda et al. (2016)</ref>). The dataset consists of 80 videos recording the cholecystectomy procedures. The videos are obtained at 25 fps and each frame has the resolution of 1920 × 1080 or 854 × 480. All the frames are annotated with 7 defined phases by experienced surgeons. Tool annotations, also consisting of 7 categories, are conducted at 1 fps re-sampling. The tool presence is defined based on the visual information of a tool in the single frame and annotated as a positive one if at least half of the tool tip is visible. The detailed definition and typical appearance of phases and tools in the Cholec80 dataset are presented in <ref type="figure" target="#fig_0">Figure 1</ref>. Following the same procedure reported in <ref type="bibr" target="#b45">Twinanda et al. (2017)</ref>, we split the dataset Cholec80 into two subsets with equal size, with 40 videos for training and the rest 40 videos for testing. All our experiments are conducted in online mode, i.e., without using future information {x t+1 , x t+2 , ...} when making predictions for frame x t .</p><p>To quantitatively evaluate the performance of our method, we employ the evaluation metrics utilized in <ref type="bibr" target="#b45">Twinanda et al. (2017)</ref>. For phase recognition, we use precision (PR), recall (RE) and accuracy (AC) to validate the performance. The PR and RE are computed in phase-wise, defined as:</p><formula xml:id="formula_5">PR = |GT ∩ P| |P| , RE = |GT ∩ P| |GT| ,<label>(6)</label></formula><p>where GT and P represent the ground truth set and prediction set of one phase, respectively. After PR and RE of each phase are calculated, we average these values over all the phases and obtain the PR and RE of the entire video. The AC is calculated at video-level, defined as the percentage of frames correctly classified into the ground truths in the entire video. For tool recognition, the performance is evaluated by mean average precision (mAP). We first calculate the AP of each tool and average them over all the seven tools. In the following result tables, we list the average and standard deviation values computed in all the test videos, to show the mean and variation among different surgical videos. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Effectiveness of Key Components of MTRCNet-CL</head><p>We conduct extensive ablation experiments to validate the effectiveness of different key components in the proposed MTRCNet-CL model. In <ref type="table" target="#tab_1">Table 1</ref>, we list the results of three configurations: (1) we independently train two networks for tool presence detection and phase recognition (SingleNet in Table 1) as the baselines of our experiments, where we employ the same network architectures as used in the MTRCNet-CL to guarantee the comparison fairness; (2) we train the multi-task network with two branches in an end-to-end manner, but without any correlation loss, i.e., MTRCNet which follows classical multi-task learning practice; (3) we add the correlation loss to the multi-task learning framework to unleash the relatedness between two tasks to a large extent, i.e., our proposed MTRCNet-CL method.</p><p>Benefits of Video Length. The length of the input video clip has the beneficial influence on the quality of the temporal features learned from the LSTM, therefore it is considered to be a key factor for accurate phase recognition. In order to lengthen the input video and thus enhance the temporal representation capability of our model, we implement our networks in a distributed way with multiple GPUs. We first conduct experiments using three different input lengths, i.e., 4, 6 and 10 seconds, with both SingleNet and MTRCNet to validate the effectiveness of increasing the length of the input video.</p><p>In <ref type="table" target="#tab_1">Table 1</ref>, we can observe that the phase recognition results produced by the single phase network (SingleNet) gradually improve with the increase of the video input length. In particular, the metric AC improves from 85.3% to 86.4% when the length increases from 4 seconds to 10 seconds, demonstrating the importance of learning long-term temporal dependencies for phase recognition task. The beneficial impact can also be witnessed when we use multi-task learning architecture (MTRCNet), increasing the AC of phase recognition from 85.9% to 87.3%. To take advantage of the temporal information from long video clips, we employ 10-second videos as inputs for the MTRCNet-CL. Effectiveness of Multi-task Learning. For phase recognition task, compared with the counterparts using SingleNet, our proposed MTRCNet, although with absence of correlation loss, can achieve consistent improvements in all three evaluation metrics across all different input lengths. The PR, RE and AC of MTR-CNet with 10-second video input reach 85.0%, 85.1% and 87.3%, respectively. For tool presence detection task, MTRCNets also achieve better performance compared with the independently trained network. In addition, it is observed that the increase of input length of MTRCNets also benefits the results of tool recognition task via the multi-task learning. The underlying reason is that, the positive effect of longer temporal dependencies for phase recognition can be transferred into the shared spatial-temporal features in earlier layers. By jointly training with two branches, the more discriminative spatial-temporal features in the shared layers can benefit the tool detection task to some extent. More importantly, with correlation loss added to enforce the prediction consistency, the results of our MTRCNet-CL for both phase and tool tasks are further improved, peaking at 89.2% AC for phase and 89.1% mAP for tool. This demonstrates that the high-level constraint assists the network to further capture the intrinsic relatedness through penalizing the difference of prediction from two branches, and then benefits the training of both branches. We also evaluate tool predictions from the mapping matrix of our MTRCNet-CL, achieving 88.4% mAP. It indicates that through leveraging the high correlation, we can infer a reliable prior for tool from the phase features, forming a strong base for the correlation loss calculation. We further average these two outputs for tool recognition, with 88.8% mAP performance obtained, which is not as good as the results directly output from tool branch.</p><p>In order to more comprehensively analyze effectiveness of the proposed multitask learning scheme, we first visualize the confusion matrices of phase recognition results which can show the details in phase level. Specifically, the confusion matrices of three methods, i.e., SingleNet, MTRCNet and MTRCNet-CL with 10-second video input, are illustrated in <ref type="figure" target="#fig_3">Figure 4</ref>. We omit the detailed phase names and do further abbreviation, e.g. Phase 1 to P1, to increase the concision and aesthetics of table. We can observe that from (a) to (c), the probability percentages on diagonals (recall) tend to increase with misclassification gradually decreasing. Particularly, it is clearly shown that the condition of incorrectly recognizing P1 into P2, P6 or P7 is consistently alleviated by using our multi-task learning scheme. The same situation can also be witnessed in P5 recognition process from <ref type="figure" target="#fig_3">Figure 4</ref>. These observations demonstrate that joint training with tool annotations is of great benefit to increasing recognition performance of some phases. Apart from low-level features shared in the early convolutional layers, our high-level correlating mechanism is capable of reinforcing the interaction between two branches and can further enhance the leverage of both annotations and improve the performance.</p><p>We further draw bar charts (see <ref type="figure" target="#fig_4">Figure 5</ref>) in order to detailedly illustrate the results of PR and RE in each phase-level and AP in each tool-level. Tool names  <ref type="table" target="#tab_1">0  P1  P2  P3  P4  P5  P6  P7  T1  T2  T3  T4  T5  T6</ref>  are also abbreviated, for example, from Tool 1: Grasper to T1. It is observed that compared with other two schemes, the MTRCNet-CL improves the PR performances in the most phases, especially in P3. Similarly, RE performances in P1 and P5 have especially significant increase by using MTRCNet-CL. For AP, the MTRCNet-CL dominates other two schemes across all the seven tools.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Detailed Ablation Study of Mapping Matrix and Correlation Loss</head><p>The learned mapping matrix infers the tool predictions from the features in phase branch, and plays the key role on the calculation of the correlation loss. Therefore, the configurations of mapping matrix, such as training strategy, position and mapping direction, have an important effect on the effectiveness of the correlation loss and indirectly determines the final performance of our MTRCNet-CL. In this regard, we establish several schemes to obtain a comprehensive insight on how the mapping matrix influences the interaction of the  <ref type="table" target="#tab_2">Table 2</ref>.</p><p>Different Training Strategies. We first investigate the influence of training strategies with three schemes: (1) jointly train the two branches with mapping matrix (TS1); (2) train the two branches firstly, next freeze two branches and train mapping matrix, and finally freeze the mapping matrix and continue to train two branches (TS2); (3) train two branches firstly, then freeze two branches and train mapping matrix, and finally jointly train the entire network, i.e. our MTRCNet-CL (see Section 3.4 for more details). It is observed from <ref type="table" target="#tab_2">Table 2</ref> that compared with TS1, the training scheme TS2 and ours both significantly improve the AC score for phase recognition and mAP score for tool presence detection. The difference between these schemes is that the latter two have a separate mapping matrix pre-training step, which provides a relatively better initialization before joint training. This observation validates that balancing the learning difficulties of different components in the network to a comparable level helps to sufficiently leverage the benefit of correlation loss. In addition, MTRCNet-CL achieves marginally better performance over training strategy TS2. The underlying reason is that MTRCNet-CL unfreezes mapping matrix in the final stage and allows a joint training process between the mapping matrix and two branches. Mapping matrix then can be optimized by the loss from two tasks through the back-propagation procedure. In the three-step training strategy of our MTRCNet-CL, the performances for tool recognition reach 87.3% in step-1 (from tool branch), 86.9% in step-2 (from mapping matrix), and increase to 89.1% in the final step (from tool branch). Such results demonstrate that with effectively leveraging the high correlation between two tasks, a reliable prior can be obtained based on the phase feature. In addition, the designed correlation loss can further encourage the interaction between two branches, benefiting the network to model more powerful spatio-temporal feature. We further carefully study the training processes of the three training strategies and visualize the loss plots in <ref type="figure" target="#fig_5">Figure 6</ref>. From (a) and (b), we observe that compared with step-1 of TS2 and MTRCNet-CL, both tool and phase losses of TS1 decrease slower when we attempt to jointly train the whole network with the mapping matrix at the beginning, which further leads to the higher loss at the end of the training process (see (f) and (g)). It is observed that from (d) and (e), the correlation losses consistently decrease with the tool and phase losses going down, demonstrating the beneficial complement between the learning of tool, phase branches and mapping matrix. In addition, our MTRCNet-CL with the designed multi-step training strategy converges much faster than using TS2, and achieves lower values of all the three losses (see (f) and (g) for the clearer comparison). It verifies the effectiveness and importance of our strategy design.</p><p>Mapping in Label Space. We then set up the mapping matrix in label space, i.e., mapping the predicted 7-bit vector of phase recognition (from phase branch) to tool prediction. In this regard, the ground truth of tool and phase correlation can be utilized to initialize the mapping matrix. However, in <ref type="table" target="#tab_2">Table 2</ref>, we can see that MTRCNet-CL still achieves much better performance than the label-space mapping, increasing PR and RE around 3%. The difference between these two settings is that MTRCNet-CL is trained to learn a phase-feature to tool-label mapping matrix, while the other is to learn a phase-label to tool-label mapping matrix. The latter utilized space is too compact and the learned mapping matrix is too sparse, since only one or two tools appear in each phase and the probabilities of the absent tools are around zero (see the ground truth correlation (a) in <ref type="figure" target="#fig_7">Figure 7)</ref>. Instead, the learned matrix of MTRCNet-CL can leverage richer information with more details in semantic feature level, and therefore can unleash the effectiveness of correlation loss.</p><p>Using Mutual Mapping Matrix. We practically explore whether also adding a mapping matrix from tool branch to phase branch, i.e., using mutual mapping  <ref type="table" target="#tab_1">T1 T2 T3 T4 T5 T6 T7 T-P1   P2   P3   P4   P5   P6   P7  T1 T2 T3 T4 T5 T6 T7 T-P1   P2   P3   P4   P5   P6   P7  T1 T2 T3 T4 T5 T6 T7 T-P1   P2   P3   P4   P5   P6   P7  T1 T2 T3 T4 T5 T6 T7 T-P1   P2   P3   P4   P5   P6   P7  T1 T2 T3 T4 T5 T6 T7 T</ref> ; the total number of tool h appearing in phase k in Cholec80 minus the total number in predictions is shown in element (h, k) of each matrix; these difference values are normalized to the range from zero to one.</p><p>'T-' denotes the situation that no tool appears in the frame.</p><p>matrice between two branches, is useful. <ref type="table" target="#tab_2">Table 2</ref> shows that our MTRCNet-CL consistently surpasses the scheme of mutual mapping, improving all the evaluation matrices around 2%. The difference between the two schemes is the network additionally uses the mapping matrix from tool branch to phase predictions or not. Since one tool may appear in different phases, for example, T1 has the high probabilities of appearance in all the phases, see <ref type="figure" target="#fig_7">Figure 7</ref> (a), the mapping from the tool branch fails to provide an ideal prior and may even cause confusion in the joint learning. In this regard, the configuration of our MTRCNet-CL is practically optimal to leverage the correlation loss. Adding the extra mapping matrix from the tool branch to phase branch is practically unnecessary.</p><p>Visualization of Learned Correlations of Two Tasks. To intuitively show the correlation existing in two tasks and provide the insight of what correlations the networks learn, we compute the correlations between phases and tools and visualize them. The <ref type="figure" target="#fig_7">Figure 7 (a-d)</ref> visualizes the phase-tool correlation matrix for (a) ground truth (b)SingleNet predictions (c) MTRCNet predictions (d) MTRCNet-CL predictions through calculating the co-occurrence between the tool and phase. The green and red color brightnesses indicating the strength of correlation for a particular phase-tool pair. For example, we easily observe that there exist high correlations in the cholecystectomy surgical procedure, demonstrating our idea of leveraging the relatedness to improve performance has fundamental support. With multi-task learning, both MTRCNet and MTRCNet-CL are able to reconstruct the correlations quite well, with consistency to the patterns in the ground truth. To further clearly demonstrate the effectiveness of using correlation loss to capture the relatedness between tool and phase, we visualize the differences between the ground truth and the predictions in <ref type="figure" target="#fig_7">Figure 7</ref> (e-g), with the blue color brightness indicating strength. Note that the lower value with lighter color indicates a higher performance. We can find that there exist largest differences between the correlations from two separately trained SingleNets and the ground truth. The visualized difference map is noisy and scattered with large values, e.g. T1 in P6, T4 and T5 in P3. For two multitask learning networks, it is observed that for those close correlations with high probabilities, such as T3 in P2 and T1 in P4 (see the ground-truth correlation in (a)), MTRCNet-CL with explicitly defined correlation loss can penalize more incorrect predictions than solely using multi-branch network (MTRCNet). This observation clearly presents the efficacy of correlation loss, with insight to retain such correlation in predictions for related tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with State-of-the-art Methods</head><p>We compare the performance of our MTRCNet-CL with several well-known or the state-of-the-art approaches; most of the results on the same dataset are reported in <ref type="bibr" target="#b45">Twinanda et al. (2017)</ref>. As for tool presence detection, we compare the results of our method with three approaches. The first one is deformable part model (DPM). This method employs three components to model each tool and uses HOG features to represent the images. The second one is 8-layer CNN (ToolNet) which is trained in a single-task way to solely perform the tool presence detection task. The third one is a 9-layer multi-task network which leverages both tool and phase annotations (EndoNet). This method is regarded as the state-of-the-art method for tool presence detection task in literature.</p><p>As for phase recognition, the first four comparison methods input different visual features followed by hierarchical HMM to refine the results. They consist of 1) binary tool usage information generated from the manual annotations; 2) bag-of-word handcrafted features followed by canonical correlation analysis (CCA); 3) features extracted by 9-layer CNN which solely utilizes the phase annotations (PhaseNet); and 4) features extracted by EndoNet. In addition, <ref type="bibr" target="#b23">Jin et al. (2018)</ref> proposes to seamlessly integrate CNN and LSTM to jointly learn spatial and temporal feature (SV-RCNet). <ref type="bibr" target="#b43">Twinanda (2017)</ref> presents to replace HHMM by LSTM to enforce the sequential constraints on the visual feature from EndoNet (EndoNet+LSTM), which achieves the state-of-the-art performance on phase recognition. The comparison results of tool detection and phase recognition are shown in <ref type="table" target="#tab_3">Table 3</ref> and <ref type="table" target="#tab_4">Table 4</ref>, respectively. We omit the standard deviation in these tables as not all referenced papers reported that. F1 scores are calculated to provide the overall results of phase recognition task for better comparison.</p><p>In these two tables, we can find that all the CNN-based methods, including our MTRCNet-CL, achieve much higher performance than those approaches based on the handcrafted features, demonstrating that deep CNNs can extract more discriminative representations. Compared with two independently trained Note: the * means the methods with multi-task learning. Note: the * means the methods with multi-task learning.</p><p>networks, i.e. ToolNet and PhaseNet, our method achieves striking improvement in both tasks, demonstrating that multi-task learning strategy is beneficial for both tool presence detection and phase recognition tasks of surgical video. Our MTRCNet-CL also outperforms another multi-task based method, i.e. EndoNet by a large margin. These comparison results verify that with low-level spatialtemporal feature sharing by CNN and RNN modules, and high-level constraint by correlation loss, our MTRCNet-CL can sufficiently facilitate the interaction of two branches and therefore catch the close relatedness of two tasks. Moreover, our approach achieves better results than the two state-of-the-art methods, in particular, boosting the tool presence detection results from 81.0% to 89.1% and F1 score of phase recognition from 84.5% to 87.4%, which corroborates the effectiveness of recurrent convolutional joint leaning and correlation loss. The detail results for all the seven tools are also reported in <ref type="table" target="#tab_3">Table 3</ref>; our approach achieves superior performances over other methods in most tool categories, especially for T4 Scissors (improving AP over 25% compared with the state-of-the-art method).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Qualitative Results</head><p>We present results of tool and phase recognitions in some challenging cases to illustrate the effectiveness of the proposed method, as shown in <ref type="figure">Figure 8</ref>. Although the partial appearances and overlap of multiple tools increase the  <ref type="figure">Figure 8</ref>: Typical results of the proposed MTRCNet-CL for tool presence detection and phase recognition. For tool, the labels and the probability predictions towards ground truths are indicated through white arrows; and for phase, they are present below each frame.</p><p>recognition difficulty (case (a), (b), (d) and (e)), our method can achieve rather high prediction confidence towards ground truth. For example, T2 Bipolar in case (b) and T1 Grasper in case (d) are easy to be ignored. However, our MTCNet-CL can witness them through the discriminative spatial-temporal feature and constructed correlation in the learned mapping matrix. Our method is robust and able to distinguish the right phases though there exists a high intraclass variance (case (a), (b) and (c)). Our method can identify the tools which are not normally present in some phases. For example, T2 Bipolar is hardly utilized in Phase2 (case (b)) while T6 Irrigator is rare to appear in Phase4 (case (d)). In other words, although the correlation between tools and phases may be unstable and complicated, our method is capable of addressing the obstacles brought from that. In addition, our network can reduce the effect of the blur scene and noise (case (c) and (f)). We further illustrate tool and phase recognition results of several complete surgical videos in <ref type="figure" target="#fig_8">Figure 9</ref> and <ref type="figure" target="#fig_0">Figure 10</ref>, respectively. From <ref type="figure" target="#fig_8">Figure 9</ref>, we observe that the presence of tool is fitful and inconsistent under the camera, even within several adjacent frames. This phenomenon may be caused by the quite rapid operation action and the unstable surgical camera. Even so, our method can precisely detect different tool presences during the whole surgical procedures, demonstrating the efficacy of our multi-task learning strategy. It can be clearly observed from <ref type="figure" target="#fig_0">Figure 10</ref> that even without any post-processing method, our MTRCNet-CL can produce the smooth phase predictions with the jointly learned spatio-temporal features. Moreover, we find that our method can accurately identify the phase transition frames, which plays a very valuable role for many computer-assisted and robotic surgery to automatically adjust the configurations and parameters to go into next phase <ref type="table" target="#tab_1">.  T4  T1  T2  T3  T5  T6  T7</ref> T-(a) 51min 40s (b) 56min 53s  In each case, we present the recognition results from our MTRCNet-CL (above) and the ground truth (bottom). We scale the temporal axes for better visualization as the different duration in these cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>Tool presence detection and phase recognition have become a key component when developing the context-aware systems for surgical process monitoring and surgeon scheduling. In this paper, we present a multi-task network supervised by both annotations to simultaneously address two tasks. With the convolutional backbone shared in the earlier layers, CNN in the tool branch and RNN in the phase branch can be jointly optimized. The entire framework is optimized in an end-to-end manner which introduces the temporal information in the whole training process. Extensive experiments have demonstrated effectiveness of this carefully architectural design for both tasks. More importantly, we propose a new correlation loss to provide the additional supervision by learning a mapping matrix. This mechanism can reinforce the interaction between two associated tasks and further enhance the high relevancy learning. The performance improvement in the experimental results demonstrates the advantage of this additional penalization for capturing the task correlation. A set of tailored training schemes of mapping matrix is designed to yield the maximum efficacy of the correlation loss for tackling both tool and phase tasks. To this end, our work not only can encourage researchers to simultaneously address the associated tasks in surgical videos, but also can inspire them to develop some strategies to facilitate the leverage of correlation for the analysis of surgical videos, as well as other interconnected multiple tasks.</p><p>According to our multi-step training strategy, in step-1, though there exist the shared earlier layers, the phase branch is jointly trained with tool branch towards the phase annotation. In step-2, the branch parameters are frozen and only the mapping matrix is trained from the modeled phase feature towards tool labels. Based on these two steps, the pathway from the phase branch focuses on the phase recognition tasks and models the spatio-temporal feature to increase the phase distinctiveness. Through the inherent high correlation learned in the mapping layer, the tool presence can be roughly inferred and derived from the modeled phase feature. Therefore, we regard the output of the mapping matrix from phase branch as an additional prior when training the whole network in step-3. During the step-3, we set hyper-parameters of phase loss and correlation loss as 1 and 0.5 respectively, to balance the loss and make the phase branch still bias to the phase recognition task. In this regard, the probability distributions obtained by the mapping matrix diverged from phase branch can still serve as a reference to tool recognition task. In addition, the additional correlation loss have two more effects. Firstly, the two outputs to calculate correlation loss are from two possible pathways to produce tool predictions. Although one is mainly based on the tool feature and the other is mainly based on phase feature, the correlation loss between them indeed can play the ensemble role to enforce the similarity, which enhances the interaction between the two branches. Secondly, in step-3, LSTM weights can also be trained for tool recognition, which introduces some beneficial temporal information for the tool task in the form of soft label through the mapping matrix.</p><p>Although temporal information plays an essential role in the video analysis tasks, LSTM unit is not employed in tool branch based on our careful consideration on the task definition and setting. In the surgical video analysis community, the clinical physicians define the tool presence solely based on one frame scene without looking at adjacent frames ). It is annotated as a positive one only if at least half of the tool tip is visible. In other words, when a tool is partially obscured in a single frame, once the visible part is less than a half, although the tool present in the surrounding frames, it will be regarded as a negative one. In addition, the actions (e.g., hooking) in surgical videos are very rapid. Under the camera, the presence of tool is fitful and inconsistent, even within several adjacent frames. Hence, using LSTM for tool detection maybe not help improve the performance too much. We have conducted the preliminary experiments on this configuration with no obvious improvement shown in final performance, and even encountering a more difficult training process and suffering from longer training time. Therefore, we choose to simplify the network architecture for computational resource saving and easier network training.</p><p>Consistency enhancement can dramatically increase the performance of phase recognition based on the prior knowledge of surgical operations. Therefore, some works proposed to use some simple yet effective post-processing strategies to boost the final results, such as averaging smoothing in <ref type="bibr" target="#b10">Cadene et al. (2016)</ref> and prior knowledge inference, called PKI, in our previous work <ref type="bibr" target="#b23">(Jin et al. (2018)</ref>). In this work, all the results are purely predicted by our end-to-end network (MTRCNet-CL) without using any post-processing strategies. Nevertheless, we have also performed the experiment to investigate how PKI affects the results of our method, with PR, RE and AC peaking at 91.6%, 90.1 and 93.3%, surpassing results of 90.6% PR, 86.2% RE and 92.4% AC in <ref type="bibr" target="#b23">Jin et al. (2018)</ref>.</p><p>One main concern in deep learning (a data-driven methodology) is the lack of available data, especially for the surgical video. The multi-task learning of tool and phase recognition requires the simultaneous annotations for both tasks on the same dataset, which restricts the development to a certain extent. Fortunately, most works regard it as a worthy trade-off: some label the two tasks with utilizing binary tool usage to address phase recognition task <ref type="bibr" target="#b35">(Padoy et al. (2012)</ref>; <ref type="bibr" target="#b50">Yu et al. (2019)</ref>); others are dedicated to establishing more advanced multi-task strategies <ref type="bibr" target="#b53">(Zisimopoulos et al. (2018)</ref>; <ref type="bibr" target="#b34">Nakawala et al. (2019)</ref>). In addition, more relevant datasets begin to be released for public usage, which alleviate the annotation problem to a great extent <ref type="bibr" target="#b34">(Nakawala et al. (2019)</ref>; <ref type="bibr" target="#b42">Stefanie et al. (2018)</ref>). We choose a large-scale and well-organized dataset (Cholec80) to validate our MTRCNet-CL. The outstanding results on this typical dataset demonstrate the effectiveness of our method for surgical tool and phase recognitions. More importantly, compared with other aforementioned multi-task methods, our network has the capability to be extended for semi-supervised learning with less annotations. The correlation loss can be utilized as an unsupervised loss for the unlabeled data. We will explore this promising direction in the future.</p><p>Our proposed method can recognize tool and phase at a quick speed (around 0.3s per frame with one GPU), which can be applied in the real-time contextaware system and assist surgeons in the real-world surgical operating, including warning generating, process monitoring as well as staff scheduling. Such realtime notification and online assistance systems have large potentials to become the key component in the modern operating rooms, especially with the gradual development of robotic minimally invasive surgery. In addition, automatic tool presence detection and phase recognition of surgical videos play the significant roles in some postoperative applications, such as surgical report writing, video database indexing, skill assessment and postoperative review. The proposed MTRCNet-CL is general enough that not only can analyze the cholecystectomy, but also can be extended to address multiple tasks in other types of surgical videos, such as cataract surgery, robotic surgery, etc. In essence, there also exist high correlations between tool usage and surgical activity in other surgical videos. As long as there exist high correlations in the videos, the mapping matrix can be learned and the proposed correlation loss can be leveraged to improve the performance by penalizing the inconsistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we present a novel architecture with correlation loss (MTRCNet-CL) to simultaneously detect surgical tool and recognize phase. Specifically, the designed architecture shares the features in the early layers and holds respective higher layers for corresponding tasks. LSTM is employed in the phase branch to model sequential dependencies. More importantly, the correlation loss with learned mapping matrix is proposed to enforce the consistency of predictions of two tasks. To this end, our framework is able to sufficiently capture the close relatedness by encouraging the interaction of two branches. Extensive experiments have validated the effectiveness of our method on a large surgical video dataset, outperforming the state-of-the-art methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of definition and correlation of each phase and tool presence in surgical videos, taking the cholecystectomy procedure as an example. pends on domain knowledge and would be insufficient to represent the complicated characteristics of surgical videos. With the revolution of deep learning, many attempts of adapting convolutional neural networks (CNNs) and recurrent neural networks (RNNs) on surgical video analysis have been explored and achieved promising performance (DiPietro et al. (2016); Sahu et al. (2017); Jin et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of the calculation process of the proposed correlation loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Confusion matrices visualized by the color brightness of three methods (a) SingleNet, (b) MTRCNet, and (c) MTRCNet-CL. In each confusion matrix, the X and Y-axis indicate predicted phase label and ground truth, respectively; element (x, y) represents the empirical probability of predicting class x given that the ground truth is class y; the probability number on diagonal is the recall for each surgical phase.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>The bar chart results of (a) Precision and (b) Recall in phase-level and (c) Average Precision in tool-level of three methods: SingleNet, MTRCNet, and MTRCNet-CL. The standard deviations are shown through the error bars in each chart.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Visualization of loss curves from different steps of (a) TS1, (b c d) TS2, and (b c e) MTRCNet-CL. The loss curves of each training strategy in the last step are shown together in (f) for the clearer comparison. (g) is presented to show a closer look at the end of training process (red region of (f)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Visualization of correlation between tool and phase in Cholec80 dataset ((a) GT), and constructed correlations by our methods (b) SingleNet (c) MTRCNet and (d) MTRCNet-CL; in each correlation, element (x, y) represents the empirical probability of tool x presenting in phase y; the sum of probability numbers in each row equals to one. The discrepancies between counted numbers in Cholec80 dataset and those predicted by SingleNet, MTRCNet and MTRCNet-CL are illustrated in (e), (f) and (g), respectively</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Color-coded ribbon visualization of tool predictions from MTRCNet-CL (above) and ground truth (bottom) in two complete surgical videos. In each case, we show the seven tool presence with different colors and no tool appearance with blank. The horizontal axes indicate the time progression of different surgical procedure, which are scaled to the same length for better visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Color-coded ribbon illustration of phase during four complete surgical videos, whose horizontal axis represents the time progression.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Experimental results of ablation analysis for different network components.</figDesc><table><row><cell>Method</cell><cell>Length(s)</cell><cell></cell><cell>Phase</cell><cell>Tool</cell></row><row><cell></cell><cell></cell><cell>Precision</cell><cell>Recall</cell><cell>Accuracy</cell><cell>mAP</cell></row><row><cell></cell><cell>4</cell><cell cols="3">81.3 ± 5.9 81.9 ± 9.3 85.3 ± 6.9</cell></row><row><cell>SingleNet</cell><cell>6</cell><cell cols="3">81.3 ± 9.2 83.2 ± 6.8 85.7 ± 7.2</cell><cell>85.4 ± 8.3</cell></row><row><cell></cell><cell>10</cell><cell cols="3">82.9 ± 5.9 84.5 ± 8.0 86.4 ± 7.3</cell></row><row><cell></cell><cell>4</cell><cell cols="3">82.5 ± 5.9 82.2 ± 9.0 85.9 ± 7.6 86.4 ± 7.8</cell></row><row><cell>MTRCNet</cell><cell>6</cell><cell cols="3">82.6 ± 6.4 84.1 ± 9.9 86.7 ± 7.2 86.5 ± 7.3</cell></row><row><cell></cell><cell>10</cell><cell cols="3">85.0 ± 4.1 85.1 ± 7.1 87.3 ± 7.4 87.5 ± 7.6</cell></row><row><cell>MTRCNet-CL</cell><cell>10</cell><cell cols="3">86.9 ± 4.3 86.9 ± 4.3 86.9 ± 4.3 88.0 ± 6.9 88.0 ± 6.9 88.0 ± 6.9 89.2 ± 7.6 89.2 ± 7.6 89.2 ± 7.6 89.1 ± 7.0 89.1 ± 7.0 89.1 ± 7.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Experimental results of ablation analysis for mapping matrix and correlation loss.</figDesc><table><row><cell>Different Settings</cell><cell></cell><cell>Phase</cell><cell></cell><cell>Tool</cell></row><row><cell></cell><cell>Precision</cell><cell>Recall</cell><cell>Accuracy</cell><cell>mAP</cell></row><row><cell cols="5">Training strategy 1 (TS1) 80.1 ± 7.1 79.2 ± 10.4 83.4 ± 8.5 85.1 ± 8.0</cell></row><row><cell cols="5">Training strategy 2 (TS2) 85.5 ± 4.6 85.6 ± 7.6 87.8 ± 7.1 88.1 ± 7.2</cell></row><row><cell>Mapping in label space</cell><cell cols="4">83.5 ± 5.8 84.5 ± 8.1 87.1 ± 7.3 86.8 ± 7.1</cell></row><row><cell cols="5">Mutual mapping matrix 84.7 ± 6.2 85.5 ± 8.9 87.4 ± 7.9 87.1 ± 7.7</cell></row><row><cell>Ours (MTRCNet-CL)</cell><cell cols="4">86.9 ± 4.3 86.9 ± 4.3 86.9 ± 4.3 88.0 ± 6.9 88.0 ± 6.9 88.0 ± 6.9 89.2 ± 7.6 89.2 ± 7.6 89.2 ± 7.6 89.1 ± 7.0 89.1 ± 7.0 89.1 ± 7.0</cell></row><row><cell cols="5">two tasks. All experiments have the same network architectures and data aug-</cell></row><row><cell cols="5">mentation strategies for fair comparison. The experimental results are listed in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Average precisions for recognizing the seven tools (rows) using different approaches (2nd to 5th columns).</figDesc><table><row><cell>Tool</cell><cell>DPM</cell><cell cols="3">ToolNet EndoNet  *  Ours  *</cell></row><row><cell>Grasper</cell><cell>82.3</cell><cell>84.7</cell><cell>84.8 84.8 84.8</cell><cell>84.7</cell></row><row><cell>Bipolar</cell><cell>60.6</cell><cell>85.9</cell><cell>86.9</cell><cell>90.1 90.1 90.1</cell></row><row><cell>Hook</cell><cell>93.4</cell><cell>95.5</cell><cell>95.6 95.6 95.6</cell><cell>95.6 95.6 95.6</cell></row><row><cell>Scissors</cell><cell>23.4</cell><cell>60.9</cell><cell>58.6</cell><cell>86.7 86.7 86.7</cell></row><row><cell>Clipper</cell><cell>68.4</cell><cell>79.8</cell><cell>80.1</cell><cell>89.8 89.8 89.8</cell></row><row><cell>Irrigator</cell><cell>40.5</cell><cell>73.0</cell><cell>74.4</cell><cell>88.2 88.2 88.2</cell></row><row><cell>Specimen bag</cell><cell>40.0</cell><cell>86.3</cell><cell>86.8</cell><cell>88.9 88.9 88.9</cell></row><row><cell>Mean</cell><cell>58.4</cell><cell>80.9</cell><cell>81.0</cell><cell>89.1 89.1 89.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Phase recognition results using different approaches (rows).</figDesc><table><row><cell>Methods</cell><cell cols="4">Accuracy Precision Recall F1 Score</cell></row><row><cell>Binary Tool</cell><cell>47.5</cell><cell>54.4</cell><cell>60.2</cell><cell>57.2</cell></row><row><cell>Handcrafted+CCA</cell><cell>38.2</cell><cell>39.4</cell><cell>41.5</cell><cell>40.4</cell></row><row><cell>PhaseNet</cell><cell>78.8</cell><cell>71.3</cell><cell>76.6</cell><cell>73.8</cell></row><row><cell>EndoNet  *</cell><cell>81.7</cell><cell>73.7</cell><cell>79.6</cell><cell>76.5</cell></row><row><cell>SV-RCNet</cell><cell>85.3</cell><cell>80.7</cell><cell>83.5</cell><cell>82.1</cell></row><row><cell>EndoNet+LSTM  *</cell><cell>88.6</cell><cell>84.4</cell><cell>84.7</cell><cell>84.5</cell></row><row><cell>Ours (MTRCNet-CL)  *</cell><cell>89.2 89.2 89.2</cell><cell>86.9 86.9 86.9</cell><cell>88.0 88.0 88.0</cell><cell>87.4 87.4 87.4</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/YuemingJin/MTRCNet-CL</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A dataset and benchmarks for segmentation and recognition of gestures in robotic surgery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahmidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sefati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">B</forename><surname>Haro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zappella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="2025" to="2041" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Surgical tool detection in cataract surgery videos through multi-image fusion inside a convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al</forename><surname>Hajj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lamard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Charrière</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cochener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Quellec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">39th Annual International Conference of the IEEE</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2002" to="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Multi-task learning of pairwise sequence classification tasks over disparate label spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Augenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Søgaard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.09913</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning with pseudoensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Alsharif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3365" to="3373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Real-time identification of operating room state from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Oates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1761" to="1766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Modeling and segmentation of surgical workflow from laparoscopic video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feußner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="400" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Vision-based and marker-less surgical tool detection and tracking: a review of the literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bouget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jannin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="633" to="654" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Detecting surgical tools by modelling local appearance and global shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bouget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Riffaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jannin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2603" to="2617" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Uncertainty in multitask learning: Joint representations for probabilistic mr-only radiotherapy planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J S</forename><surname>Bragman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tanno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Eaton-Rosen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Hawkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Cardoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Context awareness in health care: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bricon-Souf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Medical Informatics</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="2" to="12" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cadene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.05541</idno>
		<title level="m">M2CAI workflow challenge: Convolutional neural networks with time smoothing and hidden markov model for video frames classification</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Surgical-tools detection based on convolutional neural network in laparoscopic robot-assisted surgery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">39th Annual International Conference of the IEEE</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1756" to="1759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">OR 2020: the operating room of the future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cleary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kinsella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of laparoscopic &amp; advanced surgical techniques. Part A</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="495" to="497" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatic data-driven real-time segmentation and recognition of surgical workflow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Dergachyova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bouget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huaulmé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Morandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jannin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Assisted Radiology and Surgery</title>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recognizing surgical activities with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dipietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Malpani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahmidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Vedula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="551" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automated pulmonary nodule detection via 3D convnets with online sample filtering and hybrid-loss residual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="630" to="638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-site study of surgical practice in neurosurgery based on surgical process models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Forestier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lalys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Riffaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Meixensberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Wassef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Neumuth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Goulet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jannin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Biomedical Informatics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="822" to="829" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automatic phase prediction from low-level surgical activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Forestier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Riffaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jannin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Assisted Radiology and Surgery</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="833" to="841" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fine-grained recognition in the wild: A multi-task domain adaptation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1358" to="1367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Joint detection and recounting of abnormal events by learning deep generic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hinami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3619" to="3627" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Eye-gaze driven surgical workflow segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vieira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Darzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-Z</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="110" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Workflow recognition from surgical videos using recurrent convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sv-Rcnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1114" to="1126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Automatic feature generation in endoscopic images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Klank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feussner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Assisted Radiology and Surgery</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="331" to="339" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Concurrent segmentation and localization for tracking of surgical instruments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rieke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Vizcaíno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="664" to="672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Automatic knowledgebased recognition of low-level tasks in ophthalmological procedures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lalys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bouget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Riffaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jannin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer assisted radiology and surgery</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="39" to="49" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Surgical process modelling: a review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lalys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jannin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Assisted Radiology and Surgery</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="495" to="511" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A framework for the recognition of high-level surgical tasks from video images for cataract surgeries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lalys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Riffaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bouget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jannin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="966" to="976" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Surgical phase recognition: from instrumented ors to hospitals around the world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical image computing and computer-assisted intervention M2CAIMICCAI workshop</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="45" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hierarchical clustering multi-task learning for joint human action grouping and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Z</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="102" to="114" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Surgical tool detection via multiple convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jia</surname></persName>
		</author>
		<ptr target="http://camma.u-strasbg.fr/m2cai2016/reports/Luo-Tool.pdf" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Joint prediction of activity labels and starting times in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mahmud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5773" to="5782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">deep-onto network for surgical workflow and context recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nakawala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Pescatori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>De Cobelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ferrigno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>De Momi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer assisted radiology and surgery</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="685" to="696" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Statistical modeling and recognition of surgical workflow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-A</forename><surname>Ahmadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feussner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-O</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="632" to="641" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">On-line recognition of surgical activity for monitoring in the operating room</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feussner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-O</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1718" to="1724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Real-time recognition of surgical tasks in eye surgery videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Quellec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Charrière</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lamard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Droueche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cochener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cazuguel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="579" to="590" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Real-time task recognition in cataract surgery videos using adaptive spatiotemporal polynomials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Quellec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lamard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cochener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cazuguel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="877" to="887" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Spatial aggregation of holistically-nested convolutional neural networks for automated pancreas localization and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="94" to="107" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Addressing multi-label imbalance problem of surgical tool detection using CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sahu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mukhopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zachow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Assisted Radiology and Surgery</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Detection and localization of robotic tools in robot-assisted surgery videos using deep neural networks for region proposal and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sarikaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Guru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1542" to="1549" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">2018 miccai surgical workflow challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stefanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hannes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-S</forename><surname>Lena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename></persName>
		</author>
		<ptr target="https://endovissub2017-workflow.grand-challenge.org/" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Vision-based approaches for surgical activity recognition using laparoscopic and RBGD videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Twinanda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<pubPlace>Strasbourg</pubPlace>
		</imprint>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Twinanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shehata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marescaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De Mathelin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
		<ptr target="http://camma.u-strasbg.fr/datasets" />
		<title level="m">Cholec80 dataset</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">EndoNet: A deep architecture for recognition tasks on laparoscopic videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Twinanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shehata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marescaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De Mathelin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="86" to="97" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep learning based multi-label classification for surgical tool presence detection in laparoscopic videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Biomedical Imaging</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="620" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Instrument detection and pose estimation with rigid part mixtures model in video-assisted surgeries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wesierski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jezierska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="244" to="265" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Full quantification of left ventricle via deep multitask learning network respecting intra-and inter-task relatedness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mercado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Landis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Warrington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="276" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Less is more: Surgical phase recognition with less annotations through self-supervised pretraining of CNN-LSTM networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yengera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marescaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08569</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Assessment of automated identification of phases in videos of cataract surgery using machine learning and deep learning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Croso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Vedula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sikder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA Network Open</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="191860" to="191860" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Surgical gesture classification from video and kinematic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zappella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Béjar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="732" to="745" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Sfcn-opi: Detection and fine-grained classification of nuclei using sibling fcn with objectness prior interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.08297</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">DeepPhase: Surgical phase recognition in CATARACTS videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Zisimopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Flouty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Luengo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Giataganas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nehme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="265" to="272" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
