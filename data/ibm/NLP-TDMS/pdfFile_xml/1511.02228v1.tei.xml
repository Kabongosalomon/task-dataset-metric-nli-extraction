<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Seven ways to improve example-based single image super resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
							<email>timofter@vision.ee.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Vision Lab D-ITET</orgName>
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rasmus</forename><surname>Rothe</surname></persName>
							<email>rrothe@vision.ee.ethz.ch</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Computer Vision Lab D-ITET, ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>ESAT</roleName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
							<email>vangool@vision.ee.ethz.ch</email>
							<affiliation key="aff2">
								<orgName type="department">D-ITET, ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">U</forename><surname>Leuven</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">D-ITET, ETH Zurich</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Seven ways to improve example-based single image super resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we present seven techniques that everybody should know to improve example-based single image super resolution (SR): 1) augmentation of data, 2) use of large dictionaries with efficient search structures, 3) cascading, 4) image self-similarities, 5) back projection refinement, 6) enhanced prediction by consistency check, and 7) context reasoning.</p><p>We validate our seven techniques on standard SR benchmarks (i.e. Set5, Set14, B100) and methods (i.e. A+, SR-CNN, ANR, Zeyde, Yang) and achieve substantial improvements. The techniques are widely applicable and require no changes or only minor adjustments of the SR methods.</p><p>Moreover, our Improved A+ (IA) method sets new stateof-the-art results outperforming A+ by up to 0.9dB on average PSNR whilst maintaining a low time complexity.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Single image super-resolution (SR) aims at reconstructing a high-resolution (HR) image by restoring the high frequencies details from a single low-resolution (LR) image. SR is heavily ill-posed since multiple HR patches could correspond to the same LR image patch. To address this problem, the SR literature proposes interpolation-based methods <ref type="bibr" target="#b23">[24]</ref>, reconstruction-based methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33]</ref>, and learning-based methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>The example-based SR <ref type="bibr" target="#b10">[11]</ref> uses prior knowledge under the form of corresponding pairs of LR-HR image patches extracted internally from the input LR image or from external images. Most recent methods fit into this category.</p><p>In this paper we present seven ways to improve examplebased SR. We apply them to the major recent methods: the Adjusted Anchored Neighborhood Regression (A+) method introduced recently by Timofte et al. <ref type="bibr" target="#b25">[26]</ref>, the prior Anchored Neighborhood Regression (ANR) method by the same authors <ref type="bibr" target="#b24">[25]</ref>, the efficient K-SVD/OMP method of Zeyde et al. <ref type="bibr" target="#b32">[33]</ref>, the sparse coding method of Yang et al. <ref type="bibr" target="#b31">[32]</ref>, and the convolutional neural network method (SR-  <ref type="figure">Figure 1</ref>. We largely improve (red) over the original examplebased single image super-resolution methods (blue), i.e. our IA method is 0.9dB better than A+ <ref type="bibr" target="#b25">[26]</ref> and 2dB better than Yang et al. <ref type="bibr" target="#b31">[32]</ref>. Results reported on Set5, ×3. Details in Section 4.1. CNN) of Dong et al. <ref type="bibr" target="#b5">[6]</ref>. We achieve consistently significant improvements on standard benchmarks. Also, we combine the techniques to derive our Improved A+ (IA) method. <ref type="figure">Fig. 1</ref> shows a comparison of the large relative improvements when starting from the A+, ANR, Zeyde, or Yang methods on Set5 test images for magnification factor ×3. Zeyde is improved by 0.7dB in Peak Signal to Noise Ratio (PSNR), Yang and ANR by 0.8dB, and A+ by 0.9dB. Also, in <ref type="figure" target="#fig_8">Fig. 8</ref> we draw a summary of improvements for A+ in relation to our proposed Improved A+ (IA) method.</p><p>The remainder of the paper is structured as follows. First, in Section 2 we describe the framework that we use in all our experiments and briefly review the anchored regression baseline -the A+ method <ref type="bibr" target="#b25">[26]</ref>. Then in Section 3 we present the seven ways to improve SR and introduce our Improved A+ (IA) method. In Section 4 we discuss the generality of the proposed techniques and the results, to then draw the conclusions in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">General framework</head><p>We adopt the framework of <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> for developing our methods and running the experiments. As in those papers, we use 91 training images proposed by <ref type="bibr" target="#b31">[32]</ref>, and work in the YCbCr color space on the luminance component while the chroma components are bicubically interpolated. For a given magnification factor, these HR images are (bicubically) downscaled to the corresponding LR images. The magnification factor is fixed to ×3 when comparing the 7 techniques. The LR and their corresponding HR images are then used for training example-based super-resolution methods such as A+ <ref type="bibr" target="#b25">[26]</ref>, ANR <ref type="bibr" target="#b24">[25]</ref>, or Zeyde <ref type="bibr" target="#b32">[33]</ref>. For quantitative (PSNR) and qualitative evaluation 3 datasets Set5, Set14, and B100 are used as in <ref type="bibr" target="#b25">[26]</ref>. In the next section we first describe the employed datasets, then the methods we use or compare with, to finally briefly review the A+ <ref type="bibr" target="#b25">[26]</ref> baseline method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Datasets</head><p>We use the same standard benchmarks and datasets as used in <ref type="bibr" target="#b25">[26]</ref> for introducing A+, and in <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b6">7]</ref> among others. Train91 is a training set of 91 RGB color bitmap images as proposed by Yang et al. <ref type="bibr" target="#b31">[32]</ref>. Train91 contains mainly small sized flower images. The average image size is only ∼ 6, 500 pixels. <ref type="figure">Fig. 2</ref> shows one of the training images. Set5 is used for reporting results. It contains five popular images: one medium size image ('baby', 512 × 512) and four smaller ones ('bird', 'butterfly','head', 'women'). They were used in <ref type="bibr" target="#b1">[2]</ref> and adopted under the name 'Set5' in <ref type="bibr" target="#b24">[25]</ref>. Set14 is a larger, more diverse set than Set5. It contains 14 commonly used bitmap images for reporting image processing results. The images in Set14 are larger on average than those in Set5. This selection of 14 images was proposed by Zeyde et al. <ref type="bibr" target="#b32">[33]</ref>. B100 is the testing set of 100 images from the Berkeley Segmentation Dataset <ref type="bibr" target="#b16">[17]</ref>. The images cover a large variety of real-life scenes and all have the same size of 481×321 pixels. We use them for testing as in <ref type="bibr" target="#b25">[26]</ref>. L20 is our newly proposed dataset. Since all the above mentioned datasets have images of medium-low resolution, below 0.5m pixels, we decided to created a new dataset, L20, with 20 large high resolution images. The images, as seen in <ref type="figure">Fig. 10</ref>, are diverse in content, and their sizes vary from 3m pixels to up to 29m pixels. We conduct the selfsimilarity (S) experiments on the L20 dataset as discussed in Section 3.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Methods</head><p>We report results for a number of representative SR methods.</p><p>Yang is a method of Yang et al. <ref type="bibr" target="#b31">[32]</ref> that employs sparse coding and sparse dictionaries for learning a compact representation of the LR-HR priors/training samples and for sharp HR reconstruction results.</p><p>Zeyde is a method of Zeyde et al. <ref type="bibr" target="#b32">[33]</ref> that improves the Yang method by efficiently learning dictionaries using K-SVD <ref type="bibr" target="#b0">[1]</ref> and employing Orthogonal Matching Pursuit (OMP) for sparse solutions. ANR or Anchored Neighborhood Regression of Timofte et al. <ref type="bibr" target="#b24">[25]</ref> relaxes the sparse decomposition optimization of patches from Yang and Zeyde to a ridge regression which can be solved offline and stored per each dictionary atom/anchor. This results in large speed benefits. A+ of Timofte et al. <ref type="bibr" target="#b25">[26]</ref> learns the regressors from all the training patches in the local neighborhood of the anchoring point/dictionary atom, and not solely from the anchoring points/dictionary atoms as ANR does. A+ and ANR have the same run-time complexity. See more in Section 2.3. SRCNN is a method introduced by Dong et al. <ref type="bibr" target="#b5">[6]</ref>, and is based on Convolutional Neural Networks (CNN) <ref type="bibr" target="#b15">[16]</ref>. It directly learns to map patches from low to high resolution images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Anchored regression baseline (A+)</head><p>Our main baseline is the efficient Adjusted Anchored Neighborhood Regression (A+) method <ref type="bibr" target="#b25">[26]</ref>. The choice is motivated by the low time complexity of A+ both at training and testing and the superior performance that it achieves. A+ and our improved methods share the same features for LR and HR patches and the same dictionary training (K-SVD <ref type="bibr" target="#b0">[1]</ref>) as the ANR <ref type="bibr" target="#b24">[25]</ref> and Zeyde <ref type="bibr" target="#b32">[33]</ref> methods. The LR features are vertical and horizontal gradient responses, PCA projected for 99% energy preservation. The reference LR patch size is fixed to 3 × 3 while the HR patch is s 2 larger, with s being the scaling factor, as in A+.</p><p>A+ assumes a partition of the LR space around the dictionary atoms, called anchors. For each anchor j a ridge regressor is trained on the local neighborhood N l of fixed size of LR training patches (features). Thus, for each LR input patch y we minimize Then the LR input patch y can be projected to the HR space as</p><formula xml:id="formula_0">min β y − N l β 2 2 + λ β 2 .<label>(1)</label></formula><formula xml:id="formula_1">x = N h (N T l N l + λI) −1 N T l y = P j y,<label>(2)</label></formula><p>where N h are the corresponding HR patches of the LR neighborhood N l . P j is the stored projection matrix for the anchor j. The SR process for A+ (and ANR) at test time then becomes a nearest anchor search followed by a matrix multiplication (application of the corresponding stored regressor) for each input LR patch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Augmentation of training data (A)</head><p>More training data results in an increase in performance up to a point where exponentially more data is necessary for any further improvement. This has been shown, among others, by Timofte et al. in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> for neighbor embedding methods and anchored regression methods and by Dong et al. in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> for the convolutional neural networks-based methods. Zhu et al. <ref type="bibr" target="#b34">[35]</ref> assume deformable patches and Huang et al. <ref type="bibr" target="#b12">[13]</ref> transform self-exemplars.</p><p>Around ∼ 0.5 million corresponding patches of 3×3 pixels for LR and 9 × 9 for HR are extracted from the Train91 images. By scaling the training images in <ref type="bibr" target="#b25">[26]</ref> 5 million patches are extracted for A+ and improve the PSNR performance from 32.39dB with 0.5 million to 32.55dB on Set5 and magnification ×3. Inspired by the image classification literature <ref type="bibr" target="#b3">[4]</ref>, we consider also the flipped and rotated versions of the training images/patches. If we rotate the original images by 90 • , 180 • , 270 • and flip them upside-down (see <ref type="figure">Fig. 2</ref>), we get 728 images without altered content.</p><p>In <ref type="figure" target="#fig_1">Fig. 3</ref>   samples affects the PSNR performance of the A+ method on the Set5 images. The performance of A+ with 1024 regressors varies from 31.83dB when trained on 5000 samples to 32.39dB for 0.5 million and 32.71dB when using 50 million training samples. Note that the running time at test stays the same as it does not depend on the number of training samples but on the number of regressors. By A+A we mark our setup with A+, 65,536 regressors and 50 million training samples, improving 0.3dB over A+.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Large dictionary and hierarchical search (H)</head><p>In general, if the dictionary size (basis of samples/anchoring points) is increased, the performance for sparse coding (such as Zeyde <ref type="bibr" target="#b32">[33]</ref> or Yang <ref type="bibr" target="#b31">[32]</ref>) or anchored methods (such as ANR <ref type="bibr" target="#b24">[25]</ref> or A+ <ref type="bibr" target="#b25">[26]</ref>) improves as the learned model generalizes better, as shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. We show in <ref type="figure" target="#fig_1">Fig. 3</ref> on Set5, ×3 how the performance of A+ increases when using 16 up to 65,536 regressors for any fixed size pool of training samples. In A+ each regressor is associated with an anchoring point. The anchors quantize the LR feature space. The more anchors are used, the smaller the quantization error gets and the easier is the local regression. On Set5 the PSNR is 32.17dB for 16 regressors, while it reaches 32.92dB for 65536 regressors with 50 million training samples (our A+A setup). However, the more regressors (anchors) are used, the slower the method gets. At running time, each LR patch (feature) is linearly matched to all the anchors. The regressor of the closest anchor is applied to reconstruct the HR patch. Obviously, this linear search in O(N ) can be improved. Yet, the LR features are high dimensional (30 after PCA reduction for ×3  for A+) and the speedup achievable with data search structures such as kd-trees, forests, or spherical hashing codes are rather small (3-4 times in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b21">22]</ref>).</p><p>Instead, we propose a hierarchical search structure in O( √ N ) with very good empirical precision, that does not change the training procedure of A+. Given N anchors and their N regressors, we cluster them into √ N groups using k-means, each with an l 2 -normalized centroid. To each centroid we assign the c √ N most correlated anchors. This results in a 2-layers structure. For each query we linearly search for the most correlated centroid (1 st layer) to then linearly search within the anchors assigned to it (2 nd layer). c = 4 is fixed in all our experiments, so that one anchor potentially can be assigned to more centroids to handle the cluster boundary well.</p><p>In <ref type="figure" target="#fig_4">Fig. 5</ref> we depict the performance of A+A with and without our hierarchical search structure in relation to the number of trained regressors. The hierarchical structure looses at most only 0.03dB but consistently speeds up above 1,024 regressors. A+A with hierarchical search (H) and 65,536 regressors has a running time comparable to the original A+ with linear search and 1,024 regressors, but is 0.3dB better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Back projection (B)</head><p>Applying an iterative back projection (B) refinement <ref type="bibr" target="#b13">[14]</ref> generally improves the PSNR as it makes the HR reconstruction consistent with the LR input and the employed degradation operators such as blur, downscaling, and downsampling. Knowing the degradation operators is a must for the IBP approaches and therefore they need to be estimated <ref type="bibr" target="#b17">[18]</ref>. Assuming the degradation operators to be known, (B) improves the PSNR of A+ by up to 0.06dB, depending on the settings as shown in column A+B in <ref type="table" target="#tab_5">Table 5</ref>, when starting from the A+ results.</p><p>In <ref type="table" target="#tab_2">Table 1</ref> we compare the improvements obtained with our iterative back projection (B) refinement when starting from different SR methods. The largest improvement is of 0.59dB when starting from the sparse coding method of Yang et al. <ref type="bibr" target="#b31">[32]</ref>, whereas for A+ it only improves 0.04dB. This behaviour can be explained by the fact that the reference A+ is 1.18dB better than the reference Yang method. Therefore A+'s HR reconstruction is much more consistent with the LR image than Yang's and improving by using (B) is more difficult. The refined Yang result is 0.59dB better than the baseline Yang method but still 0.59dB behind A+ without refinement. Note that generally the degradation operators are unknown and their estimation is not precise, therefore our reported results with (B) refinement are an upper bound for a practical implementation and difficult to reach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Cascade of anchored regressors (C)</head><p>As the magnification factor is decreased, the superresolution becomes more accurate, since the space of possible HR solutions for each LR patch and thus the ambiguity decreases. Glasner et al. <ref type="bibr" target="#b11">[12]</ref> use small SR steps to gradually refine the contents up to the desired HR. The errors are usually enlarged by the subsequent steps and the time complexity depends on the number of steps. Instead of superresolving the LR image in small steps, we can go in one step (stage) and then refine the prediction using the same SR method again adapted to this input. We consider the output of the previous stage as LR image input and as target the HR image for each stage. Thus, we build a cascade of trained models, where each stage brings the prediction closer to the target HR image. The cascades and the layered or recurrent processing are broadly used concepts in vision tasks (i.e. object detection <ref type="bibr" target="#b26">[27]</ref> and deep learning <ref type="bibr" target="#b3">[4]</ref>). The method of Peleg and Elad <ref type="bibr" target="#b18">[19]</ref> and the SRCNN method of Dong et al. <ref type="bibr" target="#b5">[6]</ref> are layered by design. While the incremental approach has a loose control over the errors, the cascade explicitly minimizes the prediction errors at each stage.</p><p>In our cascaded A+, called A+C, with 50 million training samples, we keep the same features and settings for all the stages but have models that have been trained per stage. As shown in <ref type="figure" target="#fig_6">Fig. 6</ref> and <ref type="table" target="#tab_3">Table 2</ref> the performance improves from 32.92dB after the 1 st stage of the cascade and saturates at 33.21dB after the 4 th stage of the cascade. The running time is linear in the number of stages. The same cascading idea of A+ was applied for image demosaicing in <ref type="bibr" target="#b27">[28]</ref> with two stages.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Enhanced prediction (E)</head><p>In image classification <ref type="bibr" target="#b3">[4]</ref> often the prediction for an input image is enhanced by averaging the predictions on a set of transformed images derived from it. The most common transformations include cropping, flipping, and rotations. In SR image rotations and flips should lead to the same HR results at pixel level. Therefore, we apply rotations and flips on the LR image as shown in see <ref type="figure">Fig. 2</ref> to get a set of 8 LR images, then apply the SR method on each, reverse the transformation on the HR outputs and average for the final HR result. On Set5 (see <ref type="figure" target="#fig_6">Fig. 6</ref> and <ref type="table" target="#tab_3">Table 2</ref>) the enhanced prediction (E) gives a 0.05dB improvement for a single stage and more than 0.24dB when 4 stages are employed in the cascade. The running time is linear in the number of transformations. In <ref type="table">Table 3</ref> we report the improvements due to (E) for different SR methods. It varies from +0.05dB for ANR up to +0.25dB for the Yang method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Self-similarities (S)</head><p>The image self-similarities (or patch redundancy) at different image scales can help to discriminate between equally possible HR reconstructions. While we considered external dictionaries, thus priors from LR and HR training images, some advocate internal dictionaries, i.e. dictionaries built from the input LR image, matching the image context. Exponents are Glasner et al. <ref type="bibr" target="#b11">[12]</ref> or Dong et al. <ref type="bibr" target="#b7">[8]</ref>, among others <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b12">13]</ref>. Extracting and building models adapted to each new input image is expensive. Also, in recent works, and on the standard benchmarks, methods such as SRCNN <ref type="bibr" target="#b5">[6]</ref> and A+ <ref type="bibr" target="#b25">[26]</ref> based on external dictionaries proved better in terms of PSNR and running time.</p><p>We point out that depending on the size of the input LR image and the textural complexity, the internal dictionaries can be better than the external dictionaries. Huang et al. <ref type="bibr" target="#b12">[13]</ref> report better results with internal dictionaries when using urban HR images with high geometric regularities. We downsize the L20 images and plot the improvements over an external dictionary in <ref type="figure" target="#fig_7">Fig. 7</ref>. Above 246, 000 LR pixels the internal dictionary improves over the external one. However, the best results are obtained using both external and internal dictionaries. Intuition says that using the immediate surrounding of a LR patch should help. For example, Dong et al. <ref type="bibr" target="#b8">[9]</ref> train domain specific models and Sun et al. <ref type="bibr" target="#b22">[23]</ref> hallucinate using context constraints. We consider context images centered on each LR patch of size equal with the LR patch size times the scaling factor (×3). We extract the same features as for the LR patches but in the (×3) downscaled image and cluster them into 4 groups with 4 centroids. A small number that does not increase the time complexity much but it is still relevant for analyzing the context idea. We keep the standard A+ pipeline with 1024 anchors and 0.5 million training patches ( A+(0.5m) ). To each anchor we assign the closest patches and instead of training one regressor as A+ would, we train 4 context specific regressors. For each context we compute a regressor using the 1024 patches closest to both anchor and context centroid, in a 10 to 1 contribution. For patches of comparable distances to the anchor the distance to the context centroid makes the difference. At testing time, each LR patch is first matched against the anchors and then the regressor of the closest context is used to get the HR output. By reasoning with context we improve from 32.39dB to 32.55dB on Set5, while the running time only slightly increases. In <ref type="table" target="#tab_4">Table 4</ref> we report the improvements achieved using reasoning with context (R) over original SR methods. The (R) derivations were similar to the one explained for the A+ (0.5m) setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8.">Improved A+ (IA)</head><p>Any combination of the proposed techniques would likely improve over the baseline example-based superresolution method. If we start from the A+ method, and (A) add augmentation (50 million training samples), increase the number of regressors (to 65536) and (H) use the hierarchical search structure, we achieve 0.33dB improvement over A+ (Set5, ×3) without an increase in running time. Adding reasoning with context (R) slightly increases the running time for a gain of 0.1dB. The cascade (C) allows for another jump in performance, +0.27dB, while the enhanced prediction (E) brings another 0.25dB. The gain brought by (C) and (E) comes at the price of increasing the computation time. The full setup, using (A, H, R, C, E) is marked as our proposed Improved A+ (IA) method. The addition of internal dictionaries (S) is possible but undesirable due to the computational cost. Adding IBP (B) to the IA method can further improve the performance by 0.05dB.</p><p>The seven ways to improve A+ are summarized in <ref type="figure" target="#fig_8">Fig. 8</ref>. The Improved A+ (IA) method is 0.9dB better than the baseline A+ method by using 5 techniques (A, H, R, C, E). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Generality of the seven ways</head><p>Our study focused and demonstrated the seven ways to improve SR mainly on the A+ method. As a result, the IA method has been proposed, combining 5 out of 7 ways, namely (A, H, R, C, E). The effect of applying the different techniques is additive, each contributing to the final performance. These techniques are general in the sense that they can be applied to other example-based single image super-resolution methods as well. We demonstrated the techniques on five methods.</p><p>In <ref type="figure">Fig. 1</ref> we report on a running time versus PSNR performance scale the results (Set5, ×3) of the reference methods A+, ANR, Zeyde, and Yang together with the improved results starting from these methods. The A+A method combines A+ with A and H, while the A+C method combines A+ with A, H, and C. A+A and A+C are lighter versions of our IA. For the improved ANR result we combined the A, H, R, B, and E techniques, for the improved Zeyde result we combined A, R, B, and E, while for Yang we combined B and E without retraining the original model.</p><p>Note that using combinations of the seven techniques we are able to improve significantly all the methods considered in our study which validates the wide applicability of these techniques. Thus, A+ is improved by 0.9dB in PSNR, Yang and ANR by 0.8dB and Zeyde by 0.7dB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Benchmark results</head><p>All the experiments until now used Set5 and L20 and magnification factor ×3. In <ref type="table" target="#tab_5">Table 5</ref> we report the average  <ref type="bibr" target="#b2">[3]</ref> as adapted and implemented in <ref type="bibr" target="#b24">[25]</ref>. All the methods used the same Train91 dataset for training. For reporting improved results also for magnification factors ×2 and ×4, we keep the same parameters/settings as used for the case of magnification ×3 for our A+B, A+A, A+C, and IA methods. A+B is provided for reference as the degradation operators usually are not known and difficult to estimate in practice. A+B just slightly improves over A+. A+A improves 0.13dB up to 0.34dB over A+ while preserving the running time. A+C further improves at the price of running time, using a cascade with 3 stages. IA improves 0.4dB up to 0.9dB over the A+ results, and significantly more over SRCNN, Zeyde, and ANR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Qualitative assessment</head><p>For qualitatively assessing the IA performance we depict in <ref type="figure">Fig. 11</ref> several cropped images for magnification factors ×3 and ×4. Generally IA restores more sharp details with fewer artifacts than the A+ and Zeyde methods do. For example, the clarity and sharpness of the HR reconstruction for the text image visibly improves from the Zeyde to A+ and then to our IA result. The same for other face features such as chin, mouth or eyes in the image from the second row of <ref type="figure">Fig. 9</ref>.</p><p>In <ref type="figure">Fig. 11</ref> we show image results for magnification ×4 on Set14 for our IA method in comparison with the bicubic, Zeyde, ANR, and A+ methods.</p><p>The supplementary material contains more per image PSNR results and HR outputs for qualitative assessment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We proposed seven ways to effectively improve the performance of example-based super-resolution. Combined, we obtain a new highly efficient method, called Improved</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Zeyde <ref type="bibr" target="#b32">[33]</ref> A+ <ref type="bibr" target="#b25">[26]</ref> IA (ours) <ref type="figure">Figure 9</ref>. SR visual comparison. Best zoomed in on screen. A+ (IA), based on the anchored regressors idea of A+. Noninvasive techniques such as augmentation of the training data, enhanced prediction by consistency checks, context reasoning, or iterative back projection lead to a significant boost in PSNR performance without significant increases in running time. Our hierarchical organization of the anchors in the IA method allows us to handle orders of magnitude more regressors than the original A+ at the same running time. Another technique, often overlooked, is the cascaded application of the core super-resolution method towards HR restoration. Using the image self-similarities or the context is shown also to improve PSNR. On standard benchmarks IA improves 0.4dB up to 0.9dB over state-ofthe-art methods such as A+ <ref type="bibr" target="#b25">[26]</ref> and SRCNN <ref type="bibr" target="#b5">[6]</ref>. While we demonstrated the large improvements mainly on the A+ framework, and several other methods (ANR, Yang, Zeyde, SRCNN), we strongly believe that the proposed techniques provide similar benefits for other example-based superresolution methods. The proposed techniques are generic and require no changes to the core baseline method. <ref type="figure">Figure 10</ref>. L20 dataset. 20 high resolution large images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bicubic</head><p>Zeyde <ref type="bibr" target="#b32">[33]</ref> ANR <ref type="bibr" target="#b24">[25]</ref> A+ <ref type="bibr" target="#b25">[26]</ref> IA (ours) <ref type="figure">Figure 11</ref>. SR visual results for ×4. Images from Set14. Best zoomed in on screen.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>original rotated 90 •Figure 2 .</head><label>902</label><figDesc>rotated 180 • rotated 270 • flipped 90 • &amp; flipped 180 • &amp; flipped 270 • &amp; flipped Augmentation of training images by rotation and flip.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Average PSNR performance of A+ on (Set5, ×3) improves with the number of training samples and regressors</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Performance (Set5, ×3) improves with the number of regressors/atoms (dictionary size).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Performance (A+A on Set5, ×3) depends on the number of regressors and the data search structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Average PSNR performance of IA improves with the number of cascade stages (Set5, ×3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Average PSNR gain comparison of internal dictionary, external dictionary and combined dictionary with respect to the input LR image size (L20, ×3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>Seven ways to Improve A+. PSNR gains for Set5, ×3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Back projection (B) improves the super-resolution PSNR results (Set5, ×3). × 31.41 31.92 31.90 32.39 32.59 33.46 32.00 31.99 32.04 32.52 32.63 33.51 Improv. +0.59 +0.07 +0.14 +0.13 +0.04 +0.05</figDesc><table><row><cell>(B)</cell><cell cols="3">Yang ANR Zeyde SRCNN A+</cell><cell>IA</cell></row><row><cell></cell><cell>[32] [25] [33]</cell><cell>[6]</cell><cell>[26] (ours)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Cascading (C) and enhanced prediction (E) improve the super-resolution PSNR results (Set5, ×3). 31.92 31.90 32.39 32.59 33.21 31.65 31.97 31.96 32.61 32.68 33.46 Improv. +0.24 +0.05 +0.06 +0.22 +0.09 +0.25</figDesc><table><row><cell cols="2">Cascade (E)</cell><cell cols="2">ANR Zeyde</cell><cell>A+</cell><cell>IA</cell></row><row><cell></cell><cell></cell><cell>[25]</cell><cell>[33]</cell><cell>[26]</cell><cell>(ours)</cell></row><row><cell>1 stage</cell><cell>×</cell><cell cols="3">31.92 31.90 32.59 32.77</cell></row><row><cell>1 stage</cell><cell></cell><cell cols="3">32.15 32.16 32.70 32.91</cell></row><row><cell>2 stages</cell><cell>×</cell><cell cols="3">32.19 32.20 32.70 33.05</cell></row><row><cell>2 stages</cell><cell></cell><cell cols="3">32.25 32.26 32.81 33.21</cell></row><row><cell>3 stages</cell><cell>×</cell><cell cols="3">32.22 32.23 32.76 33.18</cell></row><row><cell>3 stages</cell><cell></cell><cell cols="3">32.28 32.29 32.87 33.34</cell></row><row><cell>4 stages</cell><cell>×</cell><cell cols="3">32.24 32.24 32.79 33.33</cell></row><row><cell>4 stages</cell><cell></cell><cell cols="3">32.30 32.31 32.89 33.46</cell></row><row><cell cols="2">Improvement</cell><cell cols="3">+0.38 +0.41 +0.30 +0.69</cell></row><row><cell cols="5">Table 3. Enhanced prediction (E) improves the super-resolution</cell></row><row><cell cols="3">PSNR results (Set5, ×3).</cell><cell></cell></row><row><cell>(E)</cell><cell cols="4">Yang ANR Zeyde SRCNN A+</cell><cell>IA</cell></row><row><cell></cell><cell cols="2">[32] [25] [33]</cell><cell>[6]</cell><cell>[26] (ours)</cell></row><row><cell>×</cell><cell>31.41</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Reasoning with context (R) improves the super-resolution PSNR results (Set5, ×3).</figDesc><table><row><cell>(R)</cell><cell cols="3">ANR Zeyde A+(0.5m)</cell><cell>A+</cell><cell>IA</cell></row><row><cell>Context</cell><cell>[25]</cell><cell>[33]</cell><cell>[26]</cell><cell>[26]</cell><cell>(ours)</cell></row><row><cell>×</cell><cell cols="2">31.92 31.90</cell><cell>32.39</cell><cell cols="2">32.59 33.46</cell></row><row><cell></cell><cell cols="2">32.12 32.11</cell><cell>32.55</cell><cell cols="2">32.71 33.51</cell></row><row><cell cols="3">Improv. +0.20 +0.21</cell><cell>+0.16</cell><cell cols="2">+0.12 +0.05</cell></row><row><cell cols="4">3.7. Reasoning with context (R)</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>compares the results with A+ [26], Zeyde [33],</cell></row><row><cell>and SRCNN [6] on standard benchmarks and for magnifi-</cell></row><row><cell>cations ×2, ×3, ×4. Figs. 9 and 11 show visual results.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Average PSNR on Set5, Set14, and B100 and the improvement (red) of our IA (blue) over A+ (bold) method. (ours) (ours) (ours) (ours) of IA over A+ x2 33.66 35.77 35.78 35.83 36.34 36.55 36.60 36.89 37.26 37.39 +0.84 Set5 x3 30.39 31.84 31.90 31.92 32.39 32.59 32.63 32.92 33.20 33.46 +0.87 x4 28.42 29.61 29.69 29.69 30.09 30.29 30.33 30.58 30.86 31.10</figDesc><table><row><cell cols="6">Benchmark Bicubic NE+LLE Zeyde ANR SRCNN A+</cell><cell>A+B A+A A+C</cell><cell>IA</cell><cell>Improvement</cell></row><row><cell></cell><cell></cell><cell>[25]</cell><cell>[33]</cell><cell>[25]</cell><cell>[6]</cell><cell>[26] +0.81</cell></row><row><cell>x2</cell><cell>30.23</cell><cell>31.76</cell><cell cols="3">31.81 31.80 32.18 32.28 32.33 32.48 32.73 32.87</cell><cell>+0.59</cell></row><row><cell>Set14 x3</cell><cell>27.54</cell><cell>28.60</cell><cell cols="3">28.67 28.65 29.00 29.13 29.16 29.33 29.51 29.69</cell><cell>+0.56</cell></row><row><cell>x4</cell><cell>26.00</cell><cell>26.81</cell><cell cols="3">26.88 26.85 27.20 27.32 27.35 27.54 27.74 27.88</cell><cell>+0.56</cell></row><row><cell>x2</cell><cell>29.32</cell><cell>30.41</cell><cell cols="3">30.40 30.44 30.71 30.78 30.81 30.91 31.15 31.33</cell><cell>+0.55</cell></row><row><cell>B100 x3</cell><cell>27.15</cell><cell>27.85</cell><cell cols="3">27.87 27.89 28.10 28.18 28.20 28.32 28.45 28.58</cell><cell>+0.40</cell></row><row><cell>x4</cell><cell>25.92</cell><cell>26.47</cell><cell cols="3">26.51 26.51 26.66 26.77 26.79 26.91 27.03 27.16</cell><cell>+0.39</cell></row><row><cell cols="6">PSNR performance on Set5, Set14, and B100, and for mag-</cell></row><row><cell cols="6">nification factors ×2, ×3, and ×4 of our methods in com-</cell></row><row><cell cols="6">parison with the baseline A+ [26], ANR [25], Zeyde [33],</cell></row><row><cell cols="6">and SRCNN [6] methods. Also we report the result of the</cell></row><row><cell cols="6">bicubic interpolation and the one for the Neighbor Embed-</cell></row><row><cell cols="6">ding with Locally Linear Embedding (NE+LLE) method of</cell></row><row><cell>Chang et al.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">K-SVD: an algorithm for designing overcomplete dictionaries for sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Low-complexity single-image super-resolution based on nonnegative neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-L. Alberi</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Super-resolution through neighbor embedding. CVPR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Jointly optimized regressors for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="95" to="104" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image superresolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Nonlocally centralized sparse representation for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1620" to="1630" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image deblurring and super-resolution by adaptive sparse domain selection and adaptive regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1838" to="1857" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image and video upscaling from local self-examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<idno>12:1-12:11</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Examplebased super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">C</forename><surname>Pasztor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="56" to="65" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Super-resolution from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Glasner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bagon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Single image superresolution from transformed self-exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving resolution by image registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVGIP</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="231" to="239" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Single-image super-resolution using sparse regression and natural image prior. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1127" to="1133" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Nonparametric blind superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Michaeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A statistical prediction model based on sparse representations for single image super-resolution. Image Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Peleg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2569" to="2582" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fast super-resolution via dense local training and inverse regressor search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pérez-Pellitero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Torres-Xirau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ruiz-Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generalizing the nonlocal-means to super-resolution reconstruction. Image Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="36" to="51" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fast and accurate image upscaling with super-resolution forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Context-constrained hallucination for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tappen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2010-06" />
			<biblScope unit="page" from="231" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image interpolation and resampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Thévenaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Blu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Unser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Medical Imaging, Processing and Analysis</title>
		<editor>I. Bankman</editor>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="393" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Anchored neighborhood regression for fast example-based super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">De</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A+: Adjusted anchored neighborhood regression for fast super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">De</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient regression priors for post-processing demosaiced images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Exploiting selfsimilarities for single frame super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">6494</biblScope>
			<biblScope unit="page" from="497" to="510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fast image super-resolution based on in-place example regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="1059" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Image superresolution via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Image superresolution as sparse representation of raw image patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On single image scale-up using sparse-representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Curves and Surfaces</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="711" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning multiple linear mappings for efficient single image super-resolution. Image Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="846" to="861" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Single image superresolution using deformable patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="2917" to="2924" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
