<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Constructing Self-motivated Pyramid Curriculums for Cross-Domain Semantic Segmentation: A Non-Adversarial Approach</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Lian</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengmao</forename><surname>Lv</surname></persName>
							<email>fengmaolv@126.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Duan</surname></persName>
							<email>lxduan@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
							<email>boqinggo@outlook.com</email>
							<affiliation key="aff1">
								<address>
									<settlement>Google</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Constructing Self-motivated Pyramid Curriculums for Cross-Domain Semantic Segmentation: A Non-Adversarial Approach</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a new approach, called self-motivated pyramid curriculum domain adaptation (PyCDA), to facilitate the adaptation of semantic segmentation neural networks from synthetic source domains to real target domains. Our approach draws on an insight connecting two existing works: curriculum domain adaptation and self-training. Inspired by the former, PyCDA constructs a pyramid curriculum which contains various properties about the target domain. Those properties are mainly about the desired label distributions over the target domain images, image regions, and pixels. By enforcing the segmentation neural network to observe those properties, we can improve the network's generalization capability to the target domain. Motivated by the self-training, we infer this pyramid of properties by resorting to the semantic segmentation network itself. Unlike prior work, we do not need to maintain any additional models (e.g., logistic regression or discriminator networks) or to solve min max problems which are often difficult to optimize. We report state-of-the-art results for the adaptation from both GTAV and SYNTHIA to Cityscapes, two popular settings in unsupervised domain adaptation for semantic segmentation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The objective of semantic segmentation is to assign a semantic label to every pixel of an image. Over the past few years, a great amount of effort has been made by the community to tackle this problem <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b20">21]</ref>, leading to sophisticated and high-performing deep convolutional neural networks as the main solution. However, to collect and label images for training such networks is a very daunting work <ref type="bibr" target="#b5">[6]</ref>. To alleviate the heavy annotation burden, a promising alternative is to employ photo-realistic simulators to efficiently collect and label training data. Richter et al. <ref type="bibr" target="#b27">[28]</ref> use the GTAV game engine to aid user annotations, resulting a dataset of 25 thousand synthetic urban scene im-(a) Synthetic images with labeling-free pixel-wise groundtruth annotations.  The clear visual mismatch between the synthetic (source) domains and real (target) domains (cf. <ref type="figure" target="#fig_4">Figs. 3  and 4</ref>), however, inevitably causes significant performance degradation when one applies the model trained on the source domain to the real images of the target domain.</p><p>In order to better take advantage of the synthetic imagery for the semantic segmentation of real scenes, we pro-arXiv:1908.09547v1 [cs.CV] <ref type="bibr" target="#b25">26</ref> Aug 2019 pose a novel domain adaptation approach by drawing an intriguing connection between two previous works: curriculum domain adaptation <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b37">38]</ref> and self-training <ref type="bibr" target="#b42">[43]</ref>. This connection naturally leads to self-motivated pyramid curriculums and a new training algorithm for the cross-domain adaptation of semantic segmentation networks. Compared with the prevalent adversarial training methods in domain adaptation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b32">33]</ref>, our approach gives rise to results on par with or better than theirs and yet is lighter-weight, without the need of learning extra discriminator nets, and easier to optimize, without the need of solving any min max problems. More importantly, it outperforms both the original curriculum adaptation <ref type="bibr" target="#b37">[38]</ref> and the original self-training method <ref type="bibr" target="#b42">[43]</ref>.</p><p>In particular, we view self-training from the perspective of curriculum domain adaptation. They share the same algorithmic format. On the one hand, the self-training alternates between two sub-tasks: 1) estimating pseudo labels for the target domain's pixels and 2) updating the weights of the segmentation network by using both the source labels and the pseudo target labels. On the other hand, the curriculum adaptation first 1) constructs a curriculum, i.e., infers properties of the target domain in the form of frequency distributions of the class labels over an image (or image region) and then 2) updates the network's weights using the source labels and the target domain's properties. Due to this analogy, we may view the pseudo labels in selftraining as one of the properties about the target domain. More interestingly, the second steps of the two works share exactly the same form in math -a cross-entropy loss between a frequency distribution / pseudo label and a differentiable function of the network's predictions.</p><p>Immediately, our approach follows the above analogy. We add the pseudo labels in self-training to the curriculum as the finest layer of properties about the target domain images. On top of that, we build a pyramid of which a layer comprises image regions of a certain size. This pyramid design resembles the original curriculum domain adaptation work, in which the frequency distributions are counted over a global image and some superpixels, respectivelyin other words, a simple two-layer pyramid.</p><p>In addition to enriching the original curriculum by the pseudo labels, we also improve it in two ways. One is to replace the superpixels by small squared regions to significantly save computation cost. The other is to infer the target domain properties -label distributions over the squared regions and full target domain images -by the semantic segmentation network itself. In each iteration of the training phase, we infer those properties from the network's pxielwise predictions over the target domain images and then use a loss defined over those properties to update the network by backpropagation.</p><p>Our main contribution is two-fold. One is that we pro-vide a new insight connecting the self-training for adapting segmentation networks <ref type="bibr" target="#b42">[43]</ref> and the curriculum adaptation method <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b37">38]</ref>. The other is that, inspired by the connection, we propose a novel self-motivated pyramid curriculum for the domain adaptation of semantic segmentation networks. Extensive experiments show that it outperforms either <ref type="bibr" target="#b42">[43]</ref> or <ref type="bibr" target="#b37">[38]</ref> individually. Moreover, it is on par with or better than state-of-the-art adversarial adaptation methods without the need of maintaining an extra discrminator network or carefully tuning the optimization procedure for min max problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Semantic segmentation. Semantic segmentation is the task that assigns labels in pixel level for an image which plays a vital role in lots of tasks including autonomous driving, disease detection, etc. In the following, we briefly review some of the works with a focus on CNN-based methods. Driven by the powerful deep neural networks <ref type="bibr" target="#b15">[16]</ref>, pixel-level prediction tasks achieve great progress mostly following the design of replacing the softmax layer in classification with the pixel-wise softmax <ref type="bibr" target="#b22">[23]</ref>. To enlarge the receptive fields and feature resolutions, methods of [1, 2, 3, 41] employ dilated convolution. To utilize different context information and multiple features, some extend the dilated convolution to a pyramid <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref> or resize it with multiple sizes <ref type="bibr" target="#b40">[41]</ref>.</p><p>Domain adaptation. A basic assumption in conventional machine learning is that the training and test data are drawn i.i.d. from the same underlying distribution. However, this does not always hold in real world scenarios, resulting in significant performance drops when the training and test data exist distribution mismatches. Domain adaptation aims to rectify this mismatch and make the model generalize well onto the test domain. Domain adaptation has been mostly addressed for image classification problems in computer vision <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>. Recent works start to study deep neural networks including learning domain-invariant models <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b11">12]</ref> and target specific models <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>Domain adaptation for semantic segmentation. Recent work on domain adaptation mainly proceeds in two directions. One is based on a curriculum learning strategy. Zhang et al. first learns to solve easy tasks in the target domain and then use them to regularize semantic segmentation <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b37">38]</ref>. Dai et al. construct a curriculum by simulating foggy images of different fog densities <ref type="bibr" target="#b6">[7]</ref>. In this work, we also view the self-training <ref type="bibr" target="#b42">[43]</ref> as a curriculum-style domain adaptation method. The other line of work is to reduce the domain shift in feature space or output space and tries to seek a better way to align both domains in an intermediate layer <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b25">26]</ref>. In <ref type="bibr" target="#b17">[18]</ref>  <ref type="figure">Figure 2</ref>: Overview of our self-motivated pyramid curriculum domain adaptation (PyCDA) approach to segmentation.</p><p>ther propose a global and class-specific feature alignment approach guided by the soft pseudo labels in the target domain. In <ref type="bibr" target="#b34">[35]</ref>, Tsai et al. proposed to align both domains at the structured output space level. In <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b39">40]</ref>, they focus on utilizing spatial information to help the discriminator distinguish the domains better. In <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b41">42]</ref>, they focus on solving the problem of class boundary or outliers in the domain adaptation. In <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b32">33]</ref>, they propose to learn domain adaptive segmentation networks through directly translating the source images to the target ones at the pixel level. We refer readers to <ref type="bibr" target="#b37">[38,</ref><ref type="bibr">Section 5]</ref> for a more thorough review.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>In this section, we first reveal the connection between the curriculum domain adaptation (CDA) <ref type="bibr" target="#b37">[38]</ref> and the selftraining (ST) for adaptation <ref type="bibr" target="#b42">[43]</ref>. This connection naturally leads to the training algorithm of this paper, dubbed self-motivated pyramid curriculum domain adaptation (Py-CDA), for the semantic segmentation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">CDA vs. ST</head><p>Denote by I t ∈ R H×W a target domain image and Y t ∈ R H×W ×C the corresponding output of a semantic segmentation network, where H, W , and C are respectively the height, width, and number of possible classes of the input image. Most neural segmentation networks employ a pixel-wise softmax function at the output layer so thatŶ t (i, j) ∈ R C is a probability vector satisfying cŶ t (i, j, c) = 1, ∀i, j andŶ t (i, j, c) ≥ 0, ∀i, j, c. Similarly, denote by I s andŶ s a source domain image and the corresponding prediction by a network. In unsupervised domain adaptation, the groundtruth labels {Y s } of the source domain (S) are given but the learner has no access to the labels of the target domain (T ). CDA <ref type="bibr" target="#b37">[38]</ref> learns a semantic segmentation network by minimizing the following objective function:</p><formula xml:id="formula_0">min s∈S L(Y s ,Ŷ s ) + λ t∈T k∈P 1 t C(p k t ,p k t )<label>(1)</label></formula><p>where the first term sums up pixel-wise cross-entropy losses over the source domain images (s ∈ S), each summand of the second term is a cross-entropy loss over a target domain image (t ∈ T ) between two label distributions which indicate the proportion of each class in the image t or in a region of it, and the set P 1 t collects all the label distributions for the target image t. As a concrete example, a desired label distribution p 0 t over a target domain image I t is calculated by</p><formula xml:id="formula_1">p 0 t (c) = 1 W H W i=1 H j=1 Y t (i, j, c), ∀c,<label>(2)</label></formula><p>and the propertyp 0 t predicted by the segmentation network, which can be obtained in a similar way as above from the network predictionŶ t , is supposed to match the desired label distribution p 0 t . In this example, the label distribution over a full image captures a global property. The others calculated within an image region capture the local properties of a target domain image. Accurate readers may wonder how to estimate the desired properties p k t , k ∈ P 1 t , in practice because the target labels are actually unknown in unsupervised domain adaptation; we explain it below.</p><p>The name of CDA attributes to the following easy-todifficult curriculum. Compared with the pixel-wise predictions, it is relatively easy to obtain these label distributions, especially when the images are about urban scenes which share common objects and spatial layouts. Zhang et al. <ref type="bibr" target="#b37">[38]</ref> train separate logistic regression models and support vector machines to estimate these distributions p k t , k ∈ P 1 t . In this paper, however, we estimate them in a self-training fashion by the segmentation network itself. Self-training (ST) <ref type="bibr" target="#b42">[43]</ref> considers the unknown labels of the target domain images as latent variables. It alternates between 1) inferring the values of the latent target labels and 2) updating the network's weights. The second step essentially solves the following problem:</p><formula xml:id="formula_2">min s∈S L(Y s ,Ŷ s ) + λ t∈T (i,j)∈P 2 t C(Y t (i, j),Ŷ t (i, j)),</formula><p>where P 2 t is the set of pixels of the target image I t whose pseudo labels {Y t (i, j)} are inferred -the other pixels are with null labels because the network predictions at those positions are probably below a certain threshold. Connection between the two. The objective functions of CDA and ST are remarkably alike. The only difference between them is on the sets P 1 t and P 2 t , where the former collects label distributions over the global target domain image I t and some of its local regions, and the latter is the pixels of I t that are pseudo-labeled by the first step in ST. What if we union the two sets, i.e., P 1 t ∪ P 2 t ? They indeed seem like mutually complementary. Thanks to CDA, we conjecture that the finest-grained pixel-level pseudo labels P 2 t may be enhanced by the label distributions of the fine-grained image regions and the coarsest-grained full image. Due to ST, we reasonably expect the label distributions may be derived from the segmentation network itself, without the need of resorting to any additional models (e.g., logistic regression or SVM used in <ref type="bibr" target="#b37">[38]</ref>). Following this line of reasoning, we devise our approach as the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Self-motivated pyramid CDA (PyCDA)</head><p>We propose a self-motivated pyramid curriculum for the domain adaptation (PyCDA) of semantic segmentation tasks. The idea faithfully follows the insight above, i.e., we union the two sets P 1 t ∪ P 2 t used in CDA and ST, respectively. This union results in a pyramid with at least three layers: pixels on the bottom layer, small image regions in the middle, and a full image on the top. Note that this pyramid is built for a target domain image, not source domain images. Similarly to CDA, we will infer the label distribution over the top-layer full image and label distributions, or more concretely one-hot vectors, for the middle-layer small image regions. Similarly to ST, we want to assign pseudo labels to some of the bottom-layer pixels. Please refer to <ref type="figure">Fig. 2</ref> for an overview of the proposed PyCDA approach. Superpixels vs. pixel squares. Before describing the main approach, we first discuss how to partition an image into small regions. Zhang et al. employ in their original CDA work <ref type="bibr" target="#b37">[38]</ref> non-overlapping superpixels, which incur additional computation overhead. We replace the superpixels by overlapped squares instead. While the pixel squares do not track object boundaries, their squared shape enables fast GPU computation as demonstrated beblow when we infer the label distributions for them. Moreover, as the squares are sufficiently small, most of them each cover pixels of the same class. In the experiments, we use the squares of 4 × 4 and 8 × 8 for the middle layers of the pyramid in PyCDA.</p><p>Self-motivated inference of target domain properties. In order to estimate the pseudo label Y t (i, j) of a target domain pixel (i, j), we use as simple as a thresholding method. Denoting by c ← arg max cŶt (i, j, c), we have</p><formula xml:id="formula_3">Y t (i, j) = c ifŶ t (i, j, c ) &gt; 0.5 null otherwise<label>(3)</label></formula><p>whereŶ t is the output of a segmentation network at a training iteration. We say a pixel survives this step and will add it to the bottom layer of the pyramid curriculum, if its pseudo label is not null. Alternatively, one may use the self-paced policy design <ref type="bibr" target="#b42">[43]</ref> to estimate the pseudo labels.</p><p>We employ a similar strategy to decide to which class each pixel square belongs to. Denote by (i 0 , j 0 ) a square (e.g., the coordinates of the top-left corner of this square). We take an average pooling of the network's predictionŝ Y square (i 0 , j 0 , c) ← mean (i,j)∈square (Ŷ t (i, j, c)) (4) and then threshold the pooled valueŶ square (i 0 , j 0 , c) to determine the label of the square, i.e., by replacingŶ t witĥ Y square in eq. (3). In implementation, we leverage efficient GPU operations by adding an average pooling layer after the network's output layer. This label for a pixel square can be converted to a one-hot vector and regarded as a label distribution over this square.</p><p>Finally, we want to estimate the label distribution over a full target domain image. Zhang et al. <ref type="bibr" target="#b37">[38]</ref> gives a few candidate methods for this sub-task, and we use the most computation-efficient one in this work. Particularly, no matter for which target domain image, we transfer to it the mean of the label distributions of all source domain images. As shown in the the experiments of <ref type="bibr" target="#b37">[38]</ref>, this actually gives rise to results on par with learning a logistic regression model from the source domain probably because the images of urban scenes share common objects and layouts. For the target domains beyond urban scenes, alternative sophisticated algorithms are desired to reliably estimate the label distributions for the target domain images.</p><p>PyCDA. We are now ready to present the overall objective function of the proposed PyCDA approach:  <ref type="figure">Figure 3</ref>: Sample images from the GTAV <ref type="bibr" target="#b27">[28]</ref> and SYNTHIA <ref type="bibr" target="#b28">[29]</ref> datasets.</p><formula xml:id="formula_4">min 1 |S| s∈S L(Y s ,Ŷ s ) + λ 1 |T | t∈T C p 0 t ,p 0 t + λ 2 |P| (t,k)∈P C Y k t ,Ŷ k t ,<label>(5)</label></formula><p>where λ 1 and λ 2 are pre-defined trade-off parameters, and P = {(t, k)|∀t ∈ T, k ∈ P 1 t ∪ P 2 t } denotes the label distributions (resp., pseudo labels) over the squared regions (resp., pixels) of the target images. Note that in Eq. (5), the second term is a cross-entropy loss defined on the label distributions over target images, and the last term takes care of the pixels and squared regions of the target domain. In the experiments, we set λ 1 = 1, λ 2 = 0.5 and tune other free parameters (e.g., learning rate) via a validation set. Effect of the pyramid curriculum. We have described a pyramid curriculum which consists of the global target domain images on the top, pixels at the bottom, and pixel squares in between. From the CDA point of view <ref type="bibr" target="#b37">[38]</ref>, the label distributions over the top-layer images macroscopically hint the network how to update its predictions while the label distributions over the middle-layer pixel squares microscopically indicate the network where to update. The pseudo labels of bottom-layer pixels give even more precise supervision to the network. From the ST perspective <ref type="bibr" target="#b42">[43]</ref>, the middle-layer pixel squares may be viewed as a way of consensus voting so that the pseudo labels (of the squares) can be estimated more reliably than thresholding the prediction at an isolated pixel. The label distributions over the target domain images act like a prior over the classes, playing a similar role to the class-balanced formulation in <ref type="bibr" target="#b42">[43]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we conduct extensive experiments on simulation-to-real unsupervised domain adaptation for the semantic segmentation task. We compare the proposed Py-CDA with several state-of-the-art methods. A majority of them uses adversarial training to bring closer the source and target domains on the feature level (ROAD <ref type="bibr" target="#b3">[4]</ref>), on both features and pixels (FCAN <ref type="bibr" target="#b39">[40]</ref>, CyCADA <ref type="bibr" target="#b16">[17]</ref>), on the output maps (OutputAdapt <ref type="bibr" target="#b34">[35]</ref>, CLAN <ref type="bibr" target="#b25">[26]</ref>), and combining adversarial learning with entropy minimization (AD-VENT <ref type="bibr" target="#b35">[36]</ref>). In contrast, our PyCDA approach, along with CDA <ref type="bibr" target="#b37">[38]</ref>, and ST <ref type="bibr" target="#b42">[43]</ref>, adapts the neural networks by posterior regularization instead. The difference between the adversarial training methods and ours is significant for practical applications because the min max adversarial problems are often hard to optimize and have to maintain an additional discrimination network. Please note that we only compare results obtained from single models without using any ensemble strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental setup</head><p>We follow the experimental setup of previous works <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b42">43]</ref> and use the standard benchmark settings (i.e., "GTAV to Cityscapes" and "SYNTHIA to Cityscapes") in the experiments.</p><p>• Cityscapes [6] is a popular dataset to benchmark semantic segmentation models. The images are collected in the real world by vehicle-carried cameras. This dataset focuses on urban scenes, covering 50 cities in Germany and nearby countries. Its official data partition has 2,975 images in the training set, 500 in the validation set and 1,175 in the test set. In total, 19 classes of the semantic labels are compatible with GTAV and 16 with SYNTHIA. • GTAV <ref type="bibr" target="#b27">[28]</ref> is a large-scale dataset with 24,966 synthetic urban scene images collected from a nearrealistically rendered computer game called Grand Theft Auto V (GTA or GTAV). We consider all the 19 semantic classes of GTAV for the adaptation to Cityscapes. • SYNTHIA <ref type="bibr" target="#b28">[29]</ref> is another synthetic image dataset and provides a particular subset, called SYNTHIA- - RANDCITYSCAPES, to pair with Cityscapes. This subset contains 9,400 images which are automatically labeled with 12 object categories, one void class, and some unnamed classes. Following <ref type="bibr" target="#b37">[38]</ref>, we manually align four unnamed classes with their counterparts in Cityscapes, forming 16 common classes between SYNTHIA and Cityscapes.</p><formula xml:id="formula_5">- - - - - - - - - - - - - - - - - -</formula><p>In this work, we consider Cityscapes containing real images as the target domain, while GTAV and SYNTHIA are respectively used as the source domain (sample images from the two domains are shown in <ref type="figure">Fig. 3</ref>). Since the groundtruth labels from the official test set of Cityscapes are not publicly available, by strictly following <ref type="bibr" target="#b37">[38]</ref>, we take the official validation set as our test set for final evaluation. Also, 500 images are randomly selected from the official training set for validation, and the remaining 2,475 images are served as unlabeled training data from the target domain. Evaluation. We directly use the evaluation code released alongside with Cityscapes, where PASCAL VOC intersection-over-union (IoU) <ref type="bibr" target="#b9">[10]</ref> is used as the evaluation metric. Specifically, for each class, we have IoU = TP TP+FP+FN , where TP, FP and FN are the numbers of true positive, false positive and false negative pixels, respectively, over the whole test set. In addtional to the per-class IoUs, we also report the mean of those IoUs (i.e., mIoU) over all classes. Note that in the experiments, we resize the images before feeding them to the segmentation network, so we resize the output segmentation mask back to the original size when running the evaluation code. Implementation details. Since existing state-of-the-art methods use different base segmentation networks as their backbones, we employ the following ones for a wider range of comparison: 1) FCN8s <ref type="bibr" target="#b22">[23]</ref> with VGG-16 <ref type="bibr" target="#b33">[34]</ref>; 2) ResNet-38 <ref type="bibr" target="#b36">[37]</ref>; and 3) PSP-Net <ref type="bibr" target="#b40">[41]</ref> with ResNet-101 <ref type="bibr" target="#b15">[16]</ref>. All the base networks are pre-trained on ImageNet <ref type="bibr" target="#b30">[31]</ref>. Regarding the data pre-processing, we firstly resize images to the same width (1024) while preserving the original aspect ratios. During training, we randomly crop regions and feed them to the network. During testing, we feed the whole images (whose width are 1024) to the network. During evaluation, we resize the output segmentation mask back to the original image size (2048 × 1024) in order to calculate the mIoUs. Regarding the training pipeline, we firstly train the model in the source images with 30000 iterations. And then we fine-tune the model using our PyCDA framework with another 30000 iterations. The training is optimized SGD with momentum of 0.9. Using the validation data, we set our initial learning rate = 0.016 and decrease it ten times in the fine-tuning stage. In the test stage, we apply adabn <ref type="bibr" target="#b19">[20]</ref> that change the mean and variance of batch-normalization layers, which were computed over the images of both do- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results on "GTAV to Cityscapes"</head><p>We report the results of unsupervised domain adaptation from GTAV to Cityscapes compared with existing state-ofthe-arts in <ref type="table" target="#tab_1">Table 1</ref>. Note that all the prior methods but CDA use the whole training set of Cityscapes in training -implying their models see more target images than oursand do not leave out a separate validation set for model selection. Unlike the others, FCAN directly works with the original high-resolution images. OutputAdapt (ResNet-101) pre-train the model using both ImageNet <ref type="bibr" target="#b30">[31]</ref> and MS COCO <ref type="bibr" target="#b21">[22]</ref>).</p><p>We draw the following observations. First, all domain adaptation methods significantly outperform the respective "source only" baselines which train segmentation networks by using only synthetic source images. Such results clearly demonstrate the benefit of explicitly using domain adaptation techniques to improve the transfer from synthetic images to real ones. Moreover, comparing our full approach (PyCDA) with the existing ones in terms of mIoU, PyCDA gives rise to the best results thus far for the adaptation from GTAV to Cityscapes. Note that the second best FCAN is a two-stage method with style transfer on the image pixel level and then an adversarial training of the features. The style transfer stage runs extremely slow, consuming about one to two hours per image. PyCDA is orthogonal to both the image style transfer and adversarial training, so our results could be further improved if we apply the style transfer to the images of the two domains.</p><p>When compared with the distribution matching methods (FCN-wild <ref type="bibr" target="#b17">[18]</ref>, ROAD <ref type="bibr" target="#b3">[4]</ref>, OutputAdapt <ref type="bibr" target="#b34">[35]</ref> and FCAN <ref type="bibr" target="#b39">[40]</ref>), PyCDA is particularly good at the dominant classes, such as "road", "building", "vegetation", and "car". Meanwhile, PyCDA is better than CBST <ref type="bibr" target="#b42">[43]</ref> at classifying small objects, such as "rider", "wall", and "fence", etc. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results on "SYNTHIA to Cityscapes"</head><p>To further validate the effectiveness of PyCDA, we also conduct experiments by using SYNTHIA as the source domain. FCN8s with VGG-16 and PSP-Net with ResNet-101 as employed as backbones to evaluate different methods. The IoU results on this setting are summarized in <ref type="table" target="#tab_3">Table 2</ref>. From the results, We can clearly see that our PyCDA again outperforms the existing state-of-the-arts by a large margin when using different backbones, and also similar observations can be drawn as in the "GTAV to Cityscapes" setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation study</head><p>To analyze the effectiveness of our PyCDA, we conduct ablation study by using the above two settings, i.e., taking GTAV and SYNTHIA as the source domains, respectively. Note that PyCDA connects curriculum domain adapation (CDA) with self-training (ST), and it can be viewed as a pyramid constructed by multiple levels from pixels (bottom) to label distributions of full images (top). In this case, we evaluate PyCDA by comparing its counterparts which eliminate different levels. Specifically, we denote "top + bottom" and "top + pixel squares" as connecting CDA with ST at the levels of pixels and pixel squares, respectively. From <ref type="table" target="#tab_4">Table 3</ref>, we can see that connect CDA with ST in either pixel level or pixel squares level outperform both CDA and ST in a large margin, which demonstrates the effectiveness of connecting both methods. And PyCDA, which considers both pixels and pixel squares, gets further boosted by a fairly big margin.</p><p>Superpixels vs. pixel squares. As discussed in Section 3, it is time-consuming to generate superpixels (about 3.6s per image). In order to avoid the computation overhead, we switch to pixel squares in our PyCDA. As turned out in <ref type="table" target="#tab_4">Table 3</ref>, the mIoU performance of using pixel squares achieves comparable results to that of using superpixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Qualitatively comparing GTAV and SYNTHIA</head><p>One may wonder how the two source domains of synthetic imagery differ from each other and what effects the difference could cause on the target domain of real images. <ref type="figure">Fig. 3</ref> shows some example images of the two domains. While GTAV images are vehicle-centric, there are more diverse views in SYNTHIA. In <ref type="figure" target="#fig_4">Fig. 4</ref>, we give some qualitative results obtained by our PyCDA models adapted from GTAV and SYNTHIA, respectively. In general, the segmentation results of PyCDA adapted from GTAV is better than that adapted from SYNTHIA, especially for the dominant "road" class. This observation can also be verified by the superior IoU for "road" (90.5%, <ref type="table" target="#tab_1">Table 1</ref>) of the Py-CDA model trained based on GTAV. Given all those results, we believe GTAV is visually more similar to the real selfdriving scenes than SYNTHIA in terms of both visual appearances and spatial layouts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose a novel method called self-motivated pyramid curriculum domain adaptation (PyCDA) for pixel-level semantic segmentation. PyCDA provides a new perspective of insight, which connects self-training for adapting segmentation networks and curriculum domain adaptation. More specifically, PyCDA constructs a curriculum based on a pyramid of pixel squares at different sizes in each real image, including the image itself as the top layer and pixels as the bottom layer. This curriculum is self-motivated, because the label distributions over the pyramid are derived from the same network of the previous iteration. By forming such a pyramid of pixel squares, we are able to better preserve and capture local information for objects appearing at different scales. Extensive experiments on two benchmark settings (i.e., "GTAV to Cityscapes" and "SYNTHIA to Cityscapes") clearly demonstrate the effectiveness of Py-CDA when compared with other state of the arts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(b) Segmentation results of a real image with and without adaptation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Unsupervised domain adaptation for semantic segmentation. The segmentation results for real images can be significantly improved by explicit domain adaptation techniques when we adapt a segmentation model trained using synthetic imagery. ages labeled in only 49 hours (about 7 seconds per image).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>this work: Training (no labels) Validation Test</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>(a) Input image (b) Groundtruth (c) Adapted from GTAV (d) Adapted from SYNTHIA Some qualitative segmentation results on the target domain. (a) displays the target images, and their corresponding groundtruth segmentation masks are shown in (b). (c) and (d) display the segmentation results obtained from our PyCDA models adapted from GTAV and SYNTHIA, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison results (in %) adapting from GTAV to Cityscapes. All the prior methods but CDA use the whole training set of Cityscapes in training -thus their models see more target images than ours -and do not leave out a separate validation set for model selection. Unlike the others, FCAN directly works with the original image size. OutputAdapt (ResNet-101) pre-trains the model using both ImageNet<ref type="bibr" target="#b30">[31]</ref> and MS COCO<ref type="bibr" target="#b21">[22]</ref>. Method road sdwk bldng wall fence pole light sign veg trrn sky psn rider car truck bus train moto bike mIoU</figDesc><table><row><cell>Network VGG</cell><cell cols="2">Source only [32] Source only (ours) 56.0 12.2 71.6 8.5 17.8 19.5 14.5 3.1 73.2 3.8 46.0 38.8 4.4 70.7 15.1 2.5 2.2 1.4 0.1 24.3 25.9 10.9 50.5 3.3 12.2 25.4 28.6 13.0 78.3 7.3 63.9 52.1 7.9 66.3 5.2 7.8 0.9 13.7 0.7 24.9 CDA [38] 72.9 30.0 74.9 12.1 13.2 15.3 16.8 14.1 79.3 14.5 75.5 35.7 10.0 62.1 20.6 19.0 0.0 19.3 12.0 31.4 ST [43] 83.8 17.4 72.1 14.3 2.9 16.5 16.0 6.8 81.4 24.2 47.2 40.7 7.6 71.7 10.2 7.6 0.5 11.1 0.9 28.1 CBST [43] 66.7 26.8 73.7 14.8 9.5 28.3 25.9 10.1 75.5 15.7 51.6 47.2 6.2 71.9 3.7 2.2 5.4 18.9 32.4 30.9</cell></row><row><cell>-16</cell><cell>ROAD [4]</cell><cell>85.4 31.2 78.6 27.9 22.2 21.9 23.7 11.4 80.7 29.3 68.9 48.5 14.1 78.0 19.1 23.8 9.4 8.3 0.0 35.9</cell></row><row><cell></cell><cell>CyCADA [17]</cell><cell>85.2 37.2 76.5 21.8 15.0 23.8 22.9 21.5 80.5 31.3 60.7 50.5 9.0 76.9 17.1 28.2 4.5 9.8 0.0 35.4</cell></row><row><cell></cell><cell>CLAN [26]</cell><cell>88.0 30.6 79.2 23.4 20.5 26.1 23.0 14.8 81.6 34.5 72.0 45.8 7.9 80.5 26.6 29.9 0.0 10.7 0.0 36.6</cell></row><row><cell></cell><cell>ADVENT [36]</cell><cell>86.8 28.5 78.1 27.6 24.2 20.7 19.3 8.9 78.8 29.3 69.0 47.9 5.9 79.8 25.9 34.1 0.0 11.3 0.3 35.6</cell></row><row><cell></cell><cell>PyCDA (ours)</cell><cell>86.7 24.8 80.9 21.4 27.3 30.2 26.6 21.1 86.6 28.9 58.8 53.2 17.9 80.4 18.8 22.4 4.1 9.7 6.2 37.2</cell></row><row><cell></cell><cell>Source only [43]</cell><cell>70.0 23.7 67.8 15.4 18.1 40.2 41.9 25.3 78.8 11.7 31.4 62.9 29.8 60.1 21.5 26.8 7.7 28.1 12.0 35.4</cell></row><row><cell>ResNet</cell><cell>ST [43]</cell><cell>90.1 56.8 77.9 28.5 23.0 41.5 45.2 39.6 84.8 26.4 49.2 59.0 27.4 82.3 39.7 45.6 20.9 34.8 46.2 41.5</cell></row><row><cell>-38</cell><cell>CBST [43]</cell><cell>86.8 46.7 76.9 26.3 24.8 42.0 46.0 38.6 80.7 15.7 48.0 57.3 27.9 78.2 24.5 49.6 17.7 25.5 45.1 45.2</cell></row><row><cell></cell><cell>PyCDA (ours)</cell><cell>92.3 49.2 84.4 33.4 30.2 33.3 37.1 35.2 86.5 36.9 77.3 63.3 30.5 86.6 34.5 40.7 7.9 17.6 35.5 48.0</cell></row><row><cell></cell><cell>Source only [35]</cell><cell>75.8 16.8 77.2 12.5 21.0 25.5 30.1 20.1 81.3 24.6 70.3 53.8 26.4 49.9 17.2 25.9 6.5 25.3 36.0 36.6</cell></row><row><cell></cell><cell cols="2">Source only (ours) 73.8 16.0 66.3 12.8 22.3 29.0 30.3 10.2 77.7 19.0 50.8 55.2 20.4 73.6 28.3 25.6 0.1 27.5 12.1 34.2</cell></row><row><cell></cell><cell>ROAD [4]</cell><cell>76.3 36.1 69.6 28.6 22.4 28.6 29.3 14.8 82.3 35.3 72.9 54.4 17.8 78.9 27.7 30.3 4.0 24.9 12.6 39.4</cell></row><row><cell>ResNet</cell><cell cols="2">OutputAdapt [35] 86.5 36.0 79.9 23.4 23.3 23.9 35.2 14.8 83.4 33.3 75.6 58.5 27.6 73.7 32.5 35.4 3.9 30.1 28.1 42.4</cell></row><row><cell>-101</cell><cell>FCAN [40]</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>27.1 79.6 27.3 23.3 28.3 35.5 24.2 83.6 27.4 74.2 58.6 28.0 76.2 33.1 36.7 6.7 31.9 31.4 43.2 ADVENT [36] 87.6 21.4 82.0 34.8 26.2 28.5 35.6 23.0 84.5 35.1 76.2 58.6 30.7 84.8 34.2 43.4 0.4 28.4 35.3 44.8 PyCDA (ours) 90.5 36.3 84.4 32.4 28.7 34.6 36.4 31.5 86.8 37.9 78.5 62.3 21.5 85.6 27.9 34.8 18.0 22.9 49.3 47.4</figDesc><table><row><cell></cell><cell>46.6</cell></row><row><cell>CLAN [26]</cell><cell>87.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison results (in %) adapting from SYNTHIA to Cityscapes. mIoU* denotes the mean IoU over 13 classes excluding those marked with *. All the prior methods but CDA use the whole training set of Cityscapes in training -thus their models see more target images than ours -and do not leave out a separate validation set for model selection.<ref type="bibr" target="#b10">11</ref>.6 62.3 10.7 0.0 22.8 4.3 15.3 68.0 70.8 49.7 6.4 60.5 11.8 2.6 4.3 25.4 28.7</figDesc><table><row><cell cols="2">Network Method</cell><cell cols="7">road sdwk bldng wall* fence* pole* light sign veg sky psn rider car bus mcycl bcycl mIoU mIoU*</cell></row><row><cell></cell><cell cols="3">Source only [4] 4.7 Source only (ours) 50.1 20.0 49.4 0.0</cell><cell cols="5">0.0 16.3 0.0 0.0 69.9 54.2 43.9 4.7 43.1 6.1 0.1 0.1 22.4 26.3</cell></row><row><cell></cell><cell>CDA [38]</cell><cell cols="2">57.4 23.1 74.7 0.5</cell><cell cols="5">0.6 14.0 5.3 4.3 77.8 73.7 45.0 11.0 44.8 21.2 1.9 20.3 29.7 35.4</cell></row><row><cell>VGG -16</cell><cell>ST [43] CBST [43] ROAD [4]</cell><cell cols="7">0.2 14.5 53.8 1.6 69.6 28.7 69.5 12.1 0.1 25.4 11.9 13.6 82.0 81.9 49.1 14.5 66.0 6.6 3.7 32.4 35.4 40.4 0.0 18.9 0.9 7.8 72.2 80.3 48.1 6.3 67.7 4.7 0.2 4.5 23.9 27.8 77.7 30.0 77.5 9.6 0.3 25.8 10.3 15.6 77.6 79.8 44.5 16.6 67.8 14.5 7.0 23.8 36.2 41.8</cell></row><row><cell></cell><cell>CLAN [26]</cell><cell>80.4 30.7 74.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1.4 8.0 77.1 79.0 46.5 8.9 73.8 18.2 2.2 9.9</cell><cell>-</cell><cell>39.3</cell></row><row><cell></cell><cell>ADVENT [36]</cell><cell cols="2">67.9 29.4 71.9 6.3</cell><cell cols="5">0.3 19.9 0.6 2.6 74.9 74.9 35.4 9.6 67.8 21.4 4.1 15.5 31.4 36.6</cell></row><row><cell></cell><cell>PyCDA (ours)</cell><cell cols="2">80.6 26.6 74.5 2.0</cell><cell cols="5">0.1 18.1 13.7 14.2 80.8 71.0 48.0 19.0 72.3 22.5 12.1 18.1 35.9 42.6</cell></row><row><cell></cell><cell>Source only [35]</cell><cell>55.6 23.8 74.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>6.1 12.1 74.8 79.0 55.3 19.1 39.6 23.3 13.7 25.0</cell><cell>-</cell><cell>38.6</cell></row><row><cell></cell><cell cols="3">Source only (ours) 55.6 22.7 68.6 4.3</cell><cell cols="5">0.1 23.0 5.6 9.1 77.2 75.9 54.7 8.7 81.5 23.9 8.4 8.8 33.0 38.5</cell></row><row><cell>ResNet</cell><cell cols="2">OutputAdapt [35] 84.3 42.7 77.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>4.7 7.0 77.9 82.5 54.3 21.0 72.3 32.2 18.9 32.3</cell><cell>-</cell><cell>46.7</cell></row><row><cell>-101</cell><cell>CLAN [26]</cell><cell>81.3 37.0 80.1</cell><cell>-</cell><cell>-</cell><cell cols="2">-16.1 13.7 78.2 81.5 53.4 21.2 73.0 32.9 22.6 30.7</cell><cell>-</cell><cell>47.8</cell></row><row><cell></cell><cell>ADVENT [36]</cell><cell cols="2">85.6 42.2 79.7 8.7</cell><cell cols="5">0.4 25.9 5.4 8.1 80.4 84.1 57.9 23.8 73.3 36.4 14.2 33.0 41.2 48.0</cell></row><row><cell></cell><cell>PyCDA (ours)</cell><cell cols="7">75.5 30.9 83.3 20.8 0.7 32.7 27.3 33.5 84.7 85.0 64.1 25.4 85.0 45.2 21.2 32.0 46.7 53.3</cell></row><row><cell cols="6">mains during training, to the mean and variance over the</cell><cell></cell><cell></cell></row><row><cell cols="2">target domain only.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation study of PyCDA (in mIoU%).</figDesc><table><row><cell>Experiment Setting</cell><cell cols="4">GTAV VGG-16 Res-101 VGG-16 Res-101 SYNTHIA</cell></row><row><cell>Source only</cell><cell>24.3</cell><cell>34.2</cell><cell>22.4</cell><cell>33.0</cell></row><row><cell>Top</cell><cell>28.0</cell><cell>42.0</cell><cell>28.7</cell><cell>40.7</cell></row><row><cell>CDA [38]</cell><cell>29.7</cell><cell>-</cell><cell>31.4</cell><cell>-</cell></row><row><cell>Bottom</cell><cell>32.6</cell><cell>40.6</cell><cell>31.3</cell><cell>41.0</cell></row><row><cell>ST [43]</cell><cell>28.1</cell><cell>-</cell><cell>23.9</cell><cell>-</cell></row><row><cell>top + bottom</cell><cell>34.9</cell><cell>46.2</cell><cell>35.1</cell><cell>44.8</cell></row><row><cell>top + pixel squares</cell><cell>35.4</cell><cell>46.3</cell><cell>35.4</cell><cell>45.6</cell></row><row><cell>top + superpixels</cell><cell>35.2</cell><cell>46.3</cell><cell>35.2</cell><cell>45.9</cell></row><row><cell>PyCDA</cell><cell>37.2</cell><cell>47.4</cell><cell>35.9</cell><cell>46.7</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work is supported by National Natural Science Foundation of China (Grant No. 61772118).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix 1. Number of middle layers</head><p>We report in <ref type="table">Table 1</ref> the experimental results of different numbers of the middle layers for adapting from GTAV to Cityscapes. Here we use ResNet-101 as the backbone network. The second column corresponds to the "top + bottom" result in PyCDA -the sixth row in <ref type="table">Table 3</ref> of the main paper.</p><p>We mainly draw two observations. One is that the middle layers do improve the overall performance. The other is that the results are relatively consistent over different numbers of the middle layers, though adding layers with too big pixel squares (e.g., larger than 128 × 128) could harm both accuracy and training speed.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Road: Reality oriented adaptation for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">No more discrimination: Cross city adaptation of road scene segmenters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo-Cheng</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Curriculum model adaptation with synthetic and real data for semantic foggy scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Hecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Domain transfer multiple kernel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="465" to="479" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Visual event recognition in videos by learning from web data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1667" to="1680" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mark Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Ali Eslamiemail</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Selfensembling for visual domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Mackiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning attributes equals multi-source domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianbao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">59</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Connecting the dots with landmarks: Discriminatively learning domaininvariant features for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Geodesic flow kernel for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cycada: Cycle consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02649</idno>
		<title level="m">Fcns in the wild: Pixel-level adversarial and constraint-based adaptation</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Conditional generative adversarial network for structured domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixiang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adaptive batch normalization for practical domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="109" to="117" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">RefineNet: Multi-path refinement networks for highresolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Taking a closer look at domain shift: Categorylevel adversaries for semantics consistent domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.09478</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Image to image translation for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zak</forename><surname>Murez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soheil</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungnam</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A dirt-t approach to unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bui</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narui</forename><surname>Hirokazu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ermon</forename><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Maximum classifier discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kohei</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning from synthetic data: Addressing domain shift for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swami</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arpit</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><surname>Ser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan-Hung</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himalaya</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.10080</idno>
		<title level="m">Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A curriculum domain adaptation approach to the semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Foroosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Curriculum domain adaptation for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fully convolutional adaptation networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Penalizing top performers: Conservative loss for semantic segmentation adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinge</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V K</forename><surname>Vijaya Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
