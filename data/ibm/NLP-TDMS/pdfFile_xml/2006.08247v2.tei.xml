<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learn to cycle: Time-consistent feature discovery for action recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Stergiou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Utrecht University</orgName>
								<address>
									<addrLine>Princetonplein 5</addrLine>
									<postCode>3584 CC</postCode>
									<settlement>Utrecht</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Poppe</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Utrecht University</orgName>
								<address>
									<addrLine>Princetonplein 5</addrLine>
									<postCode>3584 CC</postCode>
									<settlement>Utrecht</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learn to cycle: Time-consistent feature discovery for action recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T14:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generalizing over temporal variations is a prerequisite for effective action recognition in videos. Despite significant advances in deep neural networks, it remains a challenge to focus on short-term discriminative motions in relation to the overall performance of an action. We address this challenge by allowing some flexibility in discovering relevant spatio-temporal features. We introduce Squeeze and Recursion Temporal Gates (SRTG), an approach that favors inputs with similar activations with potential temporal variations. We implement this idea with a novel CNN block that uses an LSTM to encapsulate feature dynamics, in conjunction with a temporal gate that is responsible for evaluating the consistency of the discovered dynamics and the modeled features. We show consistent improvement when using SRTG blocks, with only a minimal increase in the number of GFLOPs. On Kinetics-700, we perform on par with current state-of-the-art models, and outperform these on HACS, Moments in Time, UCF-101 and HMDB-51. 1</p><p>Email address: a.g.stergiou@uu.nl (Alexandros Stergiou) 1 The code for this project can be found at: https://git.io/JfuPi Frames Frames Activations Activations A. B.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Action recognition in videos is an active field of research. A major challenge that is addressed comes from dealing with the vast variation in the temporal display of the action <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24]</ref>. In deep neural networks, temporal motion has primarily been modeled either through the inclusion of optical flow as a separate input stream <ref type="bibr" target="#b20">[21]</ref> or using 3D convolutions <ref type="bibr" target="#b12">[13]</ref>. The latter have shown consistent improvements in state-of-the-art models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>3D convolution kernels in convolutional neural networks (3D-CNNs) take into account fixed-sized temporal regions. Kernels in early layers have small receptive fields that primarily focus on simple patterns such as texture and linear movement. Later layers have significantly greater receptive fields that are capable of modeling complex spatio-temporal patterns. Through this hierarchical dependency, the relations between discriminative short-term motions within the larger motion patterns are only established in the very last network layers. Consequently, when training a 3D-CNN, the learned features might include incidental correlations instead of consistent temporal patterns. Thus, there appears to be room for improvement in the discovery of discriminative spatio-temporal features.</p><p>To improve this discovery process, we propose a method named Squeeze and Recursion Temporal Gates (SRTG) which aims at extracting features that are consistent in the temporal domain. Instead of relying on a fixed-size window, our approach relates specific short-term activations to the overall motion in the video, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. We introduce a novel block that uses an LSTM <ref type="bibr" target="#b9">[10]</ref>) to encapsulate feature dynamics, and a temporal gate to decide whether these discovered dynamics are consistent with the modeled features. The novel block can be used as at various places in a wide range of CNN architectures, with minimal computational overhead.</p><p>Our contributions are as follows:</p><p>• We implement a novel block, Squeeze and Recursion Temporal Gates (SRTG), that favors inputs that are temporally consistent with the modeled features.</p><p>• The SRTG block can be used in a wide range of 3D-CNNs, including those with residual connections, with minimal computational overhead (∼0.15% of model GFLOPs).</p><p>• We demonstrate state-of-the-art performance on five action recognition datasets when SRTG blocks are used. Networks with SRTG consistently outperform their vanilla counterparts, independent of the network depth, the convolution block type and dataset.  We discuss the advancements in the modeling of time in action recognition in Section 2. A detailed description of the main methodology is provided in Section 3. Experimental setup and results are presented in Section 4 and we conclude in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We discuss how temporal information is represented in CNNs, in particular using 3D convolutions.</p><p>Time representation in CNNs. Apart from the hand-coded calculation of optical flow <ref type="bibr" target="#b20">[21]</ref>, the predominant method for representing spatio-temporal information in CNNs is the use of 3D convolutions. These convolutions process motion information jointly with spatial information <ref type="bibr" target="#b12">[13]</ref>. Because the spatial and temporal dimensions of videos are strongly connected, this has led to great improvements especially for deeper 3D-CNN models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8]</ref>. Recent work additionally targets the efficient incorporation of temporal information at different time scales through the use of separate pathways <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>3D convolution variants. A large body of work has focused on reducing the computational requirements of 3D convolutions. Most of these attempts are targeted towards the decoupling of temporal information, for example as pseudo and (2+1)D 3D convolutions <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b26">27]</ref>. Others have proposed a decoupling of horizontal and vertical motions <ref type="bibr" target="#b24">[25]</ref>.</p><p>Information fusion of spatio-temporal activations. Squeeze and Excitation <ref type="bibr" target="#b11">[12]</ref>, Gather and Excite <ref type="bibr" target="#b10">[11]</ref> and Point-wise Spatial Attention <ref type="bibr" target="#b32">[33]</ref> consider self-attention in convolutional blocks for image-based input. In the video domain, self-attention has been implemented by Long et al. <ref type="bibr" target="#b15">[16]</ref> using clustering, to integrate local patterns with different attention units. Others have studied the use of non-local operations that capture long-range temporal dependencies through different distances <ref type="bibr" target="#b28">[29]</ref>. Wang et al. <ref type="bibr" target="#b27">[28]</ref> proposed to filter feature responses with activations decoupled to branches for appearance and spatial relations. Qiu et al. <ref type="bibr" target="#b19">[20]</ref> have extended the idea of creating separate pathways for general features that can be updated through network block activations.</p><p>While these methods have shown increased generalization performance, they do not address the discovery of local spatiotemporal features across large time sequences. As activations are constrained by the spatio-temporal locality of their receptive fields, they are not allowed to effectively consider extended temporal variations of actions based on their general motion and time of execution. Instead of attempting to map the locality of features to each of the frame-wise activations, our work combines the locally-learned spatio-temporal features with their temporal variations across the duration of the video sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Squeeze and Recursion Temporal Gates</head><p>In this section, we introduce Squeeze and Recursion Temporal Gates (SRTG) blocks, and the possible configurations for their use in CNNs. We will denote layer input a as a stack of T frames a (C × T × H × W) with C the number of channels, T the number of frames, and H and W the spatial dimensions of the video. The backbone blocks that SRTG are applied to also include residual connections where the final accumulated activations are the sum of the previous block activations (a [l−1] ) and the current computed features (z <ref type="bibr">[l]</ref> ) denoted as a [l] = z [l] + a [l−1] , with block index l.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Squeeze and Recursion</head><p>Squeeze and Recursion blocks can be built on top of any spatio-temporal activation map a [l] = g(z [l] ) for any activation function g() applied to a volume of features z <ref type="bibr">[l]</ref> , shown in Figure 2(a). This process is similar to Squeeze and Excitation <ref type="bibr" target="#b11">[12]</ref>. For each block, the activation maps are sub-sampled in both spatial dimensions to create a vectorized representation of the volume's features across time. Each element in the vector contains the intensity values of a frame squeezed, so to say, in a single average value. This process encapsulates the average temporal attention through the discovered features.</p><p>Recurrent cells. The importance of each feature in the temporal attention feature vector is decided by an LSTM subnetwork. Through the sequential chain structure of recurrent cells, the overall features that are generally informative for entire video sequences can be discovered. We briefly describe the inner workings of the LSTM sub-network <ref type="bibr" target="#b9">[10]</ref> and how the importance of each feature for the entire video is learned, as depicted in <ref type="figure" target="#fig_2">Figure 3</ref>.</p><p>To focus on salient patterns, low intensity activations are discarded in the first operation of the recurrent cell at the forget gate layer. A decision f (t) is made given the input pool(a [l] ) (t) and informative features from the previous frame h (t−1) . The features that are to be stored are decided by the product of the sigmodial (σ) input gate layer i (t) , and the vector of candidate values C (t) as computed as: </p><formula xml:id="formula_0">i (t) = {σ(w i * [h (t−1) , pool(a [l] ) (t) ] + b i )} C (t) = {tanh(w C * [h (t−1) , pool(a [l] ) (t) ] + b C )}<label>(1)</label></formula><p>The previous cell state C (t−1) is then updated based on the forget and input gates in order to ignore features that are not consistent across time and to determine the update weight. The new cell state C (t) is calculated as:</p><formula xml:id="formula_1">C (t) = f (t) * C (t−1) + i (t) * C (t)<label>(2)</label></formula><p>The output of the recurrent cell h (t) is given by the current cell state C (t) , the previous hidden state h (t−1) and current input pool(a [l] ) (t) as:</p><formula xml:id="formula_2">h (t) = a (t) * tanh(C (t) ), where a (t) = {σ(w a * [h (t−1) , pool(a [l] ) (t) ] + b a )}<label>(3)</label></formula><p>The hidden states are again squeezed together to re-create a coherent sequence of filtered spatio-temporal feature intensities a <ref type="bibr">[l]</ref> . This new attention vector considers previous cell states, thus creating a generalized vector based on the feature intensity across time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Temporal Gates for cyclic consistency</head><p>Cyclic consistency. To evaluate the similarity between two temporal volumes, cyclic consistency has been widely used <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b29">30]</ref>. The technique is based on the one-to-one mapping of frames from two time sequences, schematically summarized in <ref type="figure" target="#fig_3">Figure 4</ref>. Each of the two feature spaces can be considered an embedding space. Two embedding spaces are cycle-consistent if and only if, each point at time t in the embedding space A, has a minimum distance point in embedding space B that is also at time t. Equivalently, each point at time t in embedding space B should also have a minimum distance point in embedding space A at time t. As shown in <ref type="figure" target="#fig_3">Figure 4</ref>, when points do not cycle back to the same temporal location, they do not exhibit cyclic consistency. In this case, a temporal cyclic error occurs.</p><p>By having points that can cycle back to themselves, a similarity baseline between embedding spaces can be established. Although individual features of the two spaces may be different, they should demonstrate an overall similarity as long as their alignment in terms of cyclic consistency is the same. Therefore, comparing volumes by their cyclic consistency is a suitable measure to account for (temporal) variations. Soft nearest neighbor distance. The main challenge in creating a coherent similarity measure between two embeddings is to deal with the vast embedding spaces, as well as to discover the "nearest" point in an adjacent embedding. The idea of soft matches for projected points in embeddings <ref type="bibr" target="#b6">[7]</ref> is based on finding the closest point in an embedding space through the weighted sum of all possible matches and then selecting the closest actual observation.</p><p>To find the soft nearest neighbor of an activation a A (t) in embedding space B, the euclidean distances between observation a B</p><p>(t) and all points in B are calculated (see <ref type="figure">Figure 5</ref>). Each frame is considered a separate instance for which we want to find the minimum point in the adjacent embedding space. We weight the similarity of each frame in embedding space B to activation a A (t) using a softmax activation and by exploiting the exponential difference between activation pairs:</p><formula xml:id="formula_3">a (B→A) (t) = T i z (i) * a B (i) , where z (i) = e −||a A (t) −a B (i) || 2 T i e −||a A (t) −a B (i) || 2<label>(4)</label></formula><p>The softmax activation produces a normal distribution of similarities N(µ, σ 2 ) , centered on the frame with the minimum distance from activation a A (t) . Based on the discovery of the nearest neighbor a (B→A) (t)</p><p>, the distance to nearest frames in B can then be computed. This allows the discovery of frames that are closely related to the initially considered frame a A (t) , achieved by minimizing the L2 distance from the found soft match:</p><formula xml:id="formula_4">a (B→A) (t) = argmin i (|| a (B→A) (t) − a B (i) || 2 )<label>(5)</label></formula><p>We define a point as consistent if and only if the initial temporal location t matches precisely the temporal location of the computed point in embedding space B, a (B→A)  <ref type="figure">Figure 5</ref>: Temporal Gates. Activations of each frame (a B (t i ) ) in embedding space B are compared to the activations of every frame (a A (t j ) ) in embedding space A. We calculate for each frame-wise activation map (a B (t) ) the corresponding soft nearest neighbor ( a A→B (t) ) in encoding space A. We then equivalently obtain a B→A = a A (t) ∀t ∈ {1, ..., T }. Temporal gates. The temporal activation vector encapsulates average feature attention over time. However, it does not enforce a precise similarity to the local spatio-temporal activations. Thus, we compute cyclic consistency between the pooled activations pool(a <ref type="bibr">[l]</ref> ) and the outputted recurrent cells a <ref type="bibr">[l]</ref> . In this context, cyclic consistency is used as a gating mechanism to only fuse the recurrent cell hidden states with unpooled versions of the activations when the two volumes are temporally cycleconsistent. This condition ensures that only time-consistent information is added back to the network, as shown for the active states in <ref type="figure">Figure 2</ref>(a).</p><formula xml:id="formula_5">(t) = a B<label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">SRTG block variants</head><p>Cyclic consistency can be considered in different parts of a convolution block, and we investigate six different approaches in terms of constructing a SRTG block. In each case, the principle of global and local information fusion remains. The block configurations only differ in the relative locations of the SRTG and the LSTM input. All configurations are shown in <ref type="figure">Figure 2(b)</ref>. Similar to networks with residual connections, we consider Simple blocks with two conv operations and Bottleneck blocks with three conv operations. Not all SRTG configurations apply to the Simple blocks.</p><p>Start. SRTG is the very first process in the block to ensure that all operations will be based on both global and local information. The configuration can be used in both Simple and Bottleneck residual blocks.</p><p>Top. Activations of the first convolution are used by the LSTM, with fused features being used by the final convolution. This is specific to Bottleneck blocks.</p><p>Mid. SRTG is added at the middle of Simple blocks and after the second convolution at Bottleneck blocks.</p><p>End. Local and global features are fused at the end of the final convolution, before the concatenation of the residual connection. This is only used in Bottleneck blocks.</p><p>Res. The SRTG block can also be applied to the residual connection. This transforms the residual connection to further include global spatio-temporal features and to combine those with the convolutional activations for either Simple or Bottleneck blocks.</p><p>Final. SRTG is added at the end of the residual block, which allows for the activations to be calculated jointly with their representations across time on the entire video. This can be used in both Simple and Bottleneck blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Results</head><p>We evaluate our approach on five action recognition benchmark datasets (Section 4.1). We perform experiments with various ResNet backbones with various depths. Each network uses either 3D convolutions (r3d) or (2+1)D convolutions (r(2+1)d).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We use five action recognition datasets for our experiments: Human Action Clips and Segments (HACS, <ref type="bibr" target="#b31">[32]</ref>) includes approximately 500K clips of 200 classes. Clips are 60-frame segments extracted from 50k unique videos.</p><p>Kinetics-700 (K-700, <ref type="bibr" target="#b0">[1]</ref>) is the extension of Kinetics-400/600 to 700 classes. It contains approximately 600k clips of varying duration.</p><p>Moments in Time (MiT, <ref type="bibr" target="#b17">[18]</ref>) is one of the largest video datasets of human actions and activities. It includes 339 classes with approximately 800K, 3-second clips.</p><p>UCF-101 <ref type="bibr" target="#b21">[22]</ref> includes 101 classes and 13k clips that vary between 2 and 14 seconds in duration.</p><p>HMDB-51 <ref type="bibr" target="#b13">[14]</ref> contains 7K clips divided over 51 classes with at least 101 clips per class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental settings</head><p>Training was performed with a random sub-sampling of 16 frames, resized to 224 × 224. We adopted a multigrid training scheme <ref type="bibr" target="#b30">[31]</ref> with an initial learning rate of 0.1, halved at each cycle. We used a SGD optimizer with 1e −6 weight decay and a step-wise learning rate reduction. All tested SRTG blocks incorporate stacked dual LSTMs (2 layers). For HACS, K-700 and MiT, we use the train/test splits suggested by the authors, and report on split1 for UCF-101 and HMDB-51.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison of SRTG block configurations</head><p>We compare the different SRTG block configurations with a 34-layer r3d and r(2+1)d. ResNets-34 contain Simple blocks with two conv layers instead of the Bottleneck blocks with three conv layers. We therefore only evaluate the Start, Mid, Res and Final configurations. Results, summarized in <ref type="table" target="#tab_1">Table 1</ref>, are obtained on HACS by training from scratch. All SRTG blocks perform better than their vanilla counterparts. This demonstrates the merits of our more flexible treatment of the temporal dimension. This effect appears to be stronger when the filtering is applied later. Indeed, the best performing SRTG configuration Final achieves a top-1 accuracy improvement of 3.781% for 3D and 4.686% for (2+1)D convolution blocks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison of network architectures</head><p>To better understand the merits of our method, we compare a number of network architectures with and without SRTG (Final configuration). We summarize the performance on all five benchmark datasets in <ref type="table" target="#tab_2">Table 2</ref>. The top part of the table contains the results for state-of-the-art networks including I3D <ref type="bibr" target="#b1">[2]</ref> which is based on an Inception-v1 network. The remaining evaluated architectures use Resnet backbones. Temporal Shift Module (TSM, <ref type="bibr" target="#b14">[15]</ref>) and Multi-Fiber networks (MF, <ref type="bibr" target="#b2">[3]</ref>) use a r3d-50 backbone and Channel-Separated Convolutions (ir-CSN, <ref type="bibr" target="#b25">[26]</ref>) and SlowFast networks (SF, <ref type="bibr" target="#b5">[6]</ref>) are based on r3d-101 backbones. We further include an additional 50layer SlowFast network for an additional comparison of lowercapacity models. We have used the trained networks from the respective authors' repositories. These trained models are typically pre-trained on other datasets. Missing values are due to the lack of a trained model. Any deviations from previously reported performances are due to the use of multigrid <ref type="bibr" target="#b30">[31]</ref> with a base cycle batch size of 32.</p><p>The second and third parts of <ref type="table" target="#tab_2">Table 2</ref> summarize the performances of ResNets with various depths and 3D or (2+1)D convolutions, with and without SRTG, respectively. Models for HACS are trained from scratch. The weights of models for K-700 and MiT are initialized based on those from the pre-trained HACS model. For UCF-101 and HMDB-51, we fine-tune the HACS pre-trained models. Missing values are due to time constraints. We will add these in the final version of the paper.</p><p>For the state-of-the-art architectures, the use of larger and deeper models provides accuracy improvements. This is in line with the general trend for action recognition using CNNs with architectures that are either deeper or include higher complexity. Models implemented with (2+1)D convolution blocks perform somewhat better than their counterparts with 3D convolu-tions. These differences are modest and not consistent across datasets, however.</p><p>As shown in <ref type="table" target="#tab_2">Table 2</ref>, adding SRTG blocks to any architecture consistently improves performance. <ref type="table" target="#tab_3">Table 3</ref> shows pairwise comparisons of the performance on the three largest benchmark datasets for networks with and without SRTG. When using SRTG blocks, the improvements are in the range of 1.2-4.7% for HACS, 2.8-4.4% for K-700 and 2.1-3.7% for MiT. For smaller networks, we observe larger gains. The use of timeconsistent features obtained through our method appears to improve the generalization ability of 3D-CNNs.</p><p>The r3d and r(2+1)d networks with SRTG perform on-par with the current state-of-the-art architectures. The r3d-101 outperforms current state-of-the-art in HACS, MiT, UCF-101 and HMDB-51. For MiT, our top-1 accuracy of 33.564%, which largely surpasses other tested architectures. The (2+1)D variant further outperforms current architectures on HACS with 84.326% top-1 accuracy. We also note a performance on Kinectics-700 that is comparable to the best performing Slow-Fast r3d-101 model. While the SlowFast network achieves better top-1 accuracy, a r(2+1)d-101 network with SRTG blocks has higher top-5 accuracy. This similar performance is remarkable given the relatively low complexity of the SRTG r3d-101 and r(2+1)d-101 models. SlowFast is built on a dual-network configuration with two sub-parts responsible for long-term and short-term movements. The SlowFast network therefore includes a significantly larger number of operations than a r3d-101 or r(2+1)d network with plug-in SRTG blocks. We analyze the computation cost of the SRTG block in Section 4.5.</p><p>Finally, we observe that the performance gain with SRTG is substantial for the two smaller datasets, UCF-101 and HMDB-51. Especially for UCF-101, the action recognition accuracy is very saturated. Still, the already competitive performance of the ResNet-101 models on UCF-101 increases with 1.569% and 1.778% for the 3D and (2+1)D convolution variants, respectively. This further demonstrates that SRTG can improve the selection of features that contain less noise and generalize better, even when there is fewer training data available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Analysis of computational overhead</head><p>The SRTG block can be added to a large range of 3D-CNN architectures. It leverages the small computational costs of LSTMs compared to 3D convolutions. That enables us to increase the number of parameters without a significant increase in the number of GFLOPs. This also corresponds to the small additional memory usage compared to baseline models on both forward and backward passes. We present the number of multiaccumulative operations (MACs) 2 used for the r3/(2+1)d architectures with and without SRTG in <ref type="figure" target="#fig_6">Figure 6</ref>, with respect to the corresponding accuracies. The additional computation overhead, for models that include the proposed block, is approximately 0.15% of the total number of operations in the vanilla networks. This constitutes a negligible increase, compared to the performance gains, making SRTG a lightweight block that  can be easily used on top of networks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Evaluating feature transferability</head><p>A common practice to train CNNs is to use transfer learning on a pre-trained network. To evaluate the performance of the SRTG block after transfer learning, we pre-train on several datasets and fine-tune on smaller datasets UCF-101 and HMDB-51. Through this, we can further eliminate biases relating to the pre-training datsets and compare the accuracies achieved with respect to the SRTG blocks.</p><p>As shown in <ref type="table" target="#tab_4">Table 4</ref>, the accuracy rates remain fairly consistent for the pre-training datasets. This consistency is due to <ref type="bibr" target="#b1">2</ref> Multi-accumulative operations <ref type="bibr" target="#b16">[17]</ref> are based on the product of two numbers increased by an accumulator. They relate to the accumulated sum of convolutions between the dot product of the weights and input region. the large sizes of these datasets, as well as the overall robustness of the proposed method. The average offset between each of the pre-trained models is 0.71% for UCF-101 and 0.47% for HMDB-51. These are only minor changes in accuracy, which further demonstrates that the improvements observed are due to the inclusion of SRTG blocks in the network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We have introduced a novel Squeeze and Recursion Temporal Gates (SRTG) block that can be added to a large range of CNN architectures to create time-consistent features. The SRTG block uses an LSTM to capture multi-frame feature dynamics, and a temporal gate to evaluate the cyclic consistency between the discovered dynamics and the modeled features. SRTG blocks add a negligible computational overhead (0.03-0.4 GFLOPs), which makes both forward and backward passes efficient. Adding our proposed SRTG blocks in ResNet backbones with 3D or (2+1)D convolutions consistently leads to performance gains. We obtain results that are on par with, and in most cases outperform, the current state-of-the-art on action recognition datasets including Kinetics-700 and Moments in Time. For HACS, we obtain a state-of-the-art-performance of 84.3%. Our combined experiments demonstrate the generalization ability of the discovered time-consistent features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgments</head><p>This publication is supported by the Netherlands Organization for Scientific Research (NWO) with a TOP-C2 grant for Automatic recognition of bodily interactions (ARBITER).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>A. Original 3D convolution block. Activation maps consider a fixedsize temporal window. Features are specific to the local neighborhood. B. SRTG convolution block. Activation maps take global time information into account.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Overview of the LSTM-chained cells used for the discovery of globally informative local features. Each input corresponds to a temporal activation map and produces a feature vector of the same size as the input.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Temporal Cyclic Error. Soft nearest neighbor is used to match points between two embeddings. Cycle-consistent points cycle back to original points (visualized for t 2 ). Otherwise, a temporal cyclic error occurs (e.g. at t 7 ). Corresponding salient areas below are visualized with CFP<ref type="bibr" target="#b22">[23]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>t) ∀t ∈ {1, ..., T }. To establish a consistency check for frames in embedding space A, the same procedure is repeated in reverse for every frame in embedding space B, calculating the soft nearest neighbor in embedding space A. The two embeddings are B</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>B. The gate is open when a A→B and a B→A are exactly and sequentially equal to a A and a B . considered cycle-consistent if and only if all points on both embedding spaces map back to themselves through the other embedding space: a (B→A) (t) = a B (t) and a (A→B) (t)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Accuracy in relation to computation cost. Top-1 accuracy and operations (in GMACs) of r3/r(2+1)d with/without SRTG on HACS, K-700 and MiT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Figure 2: (a) SRTG gate states. The gates can be inactive or active. When inactive, main stream and LSTM stream are fused. When active, the output is determined by the Temporal Gate and is either the fused result (open gate) or only the main stream (close state). (b) SRTG configuration options described in Section 3.3. Similar to Residual Networks, we distinguish between Simple blocks with two conv operations and Bottleneck blocks with three conv operations.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison of r3d-34 with SRTG configurations on HACS.</figDesc><table><row><cell>Config</cell><cell>Gates</cell><cell>top-1 (%) 3D (2+1)D</cell><cell>top-5 (%) 3D (2+1)D</cell></row><row><cell>No SRTG</cell><cell></cell><cell cols="2">74.818 75.703 92.839 93.571</cell></row><row><cell>Start</cell><cell></cell><cell cols="2">75.705 76.438 93.230 93.781</cell></row><row><cell>Mid</cell><cell></cell><cell cols="2">75.489 76.685 93.224 93.746</cell></row><row><cell>Res</cell><cell></cell><cell cols="2">76.703 77.094 93.307 93.856</cell></row><row><cell>Final</cell><cell></cell><cell cols="2">78.599 80.389 93.569 94.267</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Action recognition accuracy for all five benchmark datasets. Top part of the table includes state-of-the-art models, evaluated from the trained models provided by the respective authors. Middle and bottom parts summarize the results for r3/(2+1)d with/without SRTG, respectively.</figDesc><table><row><cell>Model</cell><cell cols="2">HACS</cell><cell cols="2">Kinetics-700</cell><cell cols="2">Moments in Time</cell><cell cols="2">UCF-101</cell><cell cols="2">HMDB-51</cell></row><row><cell></cell><cell cols="10">top-1(%) top-5(%) top-1(%) top-5(%) top-1(%) top-5(%) top-1(%) top-5(%) top-1(%) top-5(%)</cell></row><row><cell>I3D</cell><cell>79.948</cell><cell>94.482</cell><cell>53.015</cell><cell>69.193</cell><cell>28.143</cell><cell>54.570</cell><cell>92.453</cell><cell>97.619</cell><cell>71.768</cell><cell>94.128</cell></row><row><cell>TSM</cell><cell>N/A</cell><cell>N/A</cell><cell>54.032</cell><cell>72.216</cell><cell>N/A</cell><cell>N/A</cell><cell>92.336</cell><cell>97.961</cell><cell>72.391</cell><cell>94.158</cell></row><row><cell>ir-CSN-101</cell><cell>N/A</cell><cell>N/A</cell><cell>54.665</cell><cell>73.784</cell><cell>N/A</cell><cell>N/A</cell><cell>94.708</cell><cell>98.681</cell><cell>73.554</cell><cell>95.394</cell></row><row><cell>MF-Net</cell><cell>N/A</cell><cell>N/A</cell><cell>54.249</cell><cell>73.378</cell><cell>27.286</cell><cell>48.237</cell><cell>93.863</cell><cell>98.372</cell><cell>72.654</cell><cell>94.896</cell></row><row><cell>SF r3d-50</cell><cell>N/A</cell><cell>N/A</cell><cell>56.167</cell><cell>75.569</cell><cell>N/A</cell><cell>N/A</cell><cell>94.619</cell><cell>98.756</cell><cell>73.291</cell><cell>95.410</cell></row><row><cell>SF r3d-101</cell><cell>N/A</cell><cell>N/A</cell><cell>57.326</cell><cell>77.194</cell><cell>N/A</cell><cell>N/A</cell><cell>95.756</cell><cell>99.138</cell><cell>74.205</cell><cell>95.974</cell></row><row><cell>r3d-34</cell><cell>74.818</cell><cell>92.839</cell><cell>46.138</cell><cell>67.108</cell><cell>24.876</cell><cell>50.104</cell><cell>89.405</cell><cell>96.883</cell><cell>69.583</cell><cell>91.833</cell></row><row><cell>r3d-50</cell><cell>78.361</cell><cell>93.763</cell><cell>49.083</cell><cell>72.541</cell><cell>28.165</cell><cell>53.492</cell><cell>93.126</cell><cell>96.293</cell><cell>72.192</cell><cell>94.562</cell></row><row><cell>r3d-101</cell><cell>80.492</cell><cell>95.179</cell><cell>52.583</cell><cell>74.631</cell><cell>31.466</cell><cell>57.382</cell><cell>95.756</cell><cell>98.423</cell><cell>75.650</cell><cell>95.917</cell></row><row><cell>r(2+1)d-34</cell><cell>75.703</cell><cell>93.571</cell><cell>46.625</cell><cell>68.229</cell><cell>25.614</cell><cell>52.731</cell><cell>88.956</cell><cell>96.972</cell><cell>69.205</cell><cell>90.750</cell></row><row><cell>r(2+1)d-50</cell><cell>81.340</cell><cell>94.514</cell><cell>49.927</cell><cell>73.396</cell><cell>29.359</cell><cell>55.241</cell><cell>93.923</cell><cell>97.843</cell><cell>73.056</cell><cell>94.381</cell></row><row><cell>r(2+1)d-101</cell><cell>82.957</cell><cell>95.683</cell><cell>52.536</cell><cell>75.177</cell><cell>N/A</cell><cell>N/A</cell><cell>95.503</cell><cell>98.705</cell><cell>75.837</cell><cell>95.512</cell></row><row><cell>SRTG r3d-34</cell><cell>78.599</cell><cell>93.569</cell><cell>49.153</cell><cell>72.682</cell><cell>28.549</cell><cell>52.347</cell><cell>94.799</cell><cell>98.064</cell><cell>74.319</cell><cell>94.784</cell></row><row><cell>SRTG r3d-50</cell><cell>80.362</cell><cell>95.548</cell><cell>53.522</cell><cell>74.171</cell><cell>30.717</cell><cell>55.650</cell><cell>95.756</cell><cell>98.550</cell><cell>75.650</cell><cell>95.674</cell></row><row><cell>SRTG r3d-101</cell><cell>81.659</cell><cell>96.326</cell><cell>56.462</cell><cell>76.819</cell><cell>33.564</cell><cell>58.491</cell><cell>97.325</cell><cell>99.557</cell><cell>77.536</cell><cell>96.253</cell></row><row><cell>SRTG r(2+1)d-34</cell><cell>80.389</cell><cell>94.267</cell><cell>49.427</cell><cell>73.233</cell><cell>28.972</cell><cell>54.176</cell><cell>94.149</cell><cell>97.814</cell><cell>72.861</cell><cell>92.667</cell></row><row><cell>SRTG r(2+1)d-50</cell><cell>83.774</cell><cell>96.560</cell><cell>54.174</cell><cell>74.620</cell><cell>31.603</cell><cell>56.796</cell><cell>95.675</cell><cell>98.842</cell><cell>75.297</cell><cell>95.141</cell></row><row><cell>SRTG r(2+1)d-101</cell><cell>84.326</cell><cell>96.852</cell><cell>56.826</cell><cell>77.439</cell><cell>N/A</cell><cell>N/A</cell><cell>97.281</cell><cell>99.160</cell><cell>77.036</cell><cell>95.985</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Pairwise comparisons of r3d and r(2+1)d networks with and without SRTG on HACS, K-700 and MiT. HACS 78.361 80.362 (+2.0) 81.340 83.474 (+2.1) 80.492 81.659 (+1.1) K-700 49.083 53.522 (+4.4) 49.927 54.174 (+4.2) 52.583 56.462 (+3.8) MiT 28.165 30.717 (+2.5) 29.359 31.603 (+3.3) 31.466 33.564 (+2.0)</figDesc><table><row><cell>Dataset</cell><cell>r3d-50</cell><cell></cell><cell>r(2+1)d-50</cell><cell></cell><cell>r3d-101</cell></row><row><cell>None</cell><cell>SRTG</cell><cell>None</cell><cell>SRTG</cell><cell>None</cell><cell>SRTG</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Results on UCF-101 and HMDB-51 based on transfer learning.</figDesc><table><row><cell>Model</cell><cell>Pre-training</cell><cell cols="3">GFLOPs UCF-101 top-1 (%) HMDB-51 top-1 (%)</cell></row><row><cell></cell><cell>HACS</cell><cell></cell><cell>94.799</cell><cell>74.319</cell></row><row><cell>SRTG r3d-34</cell><cell>HACS+K-700</cell><cell>110.48</cell><cell>95.842</cell><cell>74.183</cell></row><row><cell></cell><cell>HACS+MiT</cell><cell></cell><cell>95.166</cell><cell>74.235</cell></row><row><cell></cell><cell>HACS</cell><cell></cell><cell>94.149</cell><cell>72.861</cell></row><row><cell>SRTG r(2+1)d-34</cell><cell>HACS+K-700</cell><cell>110.8</cell><cell>94.569</cell><cell>73.217</cell></row><row><cell></cell><cell>HACS+MiT</cell><cell></cell><cell>95.648</cell><cell>74.473</cell></row><row><cell></cell><cell>HACS</cell><cell></cell><cell>95.756</cell><cell>75.650</cell></row><row><cell>SRTG r3d-50</cell><cell>HACS+K-700</cell><cell>150.98</cell><cell>96.853</cell><cell>75.972</cell></row><row><cell></cell><cell>HACS+MiT</cell><cell></cell><cell>96.533</cell><cell>76.014</cell></row><row><cell></cell><cell>HACS</cell><cell></cell><cell>95.675</cell><cell>75.297</cell></row><row><cell>SRTG r(2+1)d-50</cell><cell>HACS+K-700</cell><cell>151.6</cell><cell>95.993</cell><cell>75.743</cell></row><row><cell></cell><cell>HACS+MiT</cell><cell></cell><cell>96.278</cell><cell>75.988</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06987</idno>
		<title level="m">A short note on the Kinetics-700 human action dataset</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? A new model and the Kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), IEEE</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-fiber networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="352" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Temporal cycle-consistency learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dwibedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1801" to="1810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">X3D: Expanding architectures for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno>arxiv:2004.04730</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">SlowFast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV), IEEE</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neighbourhood components analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Can spatiotemporal 3D CNNs retrace the history of 2D CNNs and ImageNet?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="18" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Going deeper into action recognition: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Herath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and vision computing</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="4" to="21" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gather-excite: Exploiting feature context in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9401" to="9411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR), IEEE</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">3D convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">HMDB: A large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">TSM: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV), IEEE</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7083" to="7093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Attention clusters: Purely attention based local feature integration for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7834" to="7843" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">On a proposed analytical machine, in: The Origins of Digital Computers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Ludgate</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="73" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Moments in time dataset: One million videos for event understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Monfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Bargal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gutfreund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="502" to="508" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with pseudo-3D residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV), IEEE</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5534" to="5542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal representation with local and global diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12056" to="12065" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Class feature pyramids for video explanation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stergiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kapidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kalliatakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chrysoulas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Poppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Veltkamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Conference on Computer Vision Workshop</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4255" to="4264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Analyzing human-human interactions: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stergiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Poppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">188</biblScope>
			<biblScope unit="page">102799</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Spatio-temporal FAST 3D convolutions for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stergiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Poppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning Applications (ICMLA), IEEE</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1830" to="1834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Video classification with channel-separated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5552" to="5561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Appearance-and-relation networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1430" to="1439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR), IEEE</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning correspondence from the cycle-consistency of time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR), IEEE</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2566" to="2576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A multigrid method for efficiently training video models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="153" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">HACS: Human action clips and segments dataset for recognition and temporal localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8668" to="8678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Psanet: Point-wise spatial attention network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="267" to="283" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
