<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
						</author>
						<title level="a" type="main">Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Convolutional Neural Networks</term>
					<term>Spatial Pyramid Pooling</term>
					<term>Image Classification</term>
					<term>Object Detection !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g., 224×224) input image. This requirement is "artificial" and may reduce the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with another pooling strategy, "spatial pyramid pooling", to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. Pyramid pooling is also robust to object deformations. With these advantages, SPP-net should in general improve all CNN-based image classification methods. On the ImageNet 2012 dataset, we demonstrate that SPP-net boosts the accuracy of a variety of CNN architectures despite their different designs. On the Pascal VOC 2007 and Caltech101 datasets, SPP-net achieves state-of-theart classification results using a single full-image representation and no fine-tuning.</p><p>The power of SPP-net is also significant in object detection. Using SPP-net, we compute the feature maps from the entire image only once, and then pool features in arbitrary regions (sub-images) to generate fixed-length representations for training the detectors. This method avoids repeatedly computing the convolutional features. In processing test images, our method is 24-102× faster than the R-CNN method, while achieving better or comparable accuracy on Pascal VOC 2007.</p><p>In ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2014, our methods rank #2 in object detection and #3 in image classification among all 38 teams. This manuscript also introduces the improvement made for this competition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>We are witnessing a rapid, revolutionary change in our vision community, mainly caused by deep convolutional neural networks (CNNs) <ref type="bibr" target="#b0">[1]</ref> and the availability of large scale training data <ref type="bibr" target="#b1">[2]</ref>. Deep-networksbased approaches have recently been substantially improving upon the state of the art in image classification <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, object detection <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b4">[5]</ref>, many other recognition tasks <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, and even non-recognition tasks.</p><p>However, there is a technical issue in the training and testing of the CNNs: the prevalent CNNs require a fixed input image size (e.g., 224×224), which limits both the aspect ratio and the scale of the input image. When applied to images of arbitrary sizes, current methods mostly fit the input image to the fixed size, either via cropping <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> or via warping <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b6">[7]</ref>, as shown in <ref type="figure" target="#fig_0">Figure 1 (top</ref>  may not be suitable when object scales vary. Fixing input sizes overlooks the issues involving scales. So why do CNNs require a fixed input size? A CNN mainly consists of two parts: convolutional layers, and fully-connected layers that follow. The convolutional layers operate in a sliding-window manner and output feature maps which represent the spatial arrangement of the activations <ref type="figure" target="#fig_2">(Figure 2</ref>). In fact, convolutional layers do not require a fixed image size and can generate feature maps of any sizes. On the other hand, the fully-connected layers need to have fixedsize/length input by their definition. Hence, the fixedsize constraint comes only from the fully-connected layers, which exist at a deeper stage of the network.</p><p>In this paper, we introduce a spatial pyramid pooling (SPP) <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref> layer to remove the fixed-size constraint of the network. Specifically, we add an arXiv:1406.4729v4 [cs.CV] 23 Apr 2015 SPP layer on top of the last convolutional layer. The SPP layer pools the features and generates fixedlength outputs, which are then fed into the fullyconnected layers (or other classifiers). In other words, we perform some information "aggregation" at a deeper stage of the network hierarchy (between convolutional layers and fully-connected layers) to avoid the need for cropping or warping at the beginning. <ref type="figure" target="#fig_0">Figure 1 (bottom)</ref> shows the change of the network architecture by introducing the SPP layer. We call the new network structure SPP-net.</p><p>Spatial pyramid pooling <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref> (popularly known as spatial pyramid matching or SPM <ref type="bibr" target="#b14">[15]</ref>), as an extension of the Bag-of-Words (BoW) model <ref type="bibr" target="#b15">[16]</ref>, is one of the most successful methods in computer vision. It partitions the image into divisions from finer to coarser levels, and aggregates local features in them. SPP has long been a key component in the leading and competition-winning systems for classification (e.g., <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>) and detection (e.g., <ref type="bibr" target="#b19">[20]</ref>) before the recent prevalence of CNNs. Nevertheless, SPP has not been considered in the context of CNNs. We note that SPP has several remarkable properties for deep CNNs: 1) SPP is able to generate a fixedlength output regardless of the input size, while the sliding window pooling used in the previous deep networks <ref type="bibr" target="#b2">[3]</ref> cannot; 2) SPP uses multi-level spatial bins, while the sliding window pooling uses only a single window size. Multi-level pooling has been shown to be robust to object deformations <ref type="bibr" target="#b14">[15]</ref>; 3) SPP can pool features extracted at variable scales thanks to the flexibility of input scales. Through experiments we show that all these factors elevate the recognition accuracy of deep networks. SPP-net not only makes it possible to generate representations from arbitrarily sized images/windows for testing, but also allows us to feed images with varying sizes or scales during training. Training with variable-size images increases scale-invariance and reduces over-fitting. We develop a simple multi-size training method. For a single network to accept variable input sizes, we approximate it by multiple networks that share all parameters, while each of these networks is trained using a fixed input size. In each epoch we train the network with a given input size, and switch to another input size for the next epoch. Experiments show that this multi-size training converges just as the traditional single-size training, and leads to better testing accuracy.</p><p>The advantages of SPP are orthogonal to the specific CNN designs. In a series of controlled experiments on the ImageNet 2012 dataset, we demonstrate that SPP improves four different CNN architectures in existing publications <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> (or their modifications), over the no-SPP counterparts. These architectures have various filter numbers/sizes, strides, depths, or other designs. It is thus reasonable for us to conjecture that SPP should improve more sophisticated (deeper and larger) convolutional architectures. SPP-net also shows state-of-the-art classification results on Cal-tech101 <ref type="bibr" target="#b20">[21]</ref> and Pascal VOC 2007 <ref type="bibr" target="#b21">[22]</ref> using only a single full-image representation and no fine-tuning.</p><p>SPP-net also shows great strength in object detection. In the leading object detection method R-CNN <ref type="bibr" target="#b6">[7]</ref>, the features from candidate windows are extracted via deep convolutional networks. This method shows remarkable detection accuracy on both the VOC and ImageNet datasets. But the feature computation in R-CNN is time-consuming, because it repeatedly applies the deep convolutional networks to the raw pixels of thousands of warped regions per image. In this paper, we show that we can run the convolutional layers only once on the entire image (regardless of the number of windows), and then extract features by SPP-net on the feature maps. This method yields a speedup of over one hundred times over R-CNN. Note that training/running a detector on the feature maps (rather than image regions) is actually a more popular idea <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b4">[5]</ref>. But SPP-net inherits the power of the deep CNN feature maps and also the flexibility of SPP on arbitrary window sizes, which leads to outstanding accuracy and efficiency. In our experiment, the SPP-net-based system (built upon the R-CNN pipeline) computes features 24-102× faster than R-CNN, while has better or comparable accuracy. With the recent fast proposal method of EdgeBoxes <ref type="bibr" target="#b24">[25]</ref>, our system takes 0.5 seconds processing an image (including all steps). This makes our method practical for real-world applications.</p><p>A preliminary version of this manuscript has been published in ECCV 2014. Based on this work, we attended the competition of ILSVRC 2014 <ref type="bibr" target="#b25">[26]</ref>, and ranked #2 in object detection and #3 in image classification (both are provided-data-only tracks) among all 38 teams. There are a few modifications made for ILSVRC 2014. We show that the SPP-nets can boost various networks that are deeper and larger (Sec. 3.1.2-3.1.4) over the no-SPP counterparts. Further, driven by our detection framework, we find that multi-view testing on feature maps with flexibly located/sized windows (Sec. 3.1.5) can increase the classification accuracy. This manuscript also provides the details of these modifications.</p><p>We have released the code to facilitate future research (http://research.microsoft.com/en-us/um/people/kahe/).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">DEEP NETWORKS WITH SPATIAL PYRA-MID POOLING</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Convolutional Layers and Feature Maps</head><p>Consider the popular seven-layer architectures <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. The first five layers are convolutional, some of which are followed by pooling layers. These pooling layers can also be considered as "convolutional", in the sense that they are using sliding windows. The last two  layers are fully connected, with an N-way softmax as the output, where N is the number of categories. The deep network described above needs a fixed image size. However, we notice that the requirement of fixed sizes is only due to the fully-connected layers that demand fixed-length vectors as inputs. On the other hand, the convolutional layers accept inputs of arbitrary sizes. The convolutional layers use sliding filters, and their outputs have roughly the same aspect ratio as the inputs. These outputs are known as feature maps [1] -they involve not only the strength of the responses, but also their spatial positions.</p><p>In <ref type="figure" target="#fig_2">Figure 2</ref>, we visualize some feature maps. They are generated by some filters of the conv 5 layer.  <ref type="figure" target="#fig_2">Figure 2</ref>).</p><p>It is worth noticing that we generate the feature maps in <ref type="figure" target="#fig_2">Figure 2</ref> without fixing the input size. These feature maps generated by deep convolutional layers are analogous to the feature maps in traditional methods <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>. In those methods, SIFT vectors <ref type="bibr" target="#b28">[29]</ref> or image patches <ref type="bibr" target="#b27">[28]</ref> are densely extracted and then encoded, e.g., by vector quantization <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b29">[30]</ref>, sparse coding <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, or Fisher kernels <ref type="bibr" target="#b18">[19]</ref>. These encoded features consist of the feature maps, and are then pooled by Bag-of-Words (BoW) <ref type="bibr" target="#b15">[16]</ref> or spatial pyramids <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. Analogously, the deep convolutional features can be pooled in a similar way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The Spatial Pyramid Pooling Layer</head><p>The convolutional layers accept arbitrary input sizes, but they produce outputs of variable sizes. The classifiers (SVM/softmax) or fully-connected layers require  <ref type="figure">Figure 3</ref>: A network structure with a spatial pyramid pooling layer. Here 256 is the filter number of the conv 5 layer, and conv 5 is the last convolutional layer.</p><p>fixed-length vectors. Such vectors can be generated by the Bag-of-Words (BoW) approach <ref type="bibr" target="#b15">[16]</ref> that pools the features together. Spatial pyramid pooling <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref> improves BoW in that it can maintain spatial information by pooling in local spatial bins. These spatial bins have sizes proportional to the image size, so the number of bins is fixed regardless of the image size. This is in contrast to the sliding window pooling of the previous deep networks <ref type="bibr" target="#b2">[3]</ref>, where the number of sliding windows depends on the input size.</p><p>To adopt the deep network for images of arbitrary sizes, we replace the last pooling layer (e.g., pool 5 , after the last convolutional layer) with a spatial pyramid pooling layer. <ref type="figure">Figure 3</ref> illustrates our method. In each spatial bin, we pool the responses of each filter (throughout this paper we use max pooling). The outputs of the spatial pyramid pooling are kMdimensional vectors with the number of bins denoted as M (k is the number of filters in the last convolutional layer). The fixed-dimensional vectors are the input to the fully-connected layer.</p><p>With spatial pyramid pooling, the input image can be of any sizes. This not only allows arbitrary aspect ratios, but also allows arbitrary scales. We can resize the input image to any scale (e.g., min(w, h)=180, 224, ...) and apply the same deep network. When the input image is at different scales, the network (with the same filter sizes) will extract features at different scales. The scales play important roles in traditional methods, e.g., the SIFT vectors are often extracted at multiple scales <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b26">[27]</ref> (determined by the sizes of the patches and Gaussian filters). We will show that the scales are also important for the accuracy of deep networks. Interestingly, the coarsest pyramid level has a single bin that covers the entire image. This is in fact a "global pooling" operation, which is also investigated in several concurrent works. In <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref> a global average pooling is used to reduce the model size and also reduce overfitting; in <ref type="bibr" target="#b32">[33]</ref>, a global average pooling is used on the testing stage after all fc layers to improve accuracy; in <ref type="bibr" target="#b33">[34]</ref>, a global max pooling is used for weakly supervised object recognition. The global pooling operation corresponds to the traditional Bag-of-Words method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Training the Network</head><p>Theoretically, the above network structure can be trained with standard back-propagation <ref type="bibr" target="#b0">[1]</ref>, regardless of the input image size. But in practice the GPU implementations (such as cuda-convnet <ref type="bibr" target="#b2">[3]</ref> and Caffe <ref type="bibr" target="#b34">[35]</ref>) are preferably run on fixed input images. Next we describe our training solution that takes advantage of these GPU implementations while still preserving the spatial pyramid pooling behaviors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single-size training</head><p>As in previous works, we first consider a network taking a fixed-size input (224×224) cropped from images. The cropping is for the purpose of data augmentation. For an image with a given size, we can pre-compute the bin sizes needed for spatial pyramid pooling. Consider the feature maps after conv 5 that have a size of a×a (e.g., 13×13). With a pyramid level of n×n bins, we implement this pooling level as a sliding window pooling, where the window size win = a/n and stride str = a/n with · and · denoting ceiling and floor operations. With an l-level pyramid, we implement l such layers. The next fully-connected layer (fc 6 ) will concatenate the l outputs. <ref type="figure">Figure 4</ref> shows an example configuration of 3-level pyramid pooling (3×3, 2×2, 1×1) in the cuda-convnet style <ref type="bibr" target="#b2">[3]</ref>.</p><p>The main purpose of our single-size training is to enable the multi-level pooling behavior. Experiments show that this is one reason for the gain of accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-size training</head><p>Our network with SPP is expected to be applied on images of any sizes. To address the issue of varying</p><formula xml:id="formula_0">[fc6] type=fc outputs=4096 inputs=pool3x3,pool2x2,pool1x1 [pool1x1] type=pool pool=max inputs=conv5 sizeX=13 stride=13 [pool3x3] type=pool pool=max inputs=conv5 sizeX=5 stride=4</formula><p>[pool2x2] type=pool pool=max inputs=conv5 sizeX=7 stride=6 <ref type="figure">Figure 4</ref>: An example 3-level pyramid pooling in the cuda-convnet style <ref type="bibr" target="#b2">[3]</ref>. Here sizeX is the size of the pooling window. This configuration is for a network whose feature map size of conv 5 is 13×13, so the pool 3×3 , pool 2×2 , and pool 1×1 layers will have 3×3, 2×2, and 1×1 bins respectively. image sizes in training, we consider a set of predefined sizes. We consider two sizes: 180×180 in addition to 224×224. Rather than crop a smaller 180×180 region, we resize the aforementioned 224×224 region to 180×180. So the regions at both scales differ only in resolution but not in content/layout. For the network to accept 180×180 inputs, we implement another fixed-size-input (180×180) network. The feature map size after conv 5 is a×a = 10×10 in this case. Then we still use win = a/n and str = a/n to implement each pyramid pooling level. The output of the spatial pyramid pooling layer of this 180-network has the same fixed length as the 224-network. As such, this 180-network has exactly the same parameters as the 224-network in each layer. In other words, during training we implement the varying-input-size SPP-net by two fixed-size networks that share parameters.</p><p>To reduce the overhead to switch from one network (e.g., 224) to the other (e.g., 180), we train each full epoch on one network, and then switch to the other one (keeping all weights) for the next full epoch. This is iterated. In experiments, we find the convergence rate of this multi-size training to be similar to the above single-size training.</p><p>The main purpose of our multi-size training is to simulate the varying input sizes while still leveraging the existing well-optimized fixed-size implementations. Besides the above two-scale implementation, we have also tested a variant using s × s as input where s is randomly and uniformly sampled from <ref type="bibr">[180,</ref><ref type="bibr">224]</ref> at each epoch. We report the results of both variants in the experiment section.</p><p>Note that the above single/multi-size solutions are for training only. At the testing stage, it is straightforward to apply SPP-net on images of any sizes.  <ref type="table">Table 1</ref>: Network architectures: filter number×filter size (e.g., 96 × 7 2 ), filter stride (e.g., str 2), pooling window size (e.g., pool 3 2 ), and the output feature map size (e.g., map size 55 × 55). LRN represents Local Response Normalization. The padding is adjusted to produce the expected output feature map size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SPP-NET FOR IMAGE CLASSIFICATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experiments on ImageNet 2012 Classification</head><p>We train the networks on the 1000-category training set of ImageNet 2012. Our training algorithm follows the practices of previous work <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b35">[36]</ref>. The images are resized so that the smaller dimension is 256, and a 224×224 crop is picked from the center or the four corners from the entire image 1 . The data are augmented by horizontal flipping and color altering <ref type="bibr" target="#b2">[3]</ref>. Dropout <ref type="bibr" target="#b2">[3]</ref> is used on the two fully-connected layers. The learning rate starts from 0.01, and is divided by 10 (twice) when the error plateaus. Our implementation is based on the publicly available code of cuda-convnet <ref type="bibr" target="#b2">[3]</ref> and Caffe <ref type="bibr" target="#b34">[35]</ref>. All networks in this paper can be trained on a single GeForce GTX Titan GPU (6 GB memory) within two to four weeks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Baseline Network Architectures</head><p>The advantages of SPP are independent of the convolutional network architectures used. We investigate four different network architectures in existing publications <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> (or their modifications), and we show SPP improves the accuracy of all these architectures. These baseline architectures are in <ref type="table">Table 1</ref> and briefly introduced below:</p><p>• ZF-5: this architecture is based on Zeiler and Fergus's (ZF) "fast" (smaller) model <ref type="bibr" target="#b3">[4]</ref>. The number indicates five convolutional layers. • Convnet*-5: this is a modification on Krizhevsky et al.'s network <ref type="bibr" target="#b2">[3]</ref>. We put the two pooling layers after conv 2 and conv 3 (instead of after conv 1 and conv 2 ). As a result, the feature maps after each layer have the same size as ZF-5. • Overfeat-5/7: this architecture is based on the Overfeat paper <ref type="bibr" target="#b4">[5]</ref>, with some modifications as in <ref type="bibr" target="#b5">[6]</ref>. In contrast to ZF-5/Convnet*-5, this architecture produces a larger feature map (18×18 instead of 13 × 13) before the last pooling layer. A larger filter number (512) is used in conv 3 and the following convolutional layers. We also investigate 1. In <ref type="bibr" target="#b2">[3]</ref>, the four corners are picked from the corners of the central 256×256 crop. a deeper architecture with 7 convolutional layers, where conv 3 to conv 7 have the same structures.</p><p>In the baseline models, the pooling layer after the last convolutional layer generates 6×6 feature maps, with two 4096-d fc layers and a 1000-way softmax layer following. Our replications of these baseline networks are in <ref type="table" target="#tab_3">Table 2</ref> (a). We train 70 epochs for ZF-5 and 90 epochs for the others. Our replication of ZF-5 is better than the one reported in <ref type="bibr" target="#b3">[4]</ref>. This gain is because the corner crops are from the entire image, as is also reported in <ref type="bibr" target="#b35">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Multi-level Pooling Improves Accuracy</head><p>In <ref type="table" target="#tab_3">Table 2</ref> (b) we show the results using singlesize training. The training and testing sizes are both 224×224. In these networks, the convolutional layers have the same structures as the corresponding baseline models, whereas the pooling layer after the final convolutional layer is replaced with the SPP layer. For the results in <ref type="table" target="#tab_3">Table 2</ref>, we use a 4-level pyramid. The pyramid is {6×6, 3×3, 2×2, 1×1} (totally 50 bins). For fair comparison, we still use the standard 10view prediction with each view a 224×224 crop. Our results in <ref type="table" target="#tab_3">Table 2</ref> (b) show considerable improvement over the no-SPP baselines in <ref type="table" target="#tab_3">Table 2</ref> (a). Interestingly, the largest gain of top-1 error (1.65%) is given by the most accurate architecture. Since we are still using the same 10 cropped views as in (a), these gains are solely because of multi-level pooling.</p><p>It is worth noticing that the gain of multi-level pooling is not simply due to more parameters; rather, it is because the multi-level pooling is robust to the variance in object deformations and spatial layout <ref type="bibr" target="#b14">[15]</ref>. To show this, we train another ZF-5 network with a different 4-level pyramid: {4×4, 3×3, 2×2, 1×1} (totally 30 bins). This network has fewer parameters than its no-SPP counterpart, because its fc <ref type="bibr" target="#b5">6</ref>     drops to 29.68%, which is 2.33% better than its no-SPP counterpart and 0.68% better than its single-size trained counterpart. Besides using the two discrete sizes of 180 and 224, we have also evaluated using a random size uniformly sampled from <ref type="bibr">[180,</ref><ref type="bibr">224]</ref>. The top-1/5 error of SPP-net (Overfeat-7) is 30.06%/10.96%. The top-1 error is slightly worse than the two-size version, possibly because the size of 224 (which is used for testing) is visited less. But the results are still better the single-size version.</p><p>There are previous CNN solutions <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b35">[36]</ref> that deal with various scales/sizes, but they are mostly based on testing. In Overfeat <ref type="bibr" target="#b4">[5]</ref> and Howard's method <ref type="bibr" target="#b35">[36]</ref>, the single network is applied at multiple scales in the testing stage, and the scores are averaged. Howard further trains two different networks on low/highresolution image regions and averages the scores. To our knowledge, our method is the first one that trains a single network with input images of multiple sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Full-image Representations Improve Accuracy</head><p>Next we investigate the accuracy of the full-image views. We resize the image so that min(w, h)=256 while maintaining its aspect ratio. The SPP-net is applied on this full image to compute the scores of the full view. For fair comparison, we also evaluate the accuracy of the single view in the center 224×224 crop (which is used in the above evaluations). The comparisons of single-view testing accuracy are in <ref type="table" target="#tab_4">Table 3</ref>. Here we evaluate ZF-5/Overfeat-7. The top-1 error rates are all reduced by the full-view representation. This shows the importance of maintaining the complete content. Even though our network is trained using square images only, it generalizes well to other aspect ratios.</p><p>Comparing <ref type="table" target="#tab_3">Table 2</ref> and <ref type="table" target="#tab_4">Table 3</ref>, we find that the combination of multiple views is substantially better than the single full-image view. However, the fullimage representations are still of good merits. First, we empirically find that (discussed in the next subsection) even for the combination of dozens of views, the additional two full-image views (with flipping) can still boost the accuracy by about 0.2%. Second, the full-image view is methodologically consistent with the traditional methods <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b18">[19]</ref> where the encoded SIFT vectors of the entire image are pooled together. Third, in other applications such as image retrieval <ref type="bibr" target="#b36">[37]</ref>, an image representation, rather than a classification score, is required for similarity ranking. A full-image representation can be preferred.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.5">Multi-view Testing on Feature Maps</head><p>Inspired by our detection algorithm (described in the next section), we further propose a multi-view testing method on the feature maps. Thanks to the flexibility of SPP, we can easily extract the features from windows (views) of arbitrary sizes from the convolutional feature maps.</p><p>On  flipped views, we also compute the feature maps of the flipped image. Given any view (window) in the image, we map this window to the feature maps (the way of mapping is in Appendix), and then use SPP to pool the features from this window (see <ref type="figure" target="#fig_7">Figure 5</ref>). The pooled features are then fed into the fc layers to compute the softmax score of this window. These scores are averaged for the final prediction. For the standard 10-view, we use s = 256 and the views are 224×224 windows on the corners or center. Experiments show that the top-5 error of the 10-view prediction on feature maps is within 0.1% around the original 10-view prediction on image crops. We further apply this method to extract multiple views from multiple scales. We resize the image to six scales s ∈ {224, 256, 300, 360, 448, 560} and compute the feature maps on the entire image for each scale. We use 224 × 224 as the view size for any scale, so these views have different relative sizes on the original image for different scales. We use 18 views for each scale: one at the center, four at the corners, and four on the middle of each side, with/without flipping (when s = 224 there are 6 different views). The combination of these 96 views reduces the top-5 error from 10.95% to 9.36%. Combining the two fullimage views (with flipping) further reduces the top-5 error to 9.14%.</p><p>In the Overfeat paper <ref type="bibr" target="#b4">[5]</ref>, the views are also extracted from the convolutional feature maps instead of image crops. However, their views cannot have arbitrary sizes; rather, the windows are those where the pooled features match the desired dimensionality. We empirically find that these restricted windows are less beneficial than our flexibly located/sized windows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.6">Summary and Results for ILSVRC 2014</head><p>In   <ref type="bibr" target="#b25">[26]</ref>. The best entry of each team is listed.</p><p>in ILSVRC 2013. We only consider single-network performance for manageable comparisons. Our best single network achieves 9.14% top-5 error on the validation set. This is exactly the single-model entry we submitted to ILSVRC 2014 <ref type="bibr" target="#b25">[26]</ref>. The top-5 error is 9.08% on the testing set (ILSVRC 2014 has the same training/validation/testing data as ILSVRC 2012). After combining eleven models, our team's result (8.06%) is ranked #3 among all 38 teams attending ILSVRC 2014 <ref type="table" target="#tab_9">(Table 5</ref>). Since the advantages of SPPnet should be in general independent of architectures, we expect that it will further improve the deeper and larger convolutional architectures <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b31">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experiments on VOC 2007 Classification</head><p>Our method can generate a full-view image representation. With the above networks pre-trained on ImageNet, we extract these representations from the images in the target datasets and re-train SVM classifiers <ref type="bibr" target="#b37">[38]</ref>. In the SVM training, we intentionally do not use any data augmentation (flip/multi-view). We l 2 -normalize the features for SVM training.</p><p>The classification task in Pascal VOC 2007 <ref type="bibr" target="#b21">[22]</ref> involves 9,963 images in 20 categories. 5,011 images are for training, and the rest are for testing. The performance is evaluated by mean Average Precision (mAP).   <ref type="table">Table 7</ref>: Classification accuracy in Caltech101. For SPP-net, the pool 5/7 layer uses the 6×6 pyramid level.</p><p>We start from a baseline in <ref type="table" target="#tab_10">Table 6</ref> (a). The model is ZF-5 without SPP. To apply this model, we resize the image so that its smaller dimension is 224, and crop the center 224×224 region. The SVM is trained via the features of a layer. On this dataset, the deeper the layer is, the better the result is. In Table 6 (b), we replace the no-SPP net with our SPP-net. As a first-step comparison, we still apply the SPP-net on the center 224×224 crop. The results of the fc layers improve. This gain is mainly due to multi-level pooling.</p><p>Table 6 (c) shows our results on full images, where the images are resized so that the shorter side is 224. We find that the results are considerably improved (78.39% vs. 76.45%). This is due to the full-image representation that maintains the complete content.</p><p>Because the usage of our network does not depend on scale, we resize the images so that the smaller dimension is s and use the same network to extract features. We find that s = 392 gives the best results (Table 6 (d)) based on the validation set. This is mainly because the objects occupy smaller regions in VOC 2007 but larger regions in ImageNet, so the relative object scales are different between the two sets. These results indicate scale matters in the classification tasks, and SPP-net can partially address this "scale mismatch" issue.</p><p>In <ref type="table" target="#tab_10">Table 6</ref> (e) the network architecture is replaced with our best model (Overfeat-7, multi-size trained), and the mAP increases to 82.44%. <ref type="table" target="#tab_13">Table 8</ref> summarizes our results and the comparisons with the state-of-theart methods. Among these methods, VQ <ref type="bibr" target="#b14">[15]</ref>, LCC <ref type="bibr" target="#b17">[18]</ref>, and FK <ref type="bibr" target="#b18">[19]</ref> are all based on spatial pyramids matching, and <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b5">[6]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experiments on Caltech101</head><p>The Caltech101 dataset <ref type="bibr" target="#b20">[21]</ref> contains 9,144 images in 102 categories (one background). We randomly sample 30 images per category for training and up to 50 images per category for testing. We repeat 10 random splits and average the accuracy. <ref type="table">Table 7</ref> summarizes our results.</p><p>There are some common observations in the Pascal VOC 2007 and Caltech101 results: SPP-net is better than the no-SPP net <ref type="figure">(Table 7 (b) vs. (a)</ref>), and the fullview representation is better than the crop ((c) vs. (b)). But the results in Caltech101 have some differences with Pascal VOC. The fully-connected layers are less accurate, and the SPP layers are better. This is possibly because the object categories in Caltech101 are less related to those in ImageNet, and the deeper layers are more category-specialized. Further, we find that the scale 224 has the best performance among the scales we tested on this dataset. This is mainly because the objects in Caltech101 also occupy large regions of the images, as is the case of ImageNet.</p><p>Besides cropping, we also evaluate warping the image to fit the 224×224 size. This solution maintains the complete content, but introduces distortion. On the SPP (ZF-5) model, the accuracy is 89.91% using the SPP layer as features -lower than 91.44% which uses the same model on the undistorted full image.   <ref type="bibr" target="#b26">[27]</ref>. ‡ our implementation as in <ref type="table" target="#tab_10">Table 6</ref> (a). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SPP-NET FOR OBJECT DETECTION</head><p>Deep networks have been used for object detection. We briefly review the recent state-of-the-art R-CNN method <ref type="bibr" target="#b6">[7]</ref>. R-CNN first extracts about 2,000 candidate windows from each image via selective search <ref type="bibr" target="#b19">[20]</ref>. Then the image region in each window is warped to a fixed size (227×227). A pre-trained deep network is used to extract the feature of each window. A binary SVM classifier is then trained on these features for detection. R-CNN generates results of compelling quality and substantially outperforms previous methods. However, because R-CNN repeatedly applies the deep convolutional network to about 2,000 windows per image, it is time-consuming. Feature extraction is the major timing bottleneck in testing.</p><p>Our SPP-net can also be used for object detection. We extract the feature maps from the entire image only once (possibly at multiple scales). Then we apply the spatial pyramid pooling on each candidate window of the feature maps to pool a fixed-length representation of this window (see <ref type="figure" target="#fig_7">Figure 5</ref>). Because the time-consuming convolutions are only applied once, our method can run orders of magnitude faster.</p><p>Our method extracts window-wise features from regions of the feature maps, while R-CNN extracts directly from image regions. In previous works, the Deformable Part Model (DPM) <ref type="bibr" target="#b22">[23]</ref> extracts features from windows in HOG <ref type="bibr" target="#b23">[24]</ref> feature maps, and the Selective Search (SS) method <ref type="bibr" target="#b19">[20]</ref> extracts from windows in encoded SIFT feature maps. The Overfeat detection method <ref type="bibr" target="#b4">[5]</ref> also extracts from windows of deep convolutional feature maps, but needs to predefine the window size. On the contrary, our method enables feature extraction in arbitrary windows from the deep convolutional feature maps.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Detection Algorithm</head><p>We use the "fast" mode of selective search <ref type="bibr" target="#b19">[20]</ref> to generate about 2,000 candidate windows per image. Then we resize the image such that min(w, h) = s, and extract the feature maps from the entire image. We use the SPP-net model of ZF-5 (single-size trained) for the time being. In each candidate window, we use a 4-level spatial pyramid (1×1, 2×2, 3×3, 6×6, totally 50 bins) to pool the features. This generates a 12,800d (256×50) representation for each window. These representations are provided to the fully-connected layers of the network. Then we train a binary linear SVM classifier for each category on these features.</p><p>Our implementation of the SVM training follows <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b6">[7]</ref>. We use the ground-truth windows to generate the positive samples. The negative samples are those overlapping a positive window by at most 30% (measured by the intersection-over-union (IoU) ratio). Any negative sample is removed if it overlaps another negative sample by more than 70%. We apply the standard hard negative mining <ref type="bibr" target="#b22">[23]</ref> to train the SVM. This step is iterated once. It takes less than 1 hour to train SVMs for all 20 categories. In testing, the classifier is used to score the candidate windows. Then we use non-maximum suppression <ref type="bibr" target="#b22">[23]</ref> (threshold of 30%) on the scored windows.</p><p>Our method can be improved by multi-scale feature extraction. We resize the image such that min(w, h) = s ∈ S = {480, 576, 688, 864, 1200}, and compute the feature maps of conv 5 for each scale. One strategy of combining the features from these scales is to pool them channel-by-channel. But we empirically find that another strategy provides better results. For each candidate window, we choose a single scale s ∈ S such that the scaled candidate window has a number of pixels closest to 224×224. Then we only use the feature maps extracted from this scale to compute the feature of this window. If the pre-defined scales are dense enough and the window is approximately square, our method is roughly equivalent to resizing the window to 224×224 and then extracting features from it. Nevertheless, our method only requires computing the feature maps once (at each scale) from the entire image, regardless of the number of candidate windows.</p><p>We also fine-tune our pre-trained network, following <ref type="bibr" target="#b6">[7]</ref>. Since our features are pooled from the conv 5 feature maps from windows of any sizes, for simplicity we only fine-tune the fully-connected layers. In this case, the data layer accepts the fixed-length pooled features after conv 5 , and the fc 6,7 layers and a new 21-way (one extra negative category) fc 8 layer follow. The fc 8 weights are initialized with a Gaussian distribution of σ=0.01. We fix all the learning rates to 1e-4 and then adjust to 1e-5 for all three layers. During fine-tuning, the positive samples are those overlapping with a ground-truth window by [0.5, 1], and the negative samples by [0.1, 0.5). In each mini-batch, 25% of the samples are positive. We train 250k minibatches using the learning rate 1e-4, and then 50k mini-batches using 1e-5. Because we only fine-tune the fc layers, the training is very fast and takes about 2 hours on the GPU (excluding pre-caching feature maps which takes about 1 hour). Also following <ref type="bibr" target="#b6">[7]</ref>, we use bounding box regression to post-process the prediction windows. The features used for regression are the pooled features from conv 5 (as a counterpart of the pool 5 features used in <ref type="bibr" target="#b6">[7]</ref>). The windows used for the regression training are those overlapping with a ground-truth window by at least 50%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Detection Results</head><p>We evaluate our method on the detection task of the Pascal VOC 2007 dataset. <ref type="table" target="#tab_16">Table 9</ref> shows our results on various layers, by using 1-scale (s=688) or 5-scale.</p><p>Here the R-CNN results are as reported in <ref type="bibr" target="#b6">[7]</ref> using the AlexNet <ref type="bibr" target="#b2">[3]</ref> with 5 conv layers. Using the pool 5 layers (in our case the pooled features), our result (44.9%) is comparable with R-CNN's result (44.2%). But using the non-fine-tuned fc 6 layers, our results are inferior. An explanation is that our fc layers are pretrained using image regions, while in the detection case they are used on the feature map regions. The feature map regions can have strong activations near the window boundaries, while the image regions may not. This difference of usages can be addressed by fine-tuning. Using the fine-tuned fc layers (ftfc 6,7 ), our results are comparable with or slightly better than the fine-tuned results of R-CNN. After bounding box regression, our 5-scale result (59.2%) is 0.7% better than R-CNN (58.5%), and our 1-scale result (58.0%) is 0.5% worse.</p><p>In <ref type="table" target="#tab_17">Table 10</ref> we further compare with R-CNN using the same pre-trained model of SPPnet (ZF-5). In SPP (1-sc) SPP <ref type="bibr">(5-</ref>   this case, our method and R-CNN have comparable averaged scores. The R-CNN result is boosted by this pre-trained model. This is because of the better architecture of ZF-5 than AlexNet, and also because of the multi-level pooling of SPPnet (if using the no-SPP ZF-5, the R-CNN result drops). <ref type="table" target="#tab_19">Table 11</ref> shows the results for each category. <ref type="table" target="#tab_19">Table 11</ref> also includes additional methods. Selective Search (SS) <ref type="bibr" target="#b19">[20]</ref> applies spatial pyramid matching on SIFT feature maps. DPM <ref type="bibr" target="#b22">[23]</ref> and Regionlet <ref type="bibr" target="#b38">[39]</ref> are based on HOG features <ref type="bibr" target="#b23">[24]</ref>. The Regionlet method improves to 46.1% <ref type="bibr" target="#b7">[8]</ref> by combining various features including conv 5 . DetectorNet <ref type="bibr" target="#b39">[40]</ref> trains a deep network that outputs pixel-wise object masks. This method only needs to apply the deep network once to the entire image, as is the case for our method. But this method has lower mAP (30.5%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Complexity and Running Time</head><p>Despite having comparable accuracy, our method is much faster than R-CNN. The complexity of the convolutional feature computation in R-CNN is O(n · 227 2 ) with the window number n (∼2000). This complexity of our method is O(r · s 2 ) at a scale s, where r is the aspect ratio. Assume r is about 4/3. In the single-scale version when s = 688, this complexity is method mAP areo bike bird boat bottle bus car cat chair cow   In <ref type="table" target="#tab_17">Table 10</ref>, we provide a fair comparison on the running time of the feature computation using the same SPP (ZF-5) model. The implementation of R-CNN is from the code published by the authors implemented in Caffe <ref type="bibr" target="#b34">[35]</ref>. We also implement our feature computation in Caffe. In <ref type="table" target="#tab_17">Table 10</ref> we evaluate the average time of 100 random VOC images using GPU. R-CNN takes 14.37s per image for convolutions, while our 1-scale version takes only 0.053s per image. So ours is 270× faster than R-CNN. Our 5-scale version takes 0.293s per image for convolutions, so is 49× faster than R-CNN. Our convolutional feature computation is so fast that the computational time of fc layers takes a considerable portion. <ref type="table" target="#tab_17">Table 10</ref> shows that the GPU time of computing the 4,096-d fc 7 features is 0.089s per image. Considering both convolutional and fully-connected features, our 1-scale version is 102× faster than R-CNN and is 1.2% inferior; our 5-scale version is 38× faster and has comparable results.</p><p>We also compares the running time in <ref type="table" target="#tab_16">Table 9</ref> where R-CNN uses AlexNet <ref type="bibr" target="#b2">[3]</ref> as is in the original paper <ref type="bibr" target="#b6">[7]</ref>. Our method is 24× to 64× faster. Note that the AlexNet <ref type="bibr" target="#b2">[3]</ref> has the same number of filters as our ZF-5 on each conv layer. The AlexNet is faster because it uses splitting on some layers, which was designed for two GPUs in <ref type="bibr" target="#b2">[3]</ref>.</p><p>We further achieve an efficient full system with the help of the recent window proposal method <ref type="bibr" target="#b24">[25]</ref>. The Selective Search (SS) proposal <ref type="bibr" target="#b19">[20]</ref> takes about 1-2 seconds per image on a CPU. The method of EdgeBoxes <ref type="bibr" target="#b24">[25]</ref> only takes ∼ 0.2s. Note that it is sufficient to use a fast proposal method during testing only. Using the same model trained as above (using SS), we test proposals generated by EdgeBoxes only. The mAP is 52.8 without bounding box regression. This is reasonable considering that EdgeBoxes are not used for training. Then we use both SS and EdgeBox as proposals in the training stage, and adopt only EdgeBoxes in the testing stage. The mAP is 56.3 without bounding box regression, which is better than 55.2 <ref type="table" target="#tab_17">(Table 10</ref>) due to additional training samples. In this case, the overall testing time is ∼0.5s per image including all steps (proposal and recognition). This makes our method practical for real-world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Model Combination for Detection</head><p>Model combination is an important strategy for boosting CNN-based classification accuracy <ref type="bibr" target="#b2">[3]</ref>. We propose a simple combination method for detection.</p><p>We pre-train another network in ImageNet, using the same structure but different random initializations. Then we repeat the above detection algorithm. <ref type="table" target="#tab_3">Table 12</ref> (SPP-net (2)) shows the results of this network. Its mAP is comparable with the first network (59.1% vs. 59.2%), and outperforms the first network in 11 categories.</p><p>Given the two models, we first use either model to score all candidate windows on the test image. Then we perform non-maximum suppression on the union of the two sets of candidate windows (with their scores). A more confident window given by one method can suppress those less confident given by the other method. After combination, the mAP is boosted to 60.9% <ref type="table" target="#tab_3">(Table 12</ref>). In 17 out of all 20 categories the combination performs better than either individual model. This indicates that the two models are complementary.</p><p>We further find that the complementarity is mainly because of the convolutional layers. We have tried to combine two randomly initialized fine-tuned results of the same convolutional model, and found no gain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">ILSVRC 2014 Detection</head><p>The ILSVRC 2014 detection <ref type="bibr" target="#b25">[26]</ref> task involves 200 categories. There are ∼450k/20k/40k images in the training/validation/testing sets. We focus on the task of the provided-data-only track (the 1000-category CLS training data is not allowed to use).</p><p>There are three major differences between the detection (DET) and classification (CLS) training datasets, which greatly impacts the pre-training quality. First, the DET training data is merely 1/3 of the CLS training data. This seems to be a fundamental challenge of the provided-data-only DET task. Second, the category number of DET is 1/5 of CLS. To overcome this problem, we harness the provided subcategory labels 2 for pre-training. There are totally 499 nonoverlapping subcategories (i.e., the leaf nodes in the provided category hierarchy). So we pre-train a 499category network on the DET training set. Third, the distributions of object scales are different between DET/CLS training sets. The dominant object scale in CLS is about 0.8 of the image length, but in DET is about 0.5. To address the scale difference, we resize each training image to min(w, h) = 400 (instead of 256), and randomly crop 224 × 224 views for training. A crop is only used when it overlaps with a ground truth object by at least 50%.</p><p>We verify the effect of pre-training on Pascal VOC 2007. For a CLS-pre-training baseline, we consider the pool 5 features (mAP 43.0% in <ref type="table" target="#tab_16">Table 9</ref>). Replaced with a 200-category network pre-trained on DET, the mAP significantly drops to 32.7%. A 499-category pre-trained network improves the result to 35.9%. Interestingly, even if the amount of training data do not increase, training a network of more categories boosts the feature quality. Finally, training with min(w, h) = 400 instead of 256 further improves the mAP to 37.8%. Even so, we see that there is still a considerable gap to the CLS-pre-training result. This indicates the importance of big data to deep learning.</p><p>For ILSVRC 2014, we train a 499-category Overfeat-7 SPP-net. The remaining steps are similar to the VOC 2007 case. Following <ref type="bibr" target="#b6">[7]</ref>, we use the validation set to generate the positive/negative samples, with windows proposed by the selective search fast mode. The training set only contributes positive samples using the ground truth windows. We fine-tune the fc layers and then train the SVMs using the samples in both validation and training sets. The bounding box regression is trained on the validation set.</p><p>Our single model leads to 31.84% mAP in the ILSVRC 2014 testing set <ref type="bibr" target="#b25">[26]</ref>. We combine six similar models using the strategy introduced in this paper. The mAP is 35.11% in the testing set <ref type="bibr" target="#b25">[26]</ref>. This result ranks #2 in the provided-data-only track of ILSVRC 2014 <ref type="table" target="#tab_4">(Table 13</ref>) <ref type="bibr" target="#b25">[26]</ref>. The winning result is 37.21% from 2. Using the provided subcategory labels is allowed, as is explicitly stated in the competition introduction.  <ref type="table" target="#tab_4">Table 13</ref>: The competition results of ILSVRC 2014 detection (provided-data-only track) <ref type="bibr" target="#b25">[26]</ref>. The best entry of each team is listed.</p><p>NUS, which uses contextual information.</p><p>Our system still shows great advantages on speed for this dataset. It takes our single model 0.6 seconds (0.5 for conv, 0.1 for fc, excluding proposals) per testing image on a GPU extracting convolutional features from all 5 scales. Using the same model, it takes 32 seconds per image in the way of RCNN. For the 40k testing images, our method requires 8 GPU·hours to compute convolutional features, while RCNN would require 15 GPU·days.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>SPP is a flexible solution for handling different scales, sizes, and aspect ratios. These issues are important in visual recognition, but received little consideration in the context of deep networks. We have suggested a solution to train a deep network with a spatial pyramid pooling layer. The resulting SPP-net shows outstanding accuracy in classification/detection tasks and greatly accelerates DNN-based detection. Our studies also show that many time-proven techniques/insights in computer vision can still play important roles in deep-networks-based recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A</head><p>In the appendix, we describe some implementation details:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mean Subtraction.</head><p>The 224×224 cropped training/testing images are often pre-processed by subtracting the per-pixel mean <ref type="bibr" target="#b2">[3]</ref>. When input images are in any sizes, the fixedsize mean image is not directly applicable. In the ImageNet dataset, we warp the 224×224 mean image to the desired size and then subtract it. In Pascal VOC 2007 and Caltech101, we use the constant mean (128) in all the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation of Pooling Bins.</head><p>We use the following implementation to handle all bins when applying the network. Denote the width and height of the conv 5 feature maps (can be the full image or a window) as w and h. For a pyramid level with n×n bins, the (i, j)-th bin is in the range of [ i−1 n w , i n w ] × [ j−1 n h , j n h ]. Intuitively, <ref type="figure">Figure 6</ref>: Example detection results of "SPP-net ftfc 7 bb" on the Pascal VOC 2007 testing set (59.2% mAP). All windows with scores &gt; 0 are shown. The predicted category/score are marked. The window color is associated with the predicted category. These images are manually selected because we find them impressive. Visit our project website to see all 4,952 detection results in the testing set.</p><p>if rounding is needed, we take the floor operation on the left/top boundary and ceiling on the right/bottom boundary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mapping a Window to Feature Maps.</head><p>In the detection algorithm (and multi-view testing on feature maps), a window is given in the image domain, and we use it to crop the convolutional feature maps (e.g., conv 5 ) which have been sub-sampled several times. So we need to align the window on the feature maps.</p><p>In our implementation, we project the corner point of a window onto a pixel in the feature maps, such that this corner point in the image domain is closest to the center of the receptive field of that feature map pixel. The mapping is complicated by the padding of all convolutional and pooling layers. To simplify the implementation, during deployment we pad p/2 pixels for a layer with a filter size of p. As such, for a response centered at (x , y ) , its effective receptive field in the image domain is centered at (x, y) = (Sx , Sy ) where S is the product of all previous strides. In our models, S = 16 for ZF-5 on conv 5 , and S = 12 for Overfeat-5/7 on conv 5/7 . Given a window in the image domain, we project the left (top) boundary by: x = x/S + 1 and the right (bottom) boundary x = x/S − 1. If the padding is not p/2 , we need to add a proper offset to x.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Top: cropping or warping to fit a fixed size. Middle: a conventional CNN. Bottom: our spatial pyramid pooling network structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Visualization of the feature maps. (a) Two images in Pascal VOC 2007. (b) The feature maps of some conv 5 filters. The arrows indicate the strongest responses and their corresponding positions in the images. (c) The ImageNet images that have the strongest responses of the corresponding filters. The green rectangles mark the receptive fields of the strongest responses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 2(c)shows the strongest activated images of these filters in the ImageNet dataset. We see a filter can be activated by some semantic content. For example, the 55-th filter(Figure 2, bottom left) is most activated by a circle shape; the 66-th filter(Figure 2, top right) is most activated by a ∧-shape; and the 118-th filter(Figure 2, bottom right)is most activated by a ∨-shape. These shapes in the input images(Figure 2(a)) activate the feature maps at the corresponding positions (the arrows in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>layers (fc 6 , fc 7 )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>are based on deep networks. In these results, Oquab et al.'s (77.7%) and Chatfield et al.'s (82.42%) are obtained by network fine-tuning and multi-view testing. Our result is comparable with the state of the art, using only a single full-image representation and without fine-tuning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>layers (fc 6 , fc 7 )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Pooling features from arbitrary windows on feature maps. The feature maps are computed from the entire image. The pooling is performed in candidate windows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Table 12 :</head><label>12</label><figDesc>4 67.3 61.7 63.1 71.0 69.8 57.6 29.7 59.0 50.2 65.2 68.0 SPP-net (2) 59.1 65.7 71.4 57.4 42.4 39.9 67.0 71.4 70.6 32.4 66.7 61.7 64.8 71.7 70.4 56.5 30.8 59.9 53.2 63.9 64.6 combination 60.9 68.5 71.7 58.7 41.9 42.Detection results on VOC 2007 using model combination. The results of both models use "ftfc 7 bb". about 1/160 of R-CNN's; in the 5-scale version, this complexity is about 1/24 of R-CNN's.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>). But the cropped region may not contain the entire object, while the warped content may result in unwanted geometric distortion. Recognition accuracy can be compromised due to the content loss or distortion. Besides, a pre-defined scale • K. He and J. Sun are with Microsoft Research, Beijing, China. E-mail: {kahe,jiansun}@microsoft.com • X. Zhang is with Xi'an Jiaotong University, Xi'an, China. Email: xyz.clx@stu.xjtu.edu.cn</figDesc><table><row><cell></cell><cell>crop</cell><cell></cell><cell></cell><cell>warp</cell></row><row><cell>image</cell><cell cols="2">crop / warp</cell><cell cols="2">conv layers fc layers</cell><cell>output</cell></row><row><cell>image</cell><cell>conv layers</cell><cell cols="2">spatial pyramid pooling</cell><cell>fc layers</cell><cell>output</cell></row></table><note>• S. Ren is with University of Science and Technology of China, Hefei, China. Email: sqren@mail.ustc.edu.cn This work was done when X. Zhang and S. Ren were interns at Microsoft Research.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>layer has 30×256-d inputs instead of 36×256-d. The top-1/top-5 errors of this network are 35.06/14.04. This result is similar to the 50-bin pyramid above (34.98/14.14), but considerably better than the no-SPP counterpart (35.99/14.76).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">top-1 error (%)</cell><cell></cell></row><row><cell></cell><cell></cell><cell>ZF-5</cell><cell cols="3">Convnet*-5 Overfeat-5 Overfeat-7</cell></row><row><cell>(a)</cell><cell>no SPP</cell><cell>35.99</cell><cell>34.93</cell><cell>34.13</cell><cell>32.01</cell></row><row><cell cols="3">(b) SPP single-size trained 34.98 (1.01)</cell><cell>34.38 (0.55)</cell><cell>32.87 (1.26)</cell><cell>30.36 (1.65)</cell></row><row><cell>(c)</cell><cell cols="2">SPP multi-size trained 34.60 (1.39)</cell><cell>33.94 (0.99)</cell><cell>32.26 (1.87)</cell><cell>29.68 (2.33)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">top-5 error (%)</cell><cell></cell></row><row><cell></cell><cell></cell><cell>ZF-5</cell><cell cols="3">Convnet*-5 Overfeat-5 Overfeat-7</cell></row><row><cell>(a)</cell><cell>no SPP</cell><cell>14.76</cell><cell>13.92</cell><cell>13.52</cell><cell>11.97</cell></row><row><cell cols="3">(b) SPP single-size trained 14.14 (0.62)</cell><cell>13.54 (0.38)</cell><cell>12.80 (0.72)</cell><cell>11.12 (0.85)</cell></row><row><cell>(c)</cell><cell cols="2">SPP multi-size trained 13.64 (1.12)</cell><cell>13.33 (0.59)</cell><cell>12.33 (1.19)</cell><cell>10.95 (1.02)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Error rates in the validation set of ImageNet 2012. All the results are obtained using standard 10-view testing. In the brackets are the gains over the "no SPP" baselines.</figDesc><table><row><cell>SPP on</cell><cell cols="2">test view top-1 val</cell></row><row><cell>ZF-5, single-size trained</cell><cell>1 crop</cell><cell>38.01</cell></row><row><cell>ZF-5, single-size trained</cell><cell>1 full</cell><cell>37.55</cell></row><row><cell>ZF-5, multi-size trained</cell><cell>1 crop</cell><cell>37.57</cell></row><row><cell>ZF-5, multi-size trained</cell><cell>1 full</cell><cell>37.07</cell></row><row><cell cols="2">Overfeat-7, single-size trained 1 crop</cell><cell>33.18</cell></row><row><cell>Overfeat-7, single-size trained</cell><cell>1 full</cell><cell>32.72</cell></row><row><cell>Overfeat-7, multi-size trained</cell><cell>1 crop</cell><cell>32.57</cell></row><row><cell>Overfeat-7, multi-size trained</cell><cell>1 full</cell><cell>31.25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Error rates in the validation set of ImageNet 2012 using a single view. The images are resized so min(w, h) = 256. The crop view is the central 224×224 of the image.</figDesc><table /><note>3.1.3 Multi-size Training Improves Accuracy</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 (</head><label>2</label><figDesc></figDesc><table /><note>c) shows our results using multi-size training. The training sizes are 224 and 180, while the testing size is still 224. We still use the standard 10-view prediction. The top-1/top-5 errors of all architectures further drop. The top-1 error of SPP-net (Overfeat-7)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Error rates in ImageNet 2012. All the results are based on a single network. The number of views in Overfeat depends on the scales and strides, for which there are several hundreds at the finest scale.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>rank</cell><cell>team</cell><cell>top-5 test</cell></row><row><cell>1</cell><cell>GoogLeNet [32]</cell><cell>6.66</cell></row><row><cell>2</cell><cell>VGG [33]</cell><cell>7.32</cell></row><row><cell>3</cell><cell>ours</cell><cell>8.06</cell></row><row><cell>4</cell><cell>Howard</cell><cell>8.11</cell></row><row><cell>5</cell><cell>DeeperVision</cell><cell>9.50</cell></row><row><cell>6</cell><cell>NUS-BST</cell><cell>9.79</cell></row><row><cell>7</cell><cell>TTIC ECP</cell><cell>10.22</cell></row></table><note>we compare with previous state-of-the- art methods. Krizhevsky et al.'s [3] is the winning method in ILSVRC 2012; Overfeat [5], Howard's [36], and Zeiler and Fergus's [4] are the leading methods</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>The competition results of ILSVRC 2014 clas- sification</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6</head><label>6</label><figDesc>summarizes the results.</figDesc><table><row><cell></cell><cell>(a)</cell><cell>(b)</cell><cell>(c)</cell><cell>(d)</cell><cell>(e)</cell></row><row><cell>model</cell><cell cols="5">no SPP (ZF-5) SPP (ZF-5) SPP (ZF-5) SPP (ZF-5) SPP (Overfeat-7)</cell></row><row><cell></cell><cell>crop</cell><cell>crop</cell><cell>full</cell><cell>full</cell><cell>full</cell></row><row><cell>size</cell><cell>224×224</cell><cell>224×224</cell><cell>224×-</cell><cell>392×-</cell><cell>364×-</cell></row><row><cell>conv 4</cell><cell>59.96</cell><cell>57.28</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>conv 5</cell><cell>66.34</cell><cell>65.43</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>pool 5/7 (6×6)</cell><cell>69.14</cell><cell>68.76</cell><cell>70.82</cell><cell>71.67</cell><cell>76.09</cell></row><row><cell>fc 6/8</cell><cell>74.86</cell><cell>75.55</cell><cell>77.32</cell><cell>78.78</cell><cell>81.58</cell></row><row><cell>fc 7/9</cell><cell>75.90</cell><cell>76.45</cell><cell>78.39</cell><cell>80.10</cell><cell>82.44</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Classification mAP in Pascal VOC 2007. For SPP-net, the pool 5/7 layer uses the 6×6 pyramid level.</figDesc><table><row><cell></cell><cell>(a)</cell><cell>(b)</cell><cell>(c)</cell><cell>(d)</cell></row><row><cell>model</cell><cell>no SPP (ZF-5)</cell><cell>SPP (ZF-5)</cell><cell>SPP (ZF-5)</cell><cell>SPP (Overfeat-7)</cell></row><row><cell></cell><cell>crop</cell><cell>crop</cell><cell>full</cell><cell>full</cell></row><row><cell>size</cell><cell>224×224</cell><cell>224×224</cell><cell>224×-</cell><cell>224×-</cell></row><row><cell>conv 4</cell><cell>80.12</cell><cell>81.03</cell><cell>-</cell><cell>-</cell></row><row><cell>conv 5</cell><cell>84.40</cell><cell>83.76</cell><cell>-</cell><cell>-</cell></row><row><cell>pool 5/7 (6×6)</cell><cell>87.98</cell><cell>87.60</cell><cell>89.46</cell><cell>91.46</cell></row><row><cell>SPP pool 5/7</cell><cell>-</cell><cell>89.47</cell><cell>91.44</cell><cell>93.42</cell></row><row><cell>fc 6/8</cell><cell>87.86</cell><cell>88.54</cell><cell>89.50</cell><cell>91.83</cell></row><row><cell>fc 7/9</cell><cell>85.30</cell><cell>86.10</cell><cell>87.08</cell><cell>90.00</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Classification results for Pascal VOC 2007 (mAP) and Caltech101 (accuracy).</figDesc><table /><note>† numbers reported by</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell>summarizes our results compared with the</cell></row><row><cell>state-of-the-art methods on Caltech101. Our result</cell></row><row><cell>(93.42%) exceeds the previous record (88.54%) by a</cell></row><row><cell>substantial margin (4.88%).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 9</head><label>9</label><figDesc></figDesc><table><row><cell cols="4">: Detection results (mAP) on Pascal VOC 2007.</cell></row><row><cell cols="4">"ft" and "bb" denote fine-tuning and bounding box</cell></row><row><cell>regression.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">SPP (1-sc) SPP (5-sc) R-CNN</cell></row><row><cell></cell><cell>(ZF-5)</cell><cell>(ZF-5)</cell><cell>(ZF-5)</cell></row><row><cell>ftfc7</cell><cell>54.5</cell><cell>55.2</cell><cell>55.1</cell></row><row><cell>ftfc7 bb</cell><cell>58.0</cell><cell>59.2</cell><cell>59.2</cell></row><row><cell>conv time (GPU)</cell><cell>0.053s</cell><cell>0.293s</cell><cell>14.37s</cell></row><row><cell>fc time (GPU)</cell><cell>0.089s</cell><cell>0.089s</cell><cell>0.089s</cell></row><row><cell>total time (GPU)</cell><cell>0.142s</cell><cell>0.382s</cell><cell>14.46s</cell></row><row><cell>speedup (vs. RCNN)</cell><cell>102×</cell><cell>38×</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 10 :</head><label>10</label><figDesc></figDesc><table /><note>Detection results (mAP) on Pascal VOC 2007, using the same pre-trained model of SPP (ZF-5).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head></head><label></label><figDesc>table dog horse mbike person plant sheep sofa train tv DPM [23] 33.7 33.2 60.3 10.2 16.1 27.3 54.3 58.2 23.0 20.0 24.1 26.7 12.7 58.1 48.2 43.2 12.0 21.1 36.1 46.0 43.5 SS [20] 33.8 43.5 46.5 10.4 12.0 9.3 49.4 53.7 39.4 12.5 36.9 42.2 26.4 47.0 52.4 23.5 12.1 29.9 36.3 42.2 48.8 Regionlet [39] 41.7 54.2 52.0 20.3 24.0 20.1 55.5 68.7 42.6 19.2 44.2 49.1 26.6 57.0 54.5 43.4 16.4 36.6 37.7 59.4 52.3 DetNet [40] 30.5 29.2 35.2 19.4 16.7 3.7 53.2 50.2 27.2 10.2 34.8 30.2 28.2 46.6 41.7 26.2 10.3 32.8 26.8 39.8 47.0 RCNN ftfc 7 (A5) 54.2 64.2 69.7 50.0 41.9 32.0 62.6 71.0 60.7 32.7 58.5 46.5 56.1 60.6 66.8 54.2 31.5 52.8 48.9 57.9 64.7 RCNN ftfc 7 (ZF5) 55.1 64.8 68.4 47.0 39.5 30.9 59.8 70.5 65.3 33.5 62.5 50.3 59.5 61.6 67.9 54.1 33.4 57.3 52.9 60.2 62.9</figDesc><table><row><cell cols="2">SPP ftfc 7 (ZF5) 55.2 65.5 65.9 51.7 38.4 32.7 62.6 68.6 69.7 33.1 66.6 53.1 58.2 63.6 68.8 50.4 27.4 53.7 48.2 61.7 64.7</cell></row><row><cell cols="2">RCNN bb (A5) 58.5 68.1 72.8 56.8 43.0 36.8 66.3 74.2 67.6 34.4 63.5 54.5 61.2 69.1 68.6 58.7 33.4 62.9 51.1 62.5 64.8</cell></row><row><cell cols="2">RCNN bb (ZF5) 59.2 68.4 74.0 54.0 40.9 35.2 64.1 74.4 69.8 35.5 66.9 53.8 64.2 69.9 69.6 58.9 36.8 63.4 56.0 62.8 64.9</cell></row><row><cell>SPP bb (ZF5)</cell><cell>59.2 68.6 69.7 57.1 41.2 40.5 66.3 71.3 72.5 34.4 67.3 61.7 63.1 71.0 69.8 57.6 29.7 59.0 50.2 65.2 68.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 11 :</head><label>11</label><figDesc>Comparisons of detection results on Pascal VOC 2007.</figDesc><table><row><cell>method</cell><cell>mAP areo bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv</cell></row><row><cell cols="2">SPP-net (1) 59.2 68.6 69.7 57.1 41.2 40.5 66.3 71.3 72.5 34.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">dog:0.37 pe rson :0 .3 8 dog:0.99 person:1.48 person:0.22 cow:0.80 person:3.29 person:2.69 pers on:2.42 person:1.05 person:0.92 pers on:0.76 bird:1.39 bird:0.84 bottle:1.20 diningtable:0.96 pers on:1.53 pers on:1.52 pers on:0.73 car:0.12 car:0.11 car:0.04 car:0.03 car:3.98 car:1.95 car:1.39 car:0.50 bird:1.47 sofa:0.41 person:2.15 pers on:0.86 tvmonitor:2.24 motorbike:1.11 motorbike:0.74 person:1.36 pers on:1.10</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Visualizing and understanding convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1311.2901</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6229</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>ArXiv:1405.3531</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Generic object detection with dense neural patterns and regionlets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<idno>ArXiv:1404.4316</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cnn features off-the-shelf: An astounding baseline for recogniton</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2014, DeepVision Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Panda: Pose aligned networks for deep attribute modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdevr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Multi-scale orderless pooling of deep convolutional activation features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<idno>ArXiv:1403.1840</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1310.1531</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The pyramid match kernel: Discriminative classification with sets of image features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Video google: a text retrieval approach to object matching in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Linear spatial pyramid matching using sparse coding for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Localityconstrained linear coding for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improving the fisher kernel for large-scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Segmentation as selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Smeulders</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Object detection with discriminatively trained partbased models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0575</idno>
		<title level="m">Imagenet large scale visual recognition challenge</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The devil is in the details: an evaluation of recent feature encoding methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The importance of encoding versus training with sparse coding and vector quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Kernel codebooks for scene categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Geusebroek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Veenman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
	</analytic>
	<monogr>
		<title level="j">Network in network</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.4842</idno>
		<title level="m">Going deeper with convolutions</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Caffe: An open source convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<ptr target="http://caffe.berkeleyvision.org/" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Some improvements on deep convolutional neural network based image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<idno>ArXiv:1312.5402</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Aggregating local image descriptors into compact codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1704" to="1716" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Libsvm: a library for support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<date type="published" when="2011" />
			<publisher>TIST</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Regionlets for generic object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep neural networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
