<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Under review as a conference paper at ICLR 2016 NEURAL NETWORK MATRIX FACTORIZATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karolina</forename><surname>Gintare</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering</orgName>
								<orgName type="institution">University of Cambridge Cambridge</orgName>
								<address>
									<postCode>CB2 1PZ</postCode>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dziugaite</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering</orgName>
								<orgName type="institution">University of Cambridge Cambridge</orgName>
								<address>
									<postCode>CB2 1PZ</postCode>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Roy</surname></persName>
							<email>droy@utstat.toronto.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Statistical Sciences</orgName>
								<orgName type="institution">University of Toronto Toronto</orgName>
								<address>
									<postCode>M5S 3G3</postCode>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Under review as a conference paper at ICLR 2016 NEURAL NETWORK MATRIX FACTORIZATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Data often comes in the form of an array or matrix. Matrix factorization techniques attempt to recover missing or corrupted entries by assuming that the matrix can be written as the product of two low-rank matrices. In other words, matrix factorization approximates the entries of the matrix by a simple, fixed function-namely, the inner product-acting on the latent feature vectors for the corresponding row and column. Here we consider replacing the inner product by an arbitrary function that we learn from the data at the same time as we learn the latent feature vectors. In particular, we replace the inner product by a multi-layer feed-forward neural network, and learn by alternating between optimizing the network for fixed latent features, and optimizing the latent features for a fixed network. The resulting approach-which we call neural network matrix factorization or NNMF, for shortdominates standard low-rank techniques on a suite of benchmark but is dominated by some recent proposals that take advantage of the graph features. Given the vast range of architectures, activation functions, regularizers, and optimization techniques that could be used within the NNMF framework, it seems likely the true potential of the approach has yet to be reached.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>We are interested in modeling arrays of data, which arise in the analysis of networks, graphs, and, more generally, relational data. For example, in collaborative filtering and recommender system applications <ref type="bibr" target="#b4">(Goldberg et al., 1992;</ref><ref type="bibr" target="#b8">Koren et al., 2009</ref>), we may have N users and M movies, and for some collection J ⊆ [N ] × [M ] of user-movie pairs (n, m), we have recorded the rating X n,m that user n gave movie m. The inferential goal we focus on in this work is to predict the ratings for those pairs not in J. The data can be modeled as a partial observation of an N × M array X = (X n,m ). While the methods we discuss are applicable far beyond the setting of movie rating data or even two-dimensional arrays, we will rely on the user-movie metaphor throughout.</p><p>One of the most popular approaches to modeling relational data using latent features is based on matrix factorization. Here the idea is to assume that X is well approximated by the product U T V of two low-rank matrices U ∈ R D×N and V ∈ R D×M , where the rank D is much less than N and M . Write U n and V m for the D-dimensional column vectors of U and V , respectively. Informally, we will think of U n as a latent vector of features describing user n, and of V m as a latent vector of features describing movie m. The ranking X n,m is then approximated by the inner product U T n V m . Probabilistic matrix factorization <ref type="bibr" target="#b12">(Salakhutdinov and Mnih, 2008)</ref>, or simply PMF, is based on matrix factorization, and further assumes the entries of X are independent Gaussians with common variance and means given by the corresponding entries of U T V . Maximum likelihood inference of the features U and V leads one to minimize the Frobenius norm of X − U t V , or equivalently, the root mean squared error (RMSE) between the inner product of the features and the observed relations. In practice, regularization of the features vectors often improves the performance of the resulting predictions, provided the regularization parameter is chosen carefully, e.g., by cross validation. PMF is extremely effective in practice, but is also easy to improve upon when dealing with large but sparsely observed array data, as is typical in collaborative filtering. One way to improve upon PMF is to introduce row and column effects that model systematic biases associated with users and 1 arXiv:1511.06443v2 <ref type="bibr">[cs.</ref>LG] 15 Dec 2015</p><p>Under review as a conference paper at ICLR 2016 with movies, leading to a model known in collaborative filtering community as BiasedMF <ref type="bibr" target="#b8">(Koren et al., 2009)</ref>. In this approach, the mean of X n,m is taken to be U T n V m + µ n + τ m + β, where µ = (µ 1 , . . . , µ S ), τ = (τ 1 , . . . , τ M ), and β are additional latent variables representing the user, movie, and global biases, respectively. Note that the row and column effects in BiasedMF can be seen as a special case of PMF where we fix an entry of U and a distinct entry of V to take the value 1. In other words, BiasedMF implements a strong inductive bias. Again, regularization improves prediction performance in practice.</p><p>In this short paper, we describe a different approach to factorizing X. Write U n • V m for the elementwise product, i.e., the D-dimensional vector whose i'th entry is U i,n V i,m . Using this notation, PMF models the mean of X n,m by f (U n • V m ), where f is the function f (w 1 , w 2 , . . . ) = j w j . In the same notation, BiasedPMF models the mean of X n,m by f (U n • V m , µ n , τ m , β). Our idea is to learn f , rather than assume it is fixed. In particular, we take f = f θ to be a feed-forward neural network with weights θ. Given data, we learn the weights θ at the same time that we learn the latent features. Note that, for fixed latent feature vectors, we effectively have a supervised learning problem, and so we can optimize the neural network by gradient descent as is typical. Fixing the neural network, we can optimize the latent feature vectors also by gradient descent, not unlike recent applications of neural networks to the problem of transferring artistic styles to ordinary images <ref type="bibr" target="#b3">(Gatys et al., 2015)</ref>. As one would expect, regularization is critical. We used 2 -regularization for the latent feature vectors, and chose the regularization parameter λ by optimizing the error on a validation set. We call our proposal neural network matrix factorization, or simply NNMF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">MODEL</head><p>Let [N ] = {1, 2, . . . , N }. A data array is modeled as a collection of real-valued random variables X n,m , for (n, m) ∈ J, where J ⊂ [N ] × [M ] are the indices of the observed entries of the N × M data array. (The extension to higher-dimensional arrays or arrays whose entries are elements in spaces other than R is straightforward.)</p><p>To each row, n ∈ N , we associate a latent feature vector U n ∈ R D and a latent feature matrix U n ∈ R D ×K . Similarly, to each column, m ∈ M , we associate a latent feature vector V m ∈ R D and latent feature matrix V m ∈ R D ×K . Write U n,k for the k'th column of U n (and similarly for V m ), and write (U, V ) for the collection of all latent features. 1 Let f θ be a feed-forward neural network with weights θ. Viewing θ and the latent features (U, V ) as unknown parameters, we assume the entries X n,m are independent random variables with meanŝ</p><formula xml:id="formula_0">X n,m :=X(U n , V m , U n , V m ) := f θ U n , V m , U n,1 • V m,1 , . . . , U n,D • V m,D .</formula><p>(1)</p><p>In other words, the neural network f θ has 2D + D real-valued input units and one real-valued output unit. The first D are user-specific features; the next D are movie-specific features; and the last D inputs are the result of inner products between K-dimensional vectors. 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LEARNING</head><p>To learn the network weights θ and latent features (U, V ), we minimize the objective</p><formula xml:id="formula_1">(n,m)∈J (X n,m −X n,m ) 2 + λ n ||U n || 2 F + n ||U n || 2 2 + m ||V m || 2 F + m ||V m || 2 2 ,<label>(2)</label></formula><p>where λ is a regularization parameter, || · || 2 denotes the 2 norm, and || · || F denotes the Frobenius norm. This objective can be understood as a penalized log likelihood under a Gaussian model for each entry. It can also be understood as a specifying the maximum a posteriori estimate assuming independent Gaussian priors for every latent feature.</p><p>During training, we alternated between optimizing the neural network weights, while fixing the latent features, and optimizing the latent features, while fixing the network weights. Optimization was carried out by gradient descent on the entire dataset (i.e., we did not use batches). We used RMSProp to adjust the learning rate. (See Section 5 for details.) We did not evaluate other optimization algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORK</head><p>NNMF is very similar in spirit to the Random Function Model for modeling arrays/matrices proposed by <ref type="bibr" target="#b10">Lloyd et al. (2012)</ref>. Using probabilistic symmetry considerations, they arrive at a model where the mean of X n,m is given by g(U n , V m ), where g : R 2D → R is modeled by a Gaussian process. At a high level, our model replaces the Gaussian process prior with a parametric neural network one. We explored simply taking g to be a feed-forward neural network (acting on a concatenated pair of vectors in R D ), but found that we achieved better performance if some of the input dimensions first underwent an element-wise product. (We discuss this further below in relationship to NTN models.) Conceivably, a deep neural network could learn to approximate the element-wise product or even outperform it, but this was not the case in our experiments, which used gradient-descent techniques to learn the neural network weights. In experiments, we found that NNMF significantly outperformed RFM, although the RFM results were those produced by an implementation that was limited to D ≤ 3 latent dimensions due to significant algorithmic issues associated with scaling up inference for the Gaussian process component. Given some recent advances in GP inference, it would be interesting to revisit the RFM, though it is not clear to the authors whether the advances are quite enough.</p><p>NNMF is related to some methods applied to Knowledge Bases and Knowledge Graphs. (See <ref type="bibr" target="#b11">(Nickel et al., 2015)</ref> for a review of relational machine learning and Knowledge Graphs.) Knowledge bases (KBs) are relational data composed of entity-relation-entity triples. For example, a geopolitical knowledge base might contain facts such as (Rome, capitol-of, Italy) and (Lithuania, member-of, EU). Knowledge graphs (KGs) are representations of KBs as graphs whose vertices represent entities and whose (labelled) edges represent relations between entities. Given this connection, one can see that KBs can be thought of as a collection of (extremely sparsely observed) arrays, one for each relation, or as a single three-dimensional array. The key challenge in modeling KBs and KGs is to simultaneously learn many relations using shared representations in order to augment the limited data one has on each relation. This is one of the key differences with the collaborative filtering setting.</p><p>A method for KGs similar to NNMF is the Neural Tensor Network (NTN), which combines a tensor product with a single-layer neural network <ref type="bibr" target="#b15">(Socher et al., 2013)</ref>. (Methods similar to NTN have been applied to problems in speech recognition. See, e.g., <ref type="bibr" target="#b17">(Yu et al., 2013)</ref>.) Other approaches to KGs use neural networks to produce representations, rather than map representations to predictions, like NTN and NNMF. (See, e.g., <ref type="bibr" target="#b7">(Huang et al., 2015)</ref> and <ref type="bibr" target="#b0">(Bian et al., 2014</ref>).)</p><p>NTNs model each element of a two-dimensional array bŷ  </p><formula xml:id="formula_2">X n,m := a T tanh U T n Q [1:H] V m + W U n V m + b ,<label>(3)</label></formula><formula xml:id="formula_3">U T n Q h V m , where Q h ∈ R D×D is the h'th slice of Q [1:H] .</formula><formula xml:id="formula_4">U n = U n 1 D U n ∈ R 2D+D ,V m = 1 D V m V m ∈ R 2D+D , Q h ij = W h,i , if i = j, 0, otherwise,<label>(4)</label></formula><p>where 1 D denotes an D-dimensional column vector with all entries equal to 1 and W ∈ R H×(2D+D ) is the weight matrix defining the first layer of the NNMF network. Then we recover the first layer of NNMF with the third-order tensor Q [1:H] ∈ R (2D+D )×(2D+D )×H whose h'th slice is Q h .</p><p>There have been many scalable techniques proposed to model very large KGs. <ref type="bibr" target="#b11">Nickel et al. (2015)</ref> split existing models into two categories: latent feature models and graph feature models. Latent variable methods learn unobserved representations of entities and use them to predict relations, while graph feature methods learn to predict relations directly from extracted features of local graph structure. <ref type="bibr" target="#b16">Toutanova and Chen (2015)</ref> argue through empirical comparisons that these two categories of models exhibit complimentary strengths.</p><p>A number of state-of-the-art proposals for collaborative filtering are perhaps best thought of as incorporating aspects of graph feature models. An example of a method relaxing the low-rank assumption using graph features is the Local Low Rank Matrix Approximation <ref type="bibr" target="#b9">(Lee et al., 2013)</ref>, which assumes that every entry in the matrix is given by a combination of low rank matrices, where the combination is specific to the entry. LLORMA achieves impressive state-of-the-art performance.</p><p>Other approaches also use neural-network architectures but work by trying to predict the ground truth ratings directly from the observed ratings matrix X. For example, in I-AutoRec <ref type="bibr" target="#b14">(Sedhain et al., 2015)</ref>, an autoencoder is learned that takes as input the observed movie ratings vector X n for user n and produces as output the ground truth X truth n . (Missing entries are typically replaced by value 3.) AutoRec achieves state-of-the-art performance, slightly besting LLORMA on some benchmarks, but a careful comparison would likely require a fresh data set and strict controls on how the numerous parameters for both models are chosen <ref type="bibr" target="#b1">(Blum and Hardt, 2015;</ref><ref type="bibr" target="#b2">Dwork et al., 2015)</ref>. Another model in this category is the I-RBM <ref type="bibr" target="#b13">(Salakhutdinov et al., 2007)</ref>, but its performance is now far from the state of the art.</p><p>Both LLORMA and I-AutoRec can be seen as models combining aspects of both graph feature and latent feature models. LLORMA identifies similar rows and columns (entities) using graph features, but model each local low-rank approximation using latent features. I-AutoRec takes as input all observed ratings (relations) for a user (entity), allowing the network to model the graph features, which in this case are similarities and distances among movies.</p><p>In Section 5, we compare the performance of NNMF and other approaches on benchmarks including link prediction in graphs, as well as collaborative filtering in movie rating datasets. In our experiments, NNMF dominated other latent feature methods, as well as the I-RBM model. However, NNMF was dominated by both LLORMA and I-AutoRec. One possibility is that a different approach to learning the underlying neural network would deliver results on par with these methods. Another possibility is that the difference reflects some fundamental limitation of latent feature models, which assume that the ratings are conditionally independent given the latent feature representations. Local graph structure may contain information that would aid in predicting ratings. In particular, NNMF does not learn from the pattern of missing ratings,which can reveal information about a user or movie: e.g., a user might tend only to give ratings when those ratings are extreme, and movies with low ratings are less likely to be viewed in the first place. In contrast to NNMF, both LLORMA and AutoRec could, in principle, be taking advantage of the information latent in the pattern of missing ratings, although the strength of this effect has not been studied. In LLORMA, the sparsity pattern affects the notion of locality. In AutoRec, the entire pattern of ratings is fed as input, although the sparsity is obscured somewhat by missing entries being replaced by 3's. <ref type="bibr" target="#b6">Hernández-Lobato et al. (2014)</ref> demonstrates that explicitly modeling the non-random pattern of missing ratings can lead to a slight improvement in performance for latent feature models, although the gains they demonstrated were not dramatic enough that they would have closed the gap between NNMF and LLORMA/AutoRec. Indeed, we implemented a neural architecture similar in spirit to theirs, but were only able to improve the RMSE score by approximately 0.003. A more careful analysis would be necessary to make more definitive conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Some recent work by</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>We evaluated NNMF on two graph datasets (NIPS and Protein) and two collaborative filtering datasets (MovieLens 100K and 1M). See <ref type="table">Table 1</ref> for more information about the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NIPS Protein ML100k ML1m</head><p>Vertices <ref type="table">X 234  230  943  6040  Vertices Y --1682  3900  Edges  27144 52900  100000  1000209   Table 1</ref>: Data sets and their dimensions. The mark "-" highlights that the array is square.  <ref type="table">Table 2</ref>: Results across the four data sets for a variety of techniques. The token (D) specifies that a rank-D factorization was used. The token (nHL) specifies that n hidden layers were used. Scores reported for RFM and PMF (3) are taken from <ref type="bibr" target="#b10">(Lloyd et al., 2012)</ref>. Scores for BiasedMF were obtained using LibRec <ref type="bibr" target="#b5">(Guo et al., 2015)</ref>. Scores for LLORMA were taken from <ref type="bibr" target="#b9">(Lee et al., 2013)</ref>, AutoRec and RBM, were taken from <ref type="bibr" target="#b14">(Sedhain et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NIPS</head><p>NNMF, NTN, PMF model performance was evaluated on 5 randomly subsampled test sets, each comprising 10% of the data points, and then averaged. The remaining 90% of the data was split into training and validation sets: For the graph datasets, we used a 10% of the training data for validation. Due to the larger size of collaborative filtering datasets, we used 2% and 0.5% of the training data for validation on the MovieLens 100K and 1M datasets, respectively. These numbers were chosen to make the Monte Carlo error of the validation set estimate sufficiently small. (It is likely that results could be improved by better use of the training and validation data.)</p><p>The regularization parameter, λ, and optimal stopping time were chosen by optimizing the error on the validation set. For every fixed setting of λ, the network and features were learned by optimizing Eq. (2) as described in Section 3.</p><p>For simplicity, and to avoid the pitfalls of choosing parameters that produce good test set performance, the number and dimensionality of the features, as well as the network architecture, were fixed across experiments. It is conceivable that cross validating these parameters would have yielded better results. On the other hand, it would be wise to employ safeguards <ref type="bibr" target="#b2">(Dwork et al., 2015)</ref> before embarking on an adaptive search for better architectures, learning rates, activation functions, etc.</p><p>We chose D = 60 feature dimensions to be preprocessed by an element-wise product, and included D = 10 additional features for each user and each movie. The feed-forward neural network was chosen to have 3 hidden layers with 50 sigmoidal units each. The network weights were sampled uniformly in ±4 √ 6/ √ n in + n out , where n in , n out denote the number of inbound and outbound connections. The latent features were randomly initialized from a zero-mean Gaussian distribution with standard deviation 0.1. The features and weights were learned by gradient descent, and RMSPROP was used to adjust the learning rate, which was initialized to 0.001 for NIPS, Protein, and ML-100K, and to 0.005 for ML-1M.</p><p>To train the PMF model, we chose 60 dimensions after evaluating the performance of PMF with various choices for the dimensionality and finding that this worked best. On each run, the regularization parameter was chosen from a large range by optimizing the validation error. (We tried many other settings for PMF, and have reported the best numbers we obtained here to make the comparison conservative.) NTN model hyperparameters were chosen to match NNMF ones-we used 60-dimensional latent features, and 50 units in the hidden layer. This setup yields a third order tensor with 60 × 60 × 50 = 180, 000 entries. Compared to the network underlying NNMF, a NTN of approximately the same size has roughly 20 times more parameters. The model was trained with gradient descent on the same objective function as for NNMF. We had to use mini-batches for the MovieLens 1M dataset to avoid memory issues. Just as for other models, we chose the regularization parameter λ by optimizing it the error on the validation set. Note, that in the original NTN model was trained with a contrastive max-margin objective function with 2 regularization of all parameters. We applied a sigmoid nonlinearity to the output layer of the original NTN, to ensure that its outputs fell in [0, 1].</p><p>The results appear in <ref type="table">Table 2</ref>. As mentioned above, NNMF dominates PMF, RFM, and to a lesser extent NTN. In <ref type="bibr" target="#b10">(Lloyd et al., 2012)</ref>, the performance of RFM is compared with PMF when both models use the same number of latent dimensions. The performance of PMF, however, tends to improve with the higher dimension, assuming proper regularization, and so RFM (3) is seen here to perform worse than PMF (60). It is possible that recent advances in Gaussian process regression could in turn improve the performance of RFM.</p><p>NNMF outperforms BaisedMF, although the margin narrows as we move to the sparsely-observed MovieLens datasets. We note that adding bias correction terms to NNMF also improves the performance of NNMF, although the improvement is on the order of 0.003, and so may not be robust. It is also possible that using more of the training data might widen the gap.</p><p>NNMF beats the (low-rank) global version of LLORMA, but not the local version that relaxes the low-rank constraint. NNMF is also bested by AutoRec. It is also not clear if we could have reliably found much better network weights and features had we made different choices around the architecture, composition, and training of the neural network. Given that NNMF dominates PMF so handily on the graph datasets, it might stand to reason that there is a lot of room for improvement on MovieLens through better engineering of NNMF. It is worth noting that a 'local' versions of NNMF could be developed along the same lines as were for LLORMA. Given that NNMF dominates PMF, it might then also stand to reason that a local version of NNMF would dominate LLORMA, because LLORMA can be understood as a local version of PMF.</p><p>To see whether deeper networks performed better on the collaborative filtering datasets, we also evaluated NNMF on the MovieLens data sets using a 4 hidden layer network. We observed that fewer units per layer yielded better results. (We compared 50 units per layer when (D, D ) = (10, 60) to 20 units per layer when (D, D ) = (10, 80).) However, to draw any conclusions, more experiments would be needed, with care to avoid overfitting. We reported scores for 4 hidden layer networks, with 20 units per hidden layer, and (D, D ) = (10, 80) latent feature dimensions. We believe that adding additional layers would likely improve the results, though we suspect the performance would saturate quickly (and then drop if we did not very carefully initialize and regularize the network).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION</head><p>NNMF achieves state-of-the-art results among latent feature models, but is dominated by approaches that take into account local graph structure. However, it is possible that our experiments have not identified the limits of the NNMF model. It is difficult to exhaustively explore the range of network architectures, activation functions, regularization techniques, and cross-validation strategies. Even if we could explore them all, we would be in danger of overfitting and losing any hope of insight into the usefulness of NNMF. Indeed, we erred towards not trying to optimize over the model's many possible configuration. It would be interesting to apply recent advances in adaptive estimation to control the possibility of overfitting during this phase of designing and evaluating a new model <ref type="bibr" target="#b2">(Dwork et al., 2015)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>The model is trained by optimizing the contrastive max-margin objective using L-BFGS with mini-batches. Ignoring the particularly nonlinearity used, the first layer of the NNMF model can be expressed in the form Eq. (3) if we take W = 0 and allow ourselves to fix some entries of the latent features. (NTN employs no additional layers.) For example, taking K = 1 as in our experiments, define for n ∈ [N ], m ∈ [M ], i, j ∈ [2D + D ], and h ∈ [H],</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>D×D×H is a third order tensor; and the nonlinearity tanh(·) : R H → R H acts element wise. The tensor product term U T n Q [1:H] V m denotes the element in R H whose h'th entry is equal to</figDesc><table /><note>where U n , V m ∈ R D are feature vectors; a ∈ R H is a linear layer weight vector; b ∈ R H is a bias vector; W ∈ R H×2D is a weight matrix; Q [1:H] ∈ R</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that there is nothing forcing the latent feature vectors Un for users and Vm for movies to have the same dimensionality. We made this choice for simplicity.2  In our experiments, we took K = 1 and D large. Taking D = 1 results in a model where the input to the neural network is the prediction of a rank-K matrix factorization and 2D additional features. This simple modification of matrix factorization lead to improvements, but not as dramatic as those we report in Section 5.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors would like to thank Zoubin Ghahramani for feedback and helpful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Knowledge-powered deep learning for word embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Machine Learning</title>
		<meeting>European Conference on Machine Learning</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The ladder: A reliable leaderboard for machine learning competitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.04585</idno>
	</analytic>
	<monogr>
		<title level="m">Conference version appeared in ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The reusable holdout: Preserving validity in adaptive data analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pitassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Reingold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Earlier versions appeared in NIPS 2015 and STOC</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">349</biblScope>
			<biblScope unit="page" from="636" to="638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A neural algorithm of artistic style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.06576</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Using collaborative filtering to weave an information tapestry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nichols</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Oki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Terry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="61" to="70" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Librec: A java library for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yorke-Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Posters, Demos, Late-breaking Results and Workshop Proceedings of the 23rd Conference on User Modelling, Adaptation and Personalization (UMAP)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Probabilistic matrix factorization with non-random missing data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Int. Conf. on Machine Learning</title>
		<meeting>of the Int. Conf. on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Leveraging deep neural networks and knowledge graphs for entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Heck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.07678v1</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Volinsky</surname></persName>
		</author>
		<idno type="DOI">10.1109/MC.2009.263</idno>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Local low-rank matrix approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lebanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Int. Conf. on Machine Learning</title>
		<meeting>of the Int. Conf. on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Random function priors for exchangeable arrays with applications to graphs and relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lloyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Orbanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adv Neural Inform. Proc. Systems</title>
		<editor>P. Bartlett, F. Pereira, C. Burges, L. Bottou, and K. Weinberger</editor>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1007" to="1015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A review of relational machine learning for knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gabrilovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00759</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Probabilistic matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems 21</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Restricted boltzmann machines for collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1145/1273496.1273596</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Machine Learning, ICML &apos;07</title>
		<meeting>the 24th International Conference on Machine Learning, ICML &apos;07<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="791" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Autorec: Autoencoders meet collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sedhain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<idno type="DOI">10.1145/2740908.2742726</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web, WWW &apos;15 Companion</title>
		<meeting>the 24th International Conference on World Wide Web, WWW &apos;15 Companion<address><addrLine>Republic and Canton of Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="111" to="112" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Weinberger</editor>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Observed versus latent features for knowledge base and text inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd Workshop on Continuous Vector Space Models and Their Compositionality</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The deep tensor neural network with applications to large vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Seide</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="388" to="396" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
