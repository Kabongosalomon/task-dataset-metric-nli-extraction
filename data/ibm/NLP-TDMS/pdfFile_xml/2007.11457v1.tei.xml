<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning One Class Representations for Face Presentation Attack Detection using Multi-channel Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Anjith</forename><surname>George</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Sébastien</forename><forename type="middle">Marcel</forename></persName>
						</author>
						<title level="a" type="main">Learning One Class Representations for Face Presentation Attack Detection using Multi-channel Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Presentation Attack Detection</term>
					<term>Convolutional Neural Network</term>
					<term>Face Recognition</term>
					<term>Anti-spoofing</term>
					<term>Reproducible Research</term>
					<term>Unseen Attack Detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Face recognition has evolved as a widely used biometric modality. However, its vulnerability against presentation attacks poses a significant security threat. Though presentation attack detection (PAD) methods try to address this issue, they often fail in generalizing to unseen attacks. In this work, we propose a new framework for PAD using a one-class classifier, where the representation used is learned with a Multi-Channel Convolutional Neural Network (MCCNN). A novel loss function is introduced, which forces the network to learn a compact embedding for bonafide class while being far from the representation of attacks. A one-class Gaussian Mixture Model is used on top of these embeddings for the PAD task. The proposed framework introduces a novel approach to learn a robust PAD system from bonafide and available (known) attack classes. This is particularly important as collecting bonafide data and simpler attacks are much easier than collecting a wide variety of expensive attacks. The proposed system is evaluated on the publicly available WMCA multi-channel face PAD database, which contains a wide variety of 2D and 3D attacks. Further, we have performed experiments with MLFP and SiW-M datasets using RGB channels only. Superior performance in unseen attack protocols shows the effectiveness of the proposed approach. Software, data, and protocols to reproduce the results are made available publicly.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>F ACE recognition has proved to be a beneficial modality for biometric authentication. One of the main reasons for the widespread use of face recognition systems is its nonintrusive nature of acquisition and ease of use <ref type="bibr" target="#b0">[1]</ref>. Face recognition systems have matured a lot in recent years, and several approaches have reported human parity in the identification rate in 'in the wild' conditions <ref type="bibr" target="#b1">[2]</ref>. However, a critical security issue undermining the widespread use of face recognition technology is its vulnerability to presentation attacks (a.k.a spoofing attacks) <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>.</p><p>Presentation attack refers to an attack using an instrument with the intention to affect the normal operation of the biometric system. Often, features such as color, texture <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, motion <ref type="bibr" target="#b6">[7]</ref>, and physiological cues <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> and CNN based methods <ref type="bibr" target="#b9">[10]</ref> are used for detection of attacks like 2D prints and replays. However, detection of sophisticated attacks like 3D masks and partial attacks are challenging and poses a serious threat to the reliability of face recognition systems.</p><p>A. <ref type="bibr">George</ref>   Most of the presentation attack detection (PAD) methods available in prevailing literature try to solve the problem for a limited number of presentation attack instruments and on visible spectrum images <ref type="bibr" target="#b2">[3]</ref>. Though some success has been achieved in addressing 2D presentation attacks, performance of the algorithms in realistic 3D masks and other kinds of attacks is poor. With the increase in quality of attack instruments, it becomes harder to discriminate between bonafide and PAs in the visible spectrum alone. Moreover, considering a real-world situation with a wide variety of 2D, 3D, and partial attacks, PAD in visual spectra alone is challenging and inadequate for security-critical applications. Partial attacks refer to attacks where the attack instrument covers only a part of the face. These attacks are much harder to detect as they appear similar to bonafide in most of the face regions, and they can fool holistic liveliness detection systems easily. Multi-channel methods have been proposed as an alternative <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref>, since they use complementary information from different channels to improve the discrimination between bonafide and attacks. In the multi-channel scenario, the additional channels used can be any modality which can provide complementary representation such as depth, infrared, and thermal channels. Multi-channel PAD approaches are more promising in the context of a wide variety of attacks since they make PAD systems harder to fool.</p><p>Even with the use of multiple channels, one of the main issues with PAD is its poor generalization to unseen attacks <ref type="bibr" target="#b13">[14]</ref>. This is particularly important, since at the time of developing a PAD system, anticipating all possible attacks is impossible. Malicious attackers can always come up with new attacks to fool the PAD systems. In such situations, PAD systems which are robust against unseen attacks are of paramount importance. Moreover, while it is comparatively easy to collect data for attacks like 2D prints and replays, making replicas of challenging presentation attack instruments (PAI) like silicone mask are often very costly <ref type="bibr" target="#b17">[18]</ref> and resource-intensive. In this context, it will be ideal to have a framework which can be trained with bonafide alone, or with a combination of bonafide and easy to manufacture PAIs.</p><p>In real-world scenarios, it can be assumed that all presentation attacks are unseen, as it is not possible to foretell all the variations a PAD system could encounter a priori. A toy example of the decision boundary in an unseen attack scenario is illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. Performances in typical PAD databases may not be representative of the performance of a PAD system in real-world conditions. This necessitates the PAD algorithms to be robust against unseen attacks. Since it is easy (in effort and cost) to collect data from more straightforward attacks compared to complex PAIs, we try to learn the representation leveraging the information from PA classes which are available at the training stage (while not over-fitting on the available attacks). To achieves this, we propose a one-class classifier based framework, where the feature representation is learned with a CNN to have discriminative properties. The core of the framework is a multi-channel CNN trained to learn the embedding using a specific loss function. The proposed approach aims at learning a compact representation for the bonafide class while leveraging the discriminative information for PAD task.</p><p>The main contributions of the paper are listed below.</p><p>• A novel multi-channel one-class classifier-based approach is proposed for unseen attack detection. • A novel loss function is proposed which learns a compact and discriminative representation of the face for PAD task, leveraging the information provided from known attacks. The features used in the one class classifier are learned with a multi-channel CNN framework. The proposed approach was evaluated in known and unseen attack protocols in WMCA database containing a wide variety of 2D and 3D attacks, and performed significantly better than baselines in unseen protocols. We have also performed experiments using RGB channel in MLFP and SiW-M datasets.</p><p>Additionally, the source code and protocols to reproduce the results are made available publicly and are accessible at the following link <ref type="bibr" target="#b0">1</ref> .</p><p>The rest of the paper is organized as follows. Section 2 describes the related work with a particular focus on unseen attack detection. Section 3 outlines the proposed framework. Extensive evaluations, comparison with baseline methods, and ablation studies are shown in section 4. Section 5 discusses 1 Source code: https://gitlab.idiap.ch/bob/bob.paper.oneclass mccnn 2019 the importance of the results, and Section 6 presents the conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Majority of the literature in face PAD is mainly focused on 2D attacks and uses feature-based methods <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> or CNN based methods. Recently, CNN based methods have been more successful as compared to feature-based methods <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>. These methods usually leverage the quality degradation during 'recapture' and are often useful only for the detection of attacks like 2D prints and replays. Sophisticated attacks like 3D masks are more challenging and pose serious threat to the reliability of face recognition systems.</p><p>Most of these methods handle the PAD problem as binary classification, which results in classifiers over-fitting to the known attacks resulting in poor generalization to unseen attacks. We focus the further discussion on the detection of unseen attacks. However, it is imperative that methods working for unseen attacks must perform accurately for known attacks as well. One naive solution for such a task is one-class classifiers (OCC). OCC provides a straightforward way of handling the unseen attack scenario by modeling the distribution of the bonafide class alone.</p><p>Arashloo et al. <ref type="bibr" target="#b21">[22]</ref> and Nikisins et al. <ref type="bibr" target="#b22">[23]</ref> have shown the effectiveness of one class methods against unseen attacks. Even though these methods performed better than binary classifiers in an unseen attack scenario, the performance in known attack protocols was inferior to that of binary classifiers. Xiong et al. <ref type="bibr" target="#b23">[24]</ref> proposed unseen PAD methods using auto-encoders and one class classifiers with texture features extracted from images. However, the performance of the methods compared to recent CNN based methods is very poor. CNN based methods outperform most of the feature-based baselines for PAD task. Hence there is a clear need of one class classifiers or anomaly detectors in the CNN framework. One of the drawbacks of one class model is that they do not use the information provided by the known attacks. An anomaly detector framework which utilizes the information from the known attacks could be more efficient.</p><p>Perera and Patel <ref type="bibr" target="#b24">[25]</ref> presented an approach for one-class transfer learning in which labelled data from an unrelated task is used for feature learning. They used two loss functions, namely descriptive loss, and compactness loss to learn the representations. The data from the class of interest is used to calculate the compactness loss whereas an external multi-class dataset is used to compute the descriptive loss. Accuracy of the learned model in classification using another database is used as the descriptive loss. However, in the face PAD problem, this approach would be challenging since the bonafide and attack classes appear very similar.</p><p>Fatemifar et al. <ref type="bibr" target="#b25">[26]</ref> proposed an approach to ensemble multiple one-class classifiers for improving the generalization of PAD. They introduced a class-specific normalization scheme for the one class scores before fusion. Seven regions, three one class classifiers and representations from three CNNs were used in the pool of classifiers. Though their method achieved better performance as compared to client independent thresholds, the performance is inferior to CNN based state of the art methods. Specifically, many CNN based approaches have achieved 0% HTER in Replay-Attack and Replay-Mobile datasets. Moreover, the challenging unseen attack scenario is not evaluated in this work.</p><p>Pérez-Cabo et al. <ref type="bibr" target="#b26">[27]</ref> proposed a PAD formulation from an anomaly detection perspective. A deep metric learning model is proposed, where a triplet focal loss is used as a regularization for 'metric-softmax', which forces the network to learn discriminative features. The features learned in such a way is used together with an SVM with RBF kernel for classification. They have performed several experiments on an aggregated RGB only datasets showing the improvement made by their proposed approach. However, the analysis is mostly limited to RGB only models and 2D attacks. Challenging 3D and partial attacks are not considered in this work. Specifically, the effectiveness in challenging unknown attacks (2D vs 3D) is not evaluated.</p><p>Recently, Liu et al. <ref type="bibr" target="#b27">[28]</ref> proposed an approach for the detection of unknown spoof attacks as Zero-Shot Face Antispoofing (ZSFA). They proposed a Deep Tree Network (DTN) which partitions the attack samples into semantic sub-groups in an unsupervised manner. Each tree node in their network consists of a Convolutional Residual Unit (CRU) and a Tree Routing Unit (TRU). The objective is to route the unknown attacks to the most proper leaf node for correctly classifying it. They have considered a wide variety of attacks in their approach and their approach achieved superior performance compared to the considered baselines.</p><p>Jaiswal et al. <ref type="bibr" target="#b28">[29]</ref> proposed an end to end deep learning model for PAD which used unsupervised adversarial invariance. In their method, the discriminative information and nuisance factors are disentangled in an adversarial setting. They showed that by retaining only discriminative information, the PAD performance improved for the same base architecture. Mehta et al. <ref type="bibr" target="#b29">[30]</ref> trained an Alexnet model with a combination of cross-entropy and focal losses. They extracted the features from Alexnet and trained a two-class SVM for PAD task. However, results in challenging datasets such as OULU and SiW were not reported.</p><p>Recently Joshua and Jain <ref type="bibr" target="#b30">[31]</ref> utilized multiple GANs for spoof detection in fingerprints. Their method essentially consisted of training a DCGAN <ref type="bibr" target="#b31">[32]</ref> using only the bonafide samples. At the end of the training, the generator is discarded, and the discriminator is used as the PAD classifier. They combined the results from different GANs operating on different features. However, this approach may not work well for face images as the recaptured images look very similar to the bonafide samples.</p><p>In safety critical applications, extended range methods have been proposed over the years <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> to achieve reliable PAD performance. Even these methods fail in generalizing to unseen attacks.</p><p>Wang et al. <ref type="bibr" target="#b35">[36]</ref> proposed multimodal face presentation attack detection with a ResNet based network using both spatial and channel attentions. Specifically, the approach was tailored for the CASIA-SURF <ref type="bibr" target="#b36">[37]</ref> database which contained RGB, near-infrared and depth channels. The proposed model is a multi-branch model where the individual channels and fused data are used as inputs. Each input channel has its own feature extraction module and the features extracted are concatenated in a late fusion strategy. Followed by more layers to learn a discriminative representation for PAD. The network training is supervised by both center loss and softmax loss. One key point is the use of spatial and channel attention to fully utilize complementary information from different channels. Though the proposed approach achieved good results in the CASIA-SURF database, the challenging problem of unseen attack detection is not addressed.</p><p>Parkin et al. <ref type="bibr" target="#b37">[38]</ref> proposed a multi-channel face PAD network based on ResNet. Essentially, their method consists of different ResNet blocks for each channel followed by fusion. Squeeze and excitation modules (SE) are used before fusing the channels, followed by remaining residual blocks. Further, they add aggregation blocks at multiple levels to leverage inter-channel correlations. Their approach achieved state of the art results in CASIA-SURF <ref type="bibr" target="#b36">[37]</ref> database. However, the final model presented in is a combination of 24 neural networks trained with different attack specific folds, pre-trained models and random seeds, which would increase the computation greatly.</p><p>From the discussions above, it can be seen that one class classifiers could be a good alternative for binary classification in PAD task. However, the features used for one class classifiers should be discriminative and compact to outperform binary classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head><p>From a practical viewpoint, it is not possible to anticipate all the possible types of attacks and to have them in the training set. This, in turn, make the PAD task an unseen classification problem in a broad sense. In general, we can even consider attacks coming from different replay devices as unseen attacks. Typically, one class classifiers are well suited for such outlier detection tasks. However, in practice, the performance of one class classifiers are inferior compared to binary classifiers for known attacks, since they do not leverage useful information from the known attacks. Ideally, the PAD system should perform well in both known and unseen attack scenarios.</p><p>Clearly, there is a necessity of a method which can learn a compact one class representation while utilizing the discriminative information from known attacks. While the collection of attacks could be difficult and costly, collecting bonafide samples are rather easy. A new classification strategy is required to handle the realistic scenario where a limited variety of attack classes are available.</p><p>Though one class classifiers (OCC) offers a way to model the bonafide class, the efficient use of OCC requires the feature representation to be compact while containing discriminative information for PAD task. In the proposed framework, we use a CNN based approach to learn the feature representation. A novel loss function is proposed to learn a representation of bonafide samples leveraging the known attack classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Formulation of One Class Contrastive Loss (OCCL)</head><p>Consider a typical CNN architecture for PAD, where the output layer contains one node and the loss function used is Binary Cross Entropy (BCE), which is defined as:</p><formula xml:id="formula_0">L BCE = −(y log(p) + (1 − y) log(1 − p)) (1)</formula><p>where y is the ground truth, (y = 0 for attack and y = 1 for bonafide) and p is the probability. When trained only with BCE loss, the network learns a decision boundary based on the bonafide and attacks present in the training set. However, it may not generalize when encountered with an unseen attack in the test time as it could be over-fitted to attacks which are 'known' from the training set.</p><p>To overcome this issue, we propose the 'One-Class Contrastive Loss' (OCCL) function which operates on the embedding layer. Proposed One-Class Contrastive Loss (OCCL) function is used as an auxiliary loss function in conjunction with binary cross-entropy loss. The feature map obtained from the penultimate layer of the CNN is used as the embedding. The loss function is inspired from center-loss <ref type="bibr" target="#b38">[39]</ref> and contrastive loss <ref type="bibr" target="#b39">[40]</ref>, which are usually used in the face recognition applications. In face recognition applications, center loss is used as an additional auxiliary loss function, the task of the center loss is to minimize the distance of the embeddings from their corresponding class centers. The center loss is defined as:</p><formula xml:id="formula_1">L center = 1 2 m i=1 x i − c yi 2 2<label>(2)</label></formula><p>Where L center denotes the center loss, m the number of training samples in a mini-batch, x i ∈ R d denotes the i th training sample, y i denotes the label, and c yi denotes the y th i class center in the embedding space.</p><p>The main issue with center loss in the PAD application is that the loss function penalizes for large intra-class distances and does not care about the inter-class distances. Contrastive center loss <ref type="bibr" target="#b40">[41]</ref> tries to solve this issue by adding the distance between classes (inter-class) in the formulation. However, for the PAD problem, modeling the attack class as a cluster and finding a center for the attack class is not trivial. The attacks could be of different categories: 2D, 3D, and partial attacks, and it is not ideal forcing them to cluster together in the embedding space. It is only necessary to have the embeddings of attacks far from bonafide cluster in the embedding space. Hence, we put the compactness constraint only on the bonafide class, while forcing the embeddings of PAs to be far from that of bonafide.</p><p>To formulate the loss function, we start with the equation for contrastive loss function proposed by Lecun et al. <ref type="bibr" target="#b39">[40]</ref>.</p><formula xml:id="formula_2">L Contrastive (W, Y, X 1 , X 2 ) =(1 − Y ) 1 2 D 2 W + Y 1 2 max(0, m − D W ) 2<label>(3)</label></formula><p>Where W is the network weights, X 1 , X 2 are the pairs and Y the label of the pair, i.e., whether they belong to the same class or not. m is the margin, and D W is the distance function between two samples. The data is provided as pairs (X 1 , X 2 ) and the distance function D W can be computed as the Euclidean distance.</p><formula xml:id="formula_3">D W = X 1 − X 2 2 2<label>(4)</label></formula><p>Now, in our loss formulation, the critical difference is how we define D W . In the original contrastive loss, D W is the distance between samples. In our case, we need the representation of bonafide samples to be compact in an embedding space. At the same time, we want to maximize the distance between bonafide cluster and attack samples in the embedding space. This can be achieved by defining DC W to be the distance from the center of bonafide class as follows.</p><formula xml:id="formula_4">DC W = X i − c BF 2 2<label>(5)</label></formula><p>Where X i is the embedding for i th sample, and c BF is the center of bonafide class in the embedding space.</p><p>The center of the bonafide class is updated in every minibatch during training as follows.</p><formula xml:id="formula_5">c BF =ĉ BF (1 − α) + α 1 N N i=1 e i<label>(6)</label></formula><p>Where c BF andĉ BF denotes the new and old bonafidecenters. α is a scalar which prevents sudden changes in the class centers in mini-batch. e i denotes the difference between embeddings for the bonafide samples in the current mini-batch compared to the previous center, and N denotes the number of bonafide samples in the mini-batch.</p><p>Combining the equations, our auxiliary loss function becomes:</p><formula xml:id="formula_6">L OCCL (W, Y, X) =Y 1 2 DC 2 W + (1 − Y ) 1 2 max(0, m − DC W ) 2<label>(7)</label></formula><p>Where DC W denotes the Euclidean distance between the samples and the bonafide class center, Y denotes the ground <ref type="figure">Fig. 3</ref>. Preprocessed images from a rigid mask attack; channels showed are gray-scale, infrared, depth, and thermal, respectively. Channels were preprocessed with face detection, alignment and normalization. truth, i.e., Y = 0 for attacks and Y = 1 for bonafide (note the change in labels from the standard notation due to the ground truth convention). It is to be noted that, the proposed loss function does not require pairs of samples, which is a requirement in usage of contrastive loss. This makes it easier to train the model without requiring an explicit selection of pairs during training. This auxiliary loss makes the representation of bonafide compact pushing it closer to the center of bonafide class and penalizes attack samples which are closer than the margin m. Attack samples which are farther than the margin m are not penalized. An illustration of the loss functions acting on the embeddings of bonafide and attack samples are shown in <ref type="figure" target="#fig_1">Fig.  2</ref>.</p><p>We combine the proposed loss function with standard binary cross entropy for training. The combined loss function to minimize is given as:</p><formula xml:id="formula_7">L = (1 − λ)L BCE + λL OCCL<label>(8)</label></formula><p>Where L denotes the total loss for the CNN. L BCE and L OCCL denotes the binary cross entropy, and one-class contrastive loss respectively. λ denotes a scalar value to set the weight for each loss functions. In our experiments we set the value of λ as 0.5.</p><p>The combined loss function L tries to learn a decision boundary between the available attacks and bonafide while the auxiliary loss tries to make the feature representation of the bonafide compact in the embedding space. We expect the decision boundary learned in this fashion to be more robust in unseen attacks compared to the network learned only with BCE. The embedding obtained in this manner is used with a one-class classifier for the PAD task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Components of the proposed framework</head><p>Different stages of the proposed framework are described below.</p><p>1) Preprocessing: Before using the data from the sensors, a preprocessing stage consisting of face detection, alignment, and normalization is performed. MTCNN algorithm <ref type="bibr" target="#b42">[42]</ref> was used for face detection in the color channel followed by face landmark detection using Supervised Descent Method (SDM) <ref type="bibr" target="#b43">[43]</ref>. After these stages, the face image is aligned and converted to gray-scale with a resolution of 128 × 128 pixels. Since all the channels are aligned, these face locations are utilized for the alignment of other non-RGB channels as well.</p><p>Also, normalization using Mean Absolute Deviation (MAD) <ref type="bibr" target="#b44">[44]</ref> is performed to convert the raw 16-bit values to the 8-bit range. An example image after preprocessing stage is shown in <ref type="figure">Fig. 3</ref>.</p><p>2) Network architecture and training: Since the data used is multi-channel, we use a multi-channel PAD framework called 'Multi-Channel Convolutional Neural Network'(MCCNN) proposed in <ref type="bibr" target="#b13">[14]</ref> as our base network. The main idea in MCCNN was to use the joint representation from multiple channels for PAD task, leveraging a pretrained face recognition network. The MCCNN architecture constituted of an extended version of LightCNN model <ref type="bibr" target="#b45">[45]</ref> adapted specifically for multi-channel PAD task. A pretrained LightCNN face recognition model was extended to accept multiple channels, and the embeddings from all channels were concatenated, and two fully connected layers were added on top of this joint representation layer for PAD task. The advantage in this architecture is that only lower layer features (which are known as Domain Specific Units (DSU) <ref type="bibr" target="#b46">[46]</ref> ) and higherlevel fully connected layers are adapted in the training phase. The first fully connected layer contains ten nodes, and the second layer contains only one output node. The higherlevel features in the LightCNN part are shared among all the modalities. This approach has two main advantages; first, there is a smaller number of parameters since the high-level features are shared across modalities, second, adapting only DSUs and final fully connected layers reduce possible over-fitting since PAD databases are typically small in size. An optimal set of layers to be adapted was obtained empirically and was used in the baseline MCCNN and the proposed approach.</p><p>In our proposed approach, we use the same MCCNN architecture, and the output from the penultimate fully connected layer was used as the embeddings. To quantify the effectiveness of our approach, we perform experiments on the MCCNN architecture, while using both embeddings and the final output for the loss computation. An illustration of the proposed framework is shown in <ref type="figure">Fig. 4</ref>. At the time of training, both losses are used, and the model corresponding to the lowest validation score is selected. It is to be noted that, at the time of CNN training, both bonafide and (known) attack samples are used. After the CNN training, the network weights are frozen, and the bonafide samples are feed-forwarded to obtain the embeddings.</p><p>3) One-Class Gaussian Mixture Model: After the training of MCCNN with BCE and OCCL, the trained weights of the network are frozen, and it is used as a fixed feature <ref type="figure">Fig. 4</ref>. Schematic diagram of the proposed framework. The CNN architecture is trained with two losses and then used as a fixed feature extractor with frozen weights. The one-class GMM is trained using the embeddings obtained from bonafide class alone. extractor for the PAD task. Now that a compact representation is available, the objective is to learn a one-class classifier using the features obtained. We use One-Class Gaussian Mixture Model for this task. The one class GMM is a generative approach which is used for modeling the distribution of the bonafide class in the proposed framework.</p><p>A Gaussian Mixture Model is defined as the weighted sum of K multivariate Gaussian distributions as:</p><formula xml:id="formula_8">p(x|Θ) = K k=1 w k N (x; µ k , Σ k ),<label>(9)</label></formula><p>where Θ = {w k , µ k , σ k } {k=1,...,K} are the weights, means and the covariance matrix of the GMM.</p><p>Expectation-Maximization (EM) <ref type="bibr" target="#b47">[47]</ref> was used to compute the parameters of the GMM. A full covariance matrix is computed for each component, and the number of components to use was empirically selected as five (K = 5).</p><p>During the training phase, embeddings obtained from bonafide class only are used to train the One-Class GMM.</p><p>In test time, a sample is first forwarded though the network to obtain the embedding x, and then fed to the One-Class GMM to obtain the log-likelihood score as follows:</p><formula xml:id="formula_9">score = log(p(x|Θ))<label>(10)</label></formula><p>In summary, the proposed framework can be considered as a one-class classifier based framework for PAD. The crucial distinction is that, the features used are learned. The loss function proposed forces the CNN to learn a compact representation for the bonafide class leveraging the information from known attack classes. The algorithm for training the framework is shown in Algorithm 1. Update the bonafide center:</p><formula xml:id="formula_10">8 c BF =ĉ BF (1 − α) + α 1 N N i=1 e i 9 end</formula><p>10 Forward x j (bonafide, where y j = 1) through the CNN to obtain Embeddings E j 11 Estimate parameters of GMM from E j :</p><formula xml:id="formula_11">12 Θ GM M = (w k , µ k , Σ k ) 13 Parameters← (W C , Θ GM M )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation details</head><p>To increase the number of samples, data augmentation using random horizontal flips with a probability of 0.5 was used in training. Adam Optimizer <ref type="bibr" target="#b48">[48]</ref> was used to minimize the combined loss function. Learning rate of 1×10 −4 and a weight decay parameter of 1 × 10 −5 was used. The network was trained for 50 epochs on GPU grid with a batch size of 32. The model corresponding to minimum validation loss in the dev set is selected as the best model. For the four-channel models, the MCCNN architecture has about 13.1M parameters and about 14.5 GFLOPS. The implementation was done using PyTorch <ref type="bibr" target="#b49">[49]</ref> library.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In order to evaluate the effectiveness of the proposed approach, we have performed experiments in three publicly available databases, namely WMCA <ref type="bibr" target="#b13">[14]</ref>, MLFP <ref type="bibr" target="#b34">[35]</ref>, and SiW-M <ref type="bibr" target="#b27">[28]</ref> datasets. Recently published CASIA-SURF <ref type="bibr" target="#b36">[37]</ref> database also consists of multi-channel data, namely color, depth, and infrared channels with a limited set of attack instruments. However, the raw data from the sensors were not publicly available; in the publicly available version of the database, images were masked and scaled with custom preprocessing reducing the dynamic range of depth and infrared channels severely. Moreover, there was no guaranteed alignment between the channels. Therefore we can't use our framework with CASIA-SURF database due to the mentioned limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. WMCA dataset</head><p>We have conducted an extensive set of experiments on Wide Multi-Channel presentation Attack (WMCA) 2 database, which contains a total of 1679 video samples of bonafide and attack attempts from 72 identities. The database contains information from four different channels collected simultaneously, namely, color, depth, infrared, and thermal channels. The data was collected using two consumer devices, Intel R RealSense TM SR300 capturing RGB-NIR-Depth streams, and Seek Thermal CompactPRO for the thermal channel. The database contained around eighty different PAIs constituting seven different categories of attacks: print, replay, funny eyeglasses, fake head, rigid mask, flexible silicone mask, and paper masks. The RGB visualization of the attack categories is shown in <ref type="figure">Fig. 5</ref> and the different sessions in <ref type="figure">Fig. 6</ref>. Detailed information about the WMCA database can be found in the publication <ref type="bibr" target="#b13">[14]</ref>. The statistics of the number of samples in each category and their types are shown in <ref type="table" target="#tab_2">Table I</ref>. We have made challenging protocols in the WMCA dataset to perform an extensive set of evaluations emulating real-world unseen attack scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Protocols in WMCA:</head><p>To test the performance of the algorithm in known and unseen attack scenarios, we created three protocols in the WMCA dataset. The protocols are described below.</p><p>• grandtest : This is the exact same grandtest protocol available with WMCA database, here all the attack types are present in almost equal proportions in the train, development and evaluation sets. The attack types and bonafide samples are divided into three folds, and the client ids are disjoint across the three sets. Each presentation attack instrument had a separate client id. The train, dev, eval splits were made in such a way that a specific PA instrument will appear in only one fold. • unseen-2D : In this protocol, we use same splits as grandtest and removed all 2D attacks from train and development groups. and 2D attacks. This emulates the performance of a system when encountered with 2D attacks which was not seen in training. • unseen-3D : In this protocol, we use same splits as grandtest and removed all 3D attacks from train and development groups. Evaluation set contains only bonafide and 3D attacks. This emulates the performance of a system when encountered with 3D attacks which were not seen in training. This is the most challenging protocol as the model sees only the simpler 2D attacks in training and encounter challenging 3D attacks in testing.</p><p>While the grandtest protocol emulates the known attack scenario, other protocols emulate the unseen attack scenario. All protocols are made available publicly.  <ref type="bibr" target="#b34">[35]</ref> consists of attacks captured with seven 3D latex masks and three 2D print attacks. The dataset contains videos captured from color, thermal and infrared channels. Since channels were captured individually in different recording sessions, multi-channel approaches are not trivial. Also, the alignment of channels is not possible since they are not collected simultaneously. Hence, we only use the RGB videos from the MLFP dataset for our experiments. The database contains videos of 10 subjects wearing both print and latex masks. There are 440 videos are consisting of both attacks and bonafide for the RGB channel.</p><p>1) Protocols in MLFP: To emulate known and unseen attack scenarios, we created three new protocols in the MLFP dataset. There are two types of attacks, namely print and mask. Only two sets, i.e., train and evaluation are created due to the small size of the dataset. We used a subset of the train set (10%) for model selection. The protocols are described below.</p><p>• grandtest : This protocol emulates the known attack scenario. Both the attacks are present in both train and evaluation set. However, the subjects and the PAs are disjoint across the two sets. • unseen-print : In this protocol, only bonafide and mask attacks are present in train set; the evaluation set contains only bonafide and print attacks. This emulates unseen attack scenario. • unseen-mask : In this protocol, only bonafide and print attacks are present in train set; the evaluation set contains only bonafide and mask attacks. This protocol also emulates unseen attack scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. SiW-M dataset</head><p>The Spoof in the Wild database with Multiple Attack Types (SiW-M) <ref type="bibr" target="#b27">[28]</ref> consists of a wide variety of attacks captured only in RGB spectra. The database consists of images from 493 subjects, and a total of 660 bonafide and 968 attack samples. A total of 1628 files, consisting of 13 different attack types, collected in different sessions, pose, lighting, and expression (PIE) variations. The attacks consist of various types of masks, makeups, partial attacks, and 2D attacks. The videos are available in 1080P resolution.</p><p>1) Protocols in SiW-M: To emulate unseen attack scenarios, we use the leave-one-out (LOO) testing protocols available with the SiW-M <ref type="bibr" target="#b27">[28]</ref> dataset. The protocols consists of only train and eval sets. In each LOO protocol, the training set consists of 80% percentage of the live data and 12 types of spoof attacks. The evaluation set consists of 20% of bonafide data and the attack which was left out in the training phase. The subjects in bonafide sets are disjoint in train and evaluation sets. A subset of the train set (5%) was used for model selection. Additionally, we have created a grandtest protocol, specifically for cross-database testing which contains all the attack types in all the folds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Evaluation metrics</head><p>We report the standardized ISO/IEC 30107-3 metrics <ref type="bibr" target="#b3">[4]</ref>, Attack Presentation Classification Error Rate (APCER), and Bonafide Presentation Classification Error Rate (BPCER), and Average Classification Error Rate (ACER) in the test set. A BPCER threshold of 1% is used for computing the threshold in dev set. The APCER and BPCER in both dev and eval sets are also reported. Additionally, the ROC curves for experiments are also shown in all the protocols. For the MLFP dataset, we report only EER in the evaluation set since only two sets are available. For SiW-M database, we apply a threshold selected a-priori in all protocols, for computing the metrics, to be comparable with the results in <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Baselines</head><p>We have implemented three feature-based baselines and two CNN based baselines. For a fair comparison, all the benchmarks are multi-channel methods and use the same four channels. Besides, an RGB only CNN model is also added for comparison. A short description of the baselines along with the acronyms used are shown below:</p><p>• MC-RDWT-Haralick-SVM: This baseline is the multichannel extension of the RDWT-Haralick-SVM approach proposed in <ref type="bibr" target="#b34">[35]</ref>; the images from all channels are stacked together after preprocessing. For each channel, the image is divided into a 4 × 4 grid, and Haralick <ref type="bibr" target="#b50">[50]</ref> features obtained from the RDWT decompositions are concatenated from all the grids in all channels to get the joint feature vector. The joint feature is used with a linear SVM for PAD. This model only uses RGB information for PAD.</p><p>• MC-ResNetPAD: We reimplemented the architecture from <ref type="bibr" target="#b37">[38]</ref> extending it to four channels, based on their opensource implementation <ref type="bibr" target="#b2">3</ref> . This approach obtained the first place solution in the 'CASIA-SURF' challenge. For a fair comparison, instead of using an ensemble we used the best pretrained model as suggested in <ref type="bibr" target="#b37">[38]</ref>. • MCCNN(BCE) : This is the multi-channel CNN system described in <ref type="bibr" target="#b13">[14]</ref>, which achieved state of the art performance in the grandtest protocol. The model is trained using Binary Cross-Entropy (BCE) loss only. All the baseline methods described are reproducible, and the details about the parameters can be found in our open-source package <ref type="bibr" target="#b3">4</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Experiments and Results in WMCA dataset</head><p>We have tested the baselines and the proposed approach in three different protocols in WMCA. The proposed approach is denoted as MCCNN(BCE+OCCL)-GMM.</p><p>• MCCNN(BCE+OCCL)-GMM: Here, the bonafide embeddings from the MCCNN trained using both the losses are used to train a GMM, and in the evaluation stage, the score from the one class GMM is used as the PAD score. The results in each protocol are described below. 1) Experiments in grandtest protocol: The grandtest protocol emulates the known attack scenario. <ref type="table" target="#tab_2">Table II</ref> tabulates the results in the grandtest protocol. The proposed approach outperforms the feature-based methods by a large margin as expected. The model MC-RDWT-Haralick-GMM trained using a one-class model achieves the worse results. It is interesting to note that the MC-RDWT-Haralick-SVM model, trained using the same feature as a binary classifier performed much better. This shows one weakness of one-class classifiers in a known attack scenario, as they do not use the known attacks in training. The MCCNN(BCE) achieves much better performance as compared to MC-ResNetPAD. The MCCNN(BCE) trained as a binary classifier achieves the best performance in this protocol. The proposed MCCNN(BCE+OCCL)-GMM approach achieves comparable performance to MCCNN(BCE). This indicates that the one class GMM classifier performs on par with the binary classification, provided they are trained with compact feature representations.</p><p>2) Experiments in unseen-2D and unseen-3D protocol: The unseen-2D and unseen-3D protocols emulates the unseen attack scenario. The unseen-3D is the most challenging protocol since it is trained only on 2D -print and replay attacks and encounters a wide variety of 3D attacks such as silicone masks, fake heads, mannequins, etc. in the eval set.</p><p>Most of the approaches perform well in the unseen-2D protocol. This result is intuitive as these models are trained on challenging 3D attacks, detection of 2D attacks is much easier. Moreover, the 2D attacks can be easily identified in depth, thermal, and infrared channels. Even some featurebased methods perform well in this protocol, with MC-RDWT-Haralick-GMM method achieving the best performance. This 3 Available from: https://github.com/AlexanderParkin/ChaLearn liveness challenge <ref type="bibr" target="#b3">4</ref> Source code: https://gitlab.idiap.ch/bob/bob.paper.oneclass mccnn 2019</p><p>shows the advantage of one class model in an unseen attack scenario. The proposed approach MCCNN(BCE+OCCL)-GMM and MCCNN(BCE) baseline perform comparably in this protocol. Notably, the DeepPixBiS model achieves much worse results in this protocol. This could be because discriminating between bonafide and 2D attacks are harder when only RGB information is used.</p><p>The unseen-3D protocol shows important results. All the baselines show inferior performance when encountered with unseen 3D samples. This shows the failure of binary classifiers in generalizing to challenging unseen attacks. The MCCNN(BCE) approach, while being architecturally similar, fails to generalize when trained in the binary classification setting. With the proposed approach, performance improves to 9.7% when the one class GMM is used on the bonafide representations. Since the network learns to map the bonafide samples to a compact cluster in the feature space, even in the presence of unseen attacks, the decision boundary learned for the bonafide class is robust. The unseen attacks map far from the bonafide cluster and hence becomes easy to discriminate from bonafide samples. This result is encouraging since the network was shown only 2D attacks in training, and still, it manages to achieve good performance against challenging 3D attacks. The ROCs for all the protocols are shown in <ref type="figure" target="#fig_4">Fig. 7</ref>.</p><p>The t-SNE <ref type="bibr" target="#b51">[51]</ref> plots of the embeddings for all protocols are shown in <ref type="figure" target="#fig_5">Fig. 8</ref>. Five frames from each video in the evaluation sets of the protocols are used for this visualization. While the difference between bonafide and attacks are clear in the grandtest and unseen-2D, difference in unseen-3D protocol is very evident. It can be clearly seen that the bonafide class clusters together and is far from the bonafide representation in the embedding space in the unseen-3D protocol when the proposed loss is used. Unseen attacks overlaps with bonafide embeddings when only BCE is used. This clearly demonstrates the effectiveness of the proposed approach for unseen attack detection. The unseen attacks which are overlapping with the bonafide region are shown in <ref type="figure">Fig. 9</ref>. It can be seen that some video replay samples and flexible silicone 3D masks get misclassified in unseen-2D and unseen-3D protocols respectively.</p><p>3) Ablation study with channels: To evaluate the performance of the proposed framework on different set of channels, we perform an ablation study by including a different set of channels. We used only the best performing MCCNN(BCE+OCCL)-GMM approach in this ablation study. In all combinations, the gray-scale channel is present since it is used as a reference. This is required as the embedding from the gray-scale part can be used for face recognition as well.</p><p>The acronyms for different channels are shown below:</p><p>• G: Gray-scale image • D: Depth image • I: Infrared channel • T: Thermal channel Various combinations of these channels are experimented with, and the results are tabulated in <ref type="table" target="#tab_2">Table IV</ref>. It is to be noted that the channels G, D and I come from the same device and T is coming from a different device. Usually, thermal cameras are expensive, compared to RGB-D cameras, and   Grandtest protocol contains only known attacks in the test set. <ref type="figure">Fig. 9</ref>. The attack samples which are closer to bonafide cluster in a) unseen-2D ( <ref type="figure" target="#fig_5">Fig.8(E)</ref>) and b) unseen-3D (( <ref type="figure" target="#fig_5">Fig.8(F)</ref>)) protocol for the proposed framework.</p><p>hence the combinations involving subsets of G, D and I are more interesting from a deployment point of view. From <ref type="table" target="#tab_2">Table IV</ref>, it can be seen that the performance degrades as channels are removed. However, the combination GI achieves reasonable performance while considering the performance-cost ratio. The ROCs for different protocols are shown in <ref type="figure" target="#fig_0">Fig. 10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Experiments and Results in MLFP dataset</head><p>We have used only the RGB channel for the experiments since the other channels were not captured simultaneously. For the MCCNN framework and other baselines, 'R', 'G', and 'B' are considered as the different channels in these experiments. We have performed the experiments in the three newly created protocols and the results are tabulated in <ref type="table" target="#tab_5">Table V</ref>.</p><p>From the results in <ref type="table" target="#tab_5">Table V</ref>, it can be seen that the CNN based approach outperforms the feature-based approaches. The MCCNN framework, with the addition of the newly proposed loss outperforms the architecture trained with BCE only, showing the effectiveness of the proposed approach.</p><p>Even though the proposed approach performs better than the baselines, it is to be noted that the key point of the proposed approach, leveraging multi-channel information, is not utilized here. The architecture is not optimized for PAD in RGB and this experiment is performed only to show the change in performance with the new loss function. Nevertheless, the proposed approach achieves better performance as compared to the baselines in all the protocols.  <ref type="table" target="#tab_2">Table VI</ref> shows the performance of the proposed framework, again in RGB only scenario. CNN based methods performs much better than feature based methods in this case. It can be seen that the proposed approach achieves better performance as compared to baseline methods. The performance of the MCCNN (BCE+OCCL)-GMM is better compared to the MCCNN(BCE) model. It can be seen that the addition of the new loss function makes the classification of unseen attacks more accurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Experiments and Results in SiW-M dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Cross-database evaluations</head><p>As we could not perform cross-database evaluation between a multi-channel database and an RGB only database, we used only the RGB channels from two datasets for the crossdatabase evaluation. We have selected WMCA and SiW-M datasets as they are relatively large and consist of a wide variety of attacks.</p><p>From <ref type="table" target="#tab_2">Table VII</ref>, it can be seen that the MCCNN model with and without the new loss performs comparably. In general, the performance in the cross-database setting is poor for all the models. The poor performance could be due to the disparity in acquisition conditions and the attack types. A wider variety of attacks makes it difficult for the classifier to identify attacks using RGB channels alone. The cross-database performance with this wide variety of attacks seems more challenging as compared to typical cross-database evaluations using only 2D attacks. Using multiple channels <ref type="bibr" target="#b13">[14]</ref> may alleviate these issues. This also points to the limitation of RGB only methods while dealing with a wide variety of attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DISCUSSIONS</head><p>From the experiments in WMCA database, it can be clearly seen that CNN based method outperforms the feature-based methods by a large margin. While comparing the MC-CNN(BCE) method to the proposed method, the performance  is comparable in the known attack scenario. This indicates that the proposed One-Class GMM based approach performs par with binary classification, thanks to the embedding learned with the proposed loss function. Most of the approaches perform well in the unseen-2D protocol since it can be clearly discriminated in many channels. Moreover, it shows that if the network is trained in challenging attacks, simpler attacks are easy to detect. While the performance is comparable in grandtest and unseen-2D protocols, the proposed method achieves a large performance boost in the most challenging unseen-3D protocol. The proposed loss function forces the network to learn a compact representation for bonafide samples in the feature space. Both known and unknown attacks get mapped far from the bonafide cluster in the feature space. The decision boundary learned by the one class model seems to be robust in identifying both seen and unseen attacks in such a scenario. This result is significant for several reasons. It is to be noted that in the unseen-3D protocol, the network is trained with only 2D attacks, i.e., prints and replays. The proposed method achieves excellent performance in a test set consisting of challenging 3D attacks such as custom silicone masks, paper masks, mannequins, etc. The real-world implications of this approach is very promising. The proposed method can be used to develop robust PAD systems without the requirement of having to manufacture costly presentation attacks. The models can be trained on easy to obtain attacks based on availability. The proposed framework utilizes the available (known) attack categories to learn a robust representation to facilitate known and unseen attack detection. It is to be noted that the compact representation is made possible by the joint multi-channel representation used. In practical deployment scenarios, it may not be possible to use all the four channels due to the computational or cost constraints. In such a situation, models trained on available channels can be selected based on the performance-cost ratio by sub selecting the channels. The results from the ablation study presented in <ref type="table" target="#tab_2">Table IV</ref> can be used to determine which channels should be used in such cases.</p><p>Similarly, the experiments in MLFP and SiW-M database also shows CNN based methods outperform feature-based baselines. Although we have not used multi-channel information in the experiments, the experiment results showcase the performance improvement with the new loss function. Using the proposed framework together with network backbones designed specifically for RGB PAD might improve the results.</p><p>The cross-database performance shows the limitations of the RGB channel when tested with a wide variety of attacks. The performance of the baselines as well as the proposed approach is poor when only RGB data is used. This shows the challenging nature of RGB only PAD while considering a wide variety of attacks. The usage of multiple channels as done in WMCA dataset might improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>Face presentation attack detection is often considered as a binary classification task which results in over-fitting to the known attacks leading to poor generalization against unseen attacks. In this paper, we address this problem with a new framework using a one-class classifier. A novel loss function is formulated, which forces the CNN to learn a compact yet discriminative representation for the face images. The bonafide samples form a compact cluster in the feature space, thanks to the proposed loss function. A decision boundary around the bonafide representation can be obtained using a one-class model. Both known and unknown attacks map far from the bonafide cluster in the feature space which can be classified by the one-class model. The proposed framework introduces a new way to learn a robust PAD system from bonafide and available (known) attack classes. The proposed system has been evaluated in the challenging WMCA, MLFP, and SiW-M databases and was shown to outperform the baseline featurebased and CNN based methods in both known and unseen attack scenarios. The drastic improvement in the performance in unseen-3D protocol in WMCA shows the robustness of the proposed approach against unseen attacks, thanks to the multi-channel information. The proposed method also shows improvement even when used together with RGB channels alone. The source code and protocols to reproduce the results are made available publicly to enable further extensions of the proposed framework.</p><p>ACKNOWLEDGMENT Part of this research is based upon work supported by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via IARPA R&amp;D Contract No. 2017-17020200005. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Illustration of the embedding space with known and unknown attack classes. The red dotted line shows the decision boundary learned when only bonafide and known attacks are present in the training set, this results in misclassification of unknown attacks as bonafide. If a one class decision boundary (green-dotted lines) is learned, then both known and unknown attacks can be classified correctly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Loss functions acting on the embedding space, left) bonafide representations are pulled closer to the center of bonafide class (green), while the attack embeddings(red) are forced to be beyond the margin. The attack samples outside the margin does not contribute to the loss, right) The loss as a function of distance from the bonafide center.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1 : 4 Forward x i through the CNN 5</head><label>145</label><figDesc>Algorithm for training the proposed framework Data: (x i , y i ), where x i is multi-channel input and y i ∈ 0, 1; 0 -for attack and 1-for bonafide Result: W C -CNN weights, Θ GM M -Parameters of GMM 1 Constants : λ -weighting factor, µ -learning rate 2 Initialize : C BF -center of bonafide class, W Cinitial weights of CNN from pretrained model 3 for mini-batch ← 1 to P do Compute the combined loss: L = (1 − λ)L BCE + λL OCCL 6 Back-propagate the loss and update the weights of DSUs and FC layers 7</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>• MC-RDWT-Haralick-GMM: Here, the feature extraction stage is same as MC-RDWT-Haralick-SVM; however, the classifier used is one class GMM. Only bonafide samples are used in training this model. This model is added to show the performance of one class models in unseen attack scenarios. • MC-LBP-SVM: Here, again, the same preprocessing is performed on all the channels first. After this, Spatially enhanced histograms of LBP representation from all the component channels are computed and concatenated to a feature vector. The features extracted are fed to an SVM for PAD task. • DeepPixBiS : This is a CNN based system<ref type="bibr" target="#b9">[10]</ref> trained using both binary and pixel-wise binary loss function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>DET curves for the eval sets of different protocols of WMCA dataset a) grandtest, b) unseen-2D, c) unseen-3D protocol.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>t-SNE plots of embeddings in the protocols in WMCA dataset. First row (a,b,c) shows the embeddings when only BCE loss was used. Second row (d,e,f) shows the embeddings when both the losses are used. Embeddings of both known and unseen attacks are shown in the figures for each protocol.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10 .</head><label>10</label><figDesc>Ablation study with different combination of channels, DET curves for the eval sets of different protocols of WMCA dataset a) grandtest, b) unseen-2D, c) unseen-3D protocol.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and S. Marcel are in Idiap Research Institute, Centre du Parc, Rue Marconi 19, CH -1920, Martigny, Switzerland. (e-mail: {anjith.george,sebastien.marcel}@idiap.ch)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I STATISTICS</head><label>I</label><figDesc>OF ATTACKS IN WMCA DATABASE</figDesc><table><row><cell>PA Category</cell><cell>Type</cell><cell>#Presentations</cell></row><row><cell>bonafide</cell><cell>-</cell><cell>347</cell></row><row><cell>glasses</cell><cell>Partial</cell><cell>75</cell></row><row><cell>print</cell><cell>2D</cell><cell>200</cell></row><row><cell>replay</cell><cell>2D</cell><cell>348</cell></row><row><cell>fake head</cell><cell>3D</cell><cell>122</cell></row><row><cell>rigid mask</cell><cell>3D</cell><cell>137</cell></row><row><cell>flexible mask</cell><cell>3D</cell><cell>379</cell></row><row><cell>paper mask</cell><cell>3D</cell><cell>71</cell></row><row><cell>TOTAL</cell><cell></cell><cell>1679</cell></row><row><cell>B. MLFP dataset</cell><cell></cell><cell></cell></row><row><cell>MLFP dataset</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II PERFORMANCE</head><label>II</label><figDesc>OF THE BASELINE SYSTEMS AND THE PROPOSED METHOD IN grandtest PROTOCOL OF WMCA DATASET. THE VALUES REPORTED ARE OBTAINED WITH A THRESHOLD COMPUTED FOR BPCER 1% IN dev SET.TABLE III PERFORMANCE OF THE BASELINE SYSTEMS AND THE PROPOSED METHOD IN UNSEEN PROTOCOLS OF WMCA DATASET. THE VALUES REPORTED ARE OBTAINED WITH A THRESHOLD COMPUTED FOR BPCER 1% IN dev SET.</figDesc><table><row><cell>Method</cell><cell cols="3">dev (%)</cell><cell></cell><cell></cell><cell cols="2">test (%)</cell></row><row><cell></cell><cell cols="2">APCER</cell><cell cols="2">ACER</cell><cell cols="2">APCER</cell><cell>BPCER</cell><cell>ACER</cell></row><row><cell>MC-RDWT-Haralick-SVM</cell><cell>3.6</cell><cell></cell><cell>2.3</cell><cell></cell><cell></cell><cell>5.4</cell><cell>1.2</cell><cell>3.3</cell></row><row><cell>MC-LBP-SVM</cell><cell>3.6</cell><cell></cell><cell>2.3</cell><cell></cell><cell></cell><cell>8.5</cell><cell>0.6</cell><cell>4.6</cell></row><row><cell>MC-RDWT-Haralick-GMM</cell><cell cols="2">43.4</cell><cell>22.2</cell><cell></cell><cell></cell><cell>47.7</cell><cell>1.7</cell><cell>24.7</cell></row><row><cell>DeepPixBiS (RGB only)[10]</cell><cell>1.0</cell><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell>8.2</cell><cell>3.7</cell><cell>6</cell></row><row><cell>MC-ResNetPAD [38]</cell><cell>3.8</cell><cell></cell><cell>2.4</cell><cell></cell><cell></cell><cell>3.5</cell><cell>1.6</cell><cell>2.6</cell></row><row><cell>MCCNN(BCE)[14]</cell><cell>0.4</cell><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell>0.5</cell><cell>0</cell><cell>0.2</cell></row><row><cell>MCCNN(BCE+OCCL)-GMM</cell><cell>0.1</cell><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell>0.6</cell><cell>0.1</cell><cell>0.4</cell></row><row><cell>Method</cell><cell cols="3">unseen-2D</cell><cell></cell><cell></cell><cell></cell><cell>unseen-3D</cell></row><row><cell cols="2">APCER</cell><cell cols="2">BPCER</cell><cell cols="2">ACER</cell><cell cols="2">APCER</cell><cell>BPCER</cell><cell>ACER</cell></row><row><cell>MC-RDWT-Haralick-SVM</cell><cell>0.3</cell><cell>0.1</cell><cell></cell><cell>0.2</cell><cell></cell><cell>66.0</cell><cell>0.1</cell><cell>33.1</cell></row><row><cell>MC-LBP-SVM</cell><cell>40.7</cell><cell>0.1</cell><cell></cell><cell cols="2">20.4</cell><cell>38.9</cell><cell>0.2</cell><cell>19.5</cell></row><row><cell>MC-RDWT-Haralick-GMM</cell><cell>0.0</cell><cell>0.2</cell><cell></cell><cell>0.1</cell><cell></cell><cell>70.8</cell><cell>1.9</cell><cell>36.4</cell></row><row><cell>DeepPixBiS (RGB only)[10]</cell><cell>77.7</cell><cell>0.3</cell><cell></cell><cell>39</cell><cell></cell><cell>74.7</cell><cell>16.3</cell><cell>45.5</cell></row><row><cell>MC-ResNetPAD [38]</cell><cell>4.1</cell><cell>0.9</cell><cell></cell><cell>2.5</cell><cell></cell><cell>92.2</cell><cell>6.4</cell><cell>49.3</cell></row><row><cell>MCCNN(BCE)[14]</cell><cell>0.0</cell><cell>1.0</cell><cell></cell><cell>0.5</cell><cell></cell><cell>62.0</cell><cell>0.0</cell><cell>31.0</cell></row><row><cell>MCCNN(BCE+OCCL)-GMM</cell><cell>0.3</cell><cell>0.6</cell><cell></cell><cell>0.5</cell><cell></cell><cell>15.4</cell><cell>3.9</cell><cell>9.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV PERFORMANCE</head><label>IV</label><figDesc>OF THE PROPOSED FRAMEWORK WITH DIFFERENT COMBINATIONS OF CHANNELS IN ALL PROTOCOLS OF WMCA DATASET. THE VALUES REPORTED ARE OBTAINED WITH A THRESHOLD COMPUTED FOR BPCER 1% IN dev SET.</figDesc><table><row><cell>Channels</cell><cell>grandtest ACER</cell><cell>unseen-2D ACER</cell><cell>unseen-3D ACER</cell></row><row><cell>GDIT</cell><cell>0.4</cell><cell>0.5</cell><cell>9.7</cell></row><row><cell>GDI</cell><cell>1.1</cell><cell>11.2</cell><cell>23.1</cell></row><row><cell>GT</cell><cell>2.2</cell><cell>3.2</cell><cell>21.5</cell></row><row><cell>GD</cell><cell>2.3</cell><cell>49.4</cell><cell>45.4</cell></row><row><cell>GI</cell><cell>1.1</cell><cell>2.2</cell><cell>22.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V PERFORMANCE</head><label>V</label><figDesc>OF THE PROPOSED FRAMEWORK IN THE PROTOCOLS IN MLFP DATASET. ONLY RGB CHANNEL WAS USED IN THIS EXPERIMENTS. THE VALUES REPORTED ARE THE EER IN THE evaluation SET.</figDesc><table><row><cell>Algorithm</cell><cell>grandtest</cell><cell>unseen print</cell><cell>unseen mask</cell></row><row><cell>MC-RDWT-Haralick-SVM</cell><cell>9.8</cell><cell>12.0</cell><cell>32.2</cell></row><row><cell>MC-LBP-SVM</cell><cell>6.3</cell><cell>27.1</cell><cell>9.3</cell></row><row><cell>MC-RDWT-Haralick-GMM</cell><cell>27.4</cell><cell>40.8</cell><cell>21.5</cell></row><row><cell>DeepPixBiS (RGB only)[10]</cell><cell>6.3</cell><cell>24.8</cell><cell>17.5</cell></row><row><cell>MCCNN (BCE)</cell><cell>5.5</cell><cell>9.2</cell><cell>5.2</cell></row><row><cell>MCCNN (BCE+OCCL)-GMM</cell><cell>1.2</cell><cell>3.3</cell><cell>3.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI PERFORMANCE</head><label>VI</label><figDesc>OF THE PROPOSED FRAMEWORK IN THE LEAVE ONE OUT PROTOCOLS IN SiW-M DATASET. ONLY RGB CHANNEL WAS PRESENT IN THIS DATASET. Paper Manne. Obfusc. Imperson. Cosmetic Funny Eye Paper Glasses Partial Paper</figDesc><table><row><cell cols="8">Methods Silicone Trans. MC-RDWT-Haralick-SVM Metrics (%) Replay Print Mask Attacks Half APCER 19.80 19.15 30.76 28.15 33.35 BPCER 14.50 13.89 14.66 16.83 15.38 16.68 0.29 ACER 17.15 16.52 22.71 22.49 24.37 8.49</cell><cell>4.50 15.88 10.19</cell><cell>68.91 16.03 42.47</cell><cell>Makeup Attacks 0.00 16.53 8.26</cell><cell>35.20 16.37 25.79</cell><cell>53.12 14.58 33.85</cell><cell>Partial Attacks 34.53 14.47 24.50</cell><cell>3.49 15.73 9.61</cell><cell>Average 25.4 ± 20.8 15.5 ± 0.9 20.4 ± 10.3</cell></row><row><cell></cell><cell>EER</cell><cell>16.88</cell><cell cols="2">16.53 21.80</cell><cell>20.73</cell><cell>21.94</cell><cell>7.34</cell><cell>9.88</cell><cell>32.56</cell><cell>2.37</cell><cell>23.51</cell><cell>31.72</cell><cell>21.94</cell><cell>10.05</cell><cell>18.2 ± 9.0</cell></row><row><cell></cell><cell>APCER</cell><cell>10.77</cell><cell cols="2">12.91 10.28</cell><cell>35.19</cell><cell>37.78</cell><cell>0.59</cell><cell>6.50</cell><cell>96.09</cell><cell>0.00</cell><cell>26.00</cell><cell>40.91</cell><cell>35.51</cell><cell>2.73</cell><cell>24.2 ± 26.3</cell></row><row><cell>MC-LBP-SVM</cell><cell>BPCER ACER</cell><cell>22.90 16.83</cell><cell cols="2">22.18 22.48 17.54 16.38</cell><cell>22.33 28.76</cell><cell>23.13 30.46</cell><cell>23.70 12.15</cell><cell>23.59 15.04</cell><cell>22.79 59.44</cell><cell>23.93 11.97</cell><cell>22.90 24.45</cell><cell>19.92 30.41</cell><cell>21.11 28.31</cell><cell>23.74 13.24</cell><cell>22.6 ± 1.1 23.4 ± 12.9</cell></row><row><cell></cell><cell>EER</cell><cell>15.96</cell><cell cols="2">16.83 16.87</cell><cell>28.51</cell><cell>29.77</cell><cell>10.54</cell><cell>12.75</cell><cell>52.60</cell><cell>1.90</cell><cell>24.61</cell><cell>28.32</cell><cell>26.76</cell><cell>11.29</cell><cell>21.2 ± 12.6</cell></row><row><cell></cell><cell>APCER</cell><cell>23.7</cell><cell>7.3</cell><cell>27.7</cell><cell>18.2</cell><cell>97.8</cell><cell>8.3</cell><cell>16.2</cell><cell>100.0</cell><cell>18.0</cell><cell>16.3</cell><cell>91.8</cell><cell>72.2</cell><cell>0.4</cell><cell>38.3 ± 37.4</cell></row><row><cell>Auxiliary [19]</cell><cell>BPCER ACER</cell><cell>10.1 16.8</cell><cell>6.5 6.9</cell><cell>10.9 19.3</cell><cell>11.6 14.9</cell><cell>6.2 52.1</cell><cell>7.8 8.0</cell><cell>9.3 12.8</cell><cell>11.6 55.8</cell><cell>9.3 13.7</cell><cell>7.1 11.7</cell><cell>6.2 49.0</cell><cell>8.8 40.5</cell><cell>10.3 5.3</cell><cell>8.9 ± 2.0 23.6 ± 18.5</cell></row><row><cell></cell><cell>EER</cell><cell>14.0</cell><cell>4.3</cell><cell>11.6</cell><cell>12.4</cell><cell>24.6</cell><cell>7.8</cell><cell>10.0</cell><cell>72.3</cell><cell>10.1</cell><cell>9.4</cell><cell>21.4</cell><cell>18.6</cell><cell>4.0</cell><cell>17.0 ± 17.7</cell></row><row><cell></cell><cell>APCER</cell><cell>1.0</cell><cell>0.0</cell><cell>0.7</cell><cell>24.5</cell><cell>58.6</cell><cell>0.5</cell><cell>3.8</cell><cell>73.2</cell><cell>13.2</cell><cell>12.4</cell><cell>17.0</cell><cell>17.0</cell><cell>0.2</cell><cell>17.1 ± 23.3</cell></row><row><cell>Deep Tree Network [28]</cell><cell>BPCER ACER</cell><cell>18.6 9.8</cell><cell>11.9 6.0</cell><cell>29.3 15.0</cell><cell>12.8 18.7</cell><cell>13.4 36.0</cell><cell>8.5 4.5</cell><cell>23.0 7.7</cell><cell>11.5 48.1</cell><cell>9.6 11.4</cell><cell>16.0 14.2</cell><cell>21.5 19.3</cell><cell>22.6 19.8</cell><cell>16.8 8.5</cell><cell>16.6 ± 6.2 16.8 ± 11.1</cell></row><row><cell></cell><cell>EER</cell><cell>10.0</cell><cell>2.1</cell><cell>14.4</cell><cell>18.6</cell><cell>26.5</cell><cell>5.7</cell><cell>9.6</cell><cell>50.2</cell><cell>10.1</cell><cell>13.2</cell><cell>19.8</cell><cell>20.5</cell><cell>8.8</cell><cell>16.1 ± 12.2</cell></row><row><cell></cell><cell>APCER</cell><cell>19.18</cell><cell>8.97</cell><cell>1.74</cell><cell>21.30</cell><cell>60.68</cell><cell>0.00</cell><cell>1.00</cell><cell>100.00</cell><cell>0.00</cell><cell>26.90</cell><cell>64.66</cell><cell>77.52</cell><cell>0.29</cell><cell>29.4± 34.4</cell></row><row><cell>DeepPixBiS [10]</cell><cell>BPCER ACER</cell><cell>8.70 13.94</cell><cell>7.63 8.30</cell><cell>11.03 6.38</cell><cell>11.76 16.53</cell><cell>10.27 35.47</cell><cell>8.85 4.43</cell><cell>8.63 4.81</cell><cell>10.53 55.27</cell><cell>11.60 5.80</cell><cell>10.99 18.95</cell><cell>10.31 37.48</cell><cell>10.23 43.87</cell><cell>7.10 3.69</cell><cell>9.8± 1.4 19.6± 17.4</cell></row><row><cell></cell><cell>EER</cell><cell>11.68</cell><cell>7.94</cell><cell>7.22</cell><cell>15.04</cell><cell>21.30</cell><cell>3.78</cell><cell>4.52</cell><cell>26.49</cell><cell>1.23</cell><cell>14.89</cell><cell>23.28</cell><cell>18.90</cell><cell>4.82</cell><cell>12.3± 8.2</cell></row><row><cell></cell><cell>APCER</cell><cell>38.93</cell><cell>30.60</cell><cell>7.85</cell><cell>20.00</cell><cell>32.56</cell><cell>0.00</cell><cell>2.00</cell><cell>70.65</cell><cell>0.00</cell><cell>29.00</cell><cell>46.69</cell><cell>57.32</cell><cell>23.20</cell><cell>27.6 ± 22.1</cell></row><row><cell>MCCNN (BCE)</cell><cell>BPCER ACER</cell><cell>7.10 23.01</cell><cell>6.45 18.52</cell><cell>7.48 7.66</cell><cell>10.04 15.02</cell><cell>12.56 22.56</cell><cell>8.59 4.29</cell><cell>10.04 6.02</cell><cell>9.96 40.31</cell><cell>11.72 5.86</cell><cell>11.37 20.19</cell><cell>12.75 29.72</cell><cell>7.71 32.52</cell><cell>9.89 16.54</cell><cell>9.6 ± 2.0 18.6 ± 11.1</cell></row><row><cell></cell><cell>EER</cell><cell>17.08</cell><cell>11.83</cell><cell>7.56</cell><cell>12.82</cell><cell>16.09</cell><cell>0.71</cell><cell>6.85</cell><cell>25.94</cell><cell>2.29</cell><cell>16.30</cell><cell>18.90</cell><cell>22.82</cell><cell>13.13</cell><cell>13.2 ± 7.4</cell></row><row><cell></cell><cell>APCER</cell><cell>11.79</cell><cell>9.53</cell><cell>3.12</cell><cell>3.70</cell><cell>39.20</cell><cell>0.00</cell><cell>3.12</cell><cell>44.57</cell><cell>0.00</cell><cell>21.60</cell><cell>19.34</cell><cell>35.55</cell><cell>0.00</cell><cell>14.7 ± 15.9</cell></row><row><cell>MCCNN (BCE+OCCL)-GMM</cell><cell>BPCER ACER</cell><cell>13.44 12.61</cell><cell cols="2">16.15 16.26 12.84 9.69</cell><cell>20.23 11.97</cell><cell>11.11 25.16</cell><cell>13.74 6.87</cell><cell>8.66 5.89</cell><cell>15.23 29.90</cell><cell>12.67 6.34</cell><cell>10.42 16.01</cell><cell>14.31 16.83</cell><cell>18.40 26.97</cell><cell>27.33 13.66</cell><cell>15.2 ± 4.8 14.9 ± 7.8</cell></row><row><cell></cell><cell>EER</cell><cell>12.82</cell><cell cols="2">12.94 11.33</cell><cell>13.70</cell><cell>13.47</cell><cell>0.56</cell><cell>5.60</cell><cell>22.17</cell><cell>0.59</cell><cell>15.14</cell><cell>14.40</cell><cell>23.93</cell><cell>9.82</cell><cell>12.0 ± 6.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VII THE</head><label>VII</label><figDesc>RESULTS FROM THE CROSS-DATABASE TESTING BETWEEN WMCA AND SIW-M DATASETS USING THE GRANDTEST PROTOCOL, ONLY RGB CHANNELS WERE USED IN THIS EXPERIMENT.</figDesc><table><row><cell></cell><cell cols="2">trained on</cell><cell cols="2">trained on</cell></row><row><cell>Method</cell><cell cols="2">WMCA</cell><cell cols="2">SiW-M</cell></row><row><cell></cell><cell>tested on</cell><cell>tested on</cell><cell>tested on</cell><cell>tested on</cell></row><row><cell></cell><cell>WMCA</cell><cell>SiW-M</cell><cell>SiW-M</cell><cell>WMCA</cell></row><row><cell>MC-RDWT-Haralick-SVM</cell><cell>14.6</cell><cell>29.6</cell><cell>15.1</cell><cell>45.3</cell></row><row><cell>MC-LBP-SVM</cell><cell>26.6</cell><cell>45.5</cell><cell>19.6</cell><cell>38.6</cell></row><row><cell>MC-RDWT-Haralick-GMM</cell><cell>27.9</cell><cell>34.0</cell><cell>25.5</cell><cell>43.6</cell></row><row><cell>DeepPixBiS</cell><cell>7.5</cell><cell>49.1</cell><cell>14.7</cell><cell>44.4</cell></row><row><cell>MCCNN (BCE)</cell><cell>12.1</cell><cell>34.0</cell><cell>9.9</cell><cell>42.3</cell></row><row><cell>MCCNN (BCE+OCCL)-GMM</cell><cell>12.3</cell><cell>31.9</cell><cell>9.5</cell><cell>41.8</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Handbook of face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Labeled faces in the wild: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in face detection and facial image analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="189" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Handbook of biometric anti-spoofing : Presentation attack detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Nixon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fierrez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<editor>Marcel, S., Nixon, M.S., Fierrez, J., Evans, N.</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Information technology International Organization for Standardization</title>
		<idno>ISO/IEC JTC 1/SC 37 Biometrics</idno>
		<imprint>
			<date type="published" when="2016-02" />
		</imprint>
		<respStmt>
			<orgName>ISO Standard</orgName>
		</respStmt>
	</monogr>
	<note>International Organization for Standardization</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Face anti-spoofing based on color texture analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Boulkenafet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Komulainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Processing (ICIP), 2015 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2636" to="2640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Face spoofing detection from single images using micro-texture analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Määttä</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biometrics (IJCB), 2011 international joint conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Counter-measures to photo attacks in face recognition: a public database and a baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anjos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biometrics (IJCB), 2011 international joint conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Presentation attack detection methods for face recognition systems: a comprehensive survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramachandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Busch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Remote blood pulse analysis for face presentation attack detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heusch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Biometric Anti-Spoofing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="267" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep pixel-wise binary supervision for face presentation attack detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Biometrics</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Extended multispectral face presentation attack detection: An approach based on fusing information from individual spectral bands</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raghavendra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">B</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Busch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Fusion (Fusion), 2017 20th International Conference on</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Reliable face anti-spoofing using multispectral swir imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">What you can&apos;t see can help youextended-range imaging for 3d-mask presentation attack detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Biometrics Special Interest Group., no. EPFL-CONF-231840</title>
		<meeting>the 16th International Conference on Biometrics Special Interest Group., no. EPFL-CONF-231840</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Gesellschaft fuer Informatik eV (GI</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Biometric face presentation attack detection with multichannel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Mostaani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Geissenbuhler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nikisins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anjos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Can your face detector do anti-spoofing? face presentation attack detection with a multi-channel face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Idiap Research Report, Idiap-RR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep models and shortwave infrared information to detect face presentation attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heusch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Geissbühler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Mostaani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Behavior, and Identity Science (T-BIOM)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Domain adaptation in multichannel autoencoder based features for robust face anti-spoofing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nikisins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Biometrics (ICB)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spoofing deep face recognition with custom silicone masks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohammadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biometrics Theory, Applications and Systems (BTAS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>IEEE 9th International Conference on</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning deep models for face antispoofing: Binary or auxiliary supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="389" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Face anti-spoofing using patch and depth-based cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Atoum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Joint Conference on</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="319" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep convolutional dynamic texture learning with adaptive channel-discriminability for 3d mask face antispoofing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="page" from="748" to="755" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An anomaly detection approach to face spoofing detection: A new formulation and evaluation protocol</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Arashloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Christmas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="13" to="868" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On effectiveness of anomaly detection approaches against unseen presentation attacks in face anti-spoofing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nikisins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohammadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anjos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
		<idno>no. EPFL-CONF-233583</idno>
	</analytic>
	<monogr>
		<title level="m">The 11th IAPR International Conference on Biometrics (ICB 2018)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unknown presentation attack detection with face rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Abdalmageed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE 9th International Conference on Biometrics Theory, Applications and Systems (BTAS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning deep features for one-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5450" to="5463" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Combining multiple one-class classifiers for anomaly based face spoofing attack detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fatemifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Awais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Arashloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Biometrics (ICB)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep anomaly detection for generalized face anti-spoofing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pérez-Cabo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jiménez-Cabello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Costa-Pazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>López-Sastre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep tree learning for zero-shot face anti-spoofing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stehouwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Ropad: Robust presentation attack detection through unsupervised adversarial invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jaiswal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Abdalmageed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.03691</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Crafting a panoptic face presentation attack detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Uberoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vatsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Generalizing fingerprint spoof detector: Learning a one-class classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Engelsma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.03918</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Spoofing face recognition with 3d masks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Erdogmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on information forensics and security</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1084" to="1097" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Disguise detection and face recognition in visible and thermal spectrums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">I</forename><surname>Dhamecha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vatsa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biometrics (ICB), 2013 International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Face presentation attack with latex masks in multispectral videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vatsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="275" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multi-modal face presentation attack detection via spatial and channel attentions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Casia-surf: A dataset and benchmark for largescale multi-modal face anti-spoofing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00408</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Recognizing multi-modal face spoofing with face recognition networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Parkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grinchuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="499" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;06)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Contrastive-center loss for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="2851" to="2855" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="532" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Detecting outliers: Do not use standard deviation around the mean, use absolute deviation around the median</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Licata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Social Psychology</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="764" to="766" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A light cnn for deep face representation with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2884" to="2896" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Heterogeneous face recognition using domain specific units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>De Freitas Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anjos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the em algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JOURNAL OF THE ROYAL STATISTICAL SOCIETY, SERIES B</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Statistical and structural approaches to texture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Haralick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="786" to="804" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">After Ph.D, he worked in Samsung Research Institute as a machine learning researcher. Currently, he is a postdoctoral researcher in the biometric security and privacy group at Idiap Research Institute, focusing on developing face presentation attack detection algorithms. His research interests are real-time signal and image processing, embedded systems, computer vision</title>
	</analytic>
	<monogr>
		<title level="m">Anjith George has received his Ph.D. and M-Tech degree from the Department of Electrical Engineering</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">2012</biblScope>
		</imprint>
		<respStmt>
			<orgName>Indian Institute of Technology (IIT) Kharagpur</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">He is a Senior Researcher at the Idiap Research Institute (CH), where he heads a research team and conducts research on face recognition, speaker recognition, vein recognition, and presentation attack detection (anti-spoofing)</title>
	</analytic>
	<monogr>
		<title level="m">He was the Principal Investigator of international research projects including MOBIO</title>
		<meeting><address><addrLine>Rennes, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
		<respStmt>
			<orgName>Sébastien Marcel received the Ph.D. degree in signal processing from Université de Rennes I ; CNET, the Research Center of France Telecom (now Orange Labs ; EU FP7 Mobile Biometry ; EU FP7 Biometrics Evaluation and Testing</orgName>
		</respStmt>
	</monogr>
	<note>TABULA RASA (EU FP7 Trusted Biometrics under Spoofing Attacks), and BEAT</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
