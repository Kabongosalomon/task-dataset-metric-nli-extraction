<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cooperative Cross-Stream Network for Discriminative Action Representation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingran</forename><surname>Zhang</surname></persName>
							<email>jrzhang339@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Future Multimedia School of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<postCode>610051</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fumin</forename><surname>Shen</surname></persName>
							<email>fumin.shen@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">Center for Future Multimedia School of Computer Science and Engineering University of Electronic Science and Technology of China Chengdu</orgName>
								<address>
									<postCode>610051</postCode>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xu</surname></persName>
							<email>xing.xu@uestc.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="department">Center for Future Multimedia School of Computer Science and Engineering University of Electronic Science and Technology of China Chengdu</orgName>
								<address>
									<postCode>610051</postCode>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Heng</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Center for Future Multimedia School of Computer Science and Engineering University of Electronic Science and Technology of China Chengdu</orgName>
								<address>
									<postCode>610051</postCode>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shen</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Center for Future Multimedia School of Computer Science and Engineering University of Electronic Science and Technology of China Chengdu</orgName>
								<address>
									<postCode>610051</postCode>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cooperative Cross-Stream Network for Discriminative Action Representation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Spatial and temporal stream model has gained great success in video action recognition. Most existing works pay more attention to designing effective features fusion methods, which train the two-stream model in a separate way. However, it's hard to ensure discriminability and explore complementary information between different streams in existing works. In this work, we propose a novel cooperative cross-stream network that investigates the conjoint information in multiple different modalities. The jointly spatial and temporal stream networks feature extraction is accomplished by an end-toend learning manner. It extracts this complementary information of different modality from a connection block, which aims at exploring correlations of different stream features. Furthermore, different from the conventional ConvNet that learns the deep separable features with only one cross entropy loss, our proposed model enhances the discriminative power of the deeply learned features and reduces the undesired modality discrepancy by jointly optimizing a modality ranking constraint and a cross entropy loss for both homogeneous and heterogeneous modalities. The modality ranking constraint constitute intra-modality discriminative embedding and intermodality triplet constraint, and it reduces both the intramodality and cross-modality feature variations. Experiments on three benchmark datasets demonstrate that by cooperating appearance and motion feature extraction, our method can achieve state-of-the-art or competitive performance compared with existing results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video analysis has attracted significant attention from the academic community in computer vision, partly due to the rapidly growing number of videos being shared on the Internet. As one of the fundamental task in video analy-  <ref type="figure">Figure 1</ref>. Traditional two-stream network for classifying "drink" and "eat" from HMDB-51 <ref type="bibr" target="#b14">[15]</ref> dataset. The input consist of video frames and optical flow images. It is hard to tell the classes when input the frames or optical flow field to TSN <ref type="bibr" target="#b37">[38]</ref> separately. The scores part is result of confusion matrix, which is derived from experiment of TSN on HMDB-51 dataset. sis, human action recognition in videos is a well-studied problem. However, traditionary CNN-based representations <ref type="bibr" target="#b13">[14]</ref> have not yet significantly made as the transformation of an impact on action representation as it does on still images, because of significant variations and complexities of video temporal sequence <ref type="bibr" target="#b19">[20]</ref>. Different from still image analysis, action representations often equip with specific spatial patterns as well as long-term temporal structure. Temporal modeling is critical aspects for action recognition and actions can be characterized by the temporal evolution of appearance governed by motion. Thus, it is crucial to design model which has the capacity to exploit long-range temporal information. Most recent works for action recognition can be generally originated from three kinds of architectures or frameworks, namely (1) 2D ConvNets with temporal modeling on top, like LSTM <ref type="bibr" target="#b4">[5]</ref>, (2) 3D based spatiotemporal convo-lutions <ref type="bibr" target="#b29">[30]</ref>  <ref type="bibr" target="#b12">[13]</ref>, (3) Two-stream based architectures <ref type="bibr" target="#b23">[24]</ref> [7] <ref type="bibr" target="#b37">[38]</ref>. Long term temporal modeling encode temporal relationship on frame-level features but has a poor capacity of capturing finer temporal relationship. Limited by complex spatiotemporal dependencies of action and computational cost, 3D based ConvNets have been so far hard to scale in terms of recognition performance. Whereas, two-stream based ConvNets <ref type="bibr" target="#b23">[24]</ref> which consists of motion and appearance streams typically train separately for each stream, and fuse the outputs in the end. Two-stream based ConvNets have been shown to outperform the 3D based convolution and 2D ConvNets with temporal modeling because they can easily utilize the pre-trained deep architectures <ref type="bibr" target="#b9">[10]</ref> for still-image recognition and have excellent motion sources to extract features.</p><p>Nevertheless, some motion features of different class extracted from two-stream framework are prone to confusing, resulting in the wrong classification, due to the similarity structure of optical flow field, for example, discriminating "eat" and "drink" from "smoking" (see <ref type="figure">Figure 1</ref>). What's more, simple fusing the clip scores of RGB ConvNet and flow ConvNet don't give large improvement. Experiments prove that existing two-stream based frameworks usually failed on categorizing those easily confused action label. <ref type="figure">Figure 1</ref> give an example, experimental data stemmed from baseline model TSN <ref type="bibr" target="#b37">[38]</ref> on HMDB-51 <ref type="bibr" target="#b14">[15]</ref>, of that case. Our human being can easily distinguish above action partly due to we focus on not only the motion features but also the appearance features when we determine an action. Hence, the reason for this case may be two stream based action recognition methods extract spatial and motion features separately, suffering from a limitation of lack of mutual spatial-temporal learning. An excellent framework should be able to capture both information simultaneously. The RGB frames ConvNets should help optical flow ConvNets in features extraction. That is to say, the features learned by the two distinct networks should enhance each other to make the features of the same class compact whereas the different class dissimilar. Actually, there is some subtle connection that is not well explored between spatial ConvNets and temporal ConvNets.</p><p>We introduce an architecture outline in which we simultaneously extract discriminative features and jointly train spatial-temporal network in an end-to-end manner for solving this issue. To efficient explore the relation of the RGB stream and optical flow stream, We propose a crossmodality features extraction paradigm to jointly learning spatiotemporal features for two heterogenous modalities, integrating modality information complementarity block and cross-modality ranking constraint to bridge the gap between two modalities and enhance the modality-invariance of the learned representation. As illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>, the main of the framework is spatial and temporal features learning and interaction of the separate part. ConvNet operates the spatial and temporal feature extraction. Inspired by the non local block that calculates the dependency of the same modality features, here we design a new block which takes the spatial and temporal features as input and calculate its' correlation.</p><p>The connection block which design to capturing dependencies and relationship of spatial and temporal features tries to enhance the interaction between spatial and temporal ConvNets and provide complementarity information to each other. To fully utilize the complementarity information of spatial and temporal features, in the shared block of our framework, we propose to use triplet constraint to force the spatial and temporal features to preserve the similarity structure and weaken the modality discrepancy.</p><p>The key contributions of our work are summarized as three-fold: <ref type="bibr" target="#b0">(1)</ref> We propose a cooperative spatial and temporal features learning model in an end-to-end manner. Comparing to exist two-stream networks, our model is uniquely able to cope with the incoordination problem between spatial and temporal features extracted in a separate manner. (2) The proposed network enhance the interaction and correlation between the spatial and temporal features by pulling a connection block between the spatial and temporal stream. <ref type="bibr" target="#b2">(3)</ref> we aggregate the identity loss with cross-modality ranking constraint to ensure the discriminability by exploiting the relation between spatial and temporal stream.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>As one of video analysis task, action recognition has been well studied for decades. Action recognition is hard partly due to the large inter-class similarity of different action temporal features and intra-class variability of same action spatial features. In this paper, we apply jointly spatial and temporal features learning in a discriminative fashion to improve connections between the two, which cloud in some way learn compact features. Many previous works related to this problem fall into two categories in term of feature learning: (1) hand-crafted features designing, and (2) ConvNets for auto-features extraction. Hand-crafted features for action recognition. Before deep learning became popular, most of the traditional CV algorithm variants apply shallow hand-crafted features to solve action recognition. Improved Dense Trajectories (IDT) <ref type="bibr" target="#b33">[34]</ref> which uses densely sampled trajectory features indicates that the temporal information could be processed differently from that of spatial information. Instead of extending the Harris corner detector into 3D, it utilizes the warp optical flow field to obtain some trajectories and eliminate the effects of camera motion in the video sequence. For each tracker corner hand-crafted features, like HOF, HOG, and MBH, are extracted along the trajectory. Despite their excellent performance, IDT and its improvements <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b36">[37]</ref> are still computationally formidable and become intractable on large-scale datasets. ConvNets for auto-feature extraction. An activate research which devotes to the design of deep networks for video representation learning has been trying to devise effective ConvNet architectures <ref type="bibr" target="#b13">[14]</ref> [33] <ref type="bibr" target="#b30">[31]</ref> [33] <ref type="bibr" target="#b4">[5]</ref>. Karparthy et al. <ref type="bibr" target="#b13">[14]</ref> attempt to design a deep network which stacks CNN-based frame-level features in a fixed size and then conduct spatiotemporal convolutions for video-level features learning. However, the results which implied the difficulty of CNNs in capturing motion information of the video is not satisfied. Later, many works in this genre leverage ConvNets trained on frames to extract low-level features an then perform high-level temporal integration of those features using pooling <ref type="bibr" target="#b35">[36]</ref>  <ref type="bibr" target="#b34">[35]</ref>, high-dimensional feature encoding <ref type="bibr" target="#b7">[8]</ref>  <ref type="bibr" target="#b3">[4]</ref>, or recurrent neural networks <ref type="bibr" target="#b4">[5]</ref> [41] [33] <ref type="bibr" target="#b43">[44]</ref>. Recently, the CNN-LSTM frameworks <ref type="bibr" target="#b4">[5]</ref> [41], using stacked LSTM network to connect frame-level representation and exploring long-term temporal relationships of video for learning a more robust representation, have yielded an improvement for modeling temporal dynamics of convolution features in videos. However, this genre using CNN as an encoder and RNN as a decoder of the video will lose low-level temporal context which is essential for action recognition.</p><p>These works implied the importance of temporal information for action recognition and the incapability of CNNs to capture such information. To exploiting the temporal information, some studies resort to the use of the 3D convolution kernel. Tran et al. <ref type="bibr" target="#b29">[30]</ref> [31] apply 3D CNN, both appearance and motion features learned with 3D convolution, simultaneously encode spatial and temporal cues. Several works explored the effect of performing 3D convolutions over the long-range temporal structure with ConvNets [39] <ref type="bibr" target="#b42">[43]</ref>. Unfortunately, the network accepts a predefined number of frames as the input, and it's unclear of the right choice of the temporal span. What's more, the 3D convolution kernel inevitably has more network parameters. Therefore, recent interests have proposed a variant of factorizing a 3D filter into a combination of a 2D and 1D filter, including "R(2+1)D" <ref type="bibr" target="#b31">[32]</ref>, "Pseudo3D network" <ref type="bibr" target="#b19">[20]</ref>, "factorized spatiotemporal convolutional networks" <ref type="bibr" target="#b27">[28]</ref>.</p><p>Another efficient way to extract temporal features is to precomputing the optical flow <ref type="bibr" target="#b28">[29]</ref> using traditional optical flow estimation methods and training a separate CNN to encode the precomputed optical flow, which is kind of escape from temporal modeling but effective in motion features extraction. The famous two-stream architecture <ref type="bibr" target="#b23">[24]</ref> proposed to apply two CNN architectures separately on visual frames and staked optical flows to extract spatiotemporal features and then fuse classification score. Further improvements base on this architecture including multi-granular structure <ref type="bibr" target="#b25">[26]</ref> [47], convolutional fusion <ref type="bibr" target="#b6">[7]</ref>  <ref type="bibr" target="#b38">[39]</ref>, key-volume mining <ref type="bibr" target="#b47">[48]</ref>, temporal segment networks <ref type="bibr" target="#b37">[38]</ref> and ActionVLAD <ref type="bibr" target="#b7">[8]</ref> for video representation learning. Remarkably, a recent work (I3D) <ref type="bibr" target="#b1">[2]</ref> which combines two-stream processing and 3D convolutions holds the state-of-art action recognition results. The work reflects the power of ultra-deep architectures and pre-trained models.</p><p>Two-stream architectures based methods generally have the best performance among those works. Nevertheless, twostream backbone networks often train spatial and temporal ConvNet separately, which will break the connections between appearance and motion information. Recently, many works have utilized cross-modality learning which could improve the discriminative of features to tackle computer vision task, like image retrieve <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref> and person Re-ID <ref type="bibr" target="#b41">[42]</ref>. In our framework, we jointly train spatial and temporal stream by cross-stream learning. Besides, to capture the complementarity information between appearance and motion across videos and encode the correlation features between different stream into a compact format, we propose a connection block and aggregate inter-modality triplet and intra-modality discriminative embedding constraint with identity loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>In this section, we illustrate the framework of our proposed architectures showed in <ref type="figure" target="#fig_1">Figure 2</ref>. In our cross-stream network, the spatial stream focuses on appearance features learning from sparsely sample frames, and the temporal stream focus on the motion features which is captured using multiple optical flows. The two parts should complementary to each other; a connection block is designed for improving the interaction of the two different modality features. The latter cross-modality feature learning focuses on learning a multi-modality sharable space to bridge the gap between two heterogenous modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Feature Extraction</head><p>We adopt the off-the-shelf features extractor to extract the features from two heterogenous modalities. Both spatial and temporal ConvNet employ similar backbone structures in our feature extraction block.</p><p>Suppose we have a video V i containing T i frames, equipped with a label l i , where l i ∈ 1, 2, 3, ..., n, n is the total number of action labels. Considering the video V i , firstly, we need to get snippet-level action features. A end-to-end deep neural network perform effective videolevel representation learning. Here, we use two-stream based framework <ref type="bibr" target="#b23">[24]</ref> to extract appearance and motion feature. Given the input</p><formula xml:id="formula_0">x t i = (s t , F t ), where s t is the t − th frame in video X i , F t = {f t } t+c2</formula><p>t =t−c1 is stacked optical flow field derived around s t , c1, c2 are constant, typically 5 x − level images and 5 y−level images. Two-stream network includes spatial and temporal networks which operate on single video frame s t and stacked optical flow field F t respectively. considering the output of</p><formula xml:id="formula_1">x t i , o t i = (x f i,t , x o i,t ), where x f i,t</formula><p>is the learned features of t − th frame in the i − th video, and x o i,t is the learned features of the stacked optical flow F t in the i − th video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Connection Block</head><p>Considering the output features of ConvNet from a video, the features of the sequence k will be o k . Here, we suppose the output feature sequences with the appearance and motion have the same size, i.e., x f , x o ∈ R T ×C×D , where T , C, D, denote the sequence number, filter number, and numbers of the output feature dimension. The goal of interaction block is to produce a vector which represents the correlation between x f k and x o k , which can be further fed into a neural network to compute the similarity.</p><p>Inspired by non-local operation for capturing long-range dependencies <ref type="bibr" target="#b39">[40]</ref> and relationship reasoning module <ref type="bibr" target="#b20">[21]</ref> and video temporal reasoning <ref type="bibr" target="#b44">[45]</ref>, we present a pairwise spatial and temporal correlation function as blow:</p><formula xml:id="formula_2">Y = i,j g θ x f i , x o j ,<label>(1)</label></formula><p>where the input is a set of feature sequences of standard CNN extracted from video frames and optical flows, x f i is the frame feature sequences of i − th video and x o j is the optical flow feature sequence of j − th video, and g θ are function typically implement by multiple layer perceptrons with parameter θ respectively.</p><p>Following non local module <ref type="bibr" target="#b39">[40]</ref> aiming at calculating relation of elements of the object, here, we adopt embedded Gaussian to compute the similarity of two different modality object pairs. Considering the similarity measure function g θ , we present it as follow:</p><formula xml:id="formula_3">g θ (x f i , x o j ) = e ϕ(x f i ) T κ(x o j ) ,<label>(2)</label></formula><p>where θ = {ϕ, κ}, and ϕ(</p><formula xml:id="formula_4">x f i ) = W ϕ x f i , κ(x o j ) = W κ x o<label>j</label></formula><p>are two embeddings implemented by multiple layer perceptrons.</p><p>We further wrap the spatial and temporal correlation reasoning Eq.(1) into interaction operation as:</p><formula xml:id="formula_5">x f i = h f φ (Y) + x f i x o j = h o φ (Y) + x o j ,<label>(3)</label></formula><p>where Y is given in Eq.</p><formula xml:id="formula_6">(1) and h f φ (Y) = w f x f i y, h o φ (Y) = w o</formula><p>x o j y are interaction function implemented by a convolution operation. <ref type="figure" target="#fig_2">Figure 3</ref> shows the details of connection block. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Shared Block</head><p>Based on the output features of the ConvNets for k − th segment from a video, it can be transformed into a D − dimension features vector through aggregation operation. Supposing that there is a collection of n instance video features, denoted</p><formula xml:id="formula_7">as O = {o i } n i=1 , o i = z f i , z o i , where z f i</formula><p>is the aggregation feature of frame-stream ConvNet of x f i and z o i is the aggregation feature of optical flow filed stream ConvNet of x o i , we build learning scheme by selecting triplets from above databases.</p><p>Inspired by triplet loss to learn discriminative embedding, we propose to use triplet constraint to extract spatial and motion features based on two-stream backbone ConvNets. Besides, to efficient explore the relation of the RGB stream and optical flow field stream, we propose cross-modality features extraction to jointly learning spatialtemporal features. Most works train the spatial ConvNet and temporal ConvNet separately under the architecture of two-stream. Actually, the RGB frame ConvNet should help optical flow ConvNet in features extraction. That is to say, the features learned by the two distinct networks should enhance each other to make the features of the same class compact whereas the different class dissimilar. The underlying idea is that we compare the distance of a positive appearance-motion pair and the minimum distance of all related negative appearance-motion pairs, rather than each of the negative pairs. More specifically, we sample frames from the entire video and extract appearance and motion features jointly using cross-modality training to enhance the connections of appearance and motion. The extracted features are then fed into a classifier which outputs the classification scores. Final results are improved by scores fusion.</p><p>We propose a cross-modality learning scheme relied on selecting triplets and discriminative embedding scheme on each modality in this section to reduce variations in both intra-modality and cross-modality. Online triplet sampling on each mini-batch <ref type="bibr" target="#b10">[11]</ref> are employed here. The joint effect of these two processes is illustrated in <ref type="figure" target="#fig_4">Figure 4</ref>. The discriminative embedding loss force the learned features of the same class compact and different class dissimilar, meanwhile, the cross-modality triplet loss force appearance and motion stream to project into common feature space.   </p><p>where α 1 is a margin that is enforced between positive and negative pairs. Since the above ranking loss constrains the feature learning process with their underlying relationships among the heterogeneous modality, it's hard to learn a robust feature representation to reduce the intra-class variations by simply exploiting the relationship cues. Inspired by linear discriminative analysis <ref type="bibr" target="#b0">[1]</ref>, we introduce discriminative embedding constraint to enhance the robustness of the learned feature representation and address intra-modality variations; the discriminative embedding loss function expresses as following:</p><formula xml:id="formula_9">L2 =   i,c i z f i −m f c i 2 2 −α2 + + c i =c j α3 − m f c i −m f c j 2 2 +   +   i,c i z o i −m o c i 2 2 −α2 + + c i =c j α3 − m o c i −m o c j 2 2 +   ,<label>(5)</label></formula><p>where m ci is the mean feature of class i, c is the number of the class and α 2 is a margin that forces the same class compact, α 3 is a margin that is enforced between the different class.</p><p>For the sake of feasibility and effectiveness in classification, the general cross entropy loss L 3 is utilized by treating each action as a class. In this manner, the identity-specific information is integrated to enhance the robustness.</p><p>Based on the above, the loss function of the proposed network, referred to as a combination of cross-modality, is formulated as the combination of the intra-modal discrimination loss and the intra-modal embedding constraint and identity loss:</p><formula xml:id="formula_10">L = λ 1 L 1 + λ 2 L 2 + L 3 ,<label>(6)</label></formula><p>where λ 1 , λ 2 control the contribution of the two terms. Algorithm 1 illustrates the steps of the proposed cooperative cross-stream network. From the backward pass, we can obtain that the connection block crosses the stream function as a bridge for information of appearance and motion stream flowing to each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we describe our method for action recognition. Firstly, we introduce the benchmark datasets and implementation details of the proposed method. Afterward, we compare our method with state-of-art methods on standard action datasets. Following, we explore the effectiveness of applying different component in our proposed model. Finally, we investigate the effect of ConvNet architectures and hyperparameters and visualize the interesting region extracted by our model on the snippet video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Datasets. We conduct our experiments on three challenging action datasets: namely, UCF-101 <ref type="bibr" target="#b26">[27]</ref>, HMD-B51 <ref type="bibr" target="#b14">[15]</ref>, Algorithm 1 Optimization step of CCS.  <ref type="bibr" target="#b15">[16]</ref> to evaluate the overall performance. The UCF-101, one of popular action recognition dataset, consists of 101 action classes with 13320 short video clips. Videos in this dataset have 320 × 240 spatial resolution. The HMDB-51 dataset has 6766 video clips with 51 categories. something-something-v2 an interesting temporal relationship reasoning dataset contain total 220,847 video clips with 174 action classes. For both datasets, we follow the standard evaluation protocol and adopt its training/testing splits for evaluation. We report accuracy on the split 1 test set of UCF-101 and HMDB-51 datasets. Implementation details. We employ the pytorch framework in this paper for Networks building, and all networks are trained on two GeForce GTX Titan X GPU with total 24G memory. We compute optical flow with a TV-L1 algorithm <ref type="bibr" target="#b18">[19]</ref>. All the input images are resized to 224 × 224 followed by the dataset processing strategy of <ref type="bibr" target="#b37">[38]</ref>. We adopt mini-batch stochastic gradient descent optimizer for model training, and initial learning rate here is 0.001 which will reduce by a factor 10 after 50 epochs. It has decay rate 5×10 −4 , and momentum 0.9 to update Network parameters. The maximum epochs are 400. The trade-off parameters λ 1 and λ 2 are all set as 0.5. We set cross-modality margin α 1 = 0.3, and intra-class α 2 = 0.3, inter-class margin α 3 = 0.8 of the same modality.</p><formula xml:id="formula_11">Input: N videos with n class {(X i , l i )} N i=1 , where l i ∈ {1, 2, . . . , n} is the label of video X i , iteration number K. Output: The predicted action label Y = {y i } N i=1 , where y i ∈ {1,</formula><p>We introduce a balance mini-batch sampling strategy for inter-modality modality constraint and discriminative embedding constraint. Specifically, we randomly select N action categories. Then we randomly select M instance of the selected identity from two different modalities to construct the mini-batch, in which totally 2 × N × M instances are fed into the network for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with Existing Methods</head><p>We compare to the state-of-the-art action recognition methods and report the results in <ref type="table" target="#tab_1">Table 1</ref> on UCF-101, HMDB-51 (split 1) and something-something-V2 dataset. For a fair comparison, we list the important factor such as the pre-trained dataset and use RGB images and optical flow fields as input modalities. We use CCS, as above, and predict the action in a single forward pass using fully network testing. Here, we extract three segments of a video and randomly sample a video snippet of 10 frames on each segment as input for training. During testing, 25 frames are sampled for each video. The comparison against the single model without ensemble technique, like the work in <ref type="bibr" target="#b4">[5]</ref>, which attaches an LSTM to a ConvNet architecture and the one spatiotemporal C3D based network <ref type="bibr" target="#b30">[31]</ref> are impressive. Their accuracy of 85.8% is to date the best performing approach using one stream for action recognition.</p><p>Here, our gain of 12.3% further underlines the importance of two-stream framework. Comparing to the original twostream method <ref type="bibr" target="#b23">[24]</ref>, we improve by 9.7% on UCF-101 and by 22.9%on HMDB-51. Apparently, even though the original two-stream approach has the advantage than one stream method, the benefit of our cooperative cross-stream network with the interaction of heterogeneous features are still greater. Together with TSN or I3D, our cooperate two-stream architecture widens the advantage over previous models considerably, bringing overall performance to 97.4% on UCF-101 and 81.9% on HMDB-51. We observe that the combination of RGB images and optical flow image boosts the recognition performance and cooperative training the two kinds of image further yield an improvement. This result indicates that RGB images and optical flow image may encode complementary information.</p><p>These relatively larger performance increments again underline that our approach is better able to capture the available dynamic information. Overall, our result 81.9% on HMDB-51 clearly sets a new state-of-the-art on this widely used action recognition datasets. This corroborates for different modality information, enhanced by modality connection block and cross-modality training, is crucial for a better understanding of action in videos. What's more, from <ref type="table" target="#tab_1">Table 1</ref>, we also can acquire the power of pre-trained model for action recognition.</p><p>something-something-v2 is a dataset for human-object interaction recognition, which cares more about temporal relations and transformations of objects rather than the appearance and motion of the objects characterize the activities <ref type="bibr" target="#b44">[45]</ref>. In <ref type="table">Table 2</ref>, We report the accuracies of somethingsomething-v2. Comparing with the baseline methods <ref type="bibr" target="#b8">[9]</ref>, our method further improves to 61.1%. The combination of two-stream TRN <ref type="bibr" target="#b44">[45]</ref> and our CCS achieves better results. The performance demonstrates the importance of not only the temporal reasoning pooling but also the correlation of appearance and motion features on something-something dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Further Analysis</head><p>Importance of each component of the proposed model. With all the design choices set, we now apply the cooperative cross-stream network (CCS) to the action recognition with different variants, where the result is illustrated in <ref type="table" target="#tab_2">Table  3</ref>. A component-wise analysis of the components in terms of the recognition accuracies is also presented. We cooperate the CCS with TSN <ref type="bibr" target="#b37">[38]</ref> and I3D <ref type="bibr" target="#b1">[2]</ref>, to verify the importance of modality information complementarity. Instead of training spatial and temporal stream separately, CCS jointly train the two stream network to improve the interaction of deep spatiotemporal features so that the model not only captures the co-occurrence also the specific patterns in the features. We keep all the training conditions the same, and vary connection block and loss function used by two models.</p><p>We investigate the effectiveness of each component in our proposed model by conducting a series of ablation studies on all three datasets. We treat the TSN <ref type="bibr" target="#b37">[38]</ref> and I3D <ref type="bibr" target="#b1">[2]</ref> as backbone framework in this section. We first study the effectiveness of our modality features connection modules by replacing the connection module with feature concatenation or average. We first train the TSN and I3D framework with the connection module, named TSN+CM, I3D+CM. Its' RGB ensemble with optical flow accuracy increase by 0.4%, on the UCF101 dataset, and 7.8% on the HMDB51, which demonstrates that conducting modality information interaction with connection block helps deep modality features complementarity to enhance the performance. For validating the effectiveness of shared features projection layer following connection module, we remove the shared layer and only employ cross entropy loss. Instead, we directly take the results from TSN or I3D and input them into two-layer feed-forward neural networks mentioned above to obtain the similarity confidence (denoted as TSN+CS I3D+CS). The performance even becomes worse compared with TSN and I3D. However, worked with connection block, our original CCS network can achieve the best results.</p><p>We can obtain that the reported baselines typically underperform the proposed model. Both TSN <ref type="bibr" target="#b37">[38]</ref> and I3D <ref type="bibr" target="#b1">[2]</ref> produce reasonable performances but work with our original design still yield improvement. We speculate this is because the connection block considerably explores the correlation information of heterogeneous modality and therefore, the network is able to store more complementary information for cross-modal feature learning. Effect of sequence features aggregation function. The two commonly used aggregation methods are the element-wise maximum of the sequence and element-wise average of the sequence. Here, we also evaluate (1) element-wise multiplication of the sequence, (2) concatenation of sequence. The comparisons among the four late score fusion methods are shown in <ref type="figure" target="#fig_6">Figure 5</ref> (a). We can see that the element-wise average of the sequence achieves the best result on HMDB-51 dataset. This verifies that the effective of element-wise average to improve the final accuracy. Effect of model parameter. We survey the hyperparameter α 1 , α 2 and α 3 in ranking loss. The parameter α 1 refers to the margin between the anchor/positive and negative samples. The parameter α 2 refers to the margin between the sample of its center and α 3 refers to the margin between different center. A small value enforces less on the similarities between the anchor/positive against negative, but the loss in faster convergence. On the other hand, a large value may lead to a network with good performance, but slow convergence during training. We conduct an experiment on UCF-101 to illustrate the effects of this parameter, and the results are showed in   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Visualization</head><p>To verify how our model help in action classification, we would like to attain further insight into what our model has learned. As shown in <ref type="bibr" target="#b45">[46]</ref>, ConvNets are expert in capturing the basic visual concept, but it has difficulty in identifying the importance of different units for classifying different categories. Here, we use the CAM (Class Activation Map) <ref type="bibr" target="#b45">[46]</ref> to visualize the most discriminative parts of the proposed model. Thus the output after a number of iterations can be considered as class visualization based on class knowledge inside the ConvNet model. To understanding the primitives our model used for represent actions and visualizing interesting class information in CCS models, We randomly select three classes from the UCF-101 dataset, "Apply Eye Make-Up", "Archery", "Blow Dry Hair" as visualization example. For ease of visualization, we only consider the spatial stream in this example. The results are shown in <ref type="figure">Figure 6</ref>. The highlight regions that correspond to the receptive field give us same insight of what the model cares about. For example, we see that the proposed model pays more attention to the region like 'eye' and 'hand' in 'ApplyEyeMakeUp' video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Video Input</head><p>Class Activation Maps <ref type="figure">Figure 6</ref>. Visualization of "CAM" <ref type="bibr" target="#b45">[46]</ref> generated by our CCS model when jointly trained appearance and motion stream. The maps highlight the discriminative region for action classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">conclusion</head><p>In this paper, a novel CCS network for video action recognition was proposed. It cooperatively exploits the information in RGB visual appearance features and optical flow motion features by mixing a connection block and jointly optimizing a ranking loss and a cross entropy loss. The CCS network enhances the discriminative power and explore the complementary information of the deeply learned heterogeneous features and weakens the modality discrepancy. Further, it can apply to both homogeneous and heterogeneous modality-based action recognition task. The ranking loss consists of inter-modality triplet constraint and discriminative embedding constraint, and it reduces both the intra-modality and cross-modality feature variations. Experiment results on three datasets demonstrate and justify the effectiveness of the proposed method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The overall architecture of our proposed cooperative cross-stream network (CCS). Feature extraction ConvNet, connection block, and shared block constitute our model. The feature extraction ConvNet is applied to capturing appearance and motion features. The connection block is used for enhancing appearance and motion features interaction. The shared block is designed for reducing the undesired modality discrepancy. The hole model is training under inter modality triplet and discriminative embedding constraint. The class scores of all modalities are then fused for prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Cross-stream connection block. The appearance features denoted as x f and motion features denoted as x o are put into the connection block. "⊗" denotes matrix multiplication, and "⊕" denotes element-wise sum. ϕ, κ, W f and W o denote 1 × 1 convolution operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Illustration of the joint effect of inter-modality triplet and discriminative embedding constraint. Different color represents different modality while the same color indicates the class-related cross-modality item; what's more, different shape represent the different class.are built here, where z m r,i is the member of the triplet, m ∈ {f, o} denote modality, r ∈ {a, p, n} denote the kinds of match of the triplet. The inter-modality loss function using the following expression:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>The performance of different backbone architectures or feature aggregation functions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>2, . . . , n}</figDesc><table><row><cell>initialization: i = 0,</cell><cell></cell><cell></cell></row><row><cell>repeat</cell><cell></cell><cell></cell></row><row><cell cols="3">1. Forward pass: 1.1 compute the appearance features x f i and mo-</cell></row><row><cell>tion</cell><cell></cell><cell></cell></row><row><cell cols="3">features x o i within the connection block;</cell></row><row><cell cols="3">1.2 predict the video label y i after shared block;</cell></row><row><cell>2. Backward pass: using ∂L ∂θ f = ∂L ∂z f</cell><cell>∂z f ∂θ f + ∂L ∂z o</cell><cell>∂z o ∂θ f and</cell></row><row><cell cols="3">∂L ∂θo = ∂L ∂z f where θ f are the parameters of spatial stream ∂z f ∂θ o + ∂L ∂z o ∂z o ∂θ o as parameters gradient,</cell></row><row><cell>model and</cell><cell></cell><cell></cell></row><row><cell cols="3">θ o are the parameters of temporal stream model;</cell></row><row><cell>3. i = i + 1;</cell><cell></cell><cell></cell></row><row><cell cols="2">until i = K or convergence</cell><cell></cell></row><row><cell>something-something-V2</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1 .</head><label>1</label><figDesc>COMPARISON OF STATE-OF-THE-ART METHODS ON THE UCF-101 AND HMDB-51 DATASETS (SPLIT 1). WE REPORT THE ACCURACY OF RGB MODALITY, OPTICAL FLOW MODALITY, AND THE COMBINATION OF BOTH TWO MODALITY.</figDesc><table><row><cell>Methods</cell><cell></cell><cell cols="2">Pre-train dataset</cell><cell></cell><cell>RGB</cell><cell cols="3">UCF-101 Flow RGB+Flow RGB Flow RGB+Flow HMDB-51</cell></row><row><cell>ConvNets+LSTM [5]</cell><cell></cell><cell cols="2">ImageNet</cell><cell></cell><cell>68.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Two-stream Network [24]</cell><cell></cell><cell cols="2">ImageNet</cell><cell></cell><cell>73.0</cell><cell>83.7</cell><cell>88.0</cell><cell>40.5</cell><cell>54.6</cell><cell>59.4</cell></row><row><cell>ConvNet fusion [7]</cell><cell></cell><cell cols="2">ImageNet</cell><cell></cell><cell>82.6</cell><cell>86.2.7</cell><cell>90.6</cell><cell>47.0</cell><cell>55.2</cell><cell>58.2</cell></row><row><cell>ST-resNet [6]</cell><cell></cell><cell cols="2">ImageNet</cell><cell></cell><cell>82.3</cell><cell>79.1</cell><cell>93.4</cell><cell>43.2</cell><cell>55.5</cell><cell>66.4</cell></row><row><cell>DTPP [47]</cell><cell></cell><cell cols="2">ImageNet</cell><cell></cell><cell>89.7</cell><cell>89.1</cell><cell>94.9</cell><cell>61.5</cell><cell>66.3</cell><cell>75.0</cell></row><row><cell>TLE+Two-stream [4]</cell><cell></cell><cell cols="2">ImageNet</cell><cell></cell><cell>-</cell><cell>-</cell><cell>95.6</cell><cell>-</cell><cell>-</cell><cell>71.1</cell></row><row><cell>ActionVLAD [8]</cell><cell></cell><cell cols="2">ImageNet</cell><cell></cell><cell>-</cell><cell>-</cell><cell>92.7</cell><cell>49.8</cell><cell>59.1</cell><cell>66.9</cell></row><row><cell>C3D [30]</cell><cell></cell><cell cols="2">sports-1M</cell><cell></cell><cell>82.3</cell><cell>-</cell><cell>-</cell><cell>51.6</cell><cell>-</cell><cell>-</cell></row><row><cell>C3D [31]</cell><cell></cell><cell cols="2">sports-1M</cell><cell></cell><cell>85.8</cell><cell>-</cell><cell>-</cell><cell>54.9</cell><cell>-</cell><cell>-</cell></row><row><cell>R(2+1)D [32]</cell><cell></cell><cell cols="2">sports-1M</cell><cell></cell><cell>93.6</cell><cell>93.3</cell><cell>95.0</cell><cell>66.6</cell><cell>70.1</cell><cell>72.7</cell></row><row><cell>TSN [38]</cell><cell></cell><cell cols="2">ImageNet</cell><cell></cell><cell>85.7</cell><cell>87.9</cell><cell>93.5</cell><cell>-</cell><cell>-</cell><cell>68.5</cell></row><row><cell>I3D [2]</cell><cell></cell><cell cols="2">ImageNet</cell><cell></cell><cell>84.5</cell><cell>90.6</cell><cell>93.4</cell><cell>49.8</cell><cell>61.9</cell><cell>66.4</cell></row><row><cell>R(2+1)D [32]</cell><cell cols="4">ImageNet+Kinetics</cell><cell>96.8</cell><cell>95.5</cell><cell>97.3</cell><cell>74.5</cell><cell>76.4</cell><cell>78.7</cell></row><row><cell>TSN [38]</cell><cell cols="4">ImageNet+Kinetics</cell><cell>91.1</cell><cell>95.2</cell><cell>97.0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CCS + TSN</cell><cell></cell><cell cols="2">ImageNet</cell><cell></cell><cell>87.2</cell><cell>87.4</cell><cell>95.3</cell><cell>60.5</cell><cell>62.1</cell><cell>77.2</cell></row><row><cell>CCS + TSN</cell><cell cols="4">ImageNet+Kinetics</cell><cell>94.2</cell><cell>95.0</cell><cell>97.4</cell><cell>69.4</cell><cell>71.2</cell><cell>81.9</cell></row><row><cell>CCS + I3D</cell><cell></cell><cell cols="2">ImageNet</cell><cell></cell><cell>86.7</cell><cell>87.1</cell><cell>93.8</cell><cell>60.1</cell><cell>62.3</cell><cell>68.2</cell></row><row><cell cols="5">TABLE 2. RESULTS ON SOMETHING-SOMETHING-V2.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell cols="4">val top-1 top-5 top-1 top-5 Test</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Baseline</cell><cell>51.3</cell><cell>80.6</cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MultiScale TRN</cell><cell>48.8</cell><cell>77.6</cell><cell>50.9</cell><cell>79.3</cell><cell></cell><cell></cell><cell></cell></row><row><cell>two-stream + TRN</cell><cell>55.5</cell><cell>83.1</cell><cell>56.2</cell><cell>83.2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CCS + two-stream + TRN</cell><cell>61.2</cell><cell>89.3</cell><cell>60.5</cell><cell>87.9</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 3 .</head><label>3</label><figDesc>ABLATION STUDIES: RESULTS WITH DIFFERENT COMPONENTS ON THE UCF-101 , HMDB-51 DATASETS. "METHOD" DENOTES THE COMPONENT WE USE IN OUR FINAL MODEL. "CB": WITH ONLY CONNECTION BLOCK. "CS": WITHOUT CONNECTION MODULE ONLY USING CROSS-STREAM TRAINING IN THE SHARED BLOCK. "ALL": WITH THE CONNECTION BLOCK AND CROSS-STREAM TRAINING IN THE SHARED BLOCK.</figDesc><table><row><cell>Base Model</cell><cell>Methods</cell><cell cols="6">UCF-101 RGB Flow RGB+Flow RGB Flow RGB+Flow HMDB51</cell></row><row><cell></cell><cell>Baseline</cell><cell>85.7</cell><cell>87.9</cell><cell>93.5</cell><cell>-</cell><cell>-</cell><cell>68.5</cell></row><row><cell>TSN</cell><cell>CB CS</cell><cell>86.3 84.9</cell><cell>87.2 85.1</cell><cell>93.9 91.7</cell><cell>60.5 54.4</cell><cell>62.1 61.6</cell><cell>76.3 67.3</cell></row><row><cell></cell><cell>All</cell><cell>87.2</cell><cell>87.4</cell><cell>95.3</cell><cell>61.7</cell><cell>65.1</cell><cell>77.2</cell></row><row><cell></cell><cell>Baseline</cell><cell>84.5</cell><cell>90.6</cell><cell>93.4</cell><cell>49.8</cell><cell>61.9</cell><cell>66.4</cell></row><row><cell>I3D</cell><cell>CB CS</cell><cell>86.1 82.4</cell><cell>86.9 83.1</cell><cell>92.7 91.8</cell><cell>53.0 50.9</cell><cell>56.2 52.3</cell><cell>67.6 64.7</cell></row><row><cell></cell><cell>All</cell><cell>86.7</cell><cell>87.1</cell><cell>93.8</cell><cell>60.1</cell><cell>62.3</cell><cell>68.2</cell></row><row><cell cols="4">compared to the whole networks and contribution to model</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>performance.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>From the table, it can be seen that it achieves the best accuracy when α 1 and α 2 are set to 0.3, and α 3 is set as 0.8. This suggests that we should carefully choose the hyperparameter, and it is advisable to set relatively small α values for reasonable results. Effect of ConvNet structure. Furthermore, to investigate different effect of ConvNet structures, We also explore the conventional CNN model, namely VGG<ref type="bibr" target="#b24">[25]</ref>, ResNet<ref type="bibr" target="#b9">[10]</ref>, BN-Inception<ref type="bibr" target="#b11">[12]</ref>, all pre-trained on ImageNet, as the backbone of two-stream ConvNets. All those ConvNets are trained together with TSN and our CCS network on UCF-101. The results of those deep structures are shown inFigure 5(b). Among those structures, BN-Inception achieves the best accuracy.</figDesc><table><row><cell></cell><cell>85</cell><cell></cell><cell cols="3">81.3 RGB</cell><cell>80.7</cell><cell cols="2">Flow</cell><cell>80.5</cell><cell>81.9 RGB+Flow</cell></row><row><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Accuracy</cell><cell>65 70 75</cell><cell cols="2">70.1 70.5</cell><cell></cell><cell cols="2">68.3 72.1</cell><cell></cell><cell cols="2">69.1 70.9</cell><cell>69.4 71.2</cell></row><row><cell></cell><cell>60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>55</cell><cell cols="2">Max</cell><cell></cell><cell cols="6">Multiplication Concatenation Average</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">(a) Aggregation function</cell></row><row><cell cols="2">85 90 95 100</cell><cell>84.1</cell><cell>83.8</cell><cell>91.2</cell><cell>RGB</cell><cell cols="2">86.2 Flow 88.1</cell><cell>93.2</cell><cell cols="2">87.2 RGB+Flow</cell><cell>87.4</cell><cell>95.3</cell></row><row><cell>Accuracy</cell><cell>75 80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>65</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>60</cell><cell cols="3">VGG16</cell><cell></cell><cell cols="3">ResNet101</cell><cell></cell><cell>BNInception</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 4 .</head><label>4</label><figDesc>THE PERFORMANCE OF CCS WITH DIFFERENT MODEL PARAMETERS VALUES ON UCF-101 DATASET.</figDesc><table><row><cell>Parameters α 1 α 2 α 3</cell><cell>Accuracy</cell></row><row><cell>0.2 0.3 0.8</cell><cell>96.3</cell></row><row><cell>0.3 0.3 0.8</cell><cell>97.4</cell></row><row><cell>0.3 0.5 1.0</cell><cell>97.1</cell></row><row><cell>0.5 0.5 1.0</cell><cell>95.4</cell></row><row><cell>0.8 0.5 1.2</cell><cell>95.5</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider the additional parameters introduced by connection and shared block. All of them are contained in two 1×1 convolution and shared full connection operation. The computation is relatively small and worth of the cost,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work was supported in part by the National Natural Science Foundation of China under grants No. 61502081, 61602089, 61632007 and the Sichuan Science and Technology Program 2018GZDZX0032, 2019ZDZX0008 and 2019YFG0003.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Eigenfaces vs. fisherfaces: Recognition using class specific linear projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Hespanha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="711" to="720" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cross-modal retrieval in the cooking context: Learning semantic text-image embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cadène</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Soulier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="35" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep temporal linear encoding networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Spatiotemporal residual networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3468" to="3476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional twostream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Actionvlad: Learning spatio-temporal aggregation for action classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision &amp; Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The&quot; something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mueller-Freitag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person re-identification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="221" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hmdb51: A large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High Performance Computing in Science and Engineering</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="571" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mahdisoltani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gharbieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.09235</idno>
		<title level="m">Fine-grained video classification and captioning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fast reference frame selection based on content similarity for low complexity hevc encoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="516" to="524" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bag of visual words and fusion methods for action recognition: Comprehensive study and good practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">150</biblScope>
			<biblScope unit="page" from="109" to="125" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Tv-l1 optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Meinhardt-Llopis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Facciolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Processing On Line</title>
		<imprint>
			<biblScope unit="page" from="137" to="150" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5534" to="5542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4967" to="4976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep asymmetric pairwise hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international conference on Multimedia</title>
		<meeting>the 25th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1522" to="1530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Supervised discrete hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H. Tao</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="37" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Temporalspatial mapping for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Human action recognition using factorized spatio-temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4597" to="4605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Optical flow guided feature: a fast and robust motion representation for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1390" to="1399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.05038</idno>
		<title level="m">Convnet architecture search for spatiotemporal feature learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Long-term temporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1510" to="1517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning discriminative video representations using adversarial perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cherian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="685" to="701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Video representation learning using discriminative pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1149" to="1158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Action recognition with trajectorypooled deep-convolutional descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4305" to="4314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Two-stream 3-d convnet fusion for action recognition in videos with arbitrary size and length</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="634" to="644" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Modeling spatialtemporal clues in a hybrid deep learning framework for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM international conference on Multimedia</title>
		<meeting>the 23rd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="461" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning cross-modal deep representations for robust pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5363" to="5371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Describing videos by exploiting temporal structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4507" to="4515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yue-Hei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="803" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">End-to-end video-level representation learning for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">24th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="645" to="650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A key volume mining deep framework for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1991" to="1999" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
