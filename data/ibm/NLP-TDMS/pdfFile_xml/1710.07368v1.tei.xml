<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SqueezeSeg: Convolutional Neural Nets with Recurrent CRF for Real-Time Road-Object Segmentation from 3D LiDAR Point Cloud</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
							<email>bichen@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Wan</surname></persName>
							<email>alvinwan@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
							<email>xyyue@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
							<email>keutzer@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SqueezeSeg: Convolutional Neural Nets with Recurrent CRF for Real-Time Road-Object Segmentation from 3D LiDAR Point Cloud</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we address semantic segmentation of road-objects from 3D LiDAR point clouds. In particular, we wish to detect and categorize instances of interest, such as cars, pedestrians and cyclists. We formulate this problem as a pointwise classification problem, and propose an end-to-end pipeline called SqueezeSeg based on convolutional neural networks (CNN): the CNN takes a transformed LiDAR point cloud as input and directly outputs a point-wise label map, which is then refined by a conditional random field (CRF) implemented as a recurrent layer. Instance-level labels are then obtained by conventional clustering algorithms. Our CNN model is trained on LiDAR point clouds from the KITTI [1] dataset, and our point-wise segmentation labels are derived from 3D bounding boxes from KITTI. To obtain extra training data, we built a LiDAR simulator into Grand Theft Auto V (GTA-V), a popular video game, to synthesize large amounts of realistic training data. Our experiments show that SqueezeSeg achieves high accuracy with astonishingly fast and stable runtime (8.7 ± 0.5 ms per frame), highly desirable for autonomous driving applications. Furthermore, additionally training on synthesized data boosts validation accuracy on real-world data. Our source code and synthesized data will be open-sourced.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Autonomous driving systems rely on accurate, real-time and robust perception of the environment. An autonomous vehicle needs to accurately categorize and locate "roadobjects", which we define to be driving-related objects such as cars, pedestrians, cyclists, and other obstacles. Different autonomous driving solutions may have different combinations of sensors, but the 3D LiDAR scanner is one of the most prevalent components. LiDAR scanners directly produce distance measurements of the environment, which are then used by vehicle controllers and planners. Moreover, LiDAR scanners are robust under almost all lighting conditions, whether it be day or night, with or without glare and shadows. As a result, LiDAR based perception tasks have attracted significant research attention.</p><p>In this work, we focus on road-object segmentation using (Velodyne style) 3D LiDAR point clouds. Given point cloud output from a LiDAR scanner, the task aims to isolate objects of interest and predict their categories, as shown in <ref type="figure">Fig. 1</ref>. Previous approaches comprise or use parts of the following stages: Remove the ground, cluster the remaining points into instances, extract (hand-crafted) features from each cluster, and classify each cluster based on its features. This paradigm, despite its popularity <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, has several disadvantages: a) Ground segmentation in the above Ground truth segmentation Predicted segmentation <ref type="figure">Fig. 1</ref>: An example of SqueezeSeg segmentation results. Our predicted result is on the right and the ground truth is on the left. Cars are annotated in red, pedestrians in green and cyclists in blue.</p><p>pipeline usually relies on hand-crafted features or decision rules -some approaches rely on a scalar threshold <ref type="bibr" target="#b5">[6]</ref> and others require more complicated features such as surface normals <ref type="bibr" target="#b6">[7]</ref> or invariant descriptors <ref type="bibr" target="#b3">[4]</ref>, all of which may fail to generalize and the latter of which require significant preprocessing. b) Multi-stage pipelines see aggregate effects of compounded errors, and classification or clustering algorithms in the pipeline above are unable to leverage context, most importantly the immediate surroundings of an object. c) Many approaches for ground removal rely on iterative algorithms such as RANSAC (random sample consensus) <ref type="bibr" target="#b4">[5]</ref>, GP-INSAC (Gaussian Process Incremental Sample Consensus) <ref type="bibr" target="#b1">[2]</ref>, or agglomerative clustering <ref type="bibr" target="#b1">[2]</ref>. The runtime and accuracy of these algorithmic components depend on the quality of random initializations and, therefore, can be unstable. This instability is not acceptable for many embedded applications such as autonomous driving. We take an alternative approach: use deep learning to extract features, develop a single-stage pipeline and thus sidestep iterative algorithms.</p><p>In this paper, we propose an end-to-end pipeline based on convolutional neural networks (CNN) and conditional random field (CRF). CNNs and CRFs have been successfully applied to segmentation tasks on 2D images <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. To apply CNNs to 3D LiDAR point clouds, we designed a CNN that accepts transformed LiDAR point clouds and outputs a point-wise map of labels, which is further refined by a CRF model. Instance-level labels are then obtained by applying conventional clustering algorithms (such as DBSCAN) on points within a category. To feed 3D point clouds to a 2D CNN, we adopt a spherical projection to transform sparse, irregularly distributed 3D point clouds to dense, 2D grid representations. The proposed CNN model draws inspiration from SqueezeNet <ref type="bibr" target="#b11">[12]</ref> and is carefully designed to reduce parameter size and computational complexity, with an aim to reduce memory requirements and achieve real-time inference speed for our target embedded applications. The CRF model is reformulated as a recurrent neural network (RNN) module as <ref type="bibr" target="#b10">[11]</ref> and can be trained end-to-end together with the CNN model. Our model is trained on LiDAR point clouds from the KITTI dataset <ref type="bibr" target="#b0">[1]</ref> and point-wise segmentation labels are converted from 3D bounding boxes in KITTI. To obtain even more training data, we leveraged Grand Theft Auto V (GTA-V) as a simulator to retrieve LiDAR point clouds and point-wise labels.</p><p>Experiments show that SqueezeSeg achieves high accuracy and is extremely fast and stable, making it suitable for autonomous driving applications. We additionally find that supplanting our dataset with artificial, noise-injected simulation data further boosts validation accuracy on realworld data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Semantic segmentation for 3D LiDAR point clouds</head><p>Previous work saw a wide range of granularity in LiDAR segmentation, handling anything from specific components to the whole pipeline. <ref type="bibr" target="#b6">[7]</ref> proposed mesh based ground and object segmentation relying on local surface convexity conditions. <ref type="bibr" target="#b1">[2]</ref> summarized several approaches based on iterative algorithms such as RANSAC (random sample consensus) and GP-INSAC (gaussian process incremental sample consensus) for ground removal. Recent work also focused on algorithmic efficiency. <ref type="bibr" target="#b4">[5]</ref> proposed efficient algorithms for ground segmentation and clustering while <ref type="bibr" target="#b12">[13]</ref> bypassed ground segmentation to directly extract foreground objects. <ref type="bibr" target="#b3">[4]</ref> expanded its focus to the whole pipeline, including segmentation, clustering and classification. It proposed to directly classify point patches into background and foreground objects of different categories then use EMST-RANSAC <ref type="bibr" target="#b4">[5]</ref> to further cluster instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. CNN for 3D point clouds</head><p>CNN approaches consider LiDAR point clouds in either two or three dimensions. Work with two-dimensional data considers raw images with projections of LiDAR point clouds top-down <ref type="bibr" target="#b13">[14]</ref> or from a number of other views <ref type="bibr" target="#b14">[15]</ref>. Other work considers three-dimensional data itself, discretizing the space into voxels and engineering features such as disparity, mean, and saturation <ref type="bibr" target="#b15">[16]</ref>. Regardless of data preparation, deep learning methods consider endto-end models that leverage 2D convolutional <ref type="bibr" target="#b16">[17]</ref> or 3D convolutional <ref type="bibr" target="#b17">[18]</ref> neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Semantic Segmentation for Images</head><p>Both CNNs and CRFs have been applied to semantic segmentation tasks for images. <ref type="bibr" target="#b7">[8]</ref> proposed transforming CNN models, trained for classification, to fully convolutional networks to predict pixel-wise labels. <ref type="bibr" target="#b8">[9]</ref> proposed a CRF formulation for image segmentation and solved it approximately with the mean-field iteration algorithm. CNNs and CRFs are combined in <ref type="bibr" target="#b9">[10]</ref>, where the CNN is used to produce an initial probability map and the CRF is used to refine and restore details. In <ref type="bibr" target="#b10">[11]</ref>, mean-field iteration is reformulated as a recurrent neural network (RNN) module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Data Collection through Simulation</head><p>Obtaining annotations, especially point-wise or pixel-wise annotations for computer vision tasks is usually very difficult. As a consequence, synthetic datasets have seen growing interest. In the autonomous driving community, the video game Grand Theft Auto has been used to retrieve data for object detection and segmentation <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD DESCRIPTION A. Point Cloud Transformation</head><p>Conventional CNN models operate on images, which can be represented by 3-dimentional tensors of size H × W × 3. The first two dimensions encode spatial position, where H and W are the image height and width, respectively. The last dimension encodes features, most commonly RGB values. However, a 3D LiDAR point cloud is usually represented as a set of cartesian coordinates, (x, y, z). Extra features can also be included, such as intensity or RGB values. Unlike the distribution of image pixels, the distribution of LiDAR point clouds is usually sparse and irregular. Therefore, naively discretizing a 3D space into voxels results in excessively many empty voxels. Processing such sparse data is inefficient, wasting computation.</p><p>To obtain a more compact representation, we project the LiDAR point cloud onto a sphere for a dense, grid-based representation as θ = arcsin z</p><formula xml:id="formula_0">x 2 + y 2 + z 2 ,θ = θ/ θ , φ = arcsin y x 2 + y 2 ,φ = φ/ φ .<label>(1)</label></formula><p>φ and θ are azimuth and zenith angles, as shown in <ref type="figure" target="#fig_0">Fig.  2</ref> (A). θ and φ are resolutions for discretization and (θ,φ) denotes the position of a point on a 2D spherical grid. Applying equation <ref type="formula" target="#formula_0">(1)</ref> to each point in the cloud, we can obtain a 3D tensor of size H × W × C. In this paper, we consider data collected from a Velodyne HDL-64E LiDAR with 64 vertical channels, so H = 64. Limited by data annotations from the KITTI dataset, we only consider the front view area of 90 • and divide it into 512 grids so W = 512. C is the number of features for each point.</p><p>In our experiments, we used 5 features for each point: 3 cartesian coordinates (x, y, z), an intensity measurement and range r = x 2 + y 2 + z 2 . An example of a projected point cloud can be found at <ref type="figure" target="#fig_0">Fig. 2 (B)</ref>. As can be seen, such representation is dense and regularly distributed, resembling an ordinary image <ref type="figure" target="#fig_0">Fig. 2</ref> (C). This featurization allows us to avoid hand-crafted features, bettering the odds that our representation generalizes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Network structure</head><p>Our convolutional neural network structure is shown in <ref type="figure">Fig. 3</ref>. SqueezeSeg is derived from SqueezeNet <ref type="bibr" target="#b11">[12]</ref>, a lightweight CNN that achieved AlexNet <ref type="bibr" target="#b20">[21]</ref> level accuracy with 50X fewer parameters.</p><p>The input to SqueezeSeg is a 64 × 512 × 5 tensor as described in the previous section. We ported layers (conv1a to fire9) from SqueezeNet for feature extraction. SqueezeNet used max-pooling to down-sample intermediate feature maps in both width and height dimensions, but since our input tensor's height is much smaller than its width, we only downsample the width. The output of fire9 is a down-sampled feature map that encodes the semantics of the point cloud.</p><p>To obtain full resolution label predictions for each point, we used deconvolution modules (more precisely, "transposed convolutions") to up-sample feature maps in the width dimension. We used skip-connections to add up-sampled feature maps to lower-level feature maps of the same size, as shown in <ref type="figure">Fig. 3</ref>. The output probability map is generated by a convolutional layer (conv14) with softmax activation. The probability map is further refined by a recurrent CRF layer, which will be discussed in the next section.</p><p>In order to reduce the number of model parameters and computation, we replaced convolution and deconvolution layers with fireModules <ref type="bibr" target="#b11">[12]</ref> and fireDeconvs. The architecture of both modules are shown in <ref type="figure">Fig. 4</ref>. In a fireModule, the input tensor of size H × W × C is first fed into a 1x1 convolution to reduce the channel size to C/4. Next, a 3x3 convolution is used to fuse spatial information. Together with a parallel 1x1 convolution, they recover the channel size of C. The input 1x1 convolution is called the squeeze layer and the parallel 1x1 and 3x3 convolution together is called the expand layer. Given matching input and output size, a 3x3 convolutional layer requires 9C 2 parameters and 9HW C 2 computations, while the fireModule only requires 3 2 C 2 parameters and 3 2 HW C 2 computations. In a fireDeconv module, the deconvolution layer used to up-sample the feature map is placed between squeeze and expand layers. To up-sample the width dimension by 2, a regular 1x4 deconvolution layer must contain 4C 2 parameters and 4HW C 2 computations. With the fireDeconv however, we only need 7 4 C 2 parameters and 7 4 HW C 2 computations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Conditional Random Field</head><p>With image segmentation, label maps predicted by CNN models tend to have blurry boundaries. This is due to loss of low-level details in down-sampling operations such as max-pooling. Similar phenomena are also observed with SqueezeSeg.</p><p>Accurate point-wise label prediction requires understanding not only the high-level semantics of the object and scene but also low-level details. The latter are crucial for the consistency of label assignments. For example, if two points in the cloud are next to each other and have similar intensity measurements, it is likely that they belong to the same object and thus have the same label. Following <ref type="bibr" target="#b9">[10]</ref>, we used a conditional random field (CRF) to refine the label map generated by the CNN. For a given point cloud and a label prediction c where c i denotes the predicted label of the i-th point, a CRF model employs the energy function</p><formula xml:id="formula_1">E(c) = i u i (c i ) + i,j b i,j (c i , c j ).<label>(2)</label></formula><p>The unary potential term u i (c i ) = − log P (c i ) considers the predicted probability P (c i ) from the CNN classifier. The binary potential terms define the "penalty" for assigning different labels to a pair of similar points and is defined as</p><formula xml:id="formula_2">b i,j (c i , c j ) = µ(c i , c j ) M m=1 w m k m (f i , f j )</formula><p>where µ(c i , c j ) = 1 if c i = c j and 0 otherwise, k m is the m-th Gaussian kernel that depends on features f of point i and j and w m is the corresponding coefficient. In our work, we used two Gaussian kernels</p><formula xml:id="formula_3">w 1 exp(− p i − p j 2 2σ 2 α − x i − x j 2 2σ 2 β ) +w 2 exp(− p i − p j 2 2σ 2 γ ).<label>(3)</label></formula><p>The first term depends on both angular position p(θ,φ) and cartesian coordinates x(x, y, z) of two points. The second term only depends on angular positions. σ α , σ β and σ γ are three hyper parameters chosen empirically. Extra features such as intensity and RGB values can also be included. Minimizing the above CRF energy function yields a refined label assignment. Exact minimization of equation <ref type="formula" target="#formula_1">(2)</ref> is intractable, but <ref type="bibr" target="#b8">[9]</ref> proposed a mean-field iteration algorithm to solve it approximately and efficiently. <ref type="bibr" target="#b10">[11]</ref> reformulated the mean-field iteration as a recurrent neural network (RNN). We refer readers to <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b10">[11]</ref> for a detailed derivation of the mean-field iteration algorithm and its formulation as an RNN. Here, we provide just a brief description of our implementation of the mean-field iteration as an RNN module as shown in <ref type="figure" target="#fig_2">Fig. 5</ref>. The output of the CNN model is fed into the CRF module as the initial probability map. Next, we compute Gaussian kernels based on the input feature as equation <ref type="formula" target="#formula_3">(3)</ref>. The value of above Gaussian kernels drop very fast as the distance (in the 3D cartesian space and the 2D angular space) between two points increases. Therefore, for each point, we limit the kernel size to a small region of 3 × 5 on the input tensor. Next, we filter the initial probability map using above Gaussian kernels. This step is also called message passing in <ref type="bibr" target="#b10">[11]</ref> since it essentially aggregates probabilities of neighboring points. This step can be implemented as a locally connected layer with above Guassian kernels as parameters. Next, we re-weight the aggregated probability and use a "compatibilty transformation" to decide how much it changes each point's distribution. This step can be implemented as a 1x1 convolution whose parameters are learned during training. Next, we update the initial probability by adding it to the output of the 1x1 convolution and use softmax to normalize it. The output of the module is a refined probability map, which can be further refined by applying this procedure iteratively. In our experiment, we used 3 iterations to achieve an accurate label map. This recurrent CRF module together with the CNN model can be trained together end-to-end. With a single stage pipeline, we sidestep the thread of propagated errors present in multi-stage workflows and leverage contextual information accordingly.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Data collection</head><p>Our initial data is from the KITTI raw dataset, which provides images, LiDAR scans and 3D bounding boxes organized in sequences. Point-wise annotations are converted from 3D bounding boxes. All points within an object's 3D bounding box are considered part of the target object. We then assign the corresponding label to each point. An example of such a conversion can be found in <ref type="figure" target="#fig_0">Fig. 2 (A,  B)</ref>. Using this approach, we collected 10,848 images with point-wise labels.</p><p>In order to obtain more training samples (both point clouds and point-wise labels), we built a LiDAR simulator in GTA-V. The framework of the simulator is based on DeepGTAV 1 , which uses Script Hook V 2 as a plugin.</p><p>We mounted a virtual LiDAR scanner atop an in-game car, which is then set to drive autonomously. The system collects both LiDAR point clouds and the game screen. In our setup, the virtual LiDAR and game camera are placed at the same position, which offers two advantages: First, we can easily run sanity checks on the collected data, since the points and images need to be consistent. Second, the points and images can be exploited for other research projects, e.g. sensor fusion, etc.</p><p>We use ray casting to simulate each laser ray that LiDAR emits. The direction of each laser ray is based on several parameters of the LiDAR setup: vertical field of view (FOV), vertical resolution, pitch angle, and the index of the ray in the point cloud scan. Through a series of APIs, the following data associated with each ray can be obtained: a) the coordinates of the first point the ray hits, b) the class of the object hit, c) the instance ID of the object hit (which is useful for instance-wise segmentation, etc.), d) the center and bounding box of the object hit. Using this simulator, we built a synthesized dataset with 8,585 samples, roughly doubling our training set size. To make the data more realistic, we further analyzed the distribution of noise across KITTI point clouds <ref type="figure">(Fig. 7)</ref>. We took empirical frequencies of noise at each radial coordinate and normalized to obtain a valid probability distribution: 1) Let P i be a 3D tensor in the format described earlier in Section III-A denoting the spherically projected "pixel values" of the i-th KITTI point cloud. For each of the n KITTI point clouds, consider whether or not the pixel at the (θ,φ) coordinate contains "noise." For simplicity, we consider "noise" to be missing data, where all pixel channels are zero. Then, the empirical frequency of noise at the (θ,φ) coordinate is</p><formula xml:id="formula_4">(θ,φ) = 1 n n i=1 1 {Pi[θ,φ]=0} .</formula><p>2) We can then augment the synthesized data using the distribution of noise in the KITTI data. For any point cloud in the synthetic dataset, at each (θ,φ) coordinate of the point cloud, we randomly add noise by setting all feature values to 0 with probability (θ,φ).</p><p>It is worth noting that GTA-V used very simple physical models for pedestrians, often reducing people to cylinders. In addition, GTA-V does not encode a separate category for cyclists, instead marking people and vehicles separately on all accounts. For these reasons, we decided to focus on the "car" class for KITTI evaluation when training with our synthesized dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS A. Evaluation metrics</head><p>We evaluate our model's performance on both classlevel and instance-level segmentation tasks. For class-level <ref type="figure">Fig. 7</ref>: Fixing distribution of noise in synthesized data segmentation, we compare predicted with ground-truth labels, point-wise, and evaluate precision, recall and IoU (intersection-over-union) scores, which are defined as follows:</p><formula xml:id="formula_5">P r c = |P c ∩ G c | |P c | , recall c = |P c ∩ G c | |G c | , IoU c = |P c ∩ G c | |P c ∪ G c | ,</formula><p>where P c and G c respectively denote the predicted and ground-truth point sets that belong to class-c. | · | denotes the cardinality of a set. IoU score is used as the primary accuracy metric in our experiments. For instance-level segmentation, we first match each predicted instance-i with a ground truth instance. This index matching procedure can be denoted as M(i) = j where i ∈ {1, · · · , N } is the predicted instance index and j ∈ {∅, 1, · · · , M } is the ground truth index. If no ground truth is matched to instance-i, then we set M(i) to ∅. The matching procedure M(·) 1) sorts ground-truth instances by number of points and 2) for each ground-truth instance, finds the predicted instance with the largest IoU. The evaluation script will be released together with the source code.</p><p>For each class-c, we compute instance-level precision, recall, and IoU scores as</p><formula xml:id="formula_6">P r c = i |P i,c ∩ G M(i),c | |P c | , recall c = i |P i,c ∩ G M(i),c | |G c | , IoU c = i |P i,c ∩ G M(i),c | |P c ∪ G c | .</formula><p>P i,c denotes the i-th predicted instance that belongs to class-c. Different instance sets are mutually exclusive, thus i |P i,c | = |P c |. Likewise for G M(i),c . If no ground truth instance is matched with prediction-i, then G M(i),c is an empty set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Setup</head><p>Our primary dataset is the converted KITTI dataset described above. We split the publicly available raw dataset into a training set with 8,057 frames and a validation set with 2,791 frames. Note that KITTI LiDAR scans can be temporally correlated if they are from the same sequence. In our split, we ensured that frames in the training set do not appear in validation sequences. Our training/validation split will be released as well. We developed our model in Tensorflow <ref type="bibr" target="#b21">[22]</ref> and used NVIDIA TITAN X GPUs for our experiments. Since the KITTI dataset only provides reliable 3D bounding boxes for front-view LiDAR scans, we limit our horizontal field of view to the forward-facing 90 • . Details of our model training protocols will be released in our source code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experimental Results</head><p>Segmentation accuracy of SqueezeSeg is summarized in <ref type="table">Table.</ref>I. We compared two variations of SqueezeSeg, one with the recurrent CRF layer and one without. Although our proposed metric is very challenging-as a high IoU requires point-wise correctness-SqueezeSeg still achieved high IoU scores, especially for the car category. Note that both classlevel and instance-level recalls for the car category are higher than 90%, which is desirable for autonomous driving, as false negatives are more likely to lead to accidents than false positives. We attribute lower performance on pedestrian and cyclist categories to two reasons: 1) there are many fewer instances of pedestrian and cyclist in the dataset. 2) Pedestrian and cyclist instances are much smaller in size and have much finer details, making it more difficult to segment.</p><p>By combining our CNN with a CRF, we increased accuracy (IoU) for the car category significantly. The performance boost mainly comes from improvement in precision since CRF better filters mis-classified points on the borders. At the same time, we also noticed that the CRF resulted in slightly worse performance for pedestrian and cyclist segmentation tasks. This may be due to lack of CRF hyperparameter tuning for pedestrians and cyclists.  Runtime of two SqueezeSeg models are summarized in <ref type="table">Table.</ref>II. On a TITAN X GPU, SqueezeSeg without CRF only takes 8.7 ms to process one LiDAR point cloud frame. Combined with a CRF layer, the model takes 13.5 ms each frame. This is much faster than the sampling rate of most LiDAR scanners today. The maximum rotation rate for Velodyne HDL-64E LiDAR, for example, is 20Hz. On vehicle embedded processors, where computational resources are more limited, SqueezeSeg comfortably allows trade-offs between speed and other practical concerns such as energy efficiency or processor cost. Also, note that the standard deviation of runtime for both SqueezeSeg models is very small, which is crucial for the stability of the entire autonomous driving system. However, our instance-wise segmentation currently relies on conventional clustering algorithms such as DBSCAN 3 , which in comparison takes much longer and has much larger variance. A more efficient and stable clustering implementation is necessary, but it is out of the scope of this paper. We tested our model's accuracy on KITTI data, when trained on GTA simulated data-the results of which are summarized in <ref type="table">Table.</ref>III. Our GTA simulator is currently still limited in its ability to provide realistic labels for pedestrians and cyclists, so we consider only segmentation performance for cars. Additionally, our simulated point cloud does not contain intensity measurements; we therefore excluded intensity as an input feature to the network. To quantify the effects of training on synthesized data, we trained a SqueezeSeg model on the KITTI training set, without using intensity measurements, and validated on the KITTI validation set. The model's performance is shown in the first row of the <ref type="table" target="#tab_1">table. Compared with Table.</ref>I, the IoU score is worse, due to the loss of the intensity channel. If we train the model completely on GTA simulated data, we see significantly worse performance. However, combining the KITTI training set with our GTA-simulated dataset, we see significantly increased accuracy that is even better than <ref type="table" target="#tab_1">Table.I.</ref> A visualization of the segmentation result by SqueezeSeg vs. ground truth labels can be found in <ref type="figure" target="#fig_4">Fig.8</ref>. For most of the objects, the predicted result is almost identical to the groundtruth, save for the ground beneath target objects. Also notice SqueezeSeg additionally and accurately segments objects that are unlabeled in ground truth. These objects may be obscured or too small, therefore placed in the "Don't Care" category for the KITTI benchmark. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>We propose SqueezeSeg, an accurate, fast and stable endto-end approach for road-object segmentation from LiDAR point clouds. Addressing the deficiencies of previous approaches that were discussed in the Introduction, our deep learning approach 1) does not rely on hand-crafted features, but utilizes convolutional filters learned through training; 2) uses a deep neural network and therefore has no reliance on iterative algorithms such as RANSAC, GP-INSAC, and agglomerative clustering; and 3) reduces the pipeline to a single stage, sidestepping the issue of propagated errors and allowing the model to fully leverage object context. The model accomplishes very high accuracy at faster-than-realtime inference speeds with small variance, as required for applications such as autonomous driving. Additionally, we synthesize large quantities of simulated data, then demonstrate a significant boost in performance when training with synthesized data and validating on real-world data. We use select classes as a proof-of-concept, granting synthesized data a potential role in self-driving datasets of the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>LiDAR Projections. Note that each channel reflects structural information in the camera-view image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Conv1bFig. 3 :Fig. 4 :</head><label>34</label><figDesc>Network structure of SqueezeSeg. Structure of a FireModule (left) and a fireDeconv (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>Conditional Random Field (CRF) as an RNN layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 :</head><label>6</label><figDesc>Left: Image of game scene from GTA-V. Right: LiDAR point cloud corresponding to the game scene.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 :</head><label>8</label><figDesc>Visualization of SqueezeSeg's prediction on a projected LiDAR depth map. For comparison, visualization of the ground-truth labels are plotted below the predicted ones. Notice that SqueezeSeg additionally and accurately segments objects that are unlabeled in ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Segmentation Performance of SqueezeSeg Summary of SqueezeSeg's segmentation performance. P, R, IoU in the header row respectively represent precision, recall and intersection-over-union. IoU is used as the primary accuracy metric. All the values in this table are in percentages.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Class-level</cell><cell></cell><cell></cell><cell>Instance-level</cell></row><row><cell></cell><cell></cell><cell>P</cell><cell>R</cell><cell>IoU</cell><cell>P</cell><cell>R</cell><cell>IoU</cell></row><row><cell>car</cell><cell>w/ CRF w/o CRF</cell><cell cols="5">66.7 95.4 64.6 63.4 90.7 59.5 62.7 95.5 60.9 60.0 91.3 56.7</cell></row><row><cell>pedestrian</cell><cell>w/ CRF w/o CRF</cell><cell cols="5">45.2 29.7 21.8 43.5 28.6 20.8 52.9 28.6 22.8 50.8 27.5 21.7</cell></row><row><cell>cyclist</cell><cell>w/ CRF w/o CRF</cell><cell cols="5">35.7 45.8 25.1 30.4 39.0 20.6 35.2 51.1 26.4 30.1 43.7 21.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Runtime Performance of SqueezeSeg Pipeline</figDesc><table><row><cell></cell><cell>Average</cell><cell>Standard</cell></row><row><cell></cell><cell>runtime</cell><cell>deviation</cell></row><row><cell></cell><cell>(ms)</cell><cell>(ms)</cell></row><row><cell>SqueezeSeg w/o CRF</cell><cell>8.7</cell><cell>0.5</cell></row><row><cell>SqueezeSeg</cell><cell>13.5</cell><cell>0.8</cell></row><row><cell>DBSCAN clustering</cell><cell>27.3</cell><cell>45.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>Segmentation Performance on the Car Category with Simulated Data</figDesc><table><row><cell></cell><cell></cell><cell>Class-level</cell><cell></cell><cell></cell><cell cols="2">Instance-level</cell></row><row><cell></cell><cell>P</cell><cell>R</cell><cell>IoU</cell><cell>P</cell><cell>R</cell><cell>IoU</cell></row><row><cell>KITTI</cell><cell cols="3">58.9 95.0 57.1</cell><cell cols="3">56.1 90.5 53.0</cell></row><row><cell>GTA</cell><cell cols="3">30.4 86.6 29.0</cell><cell cols="3">29.7 84.6 28.2</cell></row><row><cell cols="4">KITTI + GTA 69.6 92.8 66.0</cell><cell cols="3">66.6 88.8 61.4</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/ai-tor/DeepGTAV 2 http://www.dev-c.com/gtav/scripthookv/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We used the implementation from http://scikit-learn.org/ 0.15/modules/generated/sklearn.cluster.DBSCAN.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>This work was partially supported by the DARPA PER-FECT program, Award HR0011-12-2-0016, together with ASPIRE Lab sponsor Intel, as well as lab affiliates HP, Huawei, Nvidia, and SK Hynix. This work has also been partially sponsored by individual gifts from BMW, Intel, and the Samsung Global Research Organization.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the segmentation of 3d lidar point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Douillard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Underwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kuntz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vlaskine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Quadros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Morton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frenkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2798" to="2805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Lidarbased 3d object perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Himmelsbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lüttel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Wünsche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 1st international workshop on cognition for technical systems</title>
		<meeting>1st international workshop on cognition for technical systems</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">What could move? finding cars, pedestrians and bicyclists in 3d laser data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation (ICRA), 2012 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="4038" to="4044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast segmentation of 3d point clouds: A paradigm on lidar data for autonomous vehicle applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zermas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Izzat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papanikolopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5067" to="5073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Stanley: The robot that won the darpa grand challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Montemerlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dahlkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stavens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Diebel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halpenny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of field Robotics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="661" to="692" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Segmentation of 3d lidar data in non-flat urban environments using a local convexity criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moosmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Vehicles Symposium</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="215" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1529" to="1537" />
		</imprint>
	</monogr>
	<note>Conditional random fields as recurrent neural networks</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<title level="m">SqueezeNet: Alexnet-level accuracy with 50x fewer parameters and &lt;0.5mb model size</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Real-time and accurate segmentation of 3-d point clouds based on gaussian process regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-O</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-M</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Seo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast lidar-based road detection using fully convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Caltagirone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scheidegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Svensson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wahde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1019" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07759</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fusing lidar and images for pedestrian detection using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schlosser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation (ICRA), 2016 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2198" to="2205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Vehicle detection from 3d lidar using fully convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.07916</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for landing zone detection from lidar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation (ICRA), 2015 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3471" to="3478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV), ser</title>
		<editor>LNCS, B. Leibe, J. Matas, N. Sebe, and M. Welling</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">9906</biblScope>
			<biblScope unit="page" from="102" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Driving in the matrix: Can virtual worlds replace human-generated annotations for real world tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson-Roberson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vasudevan</surname></persName>
		</author>
		<idno>abs/1610.01983</idno>
		<ptr target="http://arxiv.org/abs/1610.01983" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">TensorFlow: Largescale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Google Technical Report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
