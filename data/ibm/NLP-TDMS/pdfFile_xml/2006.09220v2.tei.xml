<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MS-TCN++: Multi-Stage Temporal Convolutional Network for Action Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazan</forename><forename type="middle">Abu</forename><surname>Farha</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Juergen</forename><surname>Gall</surname></persName>
						</author>
						<title level="a" type="main">MS-TCN++: Multi-Stage Temporal Convolutional Network for Action Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Index Terms-Temporal action segmentation, temporal convolutional network !</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the success of deep learning in classifying short trimmed videos, more attention has been focused on temporally segmenting and classifying activities in long untrimmed videos. State-of-the-art approaches for action segmentation utilize several layers of temporal convolution and temporal pooling. Despite the capabilities of these approaches in capturing temporal dependencies, their predictions suffer from over-segmentation errors. In this paper, we propose a multi-stage architecture for the temporal action segmentation task that overcomes the limitations of the previous approaches. The first stage generates an initial prediction that is refined by the next ones. In each stage we stack several layers of dilated temporal convolutions covering a large receptive field with few parameters. While this architecture already performs well, lower layers still suffer from a small receptive field. To address this limitation, we propose a dual dilated layer that combines both large and small receptive fields. We further decouple the design of the first stage from the refining stages to address the different requirements of these stages. Extensive evaluation shows the effectiveness of the proposed model in capturing long-range dependencies and recognizing action segments. Our models achieve state-of-the-art results on three datasets: 50Salads, Georgia Tech Egocentric Activities (GTEA), and the Breakfast dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>A CTION recognition from video has been an active research area in computer vision in the past few years. Most of the efforts, however, have been focused on classifying short trimmed videos <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. Despite the success of these approaches on trimmed videos with a single activity, their performance is limited on long videos containing many action segments. Since for many applications, like surveillance and robotics, it is crucial to temporally segment activities in long untrimmed videos, approaches for temporal action segmentation have received more attention. Early attempts for temporal action segmentation tried to extend the success on trimmed videos by combining these models with sliding windows <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. These approaches use temporal windows of different scales to detect and classify action segments. However, such approaches are expensive and do not scale for long videos. Other approaches apply a coarse temporal modeling using Markov models on top of frame-wise classifiers <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. While these approaches achieved good results, they are very slow as they require solving a maximization problem over very long sequences.</p><p>With the success of temporal convolutional networks (TCNs) as a powerful temporal model for speech synthesis, many researchers adapt TCN-based models for the temporal action segmentation task <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. These models were more capable in capturing long range dependencies between the video frames by relying on a large receptive field. However, these models are limited for a very low temporal resolution of a few frames per second. Furthermore, since  <ref type="figure">Fig. 1</ref>: Overview of the multi-stage temporal convolutional network. Each stage generates an initial prediction that is refined by the next stage. At each stage, several dilated 1D convolutions are applied on the activations of the previous layer. A loss layer is added after each stage.</p><p>these approaches rely on temporal pooling layers to increase the receptive field, many of the fine-grained information that is necessary for recognition is lost.</p><p>To overcome the limitations of the previous approaches, arXiv:2006.09220v2 [cs.CV] 2 Sep 2020</p><p>we propose a new model that also uses temporal convolutions. In contrast to previous approaches, the proposed model operates on the full temporal resolution of the videos and thus achieves better results. Our model consists of multiple stages where each stage outputs an initial prediction that is refined by the next one. We call the new architecture Multi-Stage Temporal Convolutional Network (MS-TCN).</p><p>In each stage, we apply a series of dilated 1D convolutions, which enables the model to have a large temporal receptive field with less parameters. <ref type="figure">Figure 1</ref> shows an overview of the proposed multi-stage model. Furthermore, we employ a smoothing loss during training which penalizes oversegmentation errors in the predictions. A preliminary version of this work introducing MS-TCN and the smoothing loss has been published in <ref type="bibr" target="#b13">[14]</ref>. While the proposed MS-TCN already achieves good performance, some of the design choices are sub-optimal. First, while the receptive field is very large for higher layers in MS-TCN, lower layers suffer from a small receptive field. Second, the first stage in MS-TCN generates an initial prediction and the remaining stages refine this prediction. Despite the differences between these two tasks, all stages share the same architecture. To address the first limitation, we propose a dual dilated layer (DDL) which combines both large and small receptive fields at each layer. For the second, we divide the whole architecture into two parts: The first part is the first stage, which is the prediction generation stage, and the second part consists of prediction refinement stages. Then we customize the architecture of each part separately and do not force all stages to have the same architecture as in MS-TCN. By incorporating these design choices on MS-TCN, we propose an improved version of the model, which we call MS-TCN++. Furthermore, we show that the parameters of the refinement stages in MS-TCN++ can be shared without compromising the accuracy. This model achieves superior performance compared to MS-TCN with much less parameters. Our contribution beyond <ref type="bibr" target="#b13">[14]</ref> is thus three folded:</p><p>• We propose a dual dilated layer that combines large and small receptive fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>We optimize the architecture design of MS-TCN by decoupling the prediction phase and the refinement phase. We call the new model MS-TCN++, which achieves superior results compared to MS-TCN. <ref type="bibr">•</ref> We further show that sharing the parameters between the refinement stages in MS-TCN++ results in a more compact model without compromising performance.</p><p>Extensive evaluation shows the effectiveness of our models in capturing long range dependencies between action classes and producing high quality predictions. Our approach achieves state-of-the-art results on three challenging benchmarks for action segmentation: 50Salads <ref type="bibr" target="#b14">[15]</ref>, Georgia Tech Egocentric Activities (GTEA) <ref type="bibr" target="#b15">[16]</ref>, and the Breakfast dataset <ref type="bibr" target="#b16">[17]</ref>. Moreover, the proposed models are viewagnostic and work well on all the three datasets, which depict videos with third person view, top view and egocentric videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Temporal action segmentation has received a lot of interest from the computer vision community. Many approaches where proposed to localize action segments in videos or assign action labels to video frames. In earlier approaches, a sliding window approach is applied with non-maximum suppression <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. However, such approaches are computationally expensive since the model has to be evaluated at different window scales. Other approaches model actions based on the change in the state of objects and materials <ref type="bibr" target="#b17">[18]</ref> or based on the interactions between hands and objects <ref type="bibr" target="#b18">[19]</ref>. Bhattacharya et al. <ref type="bibr" target="#b19">[20]</ref> use a vector time series representation of videos to model the temporal dynamics of complex actions using methods from linear dynamical systems theory. The representation is based on the output of pretrained concept detectors applied on overlapping temporal windows. Cheng et al. <ref type="bibr" target="#b20">[21]</ref> represent videos as a sequence of visual words, and model the temporal dependency by employing a Bayesian non-parametric model of discrete sequences to jointly classify and segment video sequences.</p><p>Despite the success of the previous approaches, their performance was limited as they failed in capturing the context over long video sequences. To alleviate this problem, many proposals tried to employ high level temporal modeling over frame-wise classifiers. Kuehne et al. <ref type="bibr" target="#b7">[8]</ref> represent the frames of a video using Fisher vectors of improved dense trajectories, and then each action is modeled with a hidden Markov model (HMM). These HMMs are combined with a context-free grammar for recognition to determine the most probable sequence of actions. HMMs are also used in many other approaches. <ref type="bibr" target="#b21">[22]</ref> combine HMMs with a Gaussian mixture model (GMM) as a frame-wise classifier. However, since frame-wise classifiers do not capture enough context to detect action classes, Richard et al. <ref type="bibr" target="#b9">[10]</ref> and Kuehne et al. <ref type="bibr" target="#b22">[23]</ref> use a GRU instead of the GMM that is used in <ref type="bibr" target="#b21">[22]</ref>. A hidden Markov model is also used in <ref type="bibr" target="#b23">[24]</ref> to model both transitions between states and their durations. Vo and Bobick <ref type="bibr" target="#b24">[25]</ref> use a Bayes network to segment activities. They represent compositions of actions using a stochastic contextfree grammar with AND-OR operations. <ref type="bibr" target="#b25">[26]</ref> propose a model for temporal action detection that consists of three components: an action model that maps features extracted from the video frames into action probabilities, a language model that describes the probability of actions at sequence level, and finally a length model that models the length of different action segments. To get the video segmentation, they use dynamic programming to find the solution that maximizes the joint probability of the three models. Singh et al. <ref type="bibr" target="#b26">[27]</ref> use a two-stream network to learn representations of short video chunks. These representations are then passed to a bi-directional LSTM to capture dependencies between different chunks. However, their approach is very slow due to the sequential prediction. In <ref type="bibr" target="#b27">[28]</ref>, a three-stream architecture that operates on spatial, temporal and egocentric streams is introduced to learn egocentric-specific features. These features are then classified using a multi-class SVM.</p><p>Multi-scale information is important for speech synthesis <ref type="bibr" target="#b28">[29]</ref> and image recognition <ref type="bibr" target="#b29">[30]</ref>. Inspired by these, researchers have tried to use similar ideas for the temporal action segmentation task. Lea et al. <ref type="bibr" target="#b10">[11]</ref> propose a temporal convolutional network for action segmentation and detection. Their approach follows an encoder-decoder architecture with a temporal convolution and pooling in the encoder, and upsampling followed by deconvolution in the decoder. While using temporal pooling enables the model to capture long-range dependencies, it might result in a loss of fine-grained information that is necessary for fine-grained recognition. Lei and Todorovic <ref type="bibr" target="#b11">[12]</ref> build on top of <ref type="bibr" target="#b10">[11]</ref> and use deformable convolutions instead of the normal convolution and add a residual stream to the encoderdecoder model. Ding and Xu <ref type="bibr" target="#b12">[13]</ref> add lateral connections to the encoder-decoder TCN <ref type="bibr" target="#b10">[11]</ref> and propose a temporal convolutional feature pyramid network for predicting framewise action labels. All of the approaches in <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> operate on downsampled videos with a temporal resolution of 1-3 frames per second. In contrast to these approaches, we operate on the full temporal resolution and use dilated convolutions to capture long-range dependencies. Recently, Mac et al. <ref type="bibr" target="#b30">[31]</ref> propose to learn spatio-temporal features using deformable convolutions and local consistency constraints. On the contrary, in our approach we only focus on the long-term temporal modeling.</p><p>Action detection is a related but a different task. In this context, the goal is to detect sparse action segments while most parts of the videos are unlabeled. In this work, we focus on action segmentation where the videos are densely annotated. For action detection, several approaches follow a two stage pipeline. The first stage is to generate proposals, and then classify and refine the boundaries of these proposals in the second stage <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>. Other approaches combine the proposal generation and classification in a single-stage architecture which enables end-to-end training <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">TEMPORAL ACTION SEGMENTATION</head><p>We introduce a multi-stage temporal convolutional network (MS-TCN) for the temporal action segmentation task. Then we introduce a new layer and address the limitations of MS-TCN to propose an improved model called MS-TCN++. Given the frames of a video x 1:T = (x 1 , . . . , x T ), our goal is to infer the class label for each frame c 1:T = (c 1 , . . . , c T ), where T is the video length. First, we describe the singlestage approach in Section 3.1, then we discuss the multistage model in Section 3.2. Section 3.3 introduces the dual dilated layer. In Section 3.4, we analyze the drawbacks of MS-TCN and introduce the improved model MS-TCN++. Finally, we describe the proposed loss function in Section 3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Single-Stage TCN</head><p>Our single stage model consists of only temporal convolutional layers. We do not use pooling layers, which reduce the temporal resolution, or fully connected layers, which force the model to operate on inputs of fixed size and massively increase the number of parameters. We call this model a single-stage temporal convolutional network (SS-TCN). The first layer of a single-stage TCN is a 1 × 1 convolutional layer, that adjusts the dimension of the input features to match the number of feature maps in the network. Then, this layer is followed by several layers of dilated 1D convolution. Inspired by the wavenet <ref type="bibr" target="#b28">[29]</ref> architecture, we use a dilation factor that is doubled at each layer, i.e. 1, 2, 4, ...., 512. All these layers have the same number of convolutional filters. However, instead of the causal convolution that is used in wavenet, we use acausal convolutions with kernel size 3. Each layer applies a dilated convolution with ReLU activation to the output of the previous layer. We further use residual connections to facilitate gradients flow. The set of operations at each layer can be formally described as followŝ</p><formula xml:id="formula_0">H l = ReLU (W d * H l−1 + b d ),<label>(1)</label></formula><formula xml:id="formula_1">H l = H l−1 + W * Ĥ l + b,<label>(2)</label></formula><p>where H l is the output of layer l, * denotes the convolution operator, W d ∈ R 3×D×D are the weights of the dilated convolution filters with kernel size 3 and D is the number of convolutional filters, W ∈ R 1×D×D are the weights of a 1 × 1 convolution, and b d , b ∈ R D are bias vectors. These operations are illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. Using dilated convolution increases the receptive field without the need to increase the number of parameters by increasing the number of layers or the kernel size. Since the receptive field grows exponentially with the number of layers, we can achieve a very large receptive field with a few layers, which helps in preventing the model from over-fitting the training data. The receptive field at each layer is determined by</p><formula xml:id="formula_2">ReceptiveF ield(l) = 2 l+1 − 1,<label>(3)</label></formula><p>where l ∈ [1, L] is the layer number. Note that this formula is only valid for a kernel of size 3. To get the probabilities for the output class, we apply a 1 × 1 convolution over the output of the last dilated convolution layer followed by a softmax activation, i.e.</p><formula xml:id="formula_3">Y t = Sof tmax(W h L,t + b),<label>(4)</label></formula><p>where Y t contains the class probabilities at time t, h L,t is the output of the last dilated convolution layer at time t, W ∈ R C×D and b ∈ R C are the weights and bias for the 1 × 1 convolution layer. C is the number of classes and D is the number of convolutional filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-Stage TCN</head><p>Stacking several predictors sequentially has shown significant improvements in many tasks like human pose estimation <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>. The idea of these stacked or multi-stage architectures is composing several models sequentially such that each model operates directly on the output of the previous one. The effect of such composition is an incremental refinement of the predictions from the previous stages. Motivated by the success of such architectures, we introduce a multi-stage temporal convolutional network (MS-TCN) for the temporal action segmentation task. In this multi-stage model, each stage takes an initial prediction from the previous stage and refines it. The input of the first stage are the frame-wise features of the video as follows</p><formula xml:id="formula_4">Y 0 = x 1:T ,<label>(5)</label></formula><formula xml:id="formula_5">Y s = F(Y s−1 ),<label>(6)</label></formula><p>where Y s is the output at stage s and F is the singlestage TCN discussed in Section 3.1. Using such a multistage architecture helps in providing more context to predict the class label at each frame. Furthermore, since the output of each stage is an initial prediction, the network is able to capture dependencies between action classes and learn plausible action sequences, which helps in reducing the over-segmentation errors. Note that the input to the next stage is just the framewise probabilities without any additional features. We will show in the experiments how adding features to the input of the next stage affects the quality of the predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Dual Dilated Layer</head><p>In the dilated convolution layers in MS-TCN, the dilation factor increases as we increase the number of layers. While this results in a large receptive field for higher layers, lower layers still suffer from very low receptive fields. Furthermore, higher layers in MS-TCN apply convolutions over very distant time steps due to the large dilation factor. To overcome this problem, we propose a dual dilated layer (DDL). Instead of having one dilated convolution, the DDL combines two convolutions with different dilation factor. The first convolution has a low dilation factor in lower layers and exponentially increases as we increases the number of layers. Whereas for the second convolution, we start with a large dilation factor in lower layers and exponentially decrease it with increasing the number of layers. The set of operations at each layer can be formally described as followŝ</p><formula xml:id="formula_6">H l,d1 = W d1 * H l−1 + b d1 ,<label>(7)</label></formula><formula xml:id="formula_7">H l,d2 = W d2 * H l−1 + b d2 ,<label>(8)</label></formula><formula xml:id="formula_8">H l = ReLU ([Ĥ l,d1 ,Ĥ l,d2 ]),<label>(9)</label></formula><formula xml:id="formula_9">H l = H l−1 + W * Ĥ l + b,<label>(10)</label></formula><p>where W d1 , W d2 ∈ R 3×D×D are the weights of dilated convolutions with dilation factor 2 l and 2 L−l respectively, W ∈ R 1×2D×D are the weights of a 1 × 1 convolution,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dilated Conv</head><p>Dilation:</p><formula xml:id="formula_10">2 − 1 x 1 + Dilated Conv Dilation: 2 ReLU Fig. 3:</formula><p>Overview of the dual dilated layer (DDL). At each layer l, DDL uses two convolutions with dilated factor 2 l and 2 L−l , respectively, where L is the number of layers in the network.</p><formula xml:id="formula_11">and b d1 , b d2 , b ∈ R D are bias vectors. In (9),Ĥ l,d1 andĤ l,d2</formula><p>are concatenated. An overview of the dual dilated layer is illustrated in <ref type="figure">Figure 3</ref>. While the dual dilated layer combines local and global context from the input sequence, there are other techniques in the literature for fusing multi-scale features like feature pyramid networks (FPN) <ref type="bibr" target="#b46">[47]</ref>. While applying FPN for temporal action segmentation has been successful <ref type="bibr" target="#b12">[13]</ref>, these approaches still suffer from a very limited receptive field. Moreover, the multi-scale features in FPN are obtained by applying pooling operations which results in a loss of the fine-grained information that is necessary for temporal segmentation. On the contrary, DDL combines multi-scale features and yet preserves the temporal resolution of the input sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">MS-TCN++</head><p>In this section, we introduce MS-TCN++, which utilizes the proposed dual dilated layer to improve MS-TCN. Similar to MS-TCN, the first stage in MS-TCN++ is responsible for generating the initial prediction, whereas the remaining stages incrementally refine this prediction. For the prediction generation stage, we adapt an SS-TCN with dual dilated layers ( <ref type="figure">Figure 3</ref>) replacing the simple dilated residual layers ( <ref type="figure" target="#fig_1">Figure 2</ref>) that are originally used in SS-TCN. Using the DDL enables the prediction generation stage to capture both local and global features in all layers, which results in better predictions. As refinement is easier than prediction generation, we adapt the SS-TCN architecture with dilated residual layers for the refinement stages. In our experiments, we show that using DDL only for the first stage performs best. <ref type="figure" target="#fig_2">Figure 4</ref> shows an overview of the proposed MS-TCN++.</p><p>While adding more stages incrementally refines the predictions, it also drastically increases the number of parameters. Nevertheless, as the refinement stages are sharing the This stage generates an initial prediction that is refined incrementally by a set of N r refinement stages. For the refinement stages, an SS-TCN with dilated residual layers is used. A loss layer is added after each stage.</p><p>same role, their parameters can be shared to get a more compact model. In the experiments, we show that sharing the parameters between the refinement stages significantly reduces the number of parameters with only a slight degradation in accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Loss Function</head><p>As a loss function, we use a combination of a classification loss and a smoothing loss. For the classification loss, we use a cross entropy loss</p><formula xml:id="formula_12">L cls = 1 T t −log(y t,c ),<label>(11)</label></formula><p>where y t,c is the predicted probability for the ground truth label c at time t. While the cross entropy loss already performs well, we found that the predictions for some of the videos contain a few over-segmentation errors. To further improve the quality of the predictions, we use an additional smoothing loss to reduce such over-segmentation errors. For this loss, we use a truncated mean squared error over the frame-wise log-probabilities</p><formula xml:id="formula_13">L T −M SE = 1 T C t,c∆ 2 t,c ,<label>(12)</label></formula><formula xml:id="formula_14">∆ t,c = ∆ t,c : ∆ t,c ≤ τ τ : otherwise ,<label>(13)</label></formula><formula xml:id="formula_15">∆ t,c = |log y t,c − log y t−1,c | ,<label>(14)</label></formula><p>where T is the video length, C is the number of classes, and y t,c is the probability of class c at time t.</p><p>Note that the gradients are only computed with respect to y t,c , whereas y t−1,c is not considered as a function of the model's parameters. This loss is similar to the Kullback-Leibler (KL) divergence loss where</p><formula xml:id="formula_16">L KL = 1 T t,c y t−1,c (log y t−1,c − log y t,c ).<label>(15)</label></formula><p>However, we found that the truncated mean squared error (L T −M SE ) (12) reduces the over-segmentation errors more. We will compare the KL loss and the proposed loss in the experiments.</p><p>The final loss function for a single stage is a combination of the above mentioned losses</p><formula xml:id="formula_17">L s = L cls + λL T −M SE ,<label>(16)</label></formula><p>where λ is a model hyper-parameter to determine the contribution of the different losses. Finally to train the complete model, we minimize the sum of the losses over all stages</p><formula xml:id="formula_18">L = s L s .<label>(17)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Implementation Details</head><p>For both MS-TCN and MS-TCN++, we use a multi-stage architecture with four stages. While all stages are the same for MS-TCN, the stages in MS-TCN++ consist of one prediction generation stage and three refinement stages. Each stage in MS-TCN and the refinement stages in MS-TCN++ contain ten dilated convolution layers. For the prediction generation stage in MS-TCN++, we use eleven layers. Dropout is used after each layer with probability 0.5. We set the number of filters to 64 in all layers of the model and the filter size is 3. For the loss function, we set τ = 4 and λ = 0.15. In all experiments, we use Adam optimizer with a learning rate of 0.0005.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>Datasets. We evaluate the proposed models on three challenging datasets: 50Salads <ref type="bibr" target="#b14">[15]</ref>, Georgia Tech Egocentric Activities (GTEA) <ref type="bibr" target="#b15">[16]</ref>, and the Breakfast dataset <ref type="bibr" target="#b16">[17]</ref>. <ref type="table" target="#tab_1">Table 1</ref> shows a summary of these datasets. The 50Salads dataset contains 50 videos with 17 action classes. The videos were recorded from the top view. On average, each video contains 20 action instances and is 6.4 minutes long. As the name of the dataset indicates, the videos depict salad preparation activities. These activities were performed by 25 actors where each actor prepared two different salads. For evaluation, we use five-fold crossvalidation and report the average as in <ref type="bibr" target="#b14">[15]</ref>.</p><p>The GTEA dataset contains 28 videos corresponding to 7 different activities, like preparing coffee or cheese sandwich, performed by 4 subjects. This dataset contains egocentric videos recorded by a camera that is mounted on the actor's head. The frames of the videos are annotated with 11 action  classes including background. On average, each video has 20 action instances. We use cross-validation for evaluation by leaving one subject out. The Breakfast dataset is the largest among the three datasets with 1, 712 third person view videos. The videos were recorded in 18 different kitchens showing breakfast preparation related activities. Overall, there are 48 different actions where each video contains 6 action instances on average. For evaluation, we use the standard 4 splits as proposed in <ref type="bibr" target="#b16">[17]</ref> and report the average.</p><p>For all datasets, we extract I3D <ref type="bibr" target="#b2">[3]</ref> features for the video frames and use these features as input to our model. For the GTEA and Breakfast datasets we use the temporal video resolution at 15 fps, while for 50Salads we downsampled the features from 30 fps to 15 fps to be consistent with the other datasets.</p><p>Evaluation Metrics. For evaluation, we report the framewise accuracy (Acc), segmental edit distance and the segmental F1 score at overlapping thresholds 10%, 25% and 50%, denoted by F 1@{10, 25, 50}. The overlapping threshold is determined based on the intersection over union (IoU) ratio. While the frame-wise accuracy is the most commonly used metric for action segmentation, long action classes have a higher impact than short action classes on this metric and over-segmentation errors have a very low impact. For that reason, we use the segmental F1 score as a measure of the quality of the prediction as proposed by <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Effect of the Number of Stages</head><p>We start our evaluation by showing the effect of using a multi-stage architecture (MS-TCN). <ref type="table" target="#tab_2">Table 2</ref> shows the results of a single-stage model compared to multi-stage models with different number of stages. As shown in the table, all of these models achieve a comparable framewise accuracy. Nevertheless, the quality of the predictions is very different. Looking at the segmental edit distance and F1 scores of these models, we can see that the singlestage model produces a lot of over-segmentation errors, as indicated by the low F1 score. On the other hand, using a multi-stage architecture reduces these errors and increases the F1 score. This effect is clearly visible when we use two or three stages, which gives a huge boost to the accuracy. Adding the fourth stage still improves the results but not as significant as the previous stages. However, by adding the fifth stage, we can see that the performance starts to degrade. This might be an over-fitting problem as a result of increasing the number of parameters. The effect of the multi-stage architecture can also be seen in the qualitative results shown in <ref type="figure" target="#fig_4">Figure 5</ref>. Adding more stages results in an incremental refinement of the predictions. In the rest of the experiments we use a multi-stage TCN with four stages.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Multi-Stage TCN vs. Deeper Single-Stage TCN</head><p>In the previous section, we have seen that our multi-stage architecture is better than a single-stage one. However, that comparison does not show whether the improvement is because of the multi-stage architecture or due to the increase in the number of parameters when adding more stages. For a fair comparison, we train a single-stage model that has the same number of parameters as the multi-stage one. As each stage in our MS-TCN contains 12 layers (ten dilated convolutional layers, one 1 × 1 convolutional layer and a softmax layer), we train a single-stage TCN with 48 layers, which is the number of layers in a MS-TCN with four stages. For the dilated convolutions, we use similar dilation factors as in our MS-TCN. I.e. we start with a dilation factor of 1 and double it at every layer up to a factor of 512, and then we start again from 1. As shown in <ref type="table" target="#tab_3">Table 3</ref>, our multi-stage architecture outperforms its single-stage counterpart with a large margin of up to 27%. This highlights the impact of the multi-stage architecture in improving the quality of the predictions.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparing Different Loss Functions</head><p>As a loss function, we use a combination of a cross-entropy loss, which is common practice for classification tasks, and a truncated mean squared loss over the frame-wise log-probabilities to ensure smooth predictions. While the smoothing loss slightly improves the frame-wise accuracy compared to the cross entropy loss alone, we found that this loss produces much less over-segmentation errors. <ref type="table" target="#tab_4">Table 4</ref> and <ref type="figure">Figure 6</ref> show a comparison of these losses. As shown in <ref type="table" target="#tab_4">Table 4</ref>, the proposed loss achieves better F1 and edit scores with an absolute improvement of up to 5%. This indicates that our loss produces less over-segmentation errors compared to cross entropy since it forces consecutive frames to have similar class probabilities, which results in a smoother output.</p><p>Penalizing the difference in log-probabilities is similar to the Kullback-Leibler (KL) divergence loss, which measures the difference between two probability distributions. However, the results show that the proposed loss produces better results than the KL loss as shown in <ref type="table" target="#tab_4">Table 4</ref> and <ref type="figure">Figure 6</ref>. The reason behind this is the fact that the KL divergence loss does not penalize cases where the difference between the target probability and the predicted probability is very small. Whereas the proposed loss penalizes small differences as well. Note that, in contrast to the KL loss, the proposed loss is symmetric. <ref type="figure" target="#fig_6">Figure 7</ref> shows the surface for both the KL loss and the proposed truncated mean squared loss for the case of two classes. We also tried a symmetric version of the KL loss but it performed worse than the KL loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Impact of λ and τ</head><p>The effect of the proposed smoothing loss is controlled by two hyper-parameters: λ and τ . In this section, we study the impact of these parameters and see how they affect the performance of the proposed model. Impact of λ: In all experiments, we set λ = 0.15. To analyze the effect of this parameter, we train different models with different values of λ. As shown in <ref type="table" target="#tab_5">Table 5</ref>, the impact of λ is very small on the performance. Reducing λ to 0.05 still improves the performance but not as good as the default  value of λ = 0.15. Increasing its value to λ = 0.25 also causes a degradation in performance. This drop in performance is due to the fact that the smoothing loss heavily penalizes changes in frame-wise labels, which affects the detected boundaries between action segments. Impact of τ : This hyper-parameter defines the threshold to truncate the smoothing loss. Our default value is τ = 4. While reducing the value to τ = 3 still gives an improvement over the cross entropy baseline, setting τ = 5 results in a huge drop in performance. This is mainly because when τ is too high, the smoothing loss penalizes cases where the model is very confident that the consecutive frames belong to two different classes, which indeed reduces the capability of the model in detecting the true boundaries between action segments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Effect of Passing Features to Higher Stages</head><p>In the proposed multi-stage TCN, the input to higher stages are the frame-wise probabilities only. However, in the multistage architectures that are used for human pose estimation,  additional features are usually concatenated to the output heat-maps of the previous stage. In this experiment, we therefore analyze the effect of combining additional features to the input probabilities of higher stages. To this end, we trained two multi-stage TCNs: one only with the predicted frame-wise probabilities as input to the next stage, and, for the second model, we concatenated the output of the last dilated convolutional layer in each stage to the input probabilities of the next stage. As shown in <ref type="table" target="#tab_7">Table 6</ref>, concatenating the features to the input probabilities results in a huge drop of the F1 score and the segmental edit distance (around 20%). We argue that the reason behind this degradation in performance is that a lot of action classes share similar appearance and motion. By adding the features of such classes at each stage, the model is confused and produces small separated falsely detected action segments that correspond to an over-segmentation effect. Passing only the probabilities forces the model to focus on the context of neighboring labels, which are explicitly represented by the probabilities. This effect can also be seen in the qualitative results shown in <ref type="figure">Figure 8</ref>.</p><formula xml:id="formula_19">F1@{10</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">MS-TCN++ vs. MS-TCN</head><p>In this section, we compare the two multi-stage architectures: MS-TCN++ and MS-TCN. In contrast to MS-TCN, MS-TCN++ uses the dual dilated layer (DDL) in the first stage. <ref type="table" target="#tab_8">Table 7</ref> shows the results of both architectures on the 50Salads dataset. As shown in the table, MS-TCN++ outperforms MS-TCN with a large margin of up to 6.4%. This emphasizes the importance of combining both local and global representations in the prediction generation stage by utilizing the DDL in MS-TCN++. To study the impact of using DDL in all stages, we also train an MS-TCN where we use the DDL in all stages. As shown in <ref type="table" target="#tab_8">Table 7</ref>, MS-TCN++ outperforms MS-TCN with DDL in all stages. This indicates that decoupling the design of the refinement stages and the prediction generation stage is crucial. While utilizing the global context by the DDL is crucial for the prediction generation stage, the refinement stages focus more on the local context. By adding DDL to the refinement stages, the accuracy even drops due to overfitting. Note that using DDL in all stages outperforms MS-TCN with up to 2.8%. This further highlights the gains of the DDL. The impact of DDL is also visible in the qualitative results shown in <ref type="figure" target="#fig_7">Figure 9</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Impact of the Number of Layers</head><p>For MS-TCN and the refinement stages in MS-TCN++, we fix the number of layers in each stage to 10 layers. Whereas for the prediction generation stage in MS-TCN++, we set the number of layers to 11. In this section, we study the impact of these parameters. <ref type="table" target="#tab_10">Table 8</ref> shows the impact of the number of layers (L) for the MS-TCN stages on the 50Salads dataset. Increasing L form 8 to 10 significantly improves the performance. This is mainly due to the increase in the receptive field. Using more than 10 layers (L = 11, L = 12) does not improve the frame-wise accuracy but slightly increases the F1 scores. We also tried to change the number of layers only in the refinement stages in MS-TCN. As shown in <ref type="table" target="#tab_11">Table 9</ref>, this does not have a significant impact and using 10 layers achieves the best performance. Also for the refinement stages in MS-TCN++, the number of layers L r does not have a significant impact on the performance. To be consistent with <ref type="bibr" target="#b13">[14]</ref>, we set L r = 10 which achieves a reasonable trade-off on performance with respect to all evaluation metrics as shown in <ref type="table" target="#tab_11">Table 9</ref>. A similar behavior can be observed in <ref type="table" target="#tab_1">Table 10</ref> for the number of layers L g in the prediction generation stage with L g = 11 achieving the best performance. Generally speaking, the number of layers in each stage has more impact in MS-TCN compared to MS-TCN++. As the main difference between these two models is the dual dilation layer (DDL) that is used in MS-TCN++, this indicates that the DDL can better capture both local and global features to generate much better predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Impact of the Large Receptive Field on Short Videos</head><p>To study the impact of the large receptive field on short videos, we evaluate MS-TCN and MS-TCN++ on three groups of videos based on their durations. For this evaluation, we use the GTEA dataset since it contains shorter videos compared to the other datasets. As shown in   and long videos. Nevertheless, the performance is slightly worse on longer videos due to the limited receptive field. The improvements of MS-TCN++ over MS-TCN are also noticeable for both short and long videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9">Effect of the Number of Refinement Stages</head><p>We set the number of refinement stages N r in MS-TCN++ to 3 stages, which results in a model with 4 stages in total. <ref type="table" target="#tab_1">Table 12</ref> shows the impact of the refinement stages on the 50Salads dataset. Using only the prediction generation stage (N r = 0) results in a relative low performance but it is F1@{10,25,50} Edit Acc     much better than a single stage TCN ( <ref type="table" target="#tab_2">Table 2</ref>). Adding more refinement stages improves the performance incrementally. Nevertheless, adding more than 3 refinement stages does not provide additional improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.10">Impact of Parameters Sharing</head><p>MS-TCN++ consists of a prediction generation stage and 3 refinement stages. Although adding more stages results in a better performance, it also increases the number of parameters. As the refinement stages share in principle the same task, it is hence intuitive that they can share parameters. <ref type="table" target="#tab_1">Table 13</ref> shows the impact of sharing parameters between the refinement stages. Sharing parameters significantly reduces the number of parameters with only a slight decrease in performance. For an MS-TCN++ with 3 refinement stages, sharing parameters reduces the total number of parameters to roughly 66% of the total parameters in the original model. As shown in the table, MS-TCN++ with shared parameters outperforms MS-TCN with a margin of up to 3.8% despite of having less parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.11">Impact of Temporal Resolution</head><p>Previous temporal models operate on a low temporal resolution of 1-3 frames per second <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. On the contrary, our approach is able to handle a higher resolution of 15 fps. In this experiment, we evaluate MS-TCN and MS-TCN++, with and without parameter sharing, for a low temporal resolution of 1 fps. As shown in <ref type="table" target="#tab_1">Table 14</ref>, both models are able to handle both low and high temporal resolutions. While reducing the temporal resolution for MS-TCN results in a better edit distance and segmental F1 score, using a higher resolution gives better frame-wise accuracy. Operating on a low temporal resolution makes MS-TCN less prone to the over-segmentation problem, which is reflected in the better edit and F1 scores. Due to the dual dilated layers, MS-TCN++ benefits more from a higher temporal resolution and the impact of reducing the temporal resolution for MS-TCN++ is noticeable for all evaluation metrics. Note that even when we share the parameters of the refinement stages in MS-TCN++, using a higher temporal resolution results in a better performance.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.12">Impact of Fine-tuning the Features</head><p>In our experiments, we use the I3D features without finetuning. <ref type="table" target="#tab_1">Table 15</ref> shows the effect of fine-tuning on the GTEA dataset. Both of our multi-stage architectures, MS-TCN and MS-TCN++, significantly outperform the single stage architecture -with and without fine-tuning. This also holds when the parameters of the refinement stages in MS-TCN++ are shared. Fine-tuning improves the results, but the effect of fine-tuning for action segmentation is lower than for action recognition. This is expected since the temporal model is by far more important for segmentation than for recognition. Note that without fine-tuning, sharing parameters achieves better results on GTEA. This is mainly due to the reduced number of parameters, which prevents the model from over-fitting the training data, especially for small datasets like GTEA.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.13">Comparison with the State-of-the-Art</head><p>In this section, we compare the proposed models to the state-of-the-art methods on three datasets: 50Salads, Geor-  gia Tech Egocentric Activities (GTEA), and the Breakfast datasets. The results are presented in <ref type="table" target="#tab_1">Table 16</ref>. As shown in the table, our models outperform the state-of-the-art methods on the three datasets and with respect to three evaluation metrics: F1 score, segmental edit distance, and frame-wise accuracy (Acc) with a large margin (up to 11.6% for the frame-wise accuracy on the 50Salads dataset). Qualitative results on the three datasets are shown in <ref type="figure" target="#fig_8">Figure 10</ref>.</p><p>Note that all the reported results are obtained using the I3D features. To analyze the effect of using a different type of features, we evaluated MS-TCN on the Breakfast dataset using the improved dense trajectories (IDT) features, which are the commonly used features for the Breakfast dataset. As shown in <ref type="table" target="#tab_1">Table 16</ref>, the impact of the features is very small. While the frame-wise accuracy and edit distance are slightly better using the I3D features, the model achieves a better F1 score when using the IDT features compared to I3D. This is mainly because I3D features encode both motion and appearance, whereas the IDT features encode only motion. For datasets like Breakfast, using appearance information does not help the performance since the appearance does not give a strong evidence about the action that is carried out. This can be seen in the qualitative results shown in <ref type="figure" target="#fig_8">Figure 10</ref>. The video frames share a very similar appearance. Additional appearance features therefore do not help in recognizing the activity. As shown in <ref type="table" target="#tab_1">Table 16</ref>, sharing the parameters of the refinement stages achieves similar performance to MS-TCN++, but it requires about 66% less parameters as reported in <ref type="table" target="#tab_1">Table 13</ref>. As our approaches do not use any recurrent layers, they are very fast both during training and testing. Training MS-TCN++ for 50 epochs takes only 10 minutes on the 50Salads dataset compared to 35 minutes for training a single cell of a Bi-LSTM with a 64-dimensional hidden state on a single GTX 1080 Ti GPU. This is due to the sequential prediction of the LSTM, where the activations at any time step depend on the activations from the previous steps. For the MS-TCN and MS-TCN++, activations at all time steps are computed in parallel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We presented two multi-stage architectures for the temporal action segmentation task. While the first stage generates an initial prediction, this prediction is iteratively refined by the higher stages. Instead of the commonly used temporal pooling, we used dilated convolutions to increase the temporal receptive field. The experimental evaluation demonstrated the capability of our architecture in capturing temporal dependencies between action classes and reducing oversegmentation errors. We further introduced a smoothing loss that gives an additional improvement of the predictions quality. We also introduced a dual dilated layer that captures both local and global features, which results in better performance. Moreover, we showed that sharing the parameters in the refinement stages results in a more efficient model with a slight degradation in performance. Our models outperform the state-of-the-art methods on three challenging datasets recorded from different views with a large margin. Since our model is fully convolutional, it is very efficient and fast both during training and testing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>•</head><label></label><figDesc>S. Li, Y. Abu Farha, and J. Gall are with the University of Bonn, Germany. Y. Liu and M.-M. Cheng are with the Nankai University, China. S. Li and Y. Abu Farha contributed equally. E-mails: {lishijie, abufarha, gall}@iai.uni-bonn.de (S. Li, Y. Abu Farha, and J. Gall), vagrantlyun@gmail.com (Y. Liu), cmm@nankai.edu.cn (M.-M. Cheng)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Overview of the dilated residual layer. At each layer l, the dilated residual layer uses a convolution with dilated factor 2 l .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Overview of MS-TCN++. The first stage adapts an SS-TCN model with dual dilated layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>F1@{10</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Qualitative result from the 50Salads dataset for comparing different number of stages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>F1@{10, 25</head><label>25</label><figDesc>,50} Edit Acc SS-TCN (48 layers) 49.0 46.4 40.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>Loss surface for the Kullback-Leibler (KL) divergence loss (L KL ) and the proposed truncated mean squared loss (L T −M SE ) for the case of two classes. y t,c is the predicted probability for class c and y t−1,c is the target probability corresponding to that class.Impact of λ F1@{10,25,50} Edit Acc MS-TCN (λ = 0.05, τ = 4) 74.1 71.7 62.4 66.6 80.0 MS-TCN (λ = 0.15, τ = 4) 76.3 74.0 64.5 67.9 80.7 MS-TCN (λ = 0.25, τ = 4) 74.7 72.4 63.7</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 :F1@{10</head><label>9</label><figDesc>Qualitative results for two videos from the 50Salads dataset showing the impact of the dual dilated layer (DDL).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 :</head><label>10</label><figDesc>Qualitative results for the temporal action segmentation task on (a)(b) 50Salads, (c)(d) GTEA, and (e)(f) Breakfast dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1 :</head><label>1</label><figDesc>Summary of the used datasets in the experiments.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2 :</head><label>2</label><figDesc>Effect of the number of stages on the 50Salads dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3 :</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell>F1@{10,25,50}</cell><cell>Edit</cell><cell>Acc</cell></row><row><cell>L cls</cell><cell>71.3 69.7 60.7</cell><cell>64.2</cell><cell>79.9</cell></row><row><cell>L cls + λL KL</cell><cell>71.9 69.3 60.1</cell><cell>64.6</cell><cell>80.2</cell></row><row><cell>L cls + λL T −M SE</cell><cell>76.3 74.0 64.5</cell><cell>67.9</cell><cell>80.7</cell></row></table><note>Comparing a multi-stage TCN with a deep single- stage TCN on the 50Salads dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 4 :</head><label>4</label><figDesc>Comparing different loss functions on the 50Salads dataset.</figDesc><table><row><cell>GT</cell></row><row><cell>L cls</cell></row><row><cell>L cls + λL T − MSE L cls + λL KL</cell></row><row><cell>Fig. 6: Qualitative result from the 50Salads dataset for com-</cell></row><row><cell>paring different loss functions.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 5 :</head><label>5</label><figDesc></figDesc><table /><note>Impact of λ and τ on the 50Salads dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 6 :</head><label>6</label><figDesc>Effect of passing features to higher stages on the 50Salads dataset.</figDesc><table><row><cell>GT</cell></row><row><cell>Prob. and features</cell></row><row><cell>Prob. only</cell></row><row><cell>GT</cell></row><row><cell>Prob. and features</cell></row><row><cell>Prob. only</cell></row><row><cell>Fig. 8: Qualitative results for two videos from the 50Salads</cell></row><row><cell>dataset showing the effect of passing features to higher</cell></row><row><cell>stages.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 7 :</head><label>7</label><figDesc></figDesc><table /><note>MS-TCN++ vs. MS-TCN vs. MS-TCN with DDL on the 50Salads dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 11</head><label>11</label><figDesc></figDesc><table><row><cell>,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 8 :</head><label>8</label><figDesc>Effect of the number of layers (L) in each stage of MS-TCN on the 50Salads dataset.</figDesc><table><row><cell></cell><cell cols="2">F1@{10,25,50}</cell><cell>Edit</cell><cell>Acc</cell></row><row><cell></cell><cell>L r = 6</cell><cell>74.3 71.5</cell><cell>62.8</cell><cell>66.0 78.6</cell></row><row><cell></cell><cell>L r = 8</cell><cell>75.4 72.4</cell><cell>64.3</cell><cell>68.0 79.5</cell></row><row><cell>MS-TCN</cell><cell cols="2">L r = 10 76.3 74.0</cell><cell>64.5</cell><cell>67.9 80.7</cell></row><row><cell></cell><cell cols="2">L r = 11 75.0 72.0</cell><cell>63.5</cell><cell>67.6 80.3</cell></row><row><cell></cell><cell cols="2">L r = 12 74.1 71.2</cell><cell>62.3</cell><cell>65.7 79.1</cell></row><row><cell></cell><cell>L r = 6</cell><cell>78.2 75.6</cell><cell>67.7</cell><cell>69.6 82.3</cell></row><row><cell></cell><cell>L r = 8</cell><cell>80.9 78.2</cell><cell>70.2</cell><cell>73.4 82.9</cell></row><row><cell cols="3">MS-TCN++ L r = 10 80.7 78.5</cell><cell>70.1</cell><cell>74.3 83.7</cell></row><row><cell></cell><cell cols="2">L r = 11 80.5 78.3</cell><cell>70.0</cell><cell>72.6 83.4</cell></row><row><cell></cell><cell cols="2">L r = 12 79.4 76.9</cell><cell>69.2</cell><cell>71.3 83.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 9 :</head><label>9</label><figDesc>Effect of the number of layers (L r ) in each refinement stage on the 50Salads dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 10 :</head><label>10</label><figDesc>Effect of the number of layers (L g ) in the prediction generation stage for MS-TCN++ on the 50Salads dataset.</figDesc><table><row><cell></cell><cell>Duration</cell><cell>F1@{10,25,50}</cell><cell>Edit</cell><cell>Acc</cell></row><row><cell></cell><cell>&lt; 1 min</cell><cell>89.6 87.9 77.0</cell><cell>82.5</cell><cell>76.6</cell></row><row><cell>MS-TCN</cell><cell cols="2">1 − 1.5 min 85.9 84.3 71.9</cell><cell>80.7</cell><cell>76.4</cell></row><row><cell></cell><cell>≥ 1.5 min</cell><cell>81.2 76.5 58.4</cell><cell>71.8</cell><cell>75.9</cell></row><row><cell></cell><cell>&lt; 1 min</cell><cell>90.4 90.4 80.8</cell><cell>84.4</cell><cell>79.3</cell></row><row><cell cols="3">MS-TCN++ 1 − 1.5 min 88.7 85.8 75.1</cell><cell>83.6</cell><cell>79.3</cell></row><row><cell></cell><cell>≥ 1.5 min</cell><cell>80.8 78.8 63.3</cell><cell>76.1</cell><cell>77.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE 11 :</head><label>11</label><figDesc>Evaluation of three groups of videos from the GTEA dataset based on their durations.</figDesc><table><row><cell></cell><cell>N r</cell><cell>F1@{10,25,50}</cell><cell>Edit</cell><cell>Acc</cell></row><row><cell>MS-TCN++</cell><cell>0</cell><cell>51.0 48.4 40.7</cell><cell>40.4</cell><cell>80.7</cell></row><row><cell>MS-TCN++</cell><cell>1</cell><cell>70.7 68.2 59.7</cell><cell>62.0</cell><cell>82.4</cell></row><row><cell>MS-TCN++</cell><cell>2</cell><cell>77.8 75.1 66.9</cell><cell>69.4</cell><cell>82.5</cell></row><row><cell>MS-TCN++</cell><cell>3</cell><cell>80.7 78.5 70.1</cell><cell>74.3</cell><cell>83.7</cell></row><row><cell>MS-TCN++</cell><cell>4</cell><cell>80.6 78.7 70.1</cell><cell>73.1</cell><cell>82.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE 12 :</head><label>12</label><figDesc>Impact of the number of refinement stages on the 50Salads dataset.</figDesc><table><row><cell></cell><cell>F1@{10,25,50}</cell><cell>Edit</cell><cell>Acc</cell><cell># param.(m)</cell></row><row><cell>MS-TCN</cell><cell>76.3 74.0 64.5</cell><cell>67.9</cell><cell>80.7</cell><cell>0.80</cell></row><row><cell>MS-TCN++</cell><cell>80.7 78.5 70.1</cell><cell>74.3</cell><cell>83.7</cell><cell>0.99</cell></row><row><cell cols="2">MS-TCN++(sh) 78.7 76.6 68.3</cell><cell>70.7</cell><cell>82.2</cell><cell>0.66</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>TABLE 13 :</head><label>13</label><figDesc>Impact of sharing parameters for the refinement stages on the 50Salads dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>TABLE 14 :</head><label>14</label><figDesc>Impact of temporal resolution on the 50Salads dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>TABLE 15 :</head><label>15</label><figDesc></figDesc><table /><note>Effect of fine-tuning on the GTEA dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>TABLE 16 :</head><label>16</label><figDesc>Comparison with the state-of-the-art on 50Salads, GTEA, and the Breakfast dataset. (* obtained from<ref type="bibr" target="#b12">[13]</ref>).</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The work has been funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) GA 1927/4-1 (FOR 2535 Anticipating Human Behavior) and the ERC Starting Grant ARCA (677650).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Spatiotemporal residual networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3468" to="3476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? A new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A database for fine grained activity detection of cooking activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1194" to="1201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast saliency based pooling of fisher encoded dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Seidenari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV), THUMOS Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The lear submission at THUMOS 2014</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An end-to-end generative framework for video segmentation and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Segmental spatiotemporal CNNs for fine-grained action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="36" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Weakly supervised action learning with RNN based fine-to-coarse modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Temporal convolutional networks for action segmentation and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Temporal deformable residual networks for action segmentation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6742" to="6751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Weakly-supervised action segmentation with iterative soft boundary assignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6508" to="6516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">MS-TCN: Multi-stage temporal convolutional network for action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abu</forename><surname>Farha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Combining embedded accelerometers with computer vision for recognizing food preparation activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Mckenna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Joint Conference on Pervasive and Ubiquitous Computing</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="729" to="738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to recognize objects in egocentric activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3281" to="3288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The language of actions: Recovering the syntax and semantics of goal-directed human activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="780" to="787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Modeling actions through state changes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2579" to="2586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Understanding egocentric activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="407" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Recognition of complex events: Exploiting temporal dynamics between underlying concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Kalayeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2243" to="2250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Temporal sequence modeling for video event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pankanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Choudhary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2227" to="2234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of actions from transcripts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">163</biblScope>
			<biblScope unit="page" from="78" to="89" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A Hybrid RNN-HMM Approach for Weakly Supervised Temporal Action Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page" from="765" to="779" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning latent temporal structure for complex event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1250" to="1257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">From stochastic grammar to bayes network: Probabilistic parsing of complex activity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">N</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Bobick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2641" to="2648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Temporal action detection using a statistical language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3131" to="3140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A multistream bi-directional recurrent neural network for fine-grained action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1961" to="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">First person action recognition using deep learned descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2620" to="2628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Wavenet: A generative model for raw audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA Speech Synthesis Workshop</title>
		<imprint>
			<publisher>SSW</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Res2net: A new multi-scale backbone architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning motion in feature space: Locally-consistent deformable convolution networks for fine-grained action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-N</forename><forename type="middle">C</forename><surname>Mac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">R-C3D: Region convolutional 3d network for temporal activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Temporal action detection with structured segment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rethinking the faster R-CNN architecture for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Turn tap: Temporal unit regression network for temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">SST: Single-stream temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Cascaded boundary regression for temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">BMN: Boundarymatching network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3889" to="3898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">End-to-end learning of action detection from frame glimpses in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Endto-end, single-stream temporal action detection in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Gaussian temporal awareness networks for action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4724" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Body Parts Dependent Joint Regressors for Human Pose Estimation in Still Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dantone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2131" to="2143" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in International</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
