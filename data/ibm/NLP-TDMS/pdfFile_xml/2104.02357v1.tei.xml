<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adaptive Mutual Supervision for Weakly-Supervised Temporal Action Localization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20151">AUGUST 2015 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">Adaptive Mutual Supervision for Weakly-Supervised Temporal Action Localization</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20151">AUGUST 2015 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Temporal action localization</term>
					<term>weak supervision</term>
					<term>adaptive sampling strategy</term>
					<term>mutual location supervision</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Weakly-supervised temporal action localization aims to localize actions in untrimmed videos with only video-level action category labels. Most of previous methods ignore the incompleteness issue of Class Activation Sequences (CAS), suffering from trivial localization results. To solve this issue, we introduce an adaptive mutual supervision framework (AMS) with two branches, where the base branch adopts CAS to localize the most discriminative action regions, while the supplementary branch localizes the less discriminative action regions through a novel adaptive sampler. The adaptive sampler dynamically updates the input of the supplementary branch with a sampling weight sequence negatively correlated with the CAS from the base branch, thereby prompting the supplementary branch to localize the action regions underestimated by the base branch. To promote mutual enhancement between these two branches, we construct mutual location supervision. Each branch leverages location pseudo-labels generated from the other branch as localization supervision. By alternately optimizing the two branches in multiple iterations, we progressively complete action regions. Extensive experiments on THUMOS14 and ActivityNet1.2 demonstrate that the proposed AMS method significantly outperforms the stateof-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T EMPORAL action localization, which localizes actions from untrimmed videos, plays an important role in video understanding. Although several studies <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b6">[7]</ref> have shown promising results on strongly-supervised temporal action localization, the annotations in the form of precise action boundaries are both time-consuming and noisy. Weakly-supervised temporal action localization (WTAL), which handles the same problem but only requires video-level action category labels, has recently received increasing attention <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b14">[15]</ref>.</p><p>To date in the studies, there are two main frameworks in the WTAL task. The classification-based framework <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref> adopts the idea of multiple instance learning (MIL), i.e., first training an action classifier by video-level category labels, then thresholding the class activation sequence (CAS) of the classifier to obtain action proposals; see <ref type="figure">Fig. 1 (a)</ref>. This <ref type="figure">Fig. 1</ref>: Framework comparison in WTAL. 'Class' is classification supervision, 'CAS' is class activation sequence, and arrows denote the propagation direction. (a): Classificationbased framework <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b9">[10]</ref> thresholds CAS for localization. (b): Self-training-based framework <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b17">[18]</ref> relies on location pseudo-labels generated from CAS. Both of these frameworks ignore the incompleteness issue of CAS, suffering from trivial localization results. (c): Adaptive mutual supervision framework. Red is our two key contributions. framework only optimizes the classification objective. On the other hand, regarding the CAS as a noisy location cue, the selftraining-based framework <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b17">[18]</ref> iteratively thresholds the CAS of the current step to generate location pseudo-labels for the next step, and progressively refines the action localization results; see <ref type="figure">Fig. 1 (b)</ref>.</p><p>The CAS generated from the classifier, indicating the classspecific action probability of each snippet, becomes the key to the localization performance of the above two frameworks. However, CAS has an incompleteness issue, i.e., it only covers the most discriminative regions that contribute most to action classification <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b14">[15]</ref>. Since there is a fundamental difference in optimization objectives between classification and localization, i.e., classification mainly relies on the most discriminative action regions while localization requires mining complete action regions, CAS is usually sparse and incomplete. As a result, the action proposals and the location pseudolabels produced from CAS are both low-quality, causing trivial localization results in these two frameworks.</p><p>To solve the incompleteness issue, this paper considers a novel adaptive mutual supervision framework (AMS) with two branches that are collaborative and complementary. The base branch adopts the CAS to localize the most discriminative arXiv:2104.02357v1 [cs.CV] 6 Apr 2021 action regions, which is similar to above two frameworks. While the supplementary branch localizes the less discriminative action regions to complete localization results of the whole framework. To achieve collaboration and complementarity, we propose two core designs: an adaptive sampler and mutual location supervision; see <ref type="figure">Fig. 1 (c)</ref>.</p><p>The design rationale of the adaptive sampler is to select the less discriminative regions underestimated by the base branch as inputs for the supplementary branch, so that the supplementary branch can focus on these challenging action regions. Concretely, we feed the original video into the base branch, while leverage the adaptive sampler to probabilistically select video snippets for the supplementary branch. Since the CAS of the base branch mainly localizes the most discriminative action regions, the sampling probability sequence is designed to be dynamic and negatively correlated with the CAS from the base branch. That is, we over-sample the snippets with low CAS values while under-sample those with high CAS values. As a result, the inputs of the supplementary branch mainly consist of the snippets corresponding to the low CAS regions of the base branch, which prompts the supplementary branch to purposefully mine less discriminative action regions, and thus completes detected action information.</p><p>To further promote mutual enhancement between the base branch and the supplementary branch, we are motivated to design mutual location supervision, which forces each branch to explicitly optimize the localization objective with location pseudo-labels from the other branch. Specifically, both the two branches use video-level category labels as classification supervision; meanwhile, each branch leverages location pseudo-labels generated from the CAS of the other branch as localization supervision. For optimization, we alternately freeze one branch and train the other branch. In this process, the localization results of each branch are taken as the localization objective of the other branch, so that the complementary action regions of the two branches are combined to make the localization supervision more complete and precise.</p><p>To optimize the whole framework, we apply multiple iterations, since one single iteration brings limited improvement to excavate less discriminative action regions. In each iteration, the adaptive sampler differentiates the inputs of the two branches, so that they purposefully focus on different action regions. Then, mutual location supervision obtains more complete location supervision by pushing the CASs of the two branches to be consistent. In the next iteration, the consistent CASs in turn force the adaptive sampler to further update the inputs of the supplementary branch, thereby exploring more missing action regions. Consequently, the adaptive sampler and mutual location supervision jointly contribute to more complete results in the progressive iterations.</p><p>In summary, our contributions are as follows. 1) We introduce an adaptive mutual supervision framework (AMS) for weakly-supervised temporal action localization. AMS contains a base branch and a supplementary branch, both of which generate location pseudo-labels for progressive iterative refinement. 2) We design a novel adaptive sampler, which encourages the supplementary branch to further detect the underesti-mated action regions of the base branch, thus making the localization results more complete. 3) We propose a novel mutual location supervision, which forces each branch to use location pseudo-labels obtained from the other branch, promoting mutual enhancement. 4) We verify the effectiveness of AMS on two widely used benchmarks, THUMOS14 <ref type="bibr" target="#b18">[19]</ref> and ActivityNet1.2 <ref type="bibr" target="#b19">[20]</ref>. Our AMS method outperforms previous state-of-the-art methods, both quantitatively and qualitatively. We organize the rest of this paper as follows. For a better understanding of our motivation on adaptive mutual supervision, we review the related work in Section II. In Section III, we propose the novel strategies about adaptive sampler and mutual location supervision. Section IV validates the proposed method by comparing it with existing methods. We further perform extensive ablation studies to reveal the effectiveness of each component. Finally, the conclusion is drawn in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>This section reviews previous works that motivate the proposed method. We can divide those works into three groups: strongly-supervised temporal action localization, weaklysupervised temporal action localization, and adaptive sampling strategy. We next review the previous works respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Strongly-Supervised Temporal Action Localization</head><p>Strongly-supervised temporal action localization relies on precise boundary labels and action category labels to localize action instances. The popular solutions can be summarized as the top-down framework and the bottom-up framework. The top-down framework <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b20">[21]</ref>- <ref type="bibr" target="#b28">[29]</ref> first pre-defines massive anchors according to the prior knowledge of action distribution; then adopts fixed-length sliding windows to generate initial proposals; finally utilizes a boundary adjustment module to refine results. Typically, TURN <ref type="bibr" target="#b4">[5]</ref> and TAL-Net <ref type="bibr" target="#b0">[1]</ref> explored the effect of contextual information and the dilated temporal convolution on localization performance, respectively. The bottom-up framework <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b33">[34]</ref> first predicts actionness or boundary probabilities for all video snippets, then groups the obtained start points and end points to produce proposals. Typically, BMN <ref type="bibr" target="#b2">[3]</ref> listed all possible proposal groups, and ranked each proposal by evaluating the IoU between the proposal and the ground truth. BUMR <ref type="bibr" target="#b29">[30]</ref> proposed to construct constraints between the action, start, and end curves to reduce invalid proposal groups. BC-GNN <ref type="bibr" target="#b30">[31]</ref> introduced graph convolution operations <ref type="bibr" target="#b34">[35]</ref> to group the most suitable start and end points. G-TAD <ref type="bibr" target="#b31">[32]</ref> modeled each video as a graph based on temporal and semantic relationships to enhance the continuity between snippet features. In general, the top-down framework completely discovers most action instances with few omissions, while the bottom-up framework flexibly adjusts the boundary and produces more precise predictions. For better performance, CTAP <ref type="bibr" target="#b35">[36]</ref>, MGG <ref type="bibr" target="#b36">[37]</ref>, AFNet <ref type="bibr" target="#b37">[38]</ref>, and A2Net <ref type="bibr" target="#b38">[39]</ref> further designed four fusion methods to combine these two frameworks in a supplementary manner. PBRNet <ref type="bibr" target="#b39">[40]</ref> proposed a coarse-to-fine strategy to progressively refine boundaries in an end-to-end fashion. To better model the relationship between action proposals, P-GCN <ref type="bibr" target="#b40">[41]</ref> and RAM <ref type="bibr" target="#b41">[42]</ref> utilized the graph convolution and self-attention mechanism to construct non-local networks for better feature embedding, respectively.</p><p>However, all these strongly-supervised methods demand precise action location annotations, which is time-consuming and less practical in real-world scenarios. To reduce the annotation cost, this work aims to study the same problem in the weakly-supervised setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Weakly-Supervised Temporal Action Localization</head><p>Weakly-supervised temporal action localization (WTAL) only requires video-level action category labels for training. Inspired by Class Activation Map <ref type="bibr" target="#b42">[43]</ref> in object detection, early methods <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b43">[44]</ref> usually adopted the classification-based framework. That is, use the video-level category labels to train an action classifier; calculate Class Activation Sequence (CAS) based on the parameters of the classifier; post-process CAS for final action proposals.</p><p>Due to the objective difference between classification and localization, the CAS generated from the classifier has a serious incompleteness issue: it only contains local and sparse action regions. To solve this issue, SSE <ref type="bibr" target="#b44">[45]</ref>, CPMN <ref type="bibr" target="#b13">[14]</ref>, WO <ref type="bibr" target="#b12">[13]</ref>, and A2CL-PT <ref type="bibr" target="#b45">[46]</ref> introduced the erase strategies, which cascade multiple classifiers, erase the most discriminative regions detected by the previous classifier in turn, and input the remaining regions for the latter classifier, thus gradually detecting less discriminative regions. CMCS <ref type="bibr" target="#b14">[15]</ref> trained multiple classifiers in parallel, which are used to detect different action regions. Since the CAS also has some falsepositive activation and action-context confusion, BM <ref type="bibr" target="#b46">[47]</ref> and BaSNet <ref type="bibr" target="#b8">[9]</ref> proposed two background modeling methods. DGAM <ref type="bibr" target="#b10">[11]</ref> explored separating context and action via Conditional Variational Auto-Encoder. In terms of post-processing CAS, Autoloc <ref type="bibr" target="#b11">[12]</ref> and KT-MGFN <ref type="bibr" target="#b47">[48]</ref> designed the outerinner contrastive loss to replace the simple threshold operation. CleanNet <ref type="bibr" target="#b48">[49]</ref> further proposed the action proposal evaluator for an effective boundary adjustment.</p><p>Considering that all the above methods localize with only classification supervision, some recent studies <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b17">[18]</ref> introduced the self-training-based framework, to provide explicit localization supervision for better localization. Its common practice is to empirically set thresholds on the CAS of the current step, and generate pseudo-labels as the location supervision of the next step; then perform several iterations to directly optimize the localization objective and progressively refine the pseudo-labels. Specifically, Refineloc <ref type="bibr" target="#b15">[16]</ref> was the first to introduce location pseudo-labels for the WTAL task, and explored various pseudo-label generation strategies. EM-MIL <ref type="bibr" target="#b17">[18]</ref> utilized class-specific CAS and class-agnostic attention as pseudo-labels, then formulated the WTAL task as an expectation-maximization problem for optimization. TSCN <ref type="bibr" target="#b16">[17]</ref> predicted pseudo-labels based on RGB and flow data respectively, then late fused these two pseudo-labels to alleviate false-positive results. Overall, our AMS framework differs from these studies from the following two aspects: (i) We design an adaptive sampler to encourage the supplementary branch to further detect the underestimated action regions of the base branch, which effectively completes the localization results of the whole framework; while the previous studies ignore the incompleteness of CAS, and only rely on lowquality pseudo-labels for iterative refinement; (ii) we construct mutual location supervision between two branches; while the previous studies use a single branch, whose generated location pseudo-labels provide self supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Adaptive Sampling Strategy</head><p>The role of the adaptive sampling strategy is to enlarge the local area of the image or video, thus forcing the model to focus more on some specific details. It has wide applications in fine-grained recognition, image retargeting, and small object detection. Concretely, in fine-grained image recognition, SSampler <ref type="bibr" target="#b49">[50]</ref> proposed to sample based on saliency maps for data augmentation. S3N <ref type="bibr" target="#b50">[51]</ref> leveraged class response maps as guidelines for sampling, and achieved considerable improvements. For faster action recognition, SCSampler <ref type="bibr" target="#b51">[52]</ref> selected a small subset of salient snippets to replace the entire video through a lightweight sampler. For better video representation, based on the importance of video frames, Coarse-Fine <ref type="bibr" target="#b52">[53]</ref> performs dynamic sampling to form different abstractions of time resolution. In image retargeting, EBID <ref type="bibr" target="#b53">[54]</ref> and NCV <ref type="bibr" target="#b54">[55]</ref> used the adaptive sampling strategy to formulate the task as the energy minimization and finite element problem. Inspired by the above studies, this paper adopts a novel adaptive sampler to differentiate the inputs of the two branches, so that they can localize different action regions. To the best of our knowledge, this is the first attempt to introduce the adaptive sampling strategy to the WTAL task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. ADAPTIVE MUTUAL SUPERVISION</head><p>In this section, we propose the adaptive mutual supervision framework (AMS); see the framework pipeline in <ref type="figure" target="#fig_0">Fig. 2</ref>. We first the formulate weakly-supervised temporal action localization problem; then, present the adaptive sampler and the mutual location supervision strategy; and finally, we introduce the training and testing details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem Formulation</head><p>Suppose that we are given N untrimmed videos {v i } N i=1 and their corresponding video-level category labels {y i } N i=1 , where y i is a C-dimensional binary vector (C is the total number of action categories), with y k i =1 if the i-th video contains the kth action category, and y k i =0 otherwise. Note that each video may contain multiple action categories and multiple action instances. Our goal is to predict the temporal locations of these action categories in the video, in terms of a set of quadruples {(s, e, c, p)}, where s, e, c, p represent the start time, the end time, the action category and the localization score of the action proposal, respectively.</p><p>Following recent methods <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b14">[15]</ref>, for each video, we sample T consecutive snippets to make sure all videos have the same length. Then, we adopt a pre-trained feature extractor to obtain the original feature F orig ∈ R T ×D for each video, where D is the feature dimension of each snippet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Base Branch</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Branch</head><p>Adaptive Sampler Then, based on the CAS of the base branch, the adaptive sampler selects the less discriminative snippet features as the inputs for the supplementary branch, which forces the supplementary branch to further detect the underestimated action regions of the base branch. Finally, each branch generates location pseudo-labels from CAS, and provides mutual location supervision during multiple training iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Overall Framework</head><p>The proposed AMS framework contains the base branch and the supplementary branch with identical backbones. Each backbone predicts the video category probability and respective CAS. The base branch is fed the original video feature to localize the most discriminative action regions from CAS. To strengthen collaboration and complementarity between the two branches so that the whole framework contains more complete action information, we encourage the supplementary branch to localizes the less discriminative action regions through a novel adaptive sampler. The sampler adaptively updates the inputs of the supplementary branch with a sampling weight sequence negatively correlated with the CAS of the base branch, i.e., over-sample in the low CAS regions while under-sample in the high CAS regions. Hence, the supplementary branch is encouraged to further excavate the action regions underestimated by the base branch, making the localization results more complete. To further promote mutual enhancement and explicitly optimize the localization objective, we construct mutual location supervision between the two branches. Each branch uses location pseudo-labels generated from the CAS of the other branch as localization supervision. We alternately freeze one branch and optimize the other branch, so that the complementary action information of the two branches can be combined to make the location supervision more complete and precise. In multiple iterations, our AMS framework progressively refines the localization results.</p><p>In either the base or supplementary branch, we input video features into a backbone network h(·) to predict the category probability and CAS. The backbone network is implemented by a multi-layer perceptron. For the base branch, its backbone network can be formalized as follows:</p><formula xml:id="formula_0">M base , y base = h base (F orig , φ base ),<label>(1)</label></formula><p>where M base ∈ R T ×C denotes CAS, indicating the probability distribution of each video snippet belonging to all action categories. y base ∈ R C means the predicted video category probability. φ base is trainable parameters of the backbone network. In our framework, the input of the base branch is the original feature F orig . While the input of the supplementary branch is selected based on the output CAS of the base branch through the adaptive sampler S(·, ·); that is,</p><formula xml:id="formula_1">F supp = S(F orig , M base ),<label>(2)</label></formula><p>where F supp ∈ R T ×D . The design details of the sampler are introduced in Section III-C. Formally, the backbone network of the supplementary branch is formalized as follows:</p><formula xml:id="formula_2">M supp , y supp = h supp (F supp , φ supp ),<label>(3)</label></formula><p>where y supp ∈ R C means the predicted category probability, φ supp is the trainable parameters, the output CASM supp ∈ R T ×C . To align the temporal distribution with the CAS of the base branch, we also perform temporal alignment onM supp and obtain M supp ; see details in Section III-C3.</p><p>C. Adaptive Sampler 1) Sampling Weight Sequence: Here we propose the implementation details of the adaptive sampler S(·, ·). To solve the incompleteness issue of CAS generated by the classifier, we desire to strengthen complementarity between the two branches. To achieve this, we design a novel adaptive sampler to differentiate the inputs of the two branches. Specifically, we input the original video features into the base branch, while dynamically select snippet features for the supplementary branch via the adaptive sampler. The whole process is divided into two parts as shown in <ref type="figure" target="#fig_1">Fig. 3</ref>.</p><p>Similar to previous studies <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>, the CAS of the base branch tends to focus on the most discriminative action First cumulate w to get its cumulative distribution function G(t); then uniformly sample T points on the cumulation axis, and map them to G(t); finally, map the points on G(t) to the time axis, and obtain the sampling timestamp set K. Sample the interpolated original features F orig to generate F supp for the supplementary branch.</p><p>regions. To make the supplementary branch purposefully localize the less discriminative action regions underestimated by the base branch, we need to calculate the class-agnostic sampling weight sequence, based on the CAS of the base branch. Since CAS is a class-specific action probability sequence, to generate the class-agnostic sequence, it is necessary to aggregate all action information of category channels in CAS. Specifically, given the CAS of the base branch M base ∈ R T ×C , we only keep the channels of ground truth categories, then perform the maximum operation on these channels in the temporal dimension. We also empirically compare the maximum, average, and random operations in <ref type="table" target="#tab_5">Table V</ref>. Formally, the aggregated CAS is denoted as m = {m t } ∈ R T .</p><p>According to the meaning of CAS <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>, the aggregated CAS m indicates the action confidences of all snippets in the temporal dimension. In other words, a higher value of m t indicates a higher confidence of existing an action at the t-th snippet. Considering the incompleteness issue of CAS, there might exist some missing actions (less discriminative actions) in the low-value regions of the aggregated CAS. Therefore, we aim to make the supplementary branch focus more on the lowvalue regions while less on the high-value regions. And the sampling weight sequence is hence designed to be negatively correlated with the aggregated CAS:</p><formula xml:id="formula_3">w = {w t } = max(m) − m + η ∈ R T ,<label>(4)</label></formula><p>where η means a sampling adjustment value, max(·) is the maximum operations. Each element in the sampling weight sequence w represents the probability that the corresponding snippet will be selected by the sampling operation. A lower value of the aggregated CAS m t corresponds to a higher probability of w t , which indicates that the t-th snippet with lower action confidence determined by the base branch is more likely to be sampled. With such a sampling weight sequence, we can naturally over-sample the snippets in the low-value regions of the aggregated CAS while under-sample in the corresponding high-value regions.</p><p>2) Sampling Operation: In this section, we perform the sampling operation to generate the inputs for the supplementary branch. Since the original features only contain T snippets, to achieve more fine-grained sampling in the temporal dimension, we first up-sample them by linear interpolation, and then, based on the sampling weight sequence, adaptively select T snippet features from the interpolated original features to form the inputs for the supplementary branch. The interpolated original features are denoted as F orig ∈ R HT ×D , where H is the interpolation factor. To match the temporal length, we also calculate the interpolated sampling weight sequence, which is denoted as w ∈ R HT .</p><p>Next, we detail the sampling timestamps and the sampling features in turn, as shown in <ref type="figure" target="#fig_1">Fig. 3 (b)</ref>. Sampling Timestamps. Following the inverse transformation theory <ref type="bibr" target="#b55">[56]</ref>, we adopt a cumulation-mapping manner to adaptively select T snippet features. Concretely, regarding the interpolated weight sequence w as a probability mass function, we first cumulate it along the temporal dimension to obtain the cumulative distribution function G(t):</p><formula xml:id="formula_4">G(t) = t τ =1 w τ dτ.<label>(5)</label></formula><p>Intuitively, G(t) corresponds to the black curve in <ref type="figure" target="#fig_1">Fig. 3 (b)</ref>. Its role is to map the sampling probability of HT snippets uniformly distributed in the temporal dimension, in proportion to the interval length of the cumulation axis. A larger sampling probability w t indicates a longer interval on the cumulation axis. Hence, sampling T timestamps on the time axis based on the interpolated weight sequence w is equivalent to uniformly sample T points on the cumulation axis. To achieve the sampling goal, we first uniformly sample T points on the cumulation axis; then, map these points to the cumulative distribution function G(t); next, we map the points on G(t) to the time axis; and finally, the corresponding T timestamps on the time axis constitute a candidate sampling timestamp set, denoted as K. Sampling Features. Afterward, based on the sampling timestamp set K, we obtain the input features of the supplementary branch from the interpolated original features F orig :</p><formula xml:id="formula_5">F supp = K F orig ∈ R T ×D ,<label>(6)</label></formula><p>where denotes the index operation, that is, chronologically take out the features corresponding to T sampling timestamps. Accordingly, F supp is mainly composed of the features corresponding to the low-CAS value regions of the base branch. Intuitively, the efficacy of adaptive sampler can be interpreted as slowing down the video in the low-CAS regions of the base branch while speeding up the video in the high-CAS regions. The supplementary branch is thus prompted to focus more on less discriminative regions, and further excavates missing actions to complete localization results.</p><p>3) Temporal Alignment: Since the input features of the supplementary branch are under-sampled or over-sampled in the temporal dimension, they have a non-uniform temporal distribution. Hence, the temporal distribution of the corresponding CAS also becomes non-uniform, which does not match the uniform distribution of the CAS from the base branch. To ensure the localization results of the two branches have the same temporal distribution, we need to make temporal alignment between these two CASs.</p><p>For this purpose, we uniformize the temporal distribution for the CAS of the supplementary branch. Concretely, for each timestamp in the uniform distribution, we search the two nearest timestamps on the CAS of the supplementary branch. After that, we can obtain the temporal alignment results by performing linear interpolation between these two CAS values. We denote the temporal alignment operation as A, hence the above process is formalized as:</p><formula xml:id="formula_6">M supp = A(M supp ) ∈ R T ×C ,<label>(7)</label></formula><p>whereM supp is the output CAS of the backbone network from the supplementary branch, M supp is its aligned CAS. 4) Discussion: Note that in the proposed adaptive sampler, if the sampling weights of high CAS regions are fixed at 0%, and the others are 100%, our sampling strategy will become similar to the existing erase operation <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>. The proposed adaptive sampler outperforms the erase operation from two aspects. (i) Our sampling probability is soft, while the erase probability is binary. This means that we always retain some action regions determined by the base branch, which act as action anchors for the supplementary branch to avoid paying attention to the background; while the erase operation removes all action regions found previously, usually misleading the model to the background. (ii) Our sampling strategy slows down the less discriminative action regions and speeds up the most discriminative regions, while the erase operation only erases the most discriminative action regions. This means that the inputs generated by our strategy are more fine-grained in the temporal dimension and more purposeful for less discriminative actions, which can lead to better localization results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Mutual Location Supervision</head><p>To promote mutual enhancement and explicitly optimize the localization objective, we further construct mutual location supervision between the base branch and the supplementary branch. Different from the self-training-based strategy <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b17">[18]</ref>, we force the two branches to provide location pseudolabels for each other, and progressively refine the localization results in multiple iterations.</p><p>1) Location Pseudo-labels: After making temporal alignment for the two branches, we encourage them to generate location pseudo-labels from CAS. Since CAS stands for the action confidence probability, a higher value of CAS means a higher confidence of existing an action. To avoid low-quality labels and eliminate uncertainty, we threshold CAS through a hyperparameter α to generate binary pseudo-labels, i.e., if the CAS value of a snippet for any ground truth category is greater than α, the snippet is regarded as a positive action example; otherwise, it is considered as a negative action example. The location pseudo-labels can be formulated as:</p><formula xml:id="formula_7">m k t = 1, if m k t &gt; α and y k = 1, 0, otherwise,<label>(8)</label></formula><p>where m k t is the CAS value of the t-th snippet and the k-th category channel, m k t is the corresponding pseudo-label. 2) Optimization Process: To promote mutual enhancement between these two branches, we construct mutual location supervision, that is, force each branch to leverage the location pseudo-labels from the other branch as the localization objective. The optimization process is achieved by alternately freezing one branch and training the other branch. Specifically, in Phase zero, we train the base branch with only video-level category labels, to generate initial location pseudo-labels. In Phase one, we freeze the base branch, and produce the inputs for the supplementary branch through the adaptive sampler; then optimize the supplementary branch with category labels and location pseudo-labels from the base branch; finally, update pseudo-labels based on the CAS from the supplementary branch. And in Phase two, we optimize the base branch with category labels and the updated location pseudo-labels from the supplementary branch.</p><p>To optimize the whole framework, we apply multiple iterations, since one single iteration brings limited improvement to mine less discriminative action regions. In each iteration, the adaptive sampler differentiates the inputs of the two branches, so that they purposefully focus on different action regions. Then, the mutual location supervision obtains more complete location supervision by pushing the CASs of the two branches to be consistent. In the next iteration, the consistent CASs force the adaptive sampler to further update the inputs of the supplementary branch in turn, so that more missing action regions can be explored. Consequently, the adaptive sampler and mutual location supervision jointly contribute to more complete results in progressive iterations.</p><p>3) Loss Function: For localization, following <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b29">[30]</ref>, we calculate the weighted cross-entropy loss between location pseudo-labels and the output CAS:</p><formula xml:id="formula_8">L local = 1 C C k=1 ( 1 T + t∈Λ + H(m k t , m k t ) + 1 T − t∈Λ − H(m k t , m k t ))<label>(9)</label></formula><p>where C is the number of action categories, m k t ∈ [0, 1] is the output CAS of the t-th snippet and the k-th category channel, m k t ∈ {0, 1} is the location pseudo-label from the other branch, H is the regular cross-entropy loss, Λ + and Λ − denote the positive and negative sample sets, T + and T − are the number of positive and negative samples.</p><p>For classification, we calculate the cross-entropy loss between the action category label y = [y 1 , ..., y C ] T and the predicted category probability y ∈ R C :</p><formula xml:id="formula_9">L class = 1 C C k=1 H( y k , y k ),<label>(10)</label></formula><p>where y is calculated by aggregating CAS with top-k mean technique <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b56">[57]</ref>. For better classification, we combine the classification loss L class with the Co-Activity Similarity loss in WTALC <ref type="bibr" target="#b9">[10]</ref>, as the basic loss L basic of each branch. Finally, during the training of the whole framework, we combine the basic loss and the localization loss to optimize the base branch or the supplementary branch:</p><formula xml:id="formula_10">L total = L basic + λL local ,<label>(11)</label></formula><p>where λ denotes a balance hyperparameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Inference</head><p>The AMS framework is separately optimized with RGB and flow features, and the final localization results are generated in a late-fusion fashion. During inference, for an input video, we fuse the two CASs from RGB and flow modes, then average the CASs of the two branches as the final predicted CAS M final ∈ R T ×C . The process is given by:</p><formula xml:id="formula_11">M final = 1 2 (M base flow + M supp flow + βM base rgb + βM supp rgb ),<label>(12)</label></formula><p>where β is a fusion hyperparameter. After that, we aggregate M final to derive the video-level action category probabilities. For classification, we only select the classes whose category probabilities are above the classification threshold θ cls . For the remaining categories, we directly threshold M final with the localization threshold θ loc , then concatenate consecutive candidate snippets as action proposals {(s j , e j , c j , p j )} o j=1 , where o is the number of proposals, s j , e j , c j , p j represent the start time, the end time, the action category, and the localization score of the j-th action proposal, respectively. The action category c j of the j-th proposal is the action category of the corresponding video. And the localization score p j of the j-th proposal is calculated by the maximum value of M final within the proposal interval [s j , e j ].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head><p>In this section, we conduct extensive experiments to evaluate the effectiveness of our AMS method and reveal the effect of each component. We first detail the experimental settings and network architectures, then report the corresponding experimental results. Based on two widely used datasets, i.e., THUMOS14 <ref type="bibr" target="#b18">[19]</ref> and ActivityNet <ref type="bibr" target="#b19">[20]</ref>, our AMS method significantly outperforms previous state-of-the-art methods both quantitatively and qualitatively. Besides that, both adaptive sampler and mutual location supervision strategy have a great effect on the localization performance.</p><p>A. Dataset and Evaluation THUMOS14 <ref type="bibr" target="#b18">[19]</ref>. There are 413 untrimmed videos in 20 categories, and each video contains an average of 15 action instances. The model is trained on 200 validation videos, and evaluated on 213 test videos. This dataset is widely used and challenging, because the video lengths vary widely and the actions occur very frequently. ActivityNet1.2 <ref type="bibr" target="#b19">[20]</ref>. The dataset contains 9682 videos belonging to 100 categories, which are divided into 4619 videos for training, 2383 videos for validation, and 2480 videos for testing. Since the ground-truth action intervals of test videos are not available, we train the model on the training set and evaluate on the validation set. Almost all videos contain only a single action category, and action regions take up more than half of the duration in most videos. Evaluation Metrics. Following the convention, we evaluate our method with the standard mean Average Precision (mAP) at different thresholds of temporal intersection over union (T-IoU). Note that a proposal is regarded as positive only if both the predicted category is correct and T-IoU exceeds the set threshold. Besides, each ground-truth action instance can only match one action proposal. The mAP is calculated from the evaluation code provided by the corresponding datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>Feature Extraction. Following previous methods <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b14">[15]</ref>, to reduce the computational requirements, we extract the highlevel features of the input video in advance, and then train the whole framework with these high-level features. Due to the memory constraint, we first split the input video into non-overlapping 16-frame snippets, then randomly sample T consecutive snippets from each video, since the video lengths vary greatly. T is set to 1000 on THUMOS14, and 400 on ActivityNet1.2. We leverage the TV-L1 algorithm <ref type="bibr" target="#b61">[62]</ref> to extract optical flow from RGB data; next, we use the two-stream I3D architecture <ref type="bibr" target="#b62">[63]</ref> pre-trained on the Kinetics dataset <ref type="bibr" target="#b62">[63]</ref> to extract RGB and flow features; and finally, we can obtain the 1024-dimensional feature from RGB data or flow data of each video snippet. Backbone Network. In either the base branch or the supplementary branch, the backbone network maps video features to CAS and video category probabilities. Structurally, it cascades a feature transformation module and a mapping module. The former consists of a fully connected layer, followed by ReLU activation and Dropout to fine-tune the video features from the feature extractor. The latter contains two parallel fully connected layers, followed by the softmax function respectively, to predict video category probabilities and CAS. Implementation. The proposed framework is implemented with Pytorch <ref type="bibr" target="#b63">[64]</ref>, using Adam optimizer <ref type="bibr" target="#b64">[65]</ref> with the learning rate of 10 −4 to respectively optimize on THUMOS14 and ActivityNet1.2. For a fair comparison, we fix the pretrained parameters of the feature extractor without fine-tuning. We train the framework for 20 epochs in Phase zero, then alternately train the two branches every 5 epochs in Phase one and Phase two. All hyperparameters are determined by grid search: the balance hyperparameter λ = 1.0, the fusion hyperparameter β = 0.15, the sampling adjustment value η = 0.75, the interpolation factor H = 20, the classification threshold θ cls = 0.25. The threshold α for generating location pseudo-labels is set equal to the localization threshold θ loc , which is calculated adaptively by 0.7 × avg(M).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison with State-of-the-Art Methods</head><p>Under multiple IoU thresholds, we compare the proposed AMS method with existing methods from various levels of I: Comparison with the state-of-the-art methods on THUMOS14. AVG(0.1-0.5) and AVG(0.3-0.7) are the average mAP from IoU 0.1 to 0.5 and from IoU 0.3 to 0.7, respectively. 'Single' denotes training with one single iteration. 'Multiple' denotes training with multiple iterations. In general, the performance of the multiple iteration methods is better than that of the single iteration methods. The proposed AMS method outperforms the state-of-the-art methods in the video-level weakly-supervised setting, while performs comparably with several strongly-supervised methods.  supervision settings. As introduced in Section I, existing weakly-supervised temporal action localization methods can be divided into two categories: classification-based framework and self-training-based framework. The main difference is that the latter requires multiple iterations for training compared to the former. We thus abbreviate these two frameworks as the 'single' and 'multiple' groups. Since our AMS method also uses pseudo-labels for multiple iterations, we put it into the 'multiple' group for a fair comparison. <ref type="table">Table I</ref> reports the comparison on THUMOS14. In terms of performance, the multiple iteration methods are generally better than the single iteration methods. And our AMS method achieves a new state-of-the-art in the video-level weaklysupervised setting. Compared to previous multiple iteration methods, i.e., <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b17">[18]</ref>, our method obtains a performance gain of more than 4.9% in terms of the average mAP from IoU 0.1 to 0.5, while more than 2.0% in terms of the average mAP from IoU 0.3 to 0.7. This proves the effectiveness of our proposed framework, and means that our AMS method produces more precise and complete localization. Moreover, despite being trained in the weakly supervised setting, our method performs comparably with several early stronglysupervised methods <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref>.</p><p>Table II also reports the results on ActivityNet1.2. Generally speaking, our AMS method surpasses most of previous methods under the same level of supervision. In terms of the average mAP, our method outperforms all existing weaklysupervised methods, and follows strongly-supervised SSN <ref type="bibr" target="#b6">[7]</ref> with the least gap. Note that, compared with THUMOS14, ActivityNet has only one-tenth of action instances per video TABLE III: Contribution of three components, i.e., the branch number, the adaptive sampler, and location supervision on THUMOS14. 'Single' means training with one single iteration. 'Multiple' means training with multiple iterations. AVG(0.1-0.7) denotes the average mAP from IoU 0.1 to 0.7. There are two ways to provide location supervision: mutual location supervision and self-training-based location supervision <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, which are abbreviated as 'mutual' and 'self', respectively. Both the adaptive sampler and mutual location supervision have great effects on the localization performance. on average, and almost all videos contain only one action category. Therefore, this dataset has a lower localization requirement, just as emphasized in <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b59">[60]</ref>. This lower requirement might lead to the small gain of our method to some extent. All in all, the great performance on the two datasets indicates the effectiveness of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Studies</head><p>Here we conduct ablation studies on THUMOS14 to analyze the effect of each component in our framework, i.e., the branch number, mutual location supervision, and the adaptive sampler. Recently, the self-training-based strategy <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b17">[18]</ref> has been proposed to explicitly optimize the localization objective. It adopts the pseudo-labels generated by CAS in the current step as the location supervision for the next step, and relies on the self-training strategy to iteratively refine the localization results. For a detailed comparison, we also reproduce this method under the same settings.</p><p>Specifically, we experiment with the following six setups. The baseline is set as a vanilla classification-based method <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref>, which is trained with only the basic loss L basic , then thresholds CAS for localization results. For the above setups, as the two branches could generate quite different CASs, we average their CASs as the final output CAS. <ref type="table" target="#tab_2">Table III</ref> reports all the localization results. 1) One branch v.s. Dual branch: In the proposed AMS framework, the dual-branch setting causes the model parameters to be doubled. We thus explore the effect of the branch number on the localization performance. Comparing (B) to (A), we can find that there is no significant difference between the localization performance of the dual-branch model and the one-branch model. The only 0.2% average mAP improvement suggests that simply doubling the model parameters cannot bring a significant increase in performance. Besides, (A) and (B) perform worst among these six setups. This is because relying only on classification supervision can mislead the model to focus on the most discriminative regions, resulting in sparse and incomplete localization results.</p><p>2) Self-training-based strategy v.s. Mutual location supervision: To evaluate the effect of location pseudo-labels, we add either mutual location supervision or self-training-based strategy to (B), and perform multiple training iterations. Comparing (D) or (E) to (B), we see that multiple iterations with location supervision improve the performance by 3.4% average mAP. This reflects that explicitly optimizing the localization objective can reconcile the contradiction between classification and localization in the WTAL task.</p><p>Besides, comparing (E) to (D), our proposed mutual location supervision outperforms the self-training-based strategy by 0.5% average mAP. We conjecture the reason as follows. During multiple iterations, the self-training-based strategy separately trains the two branches. While in each iteration, our mutual location supervision promotes mutual enhancement between the two branches, and combines the action information from the two branches, which plays the role of ensemble learning <ref type="bibr" target="#b65">[66]</ref>- <ref type="bibr" target="#b67">[68]</ref> to bring a certain improvement.</p><p>3) Effectiveness of the adaptive sampler: We also add the adaptive sampler on (B) and (E), to verify the effectiveness of the proposed sampling strategy. Comparing (C) to (B), the adaptive sampler purposefully differentiates the inputs of the two branches, and brings a gain of 3.1% average mAP. By adaptively selecting less discriminative snippets for the supplementary branch, the sampler promotes it to explore the actions underestimated by the base branch, thus completing the localization results of the whole framework.</p><p>Moreover, comparing (F) to (E), the adaptive sampler further boosts the effectiveness of mutual location supervision in multiple iterations, bringing a gain of 3.7% average mAP. By integrating the adaptive sampler and mutual location supervision to form our AMS framework, we achieve the best performance with large gaps from the others, which indicates that both components play essential roles and jointly contribute to more complete action localization results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Validation and Analysis Experiments</head><p>1) Effect of the adaptive sampling weights: As designed in Section III-C1, we design the sampling weight sequence to be negatively correlated with the CAS of the base branch. To verify the effectiveness of this strategy, we implement three experiments in <ref type="table" target="#tab_4">Table IV</ref>   results show that the adaptive sampling strategy significantly surpasses the other two, boosting the performance to more than 3.4% average mAP. As is evident, the adaptive sampling strategy effectively differentiates the two branches, prompting the supplementary branch to complement the detection results of the base branch. Moreover, (B) outperforms (A) by 0.8% average mAP. We speculate this is because random sampling acts as a role of data augmentation, which enlarges the amount of training data, thus bringing a certain gain.</p><p>2) Effect of the progressive iterative refinement: <ref type="figure" target="#fig_4">Fig. 4</ref> quantifies the results of four iterations to evaluate the effect of progressive refinement. The best performance is obtained in the third iteration, which gains 8.2% mAP compared to the initial iteration. Such a huge gain shows that in progressive iterations, the quality of pseudo-labels is continuously improving through mutual enhancement between the two branches. And the adaptive sampler and mutual location supervision can collaborate and promote each other. In essence, the mutual enhancement is a voting ensemble of the localization results from the two branches, which can provide more complete and precise supervision, compared to each individual branch. When we combine the location information of the two branches, the localization errors that only exist in one branch are largely ruled out, thus avoiding error propagation.</p><p>Moreover, we notice that the gain of a single iteration is decreasing until it becomes zero. In the initial iteration, the location pseudo-labels are low-quality, or even none. While after three iterations, the quality of pseudo-labels tends to be high and stable, the performance reaches the main bound.</p><p>3) Effect of the aggregation operations: As described in Section III-C1, we aggregate all action information of category  channels in CAS through the average operation. There are two other aggregation operations available, that is, the maximum operation and the operation randomly selecting a ground-truth category channel. <ref type="table" target="#tab_5">Table V</ref> summarizes the comparison. There is no significant performance difference between the average operation and the maximum operation. And the results of these two operations are superior to the random operation. The reason may be that the random operation also provides the supplementary branch with some action regions that have been found by the base branch. This increases the overlap between the actions discovered by the supplementary branch and the base branch, thus damaging the performance. 4) Effect of the sampling adjustment value η: In Eq. 4, the sampling adjustment value η is designed to adjust the sampling weight sequence. <ref type="figure" target="#fig_5">Fig. 5</ref> demonstrates the effect, where η varies from 0 to 2 with an interval of 0.25. We find that it shows a clear trend to peak at 0.75. And there are slight differences in performance when using values from 0.5 to 1. However, when we continue to increase η to 2, the performance drops a lot. This is because in this case, the sampling weights of different video snippets tend to be the same, causing the adaptive sampling to degenerate to the uniform sampling. Accordingly, the inputs of the two branches become substantially identical, and our AMS framework degenerates into the straightforward mutual location supervision model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Qualitative Results</head><p>To qualitatively demonstrate the superiority of the proposed framework, we visualize several examples in <ref type="figure" target="#fig_6">Fig. 6</ref>. For clear understanding, we provide the inputs, the CAS, and the localization results of the base branch and the supplementary branch in turn. Generally speaking, whether for videos containing sparse or dense action instances, the localization results of our framework are relatively complete and precise. More specifically, the base branch, which is fed with videos of uniformly temporal distribution, can only detect the most discriminative action regions. Therefore, its corresponding results are sparse and trivial. Relying on the adaptive sampler, we select the uncertain video snippets of the base branch as inputs for the supplementary branch. The videos of non-uniformly temporal distribution, force the supplementary branch to purposefully complement the less discriminative actions underestimated by the base branch, but also causes some false-positive background predictions. On this basis, mutual location supervision promotes the mutual enhancement between the two branches, which combines their localization results for more complete and precise final prediction results. The good qualitative results again prove the effectiveness of our proposed framework.</p><p>V. CONCLUSION In this work, to solve the incompleteness issue of CAS in WTAL, we propose an adaptive mutual supervision framework (AMS) with two branches. The base branch leverages CAS to localize the most discriminative action regions, and the supplementary branch localizes the less discriminative action regions through a novel adaptive sampler. The adaptive sampler dynamically updates the input of the supplementary branch with a sampling weight sequence negatively correlated with the CAS from the base branch, thus prompting the supplementary branch to localize the action regions underestimated by the base branch. To promote mutual enhancement between the two branches, we construct mutual location supervision. Each branch uses location pseudo-labels generated from the other branch as localization supervision. By alternately optimizing the two branches in multiple iterations, we progressively localize more complete action regions. Experiments on two benchmarks demonstrated the effectiveness and outstanding performance of the proposed AMS framework.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Framework pipeline. The pre-trained feature extractor extracts the original video features, and the backbone network predicts video category probabilities and CAS. The base branch is fed with the original video features to localize the most discriminative action regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Illustration of the adaptive sampler. (a): Sampling Weight Sequence. First calculate the sampling weight sequence w negatively correlated with the aggregated CAS m, then up-sample w to get the interpolated weight sequence w. (b): Sampling Operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(A): The baseline with only one branch; (B): The baseline with two identical branches; (C): Add the adaptive sampler on (B); (D): Add the self-training-based strategy on (B); (E): Add the mutual location supervision strategy on (B); (F): Add the adaptive sampler and the mutual location supervision strategy on (B).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>. (A): uniformly generate sample weights; (B): randomly generate sample weights; (C): adaptively generate sample weights by our proposed strategy. The</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Results of progressive iterative refinement on THU-MOS14. The best performance is obtained in the third iteration, and the improvement over four iterations is 8.2% mAP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Effects of the sampling adjustment value η on THU-MOS14. The localization results are reported at IoU threshold 0.5. And the best performance is achieved when η = 0.75.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 :</head><label>6</label><figDesc>Qualitative results on THUMOS14 (Best viewed in color). In each example, there are nine plots. The first three plots are the input video, the CAS, and localization results of the base branch. The middle three plots show the input video, the CAS, and localization results of the supplementary branch. The last three plots are the final CAS, the final localization results of the whole framework, and the ground truth action intervals. The base branch, fed with videos of uniformly temporal distribution, can only detect the most discriminative actions. Through the adaptive sampler, we input videos of non-uniformly temporal distribution to the supplementary branch, hence force it to purposefully complement the less discriminative actions underestimated by the base branch. Mutual location supervision makes our final localization results more complete and precise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II</head><label>II</label><figDesc></figDesc><table><row><cell cols="4">: Comparison with the state-of-the-art methods on</cell></row><row><cell cols="4">ActivityNet1.2. AVG(0.5-0.95) denotes the average mAP at</cell></row><row><cell cols="4">IoU thresholds 0.5:0.05:0.95. 'Single' means training with one</cell></row><row><cell cols="4">single iteration. 'Multiple' means training with multiple iter-</cell></row><row><cell cols="4">ations. The proposed AMS method outperforms all previous</cell></row><row><cell cols="4">methods in terms of the average mAP, while surpasses most</cell></row><row><cell cols="2">methods at some IoU thresholds.</cell><cell></cell><cell></cell></row><row><cell>Supervision (Training)</cell><cell>Method</cell><cell>mAP@IoU 0.5 0.75 0.95</cell><cell>AVG (0.5-0.95)</cell></row><row><cell>Strong</cell><cell>CDC [21] SSN [7]</cell><cell>45.3 26.0 0.2 41.3 27.0 6.1</cell><cell>23.8 26.6</cell></row><row><cell></cell><cell>U-Nets [44]</cell><cell>7.4 3.2 0.7</cell><cell>3.6</cell></row><row><cell></cell><cell>Autoloc [12]</cell><cell>27.3 15.1 3.3</cell><cell>16.0</cell></row><row><cell>Weak Video-level (Single)</cell><cell>TSM [61] WTALC [10] Cleannet [49] CMCS [15]</cell><cell>28.3 17.0 3.5 37.0 12.7 4.5 37.1 20.3 5.0 36.8 22.0 5.6</cell><cell>17.1 18.0 21.6 22.4</cell></row><row><cell></cell><cell>BaSNet [9]</cell><cell>38.5 24.2 5.6</cell><cell>24.3</cell></row><row><cell></cell><cell>DGAM [11]</cell><cell>41.0 23.5 5.3</cell><cell>24.4</cell></row><row><cell>Weak Video-level (Multiple)</cell><cell cols="2">EM-ML [18] RefineLoc [16] 38.0 20.8 4.9 37.4 --TSCN [17] 37.6 23.7 5.7 AMS (Ours) 40.7 23.7 5.8</cell><cell>20.3 22.2 23.6 24.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV :</head><label>IV</label><figDesc>Results of different sampling weight strategies on THUMOS14. AVG(0.1-0.7) means the average mAP from IoU 0.1 to 0.7. Our proposed adaptive sampling is significantly better than random sampling and uniform sampling.</figDesc><table><row><cell>ID</cell><cell></cell><cell cols="2">Sampling weights</cell><cell>0.1</cell><cell cols="2">mAP@IoU 0.3 0.5</cell><cell>0.7</cell><cell>AVG (0.1-0.7)</cell></row><row><cell>(A)</cell><cell></cell><cell cols="2">Uniform</cell><cell>65.3</cell><cell>47.0</cell><cell>29.3</cell><cell>10.7</cell><cell>38.1</cell></row><row><cell>(B)</cell><cell></cell><cell cols="2">Random</cell><cell>66.7</cell><cell>47.8</cell><cell>29.8</cell><cell>11.1</cell><cell>38.9</cell></row><row><cell>(C)</cell><cell></cell><cell cols="2">Adaptive</cell><cell>69.1</cell><cell>52.7</cell><cell>33.1</cell><cell>13.0</cell><cell>42.3</cell></row><row><cell>(%)</cell><cell>32.5</cell><cell></cell><cell></cell><cell></cell><cell cols="2">32.7</cell><cell>33.1</cell><cell>32.8</cell></row><row><cell>0.5</cell><cell>30.5</cell><cell></cell><cell></cell><cell>30.5</cell><cell></cell><cell></cell></row><row><cell>mAP@IoU</cell><cell>26.5 28.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>24.5</cell><cell>0</cell><cell>24.9</cell><cell>1</cell><cell>2</cell><cell></cell><cell>3</cell><cell>4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Iteration</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V :</head><label>V</label><figDesc>Results of three aggregation operations in Section III-C1 on THUMOS14. 'Random' means randomly selecting a ground-truth category channel. Both the 'Maximum' operation and the 'Average' operation bring promising results.</figDesc><table><row><cell cols="3">aggregation</cell><cell></cell><cell cols="2">mAP@IoU</cell><cell></cell><cell>AVG</cell></row><row><cell cols="3">method</cell><cell>0.1</cell><cell>0.3</cell><cell>0.5</cell><cell></cell><cell>0.7</cell><cell>(0.1-0.7)</cell></row><row><cell cols="3">Random</cell><cell>67.5</cell><cell>51.6</cell><cell>32.5</cell><cell cols="2">12.5</cell><cell>41.4</cell></row><row><cell cols="3">Average</cell><cell>68.9</cell><cell>52.5</cell><cell>33.3</cell><cell cols="2">13.1</cell><cell>42.2</cell></row><row><cell cols="3">Maximum</cell><cell>69.1</cell><cell>52.7</cell><cell>33.1</cell><cell cols="2">13.0</cell><cell>42.3</cell></row><row><cell></cell><cell>33.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(%)</cell><cell>32.5</cell><cell></cell><cell>32.8</cell><cell>33.1</cell><cell>32.9</cell><cell></cell></row><row><cell>0.5 mAP@IoU</cell><cell>31.0 31.5 32.0</cell><cell>31.8</cell><cell>32.3</cell><cell></cell><cell>32.5</cell><cell>32.0</cell><cell>31.4</cell></row><row><cell></cell><cell>30.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>30.6</cell></row><row><cell></cell><cell cols="7">0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">Sampling adjustment value</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Rethinking the faster r-cnn architecture for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1130" to="1139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bsn: Boundary sensitive network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bmn: Boundary-matching network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3889" to="3898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1049" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Turn tap: Temporal unit regression network for temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3628" to="3636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Fast learning of temporal action proposal via dense boundary generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Temporal action detection with structured segment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2914" to="2923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Weakly supervised action localization by sparse temporal pooling network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6752" to="6761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Background suppression network for weakly-supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Byun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="320" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">W-talc: Weakly-supervised temporal activity localization and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="563" to="579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Weakly-supervised action localization by generative attention modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1009" to="1019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Autoloc: Weakly-supervised temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="154" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Breaking winner-takes-all: Iterative-winners-out networks for weakly supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5797" to="5808" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cascaded pyramid mining network for weakly supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision (ACCV)</title>
		<meeting>the Asian Conference on Computer Vision (ACCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="558" to="574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Completeness modeling and context separation for weakly supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1298" to="1307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Refineloc: Iterative refinement for weakly-supervised action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3319" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Twostream consensus network for weakly-supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="37" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Weakly-supervised action localization with expectation-maximization multi-instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guillory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="729" to="745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Thumos challenge: Action recognition with a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cdc: Convolutional-de-convolutional networks for precise temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5734" to="5743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Single shot temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM international conference on Multimedia</title>
		<meeting>the ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="988" to="996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">R-c3d: Region convolutional 3d network for temporal activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5783" to="5792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Gaussian temporal awareness networks for action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="344" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos using action pattern trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Multimedia (T-MM)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="717" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fully convolutional network for multiscale temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Multimedia (T-MM)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="3428" to="3438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Exploiting informative video segments for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Temporal action localization using long short-term dependency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Kung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-scale based contextaware net for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Multimedia (T-MM)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="337" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bottom-up temporal action localization with mutual regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="539" to="555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Boundary content graph neural network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">G-tad: Subgraph localization for temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Rojas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Accurate temporal action proposal generation with relation-aware pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="810" to="820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Coarse-tofine localization of temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Multimedia (T-MM)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1577" to="1590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
	<note>in European semantic web conference (ESWC</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Ctap: Complementary temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="68" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multi-granularity generator for temporal action proposal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3604" to="3613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Afnet: Temporal locality-aware network with dual structure for accurate and fast action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Revisiting anchor mechanisms for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="8535" to="8548" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Progressive boundary refinement network for temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">619</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7094" to="7103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Relation attention for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Multimedia (T-MM)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="2723" to="2733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Untrimmednets for weakly supervised action recognition and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4325" to="4334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Stepby-step erasion, one-by-one collection: A weakly supervised temporal action detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-X</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM international conference on Multimedia (MM)</title>
		<meeting>the ACM international conference on Multimedia (MM)</meeting>
		<imprint>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Adversarial background-aware loss for weaklysupervised temporal activity localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="283" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Weakly-supervised action localization with background modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">X</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5502" to="5511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Transferable knowledgebased multi-granularity fusion network for weakly supervised temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Weakly supervised temporal action localization through contrast based evaluation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3899" to="3908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning to zoom: a saliency-based sampling layer for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kellnhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="51" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Selective sparse sampling for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6599" to="6608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Scsampler: Sampling salient clips from video for efficient action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6232" to="6242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Coarse-fine networks for temporal activity detection in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kahatapitiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.01302</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Energy-based image deformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Karni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gotsman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1257" to="1268" />
			<date type="published" when="2009" />
			<publisher>Wiley Online Library</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Non-homogeneous contentdriven video-retargeting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guttmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Sample-based non-uniform random variate generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Devroye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th conference on Winter simulation</title>
		<meeting>the 18th conference on Winter simulation</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1986" />
			<biblScope unit="page" from="260" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Background modeling via uncertainty estimation for weakly-supervised action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Byun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Segregated temporal assembly recurrent networks for weakly supervised multiple action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="9070" to="9078" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">3c-net: Category count and center loss for weakly-supervised action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8679" to="8687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Adversarial seeded sequence growing for weakly-supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM international conference on Multimedia (MM)</title>
		<meeting>the ACM international conference on Multimedia (MM)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="738" to="746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Temporal structure mining for weakly supervised action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5522" to="5531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">An improved algorithm for tv-l 1 optical flow,&quot; in Statistical and geometrical approaches to visual motion analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="23" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Ensemble learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Polikar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ensemble machine learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Ensemble learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Encyclopedia of biometrics</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="270" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Ensemble learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The handbook of brain theory and neural networks</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="110" to="125" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
