<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generative Latent Implicit Conditional Optimization when Learning from Small Sample</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idan</forename><surname>Azuri</surname></persName>
							<email>idan.azuri@cs.huji.ac.il</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">The Hebrew University</orgName>
								<address>
									<settlement>Jerusalem</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphna</forename><surname>Weinshall</surname></persName>
							<email>daphna@cs.huji.ac.il</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">The Hebrew University</orgName>
								<address>
									<settlement>Jerusalem</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Generative Latent Implicit Conditional Optimization when Learning from Small Sample</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We revisit the long-standing problem of learning from small sample, to which end we propose a novel method called GLICO (Generative Latent Implicit Conditional Optimization). GLICO learns a mapping from the training examples to a latent space, and a generator that generates images from vectors in the latent space. Unlike most recent works, which rely on access to large amounts of unlabeled data, GLICO does not require access to any additional data other than the small set of labeled points. In fact, GLICO learns to synthesize completely new samples for every class using as little as 5 or 10 examples per class, with as few as 10 such classes without imposing any prior. GLICO is then used to augment the small training set while training a classifier on the small sample. To this end our proposed method samples the learned latent space using spherical interpolation, and generates new examples using the trained generator. Empirical results show that the new sampled set is diverse enough, leading to improvement in image classification in comparison with the state of the art, when trained on small samples obtained from CIFAR-10, CIFAR-100, and CUB-200.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Modern deep Convolutional Neural Networks (CNNs) define the state of the art in image classification, as well as many other problems in a wide range of applications. Typically enormous amounts of labeled data are used to train the networks. It is not obvious whether this success can be replicated in domains where the resource of labeled data is not widely available. While hardly unexplored, the question of learning from a small sample remains a very important and challenging problem, not least so in the context of deep learning and image classification. The question remains relevant to date, as collecting a very large set of images may be difficult due to issues of privacy, image quality, geographical location, time period, or copyright status. The difficulties are further aggravated in applications that require the acquisition and processing of a new custom dataset of images, which may be a highly costly task.</p><p>In the strict small sample settings, the learner has access to a small number of labeled examples from each class, possibly as few as 5 or 10, and the number of classes can also be rather small. This setting is substantially different from the two related settings of semi-supervised learning and the few-shots learning scenario. In the few-shot scenario, the learner has access to a large number of labeled examples from classes not participating in the current classification task, while in the semi-supervised scenario the learner typically has access to a large number of unlabeled examples. Thus most few-shot <ref type="bibr">GLICO</ref> DCGAN <ref type="figure">Fig. 1</ref>. The quality of image generation in a small sample settings. We compare our method to DCGAN with the same generator. The quality of the generated images is measured using the FID score <ref type="bibr" target="#b0">[1]</ref> (lower is better). Top: new synthesized examples using our method (GLICO) and a widely used GAN model (DCGAN), both trained on CIFAR-100 with only 10 samples per class. Bottom: FID scores for each generator, when using 10, 100 and 500 examples from each class in CIFAR-100.</p><p>algorithms rely on transfer learning from tens of thousands of labeled training examples, while most semi-supervised algorithms transfer knowledge from the distribution of the unlabeled data. Methods designed to address the strict small sample scenario cannot rely on transfer learning from large amounts of peripheral data. Different approaches have been explored to address this problem, as reviewed in the next section. The problem can be notably alleviated by imposing a strong prior on the model. Unfortunately, in many applications we face an unknown domain and such prior knowledge is not available.</p><p>Data augmentation may help, reflecting the availability of some weak prior knowledge about the data. Methods employing semi-supervised <ref type="bibr" target="#b2">[2]</ref> and transductive learning <ref type="bibr" target="#b3">[3]</ref> make use of unlabeled data, when available. Self-training approaches can also boost performance when labels are scarce. Finally, one may compute a generative model from the training data, and use it to generate new samples. This is the approach we explore in this study. The different approaches are not mutually exclusive and can be used in conjunction to further boost performance as shown in Section V-C.</p><p>Why not GAN? Using generative models to augment a small training sample is very appealing, especially at present when very powerful deep generative models are becoming available. The problem is that in general, these models require a very large (possibly unlabeled) sample to achieve effective training, and therefore can only be used to augment the training set in the semi-supervised scenario. These models perform poorly in the strict small sample scenario, as can be seen in <ref type="figure">Fig. 1</ref>. To obviate this problem, our method optimizes the latent space directly, similarly to <ref type="bibr" target="#b4">[4]</ref>. It is thus able to effectively synthesize quality new images in the small sample domain, outperforming the conditional DCGAN as can be seen in <ref type="figure">Fig. 1</ref>, where image quality is measured using the FID score.</p><p>More specifically, our proposed method (see <ref type="figure" target="#fig_0">Fig. 2</ref>) learns a latent space code for each data point separately. To improve the properties of the learned latent space, we seek to push intraclass examples to lie close to each other and separated from inter-class examples as much as possible. This is accomplished by adding a classifier to the basic architecture as described in Section III-B. The classifier is trained using the known multi-class labels of the data. We note that training is not adversarial, and therefore this classifier is not equivalent to the discriminator in the GAN (Generative Adversarial Network) architecture. The full model is described in Section III.</p><p>The architecture of the model reveals geometric properties that allow us to effectively sample specific areas in the latent space and synthesize novel images as explained in Section III-B. The empirical evaluation described in Section IV shows the success of our model in improving classification performance while training with a small sample, especially in the extreme conditions where the sample size is very small indeed.</p><p>The rest of the paper is organized as follows. In Section III we describe GLICO, a novel method for data synthesis which can be effectively trained from a few labeled points from each class. In Section IV we describe how synthetic images are used to boost the training of a discriminative deep classifier. In Section V we demonstrate the superior performance of our method when compared to alternative methods under extreme low sample conditions in the strict small sample scenario with no additional unlabeled data. We provide an ablation study and further investigate alternative design choices and their effect on classification performance. In Section V-C we show empirically that the synthetic examples produced by GLICO are unlike those generated by conventional augmentation methods. Lastly, we investigate the contribution of adding unlabelled data to GLICO in the semi supervised settings in Section V-D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Earlier work addressed the small sample scenario mainly in the context of Bayesian inference, see for example <ref type="bibr" target="#b6">[5]</ref>- <ref type="bibr" target="#b8">[7]</ref>. These methods have been revisited in recent years in the context of few-shot learning, where transfer learning can be exploited. In a commonly used setup of few-shot learning, at train time there are many classes with many labeled examples from each class. At test time, some novel classes (usually 5) with only a few examples per class are given, alongside query images from the same novel classes.</p><p>Data Augmentation with Geometrical Transformations. Focusing our attention back at the small sample problem, data augmentation as discussed in <ref type="bibr" target="#b9">[8]</ref> can be effectively used. Common augmentation techniques for image classification include translating the image by a few pixels, adding Gaussian noise, and flipping the image horizontally or vertically <ref type="bibr" target="#b10">[9]</ref>- <ref type="bibr" target="#b12">[11]</ref>. Additional image augmentation methods include Cutout <ref type="bibr" target="#b13">[12]</ref>, which randomly masks a square region in an image at every training step and thus affects the nature of the learned features, and Random Erasing <ref type="bibr" target="#b14">[13]</ref>, where similarly to dropout randomly chosen rectangular regions in the image have their pixels erased or replaced by random values. MixUp <ref type="bibr" target="#b15">[14]</ref> uses Alpha-blending of two images to form a new image while regularizing the CNN to favor a simple linear behavior in between training images. MixMatch <ref type="bibr" target="#b16">[15]</ref> augments MixUp by self training, generating "guessed labels" for each unlabeled example. We note that these methods are not always effective on very small datasets, and may even degrade classification performance as shown in <ref type="table" target="#tab_0">Table I</ref>.</p><p>Data Augmentation with Generative Models. As alluded to above, generative models can be a powerful tool for data augmentation by making it possible under ideal conditions to sample new examples, see for example <ref type="bibr" target="#b15">[14]</ref>, <ref type="bibr" target="#b17">[16]</ref>- <ref type="bibr" target="#b21">[20]</ref>. These methods typically require large amounts of (labeled or unlabeled) data to be effective. They try to leverage generative models by sampling new synthetic examples from the learned distribution of the data. Unfortunately in the small sample case, when transfer learning is not an option, the few labeled examples do not represent the true data distribution very reliably, resulting in poor generalization and low-quality synthetic data. Finally, the notorious instability of their training process and the heavy computational load make GANs less appealing for mere data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. OUR MODEL</head><p>We propose a method for multi-class image classification from small sample, which consists of data augmentation with a generative model. Specifically, to use modern deep learning classifiers, which typically require large amounts of training data, we augment the small training set by sampling from a generative model. Our generative model is a latent optimization method that combines an auxiliary task of classification, and which can be trained effectively from a small sample. The architecture is designed so that it can benefit from both labeled and unlabeled data. With access to only small amounts of unlabeled data or none at all, our results surpass the state of the art. The code is available online 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Model Overview</head><p>The generative model we use to sample new data includes a basic generative latent optimization architecture with generator 1 https://github.com/IdanAzuri/glico-learning-small-sample G θ , and an added small classifier f φ which is trained to classify the labeled data (see <ref type="figure" target="#fig_0">Fig. 2</ref>). Training is not adversarial, and therefore this classifier is nothing like the discriminator in the GAN architecture. The latent space is initialized randomly</p><formula xml:id="formula_0">{z i ∈ Z} where Z denotes the unit sphere in R d . Every vector z i is mapped to an image {x i ∈ X|x i ∈ R 3×H×W } from the given (small) training set.</formula><p>The training process has two modes: With unlabeled data, the reconstruction loss in (2) is used to train G θ as done in <ref type="bibr" target="#b4">[4]</ref>. With labeled data, the reconstruction loss is augmented with the cross-entropy loss corresponding to the loss of the added classifier f φ (see <ref type="figure" target="#fig_0">Fig. 2</ref>).</p><p>Here we use the perceptual loss <ref type="bibr" target="#b23">[21]</ref> to measure the reconstruction loss. More specifically, in order to compute the perceptual loss we extract the activation vectors in layers 'conv 1 2 ', 'conv 2 2 ', 'conv 3 2 ', 'conv 4 2 ' and 'conv 5 2 ' of a VGG-16 network. Denoting the output tensor of layer 'conv j 2 ' for input image x by ξ j (x), we compute the difference between the original image and its reconstructed version by:</p><formula xml:id="formula_1">L percep (x i , z i ; θ) = j λ j ||ξ j (G θ ([z i , ε])) − ξ j (x i ))|| 1 (1)</formula><p>Above θ denotes the parameters of the generator G θ , and λ j the weight of layer j (usually the weighted average).</p><p>Our method is described in Alg. 1. Its components are described next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Generative Latent Optimization</head><p>Generative model. Generative Latent Optimization (GLO) is an effective and thin method for image reconstruction, relying on a small number of parameters. GLO maps every image x i from the dataset to a low-dimensional random vector z i in the latent space Z. It then passes the random vector into a generator G θ (·), which is optimized to minimize the reconstruction loss between G θ (z i ) and x i .</p><formula xml:id="formula_2">Formally, let {x 1 , x 2 . . . x n } ∈ X denote a set of images where x i ∈ R 3×W ×H . Choose n d-dimensional random vectors on the unit sphere {z 1 , z 2 , . . . , z n } ∈ Z where Z ⊆ R d . Pair</formula><p>Algorithm 1 GLICO. The algorithm learns codes {z i } n i=1 by minimizing the reconstruction loss L percep of generator G θ , and the cross entropy loss L ce of discriminator f φ .</p><p>Input: unlabeled data P Du , labeled data P D l , γ, epochs</p><formula xml:id="formula_3">n = |P D l | + |P Du | epoch = 0 Initialize {z i } n i=1 where {z i ∈ Z : ||z i || 2 = 1} repeat for (x i , y i ) ∈ P D l do ε ∼ N (0, σI) L percep = j λ j ||ξ j (G θ ([z i , ε])) − ξ j (x i ))|| 1 L ce = L CE (f φ (G θ ([z i , ε])), y i ) L = L percep + γL ce Update {z i }, θ, φ using the gradient of L ∀z i ∈ Z, z i := zi ||zi||2 end for // Optional: transductive learning mode for x i ∈ P Du do L = j λ j ||ξ j (G θ ([z i , ε])) − ξ j (x i ))|| 1 Update {z i }, θ using the gradient of L ∀z i ∈ Z, z i := zi ||zi||2 end for epoch+ = 1 until epoch &gt; epochs every image x i ∈ X with a random vector z i ∈ Z, to achieve the mapping {(x 1 , z 1 ), . . . , (x n , z n )}.</formula><p>Finally, learn jointly the parameters θ of the generator G θ : Z − → X, where the optimal set {z i } and parameters θ are obtained by minimizing the following objective:</p><formula xml:id="formula_4">min θ n i=1 min zi∈Z L percep (x i , z i ; θ) s.t. ||z i || 2 = 1 (2)</formula><p>While in GLO the Laplacian pyramid is used for reconstruction loss, we note that the minimization of L percep appears to yield more realistic results <ref type="bibr" target="#b24">[22]</ref>, as compared to other image metrics in common use.</p><p>Adding a Classifier. Possibly the main weakness of using GLO as a generative model is the relatively low quality of images generated when sampling new points in the latent space Z. The problem lies in the sparsity of the learned set {z i } n i=1 , which lacks structure since each z i is trained independently. To decrease the intra-class distances and increase the inter-class distances in the latent space representation, we propose the conditional model termed GLICO. In this model, the generator G θ is augmented by a weak classifier f φ , which is trained to classify the labeled data. When the label of x i is known, L percep in (2) is replaced by L percep + L ce , where L ce is the cross-entropy loss of classifier f φ . <ref type="figure" target="#fig_1">Fig. 3</ref> illustrates the structure of the latent space when the different loss functions are used.</p><p>Sampling the Latent Space. Generating images based on randomly sampling the latent space, even when restricted to the immediate vicinity of {z i } n i=1 , produces low quality somewhat meaningless images. Therefore, instead of randomly sampling Z, we generate new image codes by interpolating between the known latent vectors {z i } n i=1 . Since the latent space Z is a hyper-sphere, we employ to this end spherical linear interpolation (slerp) <ref type="bibr" target="#b25">[23]</ref>, which is defined as follows:</p><formula xml:id="formula_5">slerp(q1, q2; t) = q 1 sin (1 − t)ϑ sin ϑ + q 2 sin tϑ sin ϑ<label>(3)</label></formula><p>Above t ∈ [0, 1], and ϑ is the angle between q 1 and q 2 , computed as ϑ = cos −1 (q 1 · q 2 ). As shown in <ref type="figure" target="#fig_2">Fig. 4</ref>, the slerp interpolation follows the great circle path on a d-dimensional hyper-sphere (with elevation changes) between two points z i and z j . This technique has shown promising results in the context of both VAE and GAN generative models, with both uniform and Gaussian priors <ref type="bibr" target="#b27">[24]</ref>.</p><formula xml:id="formula_6">0.5 0.25 0.75 v 2 v 1 v 2 v 1 θ 1 θ 2 θ 3 θ 4</formula><p>Why slerp? Linear interpolation (lerp) is the simplest method to traverse the latent space manifold between two known locations. Often it is used to illustrate learned features that capture the semantics of the dataset <ref type="bibr" target="#b28">[25]</ref>. However, <ref type="bibr" target="#b29">[26]</ref> noted that linear interpolation in the latent space is often inappropriate since the latent spaces of most generative models are embedded in high dimensional spaces (over 50 dimensions). In such a space, linear interpolation traverses locations that are extremely unlikely given the prior, whether Gaussian or uniform.</p><p>Noise concatenation. Training from a small sample is more susceptible than ever to random perturbations in the data. To increase training robustness, we concatenate noise to the latent vector such that the input to the generator G θ is [z i , ε], ε ∼ N (0, σI), z i ∈ {z ∈ Z : ||z|| 2 = 1}), see <ref type="figure" target="#fig_0">Fig. 2</ref>.</p><p>Transductive learning. The setting of transductive learning was introduced by <ref type="bibr" target="#b3">[3]</ref>. Like semi-supervised learning it aims to exploit unlabeled data, using the unlabeled test set to improve the test set classification. Specifically, the learning task is defined on a given set of training points</p><formula xml:id="formula_7">{(x 1 , y 1 ), (x 2 , y 2 ), . . . , (x , y )}, x i ∈ R d , y i ∈ {−1, 1}</formula><p>and a sequence of k test vectors {x +1 , . . . , x +k }. The goal of the learner is to find among an admissible set of binary vectors the one that classifies the test vectors as accurately as possible. We assume that the set of vectors {x +1 , . . . , x +k } and their corresponding true labels is an i.i.d sample drawn according to the same unknown distribution P (x) and P (y|x). Basically, the core idea in transductive learning is to leverage the implicit information in the instances whose output is required (x +1 , . . . , x +k ), in order to improve their classification.</p><p>In the same spirit as the classical setting of transductive learning, we may use the unsupervised data to train the generator of the model to reconstruct the test points while optimizing their reconstruction loss (2), see <ref type="figure" target="#fig_1">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EVALUATION METHODOLOGY A. Datastes</head><p>We evaluate our method on three standard benchmarks for image classification. The first two are CIFAR-10 and CIFAR-100 <ref type="bibr" target="#b30">[27]</ref>, each includes 50, 000 32 × 32 color images, with 10 or 100 classes and 500 or 5000 images per class respectively. The relatively small size of the images allows us to perform an exhaustive ablation study on this dataset as described in Section V. The second dataset is CUB-200 <ref type="bibr" target="#b31">[28]</ref>, which includes high-resolution fine-grained images of 200 species of birds, with only 30 images per class. This makes this dataset a more appropriate testbed for a method that addresses the small sample problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Protocol</head><p>For each benchmark, we defined a small sample task by subsampling the original training set of the corresponding dataset.</p><p>To allow for comparison with other methods, the subset splits were adapted from <ref type="bibr" target="#b32">[29]</ref>. As classification engine we used, unless otherwise noted, WideResNet-28 <ref type="bibr" target="#b33">[30]</ref> for CIFAR-10 and CIFAR-100, and Resnet50 <ref type="bibr" target="#b34">[31]</ref> for CUB-200. Baseline results were obtained by training the corresponding model using the training set with only standard data augmentation.</p><p>When using our method, we augmented the training set using GLICO. Specifically, we start by sampling a mini-batch from the training data in each SGD optimization step. Each example x i in the mini-batch is used for training with probability 0.5. Otherwise (with probability 0.5) it is replaced by a new image obtained by sampling the latent space G([slerp(z i , z j , t), ε])). z j is the latent representation of some example from the same class c as x i , sampled uniformly from the latent codes of all remaining examples in class c. The slerp interpolation factor is sampled uniformly from the set [0.1, 0.2, 0.3, 0.4].</p><p>We compared our results with state-of-the-art methods that are suitable for the small sample domain, using as much as possible public-domain code. Thus we compared sample augmentation with GLICO to image augmentation with Cutout <ref type="bibr" target="#b13">[12]</ref>, Random Erase <ref type="bibr" target="#b14">[13]</ref> and MixMatch <ref type="bibr" target="#b16">[15]</ref>. In each case we repeated the same procedure as described above, replacing the generation of a new image using GLICO by an image obtained from the corresponding augmented set of images. We also evaluated the method described in <ref type="bibr" target="#b32">[29]</ref>, which was explicitly designed to handle small sample, using code provided by the authors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation Details</head><p>We used SGD optimization with learning rate 0.1 for CIFAR-10 and CIFAR-100 and 0.001 for CUB-200, with a batch size of 128 and 16 respectively. In all the experiments we used the standard categorical cross-entropy loss function when training the weak classifier f φ. (We note in passing that using a strong classifier decreased the quality of the generated images and harmed the final performance, see Section V-E.) Images from the CUB-200 dataset were resized to 256 pixels wide in their smaller side, and then randomly cropped to 224 × 224 pixels. As stated above, in all the experiments (including baseline) we adopted the standard transformations of random horizontal flipping and random crop for data augmentation.</p><p>In all cases, the latent space Z was the unit sphere in R 128 . For the generator, we used a standard off-the-shelf DCGAN architecture <ref type="bibr" target="#b35">[32]</ref>. The generator of any modern GAN architecture can be readily used instead and improve the reconstruction quality. However, it is worth noting that our method can improve the SOTA while using the generator of a relatively simple GAN architecture. We trained the model on 2 × RTX-2080 GPUs for 200 epochs; every epoch took around 40 seconds on CIFAR-100 and 3 minutes on CUB-200.</p><p>To achieve uniformity we oversampled the training set in proportion to the size of the training set. For example, with 50 samples per class in CIFAR-100, we trained the model ×10 iterations. Standard errors (STE) were obtained from 3 runs with different seeds in all study cases except for MixMatch, where a single result is reported since each MixMatch run took a very long time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RESULTS</head><p>The results of our empirical study are summarized in <ref type="figure">Fig. 6</ref> for CIFAR-10, and <ref type="table" target="#tab_0">Table I</ref> for CIFAR-100 and CUB-200. We used small sample partitions, with 10 to 250 labeled samples per class (SPC) in CIFAR-10, 10 to 100 labeled SPC in CIFAR-100, and 5 to 30 labeled SPC in CUB-200. We compared our model to three different methods of data augmentation and one small sample method. <ref type="figure">Fig. 5a</ref> Illustrates the kind of synthetic images we get while relying on a very small sample. Note that since the purpose of generating new images is to boost classification from small sample, low image quality does not preclude their usefulness for the task. <ref type="figure">Fig. 5b</ref> Illustrates the quality of the reconstruction when using GLICO with 10 samples per class only, demonstrating high quality reconstruction.</p><p>The two augmentation methods used for comparison in our experiments are Random Erasing <ref type="bibr" target="#b14">[13]</ref> and Cutout <ref type="bibr" target="#b13">[12]</ref>. In all  the study cases but one, augmentation with GLICO significantly outperforms the other methods (see <ref type="table" target="#tab_0">Table I</ref>). With CIFAR-100 and 100 samples per class, Cutout matches the highest accuracy (similar to GLICO) in our experiments.</p><p>MixMatch <ref type="bibr" target="#b16">[15]</ref> is a new technique that achieves state-of-theart results on multiple datasets in a semi-supervised setting. In the supervised small sample regime, this method does not perform very well as can be seen in <ref type="table" target="#tab_0">Table I</ref>, but see Section V-D. Possibly, the blending of images in pixel space, which is intended to provide some means of regularization, is only effective when enough training data are available. Otherwise, it feeds noisy examples to the model and makes it harder to generalize.</p><p>[29] describes a distance-based method that is designed to handle the small sample challenge, among other things. The results, when using the code published by the authors in our experimental design, are shown in the last column of <ref type="table" target="#tab_0">Table I</ref>. Admittedly, we were not able to reproduce their published results, which are therefore noted in parentheses 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Ablation Study</head><p>Next, we review and evaluate different design choices used in the architecture and the approach proposed in this paper. The results are summarized in <ref type="table" target="#tab_0">Table II</ref>. More specifically, we see in <ref type="table" target="#tab_0">Table II</ref> the effect of omitting different components of GLICO, including classifier f φ , noise concatenation, and replacing slerp by vanilla linear interpolation. We note that when omitting classifier f φ , the reconstruction loss achieves a better score, but the augmentation fails to generate 'good' examples to improve the classification.</p><p>The 'Baseline' case shows the results of training without sampling additional images. 'Transductive" shows the added benefit obtained from including the unlabeled test set in the training of generator G θ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Design Choices</head><p>In this section, we describe a few alternative design choices that proved less effective, as summarized in <ref type="table" target="#tab_0">Table III</ref>. Latent Classifier. One can optimize the discriminative loss L CE directly in the latent space using (z i , y i ) instead of the image space (G(z i ), y i ). Here we used a 3 layer fully connected network with inter-layer ReLU activation. Z Initialization. We investigated different ways to initialize the latent space mappings while exploiting some prior knowledge we have on the data. i) Hypercube vertices: every class is initialized in the vicinity of a different vertex of the hypercube in R d . ii) ResNet: each image is assigned the corresponding activation in the penultimate layer of a pre-trained ResNet model.</p><p>Additive Noise. GLICO relies on the concatenation of noise to the latent space representation. To investigate the contribution of this component, and following <ref type="bibr" target="#b36">[33]</ref>, we explored a simpler alternative, where random noise ε ∼ N (0, σI) is sampled i.i.d and added to z i before calculating the loss (2), so that x = G(z i + ε). The goal is to obtain a better representation of the image manifold by learning the ε ball around every example both in the latent space and the image space. However, as shown in <ref type="table" target="#tab_0">Table III</ref>, this approach leads to performance degradation in the final classification.</p><p>Cosine Loss. It is argued in <ref type="bibr" target="#b32">[29]</ref> that the cosine loss provides a better optimization function for the small sample regime. In our experimental setup, the cross-entropy classification loss provided better results, see <ref type="table" target="#tab_0">Table III</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Relation to Classical Augmentations</head><p>So far we have shown that our method boosts classification performance in the small sample settings when augmenting the small training set with images synthesized by our generative model. But are we learning to generate any significant new information? In other words, can similar images and the same boost in performance be obtained by simpler alternative means of image augmentation?</p><p>We approach this question by reevaluating the results of our method, modified so that new images are synthesized by an alternative image augmentation technique which employs classical geometric transformation. To make the challenge as hard as possible, we adopt AutoAugment (AA) <ref type="bibr" target="#b12">[11]</ref>, an RL based augmentation method, which estimates the optimal set of classical transformations to augment images in CIFAR-100. AA exhaustively searches through 16 types of color-based and geometric base transformations, while using all the images in CIFAR-100 benchmark dataset. Note that this gives an unfair advantage to this method as compared to our original method. The case studied here is CIFAR-100 with 50 labeled samples per class, and with transductive learning (similar results are obtained without transductive learning). The results of this challenge are shown in <ref type="table" target="#tab_0">Table IV</ref>. Clearly each method boosts classification performance, as can be seen in rows 2-3 when compared to the baseline in row 1. But when using the two methods -AA and GLICO -together, performance improves even further (row 4). It appears that each augmentation method provides an independent contribution, and that the effect of the two augmentation methods is additive. From this empirical result we conclude that the contribution of GLICO goes beyond the contribution of augmentation by classical image transformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Using Unlabeled Data</head><p>While our method is designed to address the strict small sample settings, the same approach can be beneficial in the semi-supervised settings (SSL), where the learner is given access in addition to a large set of unlabeled images.</p><p>Note that since GLICO is not designed to resolve the SSL problem, it does not have any mechanism of 'label guessing' like other SSL methods. The following analysis simply aims to explore the limits of the current model in different settings. Thus, while in the fully supervised scenario GLICO outperforms MixMatch as shown in <ref type="table" target="#tab_0">Table I</ref>, MixMatch slowly improves when given access to unlabeled data, and eventually outperforms GLICO as shown in <ref type="table" target="#tab_4">Table V</ref>. Clearly MixMatch benefits from unlabeled data more considerably. In order to be effective in the semi-supervised settings, we will need to enhance GLICO with some mechanism of label guessing. Our algorithm is designed to optimize a combined loss L percep + L ce , where L percep drives the generator to achieve good reconstruction, while L ce serves as a regularizer. L ce imposes structure on the latent space reflecting the known labels. The minimization of the regularizer L ce is mediated by a classifier f φ , while the minimization of the reconstruction loss L percep is mediated by a generator G θ . A strong classifier f φ , e.g. Resnet-50 or VGG-19 with 45M and 144M parameters respectively, has more than x5 parameters as compared to offthe-shelf generators G θ . This would shift the balance of the learning algorithm from the generative to the discriminative component of the algorithm. Thus it would seem that the strength of the classifier f φ should be controlled to reflect the amount of labeled data, or how small the sample is.</p><p>In <ref type="figure" target="#fig_5">Fig. 7</ref> we evaluate four classifiers: 3 strong classifiers including Resnet-50, VGG-19 and Wide-Resnet28, and one weak classifier which is a small CNN with 4 convolutional layers. Our results show that the smaller model achieves the best results in the low regime of the small sample settings, while VGG19 achieves higher accuracy when more labeled data are available. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. SUMMARY AND DISCUSSION</head><p>In this work we revisited the problem of learning from small sample. We developed a deep generative model, called Generative latent implicit conditional optimization (GLICO), which can be effectively trained to generate examples when seeing only a small sample of data. New examples are synthesized by interpolating between the latent vectors of known examples. When using small sample scenarios generated from the CIFAR-10, CIFAR-100, and CUB-200 benchmarks, we show that our method improves classification over the baseline and several alternative methods. Thus our method defines the state of the art in small sample image classification.</p><p>Our generative model is based on latent space optimization. Latent optimization does not involve an encoder like some other generative methods (such as the Variational Auto Encoder). In particular, this implies that the number of variables grows linearly with the number of data samples. Contrary to GAN, latent optimization learns every latent representation separately, and therefore it does not require much data to achieve decent reconstruction results as demonstrated in <ref type="figure">Fig. 5</ref>.</p><p>The optimization of each representation vector separately also implies that the dimensions of the latent space do not correspond to the semantic features of the data. To address this weakness and inject some semantic structure into the latent space representation, we added a classifier to the latent optimization training process. Unlike GANs, the classifier is not trained in an adversarial fashion. Rather, we use the classification loss L CE over the reconstructed examples G θ (z i ) to induce semantic relations into the latent space, and allow for better sampling and new image generation (see <ref type="figure" target="#fig_1">Fig. 3</ref>).</p><p>The unique aforementioned properties of our model allow it to improve the training efficacy of deep classifiers in the small sample regime. We suggest two complementary approaches using our proposed transductive learning option. It can be used in conjunction with our method and may benefit from unlabeled data in a semi-supervised manner, or unrelated labeled datasets which can be used for transfer learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Schematic illustration of our method. Every z i ∈ Z, where Z denotes the unit sphere, is mapped to a specific image x i in the image space. In this illustration, the color of each image frame marks the class label of the image, while black frames mark unlabeled images. The classifier is used to propagate error only when a label is given.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Illustration of the latent space Z. (a) GLO: vectors z i ∈ Z do not have semantic meaning in Z. (b) Our method: vectors from the same class are grouped. (c) Our method in transductive mode. Notations: filled colored circles represent different labeled datapoints, where color corresponds to class identity. Black circles with the symbol "?" represent unlabeled datapoints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Slerp vs. lerp. Left side: linear interpolation between v 1 and v 2 with t ∈ [0.25, 0.5, 0.75]. Right side: spherical interpolation. Note that both the length and arc length of the interpolated vectors are equal in slerp but unequal in lerp.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) CIFAR-100 image generation. (b) CUB-200 image reconstruction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Examples of synthesized images. (a) Each row shows five new images (the intermediate columns), generated based on smooth interpolation in the latent space between two reconstructed images (the left and right columns). (b) High-resolution image reconstruction. Here GLICO was trained only on 10 examples per class from CUB-200. Comparison of Top-1 Accuracy (including STE) for CIFAR-10 using WideResnet-28, with a different number of training examples per class (labeled data only).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Weak vs. strong classifier. Top-1 Accuracy (including STE) for CIFAR-10 when using different architectures for the classifier component in our method. In the small regime (10-100 samples per class) the weak classifier outperforms the stronger classifiers. When sufficient labelled examples are available, the stronger classifiers achieve higher accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I COMPARISON</head><label>I</label><figDesc>OF TOP-1 ACCURACY (INCLUDING STE) FOR CIFAR-100 AND CUB-200 USING WIDERESNET-28 AND RESNET-50 RESPECTIVELY, WITH A DIFFERENT NUMBER OF TRAINING EXAMPLES PER CLASS (LABELED DATA ONLY). THE METHODS USED FOR COMPARISON ARE DESCRIBED IN THE TEXT BELOW. BEST RESULTS ARE MARKED IN BOLD. FOR [29], *INDICATES THAT THE REPORTED RESULTS, AS OBTAINED IN OUR EXPERIMENTS USING CODE RELEASED BY THE AUTHORS, DO NOT MATCH THE RESULTS REPORTED BY THE AUTHORS WHICH ARE THEREFORE LISTED IN PARENTHESES.</figDesc><table><row><cell>DATASET</cell><cell>SAMPLES/CLASS</cell><cell>BASELINE</cell><cell>OURS</cell><cell>MIXMATCH</cell><cell>CUTOUT</cell><cell>RANDOM ERASE</cell><cell>[29]  *</cell></row><row><cell>CIFAR-100</cell><cell>10</cell><cell cols="2">22.89±0.09 28.55±0.40</cell><cell>24.80</cell><cell>23.43±0.24</cell><cell>23.26±0.27</cell><cell>23.01 (22)</cell></row><row><cell></cell><cell>25</cell><cell cols="2">38.39±0.10 43.84±0.25</cell><cell>40.17</cell><cell>39.11±0.59</cell><cell>37.45±0.15</cell><cell>28.05 (35)</cell></row><row><cell></cell><cell>50</cell><cell cols="2">47.82±0.11 52.95±0.20</cell><cell>49.87</cell><cell>52.11±0.28</cell><cell>50.50±0.41</cell><cell>44.55 (48)</cell></row><row><cell></cell><cell>100</cell><cell cols="2">61.37±0.13 64.27±0.04</cell><cell>59.03</cell><cell>64.49±0.10</cell><cell>64.03±0.22</cell><cell>55.99 (58)</cell></row><row><cell>CUB-200</cell><cell>5</cell><cell>50.79±0.19</cell><cell>51.52±0.21</cell><cell>15.01</cell><cell>50.63±0.31</cell><cell>48.90±0.45</cell><cell>17.80 (35)</cell></row><row><cell></cell><cell>10</cell><cell>64.11±0.22</cell><cell>65.13±0.12</cell><cell>36.02</cell><cell>64.33±0.02</cell><cell>63.72±0.20</cell><cell>34.23 (60)</cell></row><row><cell></cell><cell>20</cell><cell cols="2">69.11±0.55 74.16±0.17</cell><cell>60.57</cell><cell>68.47±0.20</cell><cell>66.14±0.23</cell><cell>52.00 (76)</cell></row><row><cell></cell><cell>30</cell><cell>75.15±0.10</cell><cell>77.75±0.20</cell><cell>70.41</cell><cell>74.97±0.34</cell><cell>73.74±0.34</cell><cell>62.25 (82)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II ABLATION</head><label>II</label><figDesc>STUDY: TRAINED ON CIFAR-100 WITH 25 LABELED TRAINING EXAMPLES PER CLASS. TOP 1 AND TOP 5 ACCURACY IS CALCULATED BASED ON THE ARCHITECTURAL VARIANTS DESCRIBED SECTION V-A.</figDesc><table><row><cell>Model</cell><cell>Top 1 Acc.</cell><cell>Top 5 Acc.</cell></row><row><cell>GLICO</cell><cell cols="2">43.84±0.25 70.73±0.07</cell></row><row><cell>Baseline</cell><cell cols="2">38.39±0.18 67.77±0.18</cell></row><row><cell>No Classifier</cell><cell cols="2">41.57±0.54 69.55±0.11</cell></row><row><cell>No Noise</cell><cell cols="2">43.31±0.02 70.05±0.02</cell></row><row><cell>Lerp</cell><cell cols="2">43.01±0.06 70.51±0.03</cell></row><row><cell>Transductive</cell><cell cols="2">44.79±0.12 71.28±0.09</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III</head><label>III</label><figDesc></figDesc><table><row><cell cols="3">TOP 1 AND TOP 5 ACCURACY OF ADDITIONAL DESIGN CHOICES AS</cell></row><row><cell cols="3">EXPLAINED IN THE TEXT.</cell></row><row><cell>Model</cell><cell>Top 1 Acc.</cell><cell>Top 5 Acc.</cell></row><row><cell>Latent Classifier</cell><cell cols="2">43.08±0.06 70.22±0.05</cell></row><row><cell>Hypercube Init</cell><cell cols="2">44.21±0.61 70.89±0.46</cell></row><row><cell>ResNet Init</cell><cell cols="2">42.95±0.22 70.10±0.13</cell></row><row><cell>Additive Noise</cell><cell cols="2">41.02±0.43 68.10±0.11</cell></row><row><cell>Cosine Loss</cell><cell cols="2">41.01±0.27 68.45±0.31</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV TOP</head><label>IV</label><figDesc>-1 AND TOP-5 ACCURACY WHEN AUGMENTING A SMALL DATASET (CIFAR-100, 50 SPC) BY GLICO ALONE (SECOND ROW), AUTOAUGMENT ALONE (THIRD ROW), OR BOTH (FOURTH ROW). NOTE THAT EACH METHOD BOOSTS PERFORMANCE ON ITS OWN, WHILE WHEN USED IN CONJUNCTION ADDITIONAL PERFORMANCE BOOST IS SEEN. THE FIRST ROW PROVIDES</figDesc><table><row><cell>THE BASELINE.</cell><cell></cell></row><row><cell cols="2">AutoAu. Ours Top-1 Accuracy Top-5 Accuracy</cell></row><row><cell>50.37±0.05</cell><cell>75.61±0.01</cell></row><row><cell>53.35±0.23</cell><cell>77.60±0.12</cell></row><row><cell>53.80±0.10</cell><cell>79.18±0.13</cell></row><row><cell>56.31±0.02</cell><cell>80.66±0.04</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V SEMI</head><label>V</label><figDesc>-SUPERVISED SCENARIO: TOP-1 ACCURACY OF MIXMATCH VS GLICO WHEN SHOWN CIFAR-100 WITH 25 LABELED EXAMPLES PER CLASS AND A VARYING NUMBER OF UNLABELED EXAMPLES, WHERE EACH CASE CORRESPONDS TO A DIFFERENT COLUMN.</figDesc><table><row><cell>Method</cell><cell cols="3">supervised only 1000 unlabeled 35k unlabeled</cell></row><row><cell>Baseline</cell><cell>38.39±0.18</cell><cell>-</cell><cell>-</cell></row><row><cell>MixMatch</cell><cell>40.17</cell><cell>42.39</cell><cell>50.34</cell></row><row><cell>Ours</cell><cell>43.84±0.25</cell><cell>44.52±0.12</cell><cell>44.73±0.07</cell></row><row><cell cols="2">E. Weak Vs. Strong Classifier</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The original published experiments used Resnet-110 for CIFAR-100 rather than the WideResNet-28 model used here, and Resnet-50 for CUB-200 as in our experiments.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>This work was supported by a grant from the Israel Science Foundation (ISF), a grant from the Israel Innovation Authority (Phenomics), and by the Gatsby Charitable Foundations.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno>abs/1706.08500</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<ptr target="http://arxiv.org/abs/1706.08500" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Semi-supervised learning with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01583</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Statistical learning theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">V</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Optimizing the latent space of generative networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Pas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research</title>
		<editor>J. Dy and A. Krause</editor>
		<meeting>the 35th International Conference on Machine Learning, ser. Machine Learning Research<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07-15" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="600" to="609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<ptr target="http://proceedings.mlr.press/v80/bojanowski18a.html" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A bayesian/information theoretic model of learning to learn via multiple task sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baxter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="7" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning to learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Y</forename><surname>Pratt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Springer US</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The calculation of posterior distributions by data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Tanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Wong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
	<note>in proceeding</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Document image defect models and their uses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Baird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2nd International Conference on Document Analysis and Recognition (ICDAR &apos;93)</title>
		<meeting>2nd International Conference on Document Analysis and Recognition (ICDAR &apos;93)</meeting>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="62" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation policies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno>abs/1805.09501</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04896</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02249</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Delta-encoder: an effective sample synthesis method for few-shot object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shtok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2845" to="2855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Augmenting image classifiers using data augmentation generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Edwards</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="594" to="603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dada: Deep adversarial data augmentation for extremely low data regime classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2807" to="2811" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Dataset augmentation in feature space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05538</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Data augmentation via latent space interpolation for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 24th International Conference on Pattern Recognition (ICPR)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="728" to="733" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Non-adversarial image synthesis with generative latent nearest neighbors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hoshen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno>abs/1812.08985</idno>
		<ptr target="http://arxiv.org/abs/1812.08985" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Animating rotation with quaternion curves</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shoemake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGGRAPH Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="245" to="254" />
			<date type="published" when="1985-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<idno type="DOI">http:/doi.acm.org/10.1145/325165.325242</idno>
		<ptr target="http://doi.acm.org/10.1145/325165.325242" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Sampling generative networks: Notes on a few effective techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>White</surname></persName>
		</author>
		<idno>abs/1609.04468</idno>
		<ptr target="http://arxiv.org/abs/1609.04468" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Latent space oddity: on the curvature of deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Arvanitidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hauberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cifar-100 (canadian institute for advanced research)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://www.cs.toronto.edu/∼kriz/cifar.html" />
	</analytic>
	<monogr>
		<title level="j">Dataset</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<title level="m">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep learning on small datasets without pre-training using cosine loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Barz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Denzler</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1901.09054" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Munkberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hasselgren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.04189</idno>
		<title level="m">Noise2noise: Learning image restoration without clean data</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
