<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Circle Loss: A Unified Perspective of Pair Similarity Optimization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">MEGVII Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changmao</forename><surname>Cheng</surname></persName>
							<email>chengchangmao@megvii.com</email>
							<affiliation key="aff0">
								<orgName type="institution">MEGVII Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
							<email>zhangchi@megvii.com</email>
							<affiliation key="aff0">
								<orgName type="institution">MEGVII Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongdao</forename><surname>Wang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
							<email>weiyichen@megvii.com</email>
							<affiliation key="aff0">
								<orgName type="institution">MEGVII Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Circle Loss: A Unified Perspective of Pair Similarity Optimization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper provides a pair similarity optimization viewpoint on deep feature learning, aiming to maximize the within-class similarity s p and minimize the between-class similarity s n . We find a majority of loss functions, including the triplet loss and the softmax cross-entropy loss, embed s n and s p into similarity pairs and seek to reduce (s n − s p ). Such an optimization manner is inflexible, because the penalty strength on every single similarity score is restricted to be equal. Our intuition is that if a similarity score deviates far from the optimum, it should be emphasized. To this end, we simply re-weight each similarity to highlight the less-optimized similarity scores. It results in a Circle loss, which is named due to its circular decision boundary. The Circle loss has a unified formula for two elemental deep feature learning paradigms, i.e., learning with class-level labels and pair-wise labels. Analytically, we show that the Circle loss offers a more flexible optimization approach towards a more definite convergence target, compared with the loss functions optimizing (s n − s p ). Experimentally, we demonstrate the superiority of the Circle loss on a variety of deep feature learning tasks. On face recognition, person re-identification, as well as several finegrained image retrieval datasets, the achieved performance is on par with the state of the art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This paper holds a similarity optimization view towards two elemental deep feature learning paradigms, i.e., learning from data with class-level labels and from data with pair-wise labels. The former employs a classification loss function (e.g., softmax cross-entropy loss <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b35">36]</ref>) to optimize the similarity between samples and weight vectors. The latter leverages a metric loss function (e.g., triplet loss <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22]</ref>) to optimize the similarity between samples. In our interpretation, there is no intrinsic difference between these two learning approaches. They both seek to minimize * Equal contribution. † Corresponding author.  <ref type="figure">Figure 1</ref>: Comparison between the popular optimization manner of reducing (s n −s p ) and the proposed optimization manner of reducing (α n s n − α p s p ). (a) Reducing (s n − s p ) is prone to inflexible optimization (A, B and C all have equal gradients with respect to s n and s p ), as well as ambiguous convergence status (both T and T on the decision boundary are acceptable). (b) With (α n s n − α p s p ), the Circle loss dynamically adjusts its gradients on s p and s n , and thus benefits from a flexible optimization process. For A, it emphasizes on increasing s p ; for B, it emphasizes on reducing s n . Moreover, it favors a specified point T on the circular decision boundary for convergence, setting up a definite convergence target.</p><p>between-class similarity s n , as well as to maximize withinclass similarity s p . From this viewpoint, we find that many popular loss functions (e.g., triplet loss <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22]</ref>, softmax cross-entropy loss and its variants <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b1">2]</ref>) share a similar optimization pattern. They all embed s n and s p into similarity pairs and seek to reduce (s n − s p ). In (s n − s p ), increasing s p is equivalent to reducing s n . We argue that this symmetric optimization manner is prone to the following two problems.</p><p>• Lack of flexibility for optimization. The penalty strength on s n and s p is restricted to be equal. Given the specified loss functions, the gradients with respect to s n and s p are of same amplitudes (as detailed in Section 2). In some corner cases, e.g., s p is small and s n already approaches 0 ("A" in <ref type="figure">Fig. 1 (a)</ref>), it keeps on penalizing s n with a large gradient. It is inefficient and irrational.</p><p>• Ambiguous convergence status. Optimizing (s n −s p ) usually leads to a decision boundary of s p − s n = m (m is the margin). This decision boundary allows ambiguity (e.g., "T " and "T " in <ref type="figure">Fig. 1 (a)</ref>) for convergence. For example, T has {s n , s p } = {0.2, 0.5} and T has {s n , s p } = {0.4, 0.7}. They both obtain the margin m = 0.3. However, comparing them against each other, we find the gap between s n and s p is only 0.1. Consequently, the ambiguous convergence compromises the separability of the feature space.</p><p>With these insights, we reach an intuition that different similarity scores should have different penalty strength. If a similarity score deviates far from the optimum, it should receive a strong penalty. Otherwise, if a similarity score already approaches the optimum, it should be optimized mildly. To this end, we first generalize (s n − s p ) into (α n s n − α p s p ), where α n and α p are independent weighting factors, allowing s n and s p to learn at different paces. We then implement α n and α p as linear functions w.r.t. s n and s p respectively, to make the learning pace adaptive to the optimization status: The farther a similarity score deviates from the optimum, the larger the weighting factor will be. Such optimization results in the decision boundary α n s n − α p s p = m, yielding a circle shape in the (s n , s p ) space, so we name the proposed loss function Circle loss.</p><p>Being simple, Circle loss intrinsically reshapes the characteristics of the deep feature learning from the following three aspects:</p><p>First, a unified loss function. From the unified similarity pair optimization perspective, we propose a unified loss function for two elemental learning paradigms, learning with class-level labels and with pair-wise labels.</p><p>Second, flexible optimization. During training, the gradient back-propagated to s n (s p ) will be amplified by α n (α p ). Those less-optimized similarity scores will have larger weighting factors and consequentially get larger gradients. As shown in <ref type="figure">Fig. 1 (b)</ref>, the optimization on A, B and C are different to each other.</p><p>Third, definite convergence status. On the circular decision boundary, Circle loss favors a specified convergence status ("T " in <ref type="figure">Fig. 1 (b)</ref>), as to be demonstrated in Section 3.3. Correspondingly, it sets up a definite optimization target and benefits the separability.</p><p>The main contributions of this paper are summarized as follows:</p><p>• We propose Circle loss, a simple loss function for deep feature learning. By re-weighting each similarity score under supervision, Circle loss benefits the deep feature learning with flexible optimization and definite convergence target.</p><p>• We present Circle loss with compatibility to both classlevel labels and pair-wise labels. Circle loss degenerates to triplet loss or softmax cross-entropy loss with slight modifications.</p><p>• We conduct extensive experiments on a variety of deep feature learning tasks, e.g. face recognition, person reidentification, car image retrieval and so on. On all these tasks, we demonstrate the superiority of Circle loss with performance on par with the state of the art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">A Unified Perspective</head><p>Deep feature learning aims to maximize the within-class similarity s p , as well as to minimize the between-class similarity s n . Under the cosine similarity metric, for example, we expect s p → 1 and s n → 0.</p><p>To this end, learning with class-level labels and learning with pair-wise labels are two elemental paradigms. They are conventionally considered separately and significantly differ from each other w.r.t to the loss functions. Given class-level labels, the first one basically learns to classify each training sample to its target class with a classification loss, e.g. L2-Softmax <ref type="bibr" target="#b20">[21]</ref>, Large-margin Softmax <ref type="bibr" target="#b14">[15]</ref>, Angular Softmax <ref type="bibr" target="#b15">[16]</ref>, NormFace <ref type="bibr" target="#b29">[30]</ref>, AM-Softmax <ref type="bibr" target="#b28">[29]</ref>, CosFace <ref type="bibr" target="#b31">[32]</ref>, ArcFace <ref type="bibr" target="#b1">[2]</ref>. These methods are also known as proxy-based learning, as they optimize the similarity between samples and a set of proxies representing each class. In contrast, given pair-wise labels, the second one directly learns pair-wise similarity (i.e., the similarity between samples) in the feature space and thus requires no proxies, e.g., constrastive loss <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b0">1]</ref>, triplet loss <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22]</ref>, Lifted-Structure loss <ref type="bibr" target="#b18">[19]</ref>, N-pair loss <ref type="bibr" target="#b23">[24]</ref>, Histogram loss <ref type="bibr" target="#b26">[27]</ref>, Angular loss <ref type="bibr" target="#b32">[33]</ref>, Margin based loss <ref type="bibr" target="#b37">[38]</ref>, Multi-Similarity loss <ref type="bibr" target="#b33">[34]</ref> and so on. This paper views both learning approaches from a unified perspective, with no preference for either proxy-based or pair-wise similarity. Given a single sample x in the feature space, let us assume that there are K within-class similarity scores and L between-class similarity scores associated with x. We denote these similarity scores as {s i p } (i = 1, 2, · · · , K) and {s j n } (j = 1, 2, · · · , L), respectively. To minimize each s j n as well as to maximize s i p , (∀i ∈ {1, 2, · · · , K}, ∀j ∈ {1, 2, · · · , L}), we propose a unified loss function by:</p><formula xml:id="formula_0">L uni = log 1 + K i=1 L j=1 exp(γ(s j n − s i p + m)) = log 1 + L j=1 exp(γ(s j n + m)) K i=1 exp(γ(−s i p )) ,<label>(1)</label></formula><p>in which γ is a scale factor and m is a margin for better similarity separation. Eq. 1 is intuitive. It iterates through every similarity pair to reduce (s j n − s i p ). We note that it degenerates to triplet loss or classification loss, through slight modifications.</p><p>Given class-level labels, we calculate the similarity scores between x and weight vectors w i (i = 1, 2, · · · , N ) Both triplet loss and AM-Softmax loss present the lack of flexibility for optimization. The gradients with respect to s p (left) and s n (right) are restricted to equal and undergo a sudden decrease upon convergence (the similarity pair B). For example, at A, the within-class similarity score s p already approaches 1, and still incurs a large gradient. Moreover, the decision boundaries are parallel to s p = s n , which allows ambiguous convergence. In contrast, the proposed Circle loss assigns different gradients to the similarity scores, depending on their distances to the optimum. For A (both s n and s p are large), Circle loss lays emphasis on optimizing s n . For B, since s n significantly decreases, Circle loss reduces its gradient and thus enforces a moderated penalty. Circle loss has a circular decision boundary, and promotes accurate convergence status.</p><p>(N is the number of training classes) in the classification layer. Specifically, we get (N − 1) between-class similarity scores by: s j n = w j x/( w j x ) (w j is the j-th non-target weight vector). Additionally, we get a single within-class similarity score (with the superscript omitted) s p = w y x/( w y x ). With these prerequisite, Eq. 1 degenerates to AM-Softmax <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b31">32]</ref>, an important variant of Softmax loss (i.e., softmax cross-entropy loss):</p><formula xml:id="formula_1">Lam = log 1 + N −1 j=1 exp(γ(s j n + m)) exp(−γsp) = − log exp(γ(sp − m)) exp(γ(sp − m)) + N −1 j=1 exp(γs j n ) .<label>(2)</label></formula><p>Moreover, with m = 0, Eq. 2 further degenerates to Normface <ref type="bibr" target="#b29">[30]</ref>. By replacing the cosine similarity with the inner product and setting γ = 1, it finally degenerates to Softmax loss.</p><p>Given pair-wise labels, we calculate the similarity scores between x and the other features in the minibatch. Specifically, s j n = (x j n ) x/( x j n x ) (x j n is the j-th sample in the negative sample set N ) and s i p = (</p><formula xml:id="formula_2">x i p ) x/( x i p x ) (x i p is the i-th sample in the positive sample set P). Correspondingly, K = |P|, L = |N |. Eq. 1</formula><p>degenerates to triplet loss with hard mining <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b7">8]</ref>:</p><formula xml:id="formula_3">L tri = lim γ→+∞ 1 γ L uni = lim γ→+∞ 1 γ log 1 + K i=1 L j=1 exp(γ(s j n − s i p + m)) = max s j n − s i p + m + .<label>(3)</label></formula><p>Specifically, we note that in Eq. 3, the " exp(·)" operation is utilized by Lifted-Structure loss <ref type="bibr" target="#b18">[19]</ref>, N-pair loss <ref type="bibr" target="#b23">[24]</ref>, Multi-Similarity loss <ref type="bibr" target="#b33">[34]</ref> and etc., to conduct "soft" hard mining among samples. Enlarging γ gradually reinforces the mining intensity and when γ → +∞, it results in the canonical hard mining in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>Gradient analysis. Eq. 2 and Eq. 3 show triplet loss, Softmax loss and its several variants can be interpreted as specific cases of Eq. 1. In another word, they all optimize (s n − s p ). Under the toy scenario where there are only a single s p and s n , we visualize the gradients of triplet loss and AM-Softmax loss in <ref type="figure" target="#fig_1">Fig. 2</ref> (a) and (b), from which we draw the following observations:</p><p>• First, before the loss reaches its decision boundary (upon which the gradients vanish), the gradients with respect to both s p and s n are the same to each other. The status A has {s n , s p } = {0.8, 0.8}, indicating good within-class compactness. However, A still receives a large gradient with respect to s p . It leads to a lack of flexibility during optimization.</p><p>• Second, the gradients stay (roughly) constant before convergence and undergo a sudden decrease upon convergence. The status B lies closer to the decision boundary and is better optimized, compared with A. However, the loss functions (both triplet loss and AM-Softmax loss) enforce an approximately equal penalty on A and B. It is another evidence of inflexibility.</p><p>• Third, the decision boundaries (the white dashed lines) are parallel to s n − s p = m. Any two points (e.g., T and T in <ref type="figure">Fig. 1</ref>) on this boundary have an equal similarity gap of m, and are thus of equal difficulties to achieve. In another word, loss functions minimizing (s n − s p + m) lay no preference on T or T for convergence, and are prone to ambiguous convergence. Ex-perimental evidence of this problem is to be accessed in Section 4.6.</p><p>These problems originate from the optimization manner of minimizing (s n − s p ), in which reducing s n is equivalent to increasing s p . In the following Section 3, we will transfer such an optimization manner into a more general one to facilitate higher flexibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">A New Loss Function</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Self-paced Weighting</head><p>We consider to enhance the optimization flexibility by allowing each similarity score to learn at its own pace, depending on its current optimization status. We first neglect the margin item m in Eq. 1 and transfer the unified loss function into the proposed Circle loss by:</p><formula xml:id="formula_4">L circle = log 1 + K i=1 L j=1 exp γ(α j n s j n − α i p s i p ) = log 1 + L j=1 exp(γα j n s j n ) K i=1 exp(−γα i p s i p ),<label>(4)</label></formula><p>in which α j n and α i p are non-negative weighting factors.</p><formula xml:id="formula_5">Eq. 4 is derived from Eq. 1 by generalizing (s j n −s i p ) into (α j n s j n −α i p s i p ). During training, the gradient with respect to (α j n s j n − α i p s i p ) is to be multiplied with α j n (α i p ) when back- propagated to s j n (s i p )</formula><p>. When a similarity score deviates far from its optimum (i.e., O n for s j n and O p for s i p ), it should get a large weighting factor so as to get effective update with large gradient. To this end, we define α i p and α j n in a self-paced manner:</p><formula xml:id="formula_6">α i p = [Op − s i p ] + , α j n = [s j n − On] + ,<label>(5)</label></formula><p>in which [·] + is the "cut-off at zero" operation to ensure α i p and α j n are non-negative. Discussions. Re-scaling the cosine similarity under supervision is a common practice in modern classification losses <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref>. Conventionally, all the similarity scores share an equal scale factor γ. The equal rescaling is natural when we consider the softmax value in a classification loss function as the probability of a sample belonging to a certain class. In contrast, Circle loss multiplies each similarity score with an independent weighting factor before re-scaling. It thus gets rid of the constraint of equal re-scaling and allows more flexible optimization. Besides the benefits of better optimization, another significance of such a re-weighting (or re-scaling) strategy is involved with the underlying interpretation. Circle loss abandons the interpretation of classifying a sample to its target class with a large probability. Instead, it holds a similarity pair optimization perspective, which is compatible with two learning paradigms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Within-class and Between-class Margins</head><p>In loss functions optimizing (s n − s p ), adding a margin m reinforces the optimization <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32]</ref>. Since s n and −s p are in symmetric positions, a positive margin on s n is equivalent to a negative margin on s p . It thus only requires a single margin m. In Circle loss, s n and s p are in asymmetric positions. Naturally, it requires respective margins for s n and s p , which is formulated by:</p><formula xml:id="formula_7">L circle = log 1 + L j=1 exp(γα j n (s j n − ∆n)) K i=1 exp(−γα i p (s i p − ∆p))<label>(6)</label></formula><p>in which ∆ n and ∆ p are the between-class and within-class margins, respectively.</p><p>Basically, Circle loss in Eq. 6 expects s i p &gt; ∆ p and s j n &lt; ∆ n . We further analyze the settings of ∆ n and ∆ p by deriving the decision boundary. For simplicity, we consider the case of binary classification, in which the decision boundary is achieved at α n (s n − ∆ n ) − α p (s p − ∆ p ) = 0. Combined with Eq. 5, the decision boundary is given by:</p><formula xml:id="formula_8">(sn − On + ∆n 2 ) 2 + (sp − Op + ∆p 2 ) 2 = C (7) in which C = (O n − ∆ n ) 2 + (O p − ∆ p ) 2 /4.</formula><p>Eq. 7 shows that the decision boundary is the arc of a circle, as shown in <ref type="figure">Fig. 1 (b)</ref>. The center of the circle is at s n = (O n + ∆ n )/2, s p = (O p + ∆ p )/2, and its radius equals √ C. There are five hyper-parameters for Circle loss, i.e., O p , O n in Eq. 5 and γ, ∆ p , ∆ n in Eq. 6. We reduce the hyperparameters by setting O p = 1+m, O n = −m, ∆ p = 1−m, and ∆ n = m. Consequently, the decision boundary in Eq. 7 is reduced to:</p><formula xml:id="formula_9">(s n − 0) 2 + (s p − 1) 2 = 2m 2 .<label>(8)</label></formula><p>With the decision boundary defined in Eq. 8, we have another intuitive interpretation of Circle loss. It aims to optimize s p → 1 and s n → 0. The parameter m controls the radius of the decision boundary and can be viewed as a relaxation factor. In another word, Circle loss expects s i p &gt; 1 − m and s j n &lt; m. Hence there are only two hyper-parameters, i.e., the scale factor γ and the relaxation margin m. We will experimentally analyze the impacts of m and γ in Section 4.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">The Advantages of Circle Loss</head><p>The gradients of Circle loss with respect to s j n and s i p are derived as follows:</p><formula xml:id="formula_10">∂L circle ∂s j n = Z exp γ((s j n ) 2 − m 2 ) L l=1 exp γ((s l n ) 2 − m 2 ) γ(s j n + m),<label>(9)</label></formula><p>and</p><formula xml:id="formula_11">∂L circle ∂s i p = Z exp γ((s i p − 1) 2 − m 2 ) K k=1 exp γ((s k p − 1) 2 − m 2 ) γ(s i p −1−m),<label>(10)</label></formula><p>in both of which Z = 1 − exp(−L circle ).</p><p>Under the toy scenario of binary classification (or only a single s n and s p ), we visualize the gradients under different settings of m in <ref type="figure" target="#fig_1">Fig. 2 (c)</ref>, from which we draw the following three observations:</p><p>• Balanced optimization on s n and s p . We recall that the loss functions minimizing (s n − s p ) always have equal gradients on s p and s n and is inflexible. In contrast, Circle loss presents dynamic penalty strength. Among a specified similarity pair {s n , s p }, if s p is better optimized in comparison to s n (e.g., A = {0.8, 0.8} in <ref type="figure" target="#fig_1">Fig. 2 (c)</ref>), Circle loss assigns a larger gradient to s n (and vice versa), so as to decrease s n with higher superiority. The experimental evidence of balanced optimization is to be accessed in Section 4.6.</p><p>• Gradually-attenuated gradients. At the start of training, the similarity scores deviate far from the optimum and gain large gradients (e.g., "A" in <ref type="figure" target="#fig_1">Fig. 2 (c)</ref>). As the training gradually approaches the convergence, the gradients on the similarity scores correspondingly decays (e.g., "B" in <ref type="figure" target="#fig_1">Fig. 2 (c)</ref>), elaborating mild optimization. Experimental result in Section 4.5 shows that the learning effect is robust to various settings of γ (in Eq. 6), which we attribute to the automatically-attenuated gradients.</p><p>• A (more) definite convergence target. Circle loss has a circular decision boundary and favors T rather than T <ref type="figure">(Fig. 1)</ref> for convergence. It is because T has the smallest gap between s p and s n , compared with all the other points on the decision boundary. In another word, T has a larger gap between s p and s n and is inherently more difficult to maintain. In contrast, losses that minimize (s n − s p ) have a homogeneous decision boundary, that is, every point on the decision boundary is of the same difficulty to reach. Experimentally, we observe that Circle loss leads to a more concentrated similarity distribution after convergence, as to be detailed in Section 4.6 and <ref type="figure" target="#fig_4">Fig. 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We comprehensively evaluate the effectiveness of Circle loss under two elemental learning approaches, i.e., learning with class-level labels and learning with pair-wise labels. For the former approach, we evaluate our method on face recognition (Section 4.2) and person re-identification (Section 4.3) tasks. For the latter approach, we use the fine-grained image retrieval datasets (Section 4.4), which are relatively small and encourage learning with pair-wise labels. We show that Circle loss is competent under both settings. Section 4.5 analyzes the impact of the two hyperparameters, i.e., the scale factor γ in Eq. 6 and the relaxation factor m in Eq. 8. We show that Circle loss is robust under reasonable settings. Finally, Section 4.6 experimentally confirms the characteristics of Circle loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Settings</head><p>Face recognition. We use the popular dataset MS-Celeb-1M <ref type="bibr" target="#b3">[4]</ref> for training. The native MS-Celeb-1M data is noisy and has a long-tailed data distribution. We clean the dirty samples and exclude few tail identities (≤ 3 images per identity). It results in 3.6M images and 79.9K identities. For evaluation, we adopt MegaFace Challenge 1 (MF1) <ref type="bibr" target="#b11">[12]</ref>, IJB-C <ref type="bibr" target="#b16">[17]</ref>, LFW <ref type="bibr" target="#b9">[10]</ref>, YTF <ref type="bibr" target="#b36">[37]</ref> and CFP-FP <ref type="bibr" target="#b22">[23]</ref> datasets and the official evaluation protocols are used. We also polish the probe set and 1M distractors on MF1 for more reliable evaluation, following <ref type="bibr" target="#b1">[2]</ref>. For data pre-processing, we resize the aligned face images to 112 × 112 and linearly normalize the pixel values of RGB images to [−1, 1] <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b31">32]</ref>. We only augment the training samples by random horizontal flip. We choose the popular residual networks <ref type="bibr" target="#b5">[6]</ref> as our backbones. All the models are trained with 182k iterations. The learning rate is started with 0.1 and reduced by 10× at 50%, 70% and 90% of total iterations respectively. The default hyper-parameters of our method are γ = 256 and m = 0.25 if not specified. For all the model inference, we extract the 512-D feature embeddings and use cosine distance as the metric.</p><p>Person re-identification. Person re-identification (re-ID) aims to spot the appearance of the same person in different observations. We evaluate our method on two popular datasets, i.e., Market-1501 <ref type="bibr" target="#b40">[41]</ref> and MSMT17 <ref type="bibr" target="#b34">[35]</ref>. Market-1501 contains 1,501 identities, 12,936 training images and 19,732 gallery images captured with 6 cameras. MSMT17 contains 4,101 identities, 126,411 images captured with 15 cameras and presents a long-tailed sample distribution. We adopt two network structures, i.e. a global feature learning model backboned on ResNet50 and a partfeature model named MGN <ref type="bibr" target="#b30">[31]</ref>. We use MGN with consideration of its competitive performance and relatively concise structure. The original MGN uses a Sofmax loss on each part feature branch for training. Our implementation concatenates all the part features into a single feature vector for simplicity. For Circle loss, we set γ = 128 and m = 0.25.</p><p>Fine-grained image retrieval. We use three datasets for evaluation on fine-grained image retrieval, i.e. CUB-200-2011 <ref type="bibr" target="#b27">[28]</ref>, Cars196 <ref type="bibr" target="#b13">[14]</ref> and Stanford Online Products <ref type="bibr" target="#b18">[19]</ref>. CARS-196 contains 16, 183 images which belong to 196 class of cars. The first 98 classes are used for training and the last 98 classes are used for testing. CUB-200-2010 has 200 different class of birds. We use the first 100 class with 5, 864 images for training and the last 100 class with 5, 924 images for testing. SOP is a large dataset that consists of 120, 053 images belonging to 22, 634 classes of online products. The training set contains 11, 318 class includes 59, 551 images and the rest 11, 316 class includes 60, 499 images are for testing. The experimental setup follows <ref type="bibr" target="#b18">[19]</ref>. We use BN-Inception <ref type="bibr" target="#b10">[11]</ref> as the backbone to <ref type="table">Table 1</ref>: Face identification and verification results on MFC1 dataset. "Rank 1" denotes rank-1 identification accuracy. "Veri." denotes verification TAR (True Accepted Rate) at 1e-6 FAR (False Accepted Rate) with 1M distractors. "R34" and "R100" denote using ResNet34 and ResNet100 backbones, respectively.   learn 512-D embeddings. We adopt P-K sampling trategy <ref type="bibr" target="#b7">[8]</ref> to construct mini-batch with P = 16 and K = 5. For Circle loss, we set γ = 80 and m = 0.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Face Recognition</head><p>For face recognition task, we compare Circle loss against several popular classification loss functions, i.e., vanilla Softmax, NormFace <ref type="bibr" target="#b29">[30]</ref>, AM-Softmax <ref type="bibr" target="#b28">[29]</ref> (or CosFace <ref type="bibr" target="#b31">[32]</ref>), ArcFace <ref type="bibr" target="#b1">[2]</ref>. Following the original papers <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b1">2]</ref>, we set γ = 64, m = 0.35 for AM-Softmax and γ = 64, m = 0.5 for ArcFace.</p><p>We report the identification and verification results on MegaFace Challenge 1 dataset (MFC1) in <ref type="table">Table 1</ref>. Circle loss marginally outperforms the counterparts under differ- <ref type="table">Table 4</ref>: Evaluation of Circle loss on re-ID task. We report R-1 accuracy (%) and mAP (%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Market-1501 MSMT17 The same observations also hold for the verification metric. <ref type="table" target="#tab_1">Table 2</ref> summarizes face verification results on LFW <ref type="bibr" target="#b9">[10]</ref>, YTF <ref type="bibr" target="#b36">[37]</ref> and CFP-FP <ref type="bibr" target="#b22">[23]</ref>. We note that performance on these datasets is already near saturation. Specifically, ArcFace is higher than AM-Softmax by +0.05%, +0.03%, +0.07% on three datasets, respectively. Circle loss remains the best one, surpassing ArcFace by +0.05%, +0.06% and +0.18%, respectively.</p><p>We further compare Circle loss with AM-Softmax and ArcFace on IJB-C 1:1 verification task in <ref type="table" target="#tab_2">Table 3</ref>. Under both ResNet34 and ResNet100 backbones, Circle loss presents considerable superiority. For example, with ResNet34, Circle loss significantly surpasses Arc-Face by +1.16% and +2.55% on "TAR@FAR=1e-4" and "TAR@FAR=1e-5", respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Person Re-identification</head><p>We evaluate Circle loss on re-ID task in <ref type="table">Table 4</ref>. MGN <ref type="bibr" target="#b30">[31]</ref> is one of the state-of-the-art methods and is featured for learning multi-granularity part-level features. Originally, it uses both Softmax loss and triplet loss to facilitate joint optimization. Our implementation of "MGN (ResNet50) + AM-Softmax" and "MGN (ResNet50)+ Circle loss" only use a single loss function for simplicity.</p><p>We make three observations from <ref type="table">Table 4</ref>. First, we find that Circle loss can achieve competitive re-ID accuracy against state of the art. We note that "JDGL" is slightly higher than "MGN + Circle loss" on MSMT17 <ref type="bibr" target="#b34">[35]</ref>. JDGL <ref type="bibr" target="#b41">[42]</ref> uses a generative model to augment the training data, and significantly improves re-ID over the long-tailed dataset. Second, comparing Circle loss with AM-Softmax, we observe the superiority of Circle loss, which is consistent with the experimental results on the face recognition task. Third, comparing "ResNet50 + Circle loss" against  "MGN + Circle loss", we find that part-level features bring incremental improvement to Circle loss. It implies that Circle loss is compatible with the part-model specially designed for re-ID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Fine-grained Image Retrieval</head><p>We evaluate the compatibility of Circle loss to pair-wise labeled data on three fine-grained image retrieval datasets, i.e., CUB-200-2011, Cars196, and Standford Online Products. On these datasets, majority methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b33">34]</ref> adopt the encouraged setting of learning with pairwise labels. We compare Circle loss against these stateof-the-art methods in <ref type="table" target="#tab_4">Table 5</ref>. We observe that Circle loss achieves competitive performance, on all of the three datasets. Among the competing methods, LiftedStruct <ref type="bibr" target="#b18">[19]</ref> and Multi-Simi <ref type="bibr" target="#b33">[34]</ref> are specially designed with elaborate hard mining strategies for learning with pair-wise labels. HDC <ref type="bibr" target="#b17">[18]</ref>, ABIER <ref type="bibr" target="#b19">[20]</ref> and ABE <ref type="bibr" target="#b12">[13]</ref> benefit from model ensemble. In contrast, the proposed Circle loss achieves performance on par with the state of the art, without any bells and whistles. We linearly lengthen the curves within the first 2k iterations to highlight the initial training process (in the green zone). During the early training stage, Circle loss rapidly increases s p , because s p deviates far from the optimum at the initialization and thus attracts higher optimization priority.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Impact of the Hyper-parameters</head><p>We analyze the impact of two hyper-parameters, i.e., the scale factor γ in Eq. 6 and the relaxation factor m in Eq. 8 on face recognition tasks.</p><p>The scale factor γ determines the largest scale of each similarity score. The concept of the scale factor is critical in a lot of variants of Softmax loss. We experimentally evaluate its impact on Circle loss and make a comparison with several other loss functions involving scale factors. We vary γ from 32 to 1024 for both AM-Softmax and Circle loss. For ArcFace, we only set γ to 32, 64 and 128, as it becomes unstable with larger γ in our implementation. The results are visualized in <ref type="figure" target="#fig_2">Fig. 3</ref>. Compared with AM-Softmax and ArcFace, Circle loss exhibits high robustness on γ. The main reason for the robustness of Circle loss on γ is the automatic attenuation of gradients. As the similarity scores approach the optimum during training, the weighting factors gradually decrease. Consequentially, the gradients automatically decay, leading to a moderated optimization.</p><p>The relaxation factor m determines the radius of the circular decision boundary. We vary m from −0.2 to 0.3 (with 0.05 as the interval) and visualize the results in <ref type="figure" target="#fig_2">Fig. 3  (b)</ref>. It is observed that under all the settings from −0.05 to 0.25, Circle loss surpasses the best performance of Arcface, as well as AM-Softmax, presenting a considerable degree of robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Investigation of the Characteristics</head><p>Analysis of the optimization process. To intuitively understand the learning process, we show the change of s n and s p during the whole training process in <ref type="figure" target="#fig_3">Fig. 4</ref>, from which we draw two observations:</p><p>First, at the initialization, all the s n and s p scores are small. It is because randomized features are prone to be far away from each other in the high dimensional feature space <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b6">7]</ref>. Correspondingly, s p get significantly larger weights (compared with s n ), and the optimization on s p dominates the training, incurring a fast increase in similarity values in <ref type="figure" target="#fig_3">Fig. 4</ref>. This phenomenon evidences that Circle loss maintains a flexible and balanced optimization.</p><p>Second, at the end of the training, Circle loss achieves both better within-class compactness and between-class discrepancy (on the training set), compared with AM-Softmax. Because Circle loss achieves higher performance on the testing set, we believe that it indicates better optimization.</p><p>Analysis of the convergence. We analyze the convergence status of Circle loss in <ref type="figure" target="#fig_4">Fig. 5</ref>. We investigate two issues: how the similarity pairs consisted of s n and s p cross the decision boundary during training and how they are distributed in the (s n , s p ) space after convergence. The results are shown in <ref type="figure" target="#fig_4">Fig. 5</ref>. In <ref type="figure" target="#fig_4">Fig. 5 (a)</ref>, AM-Softmax loss adopts the optimal setting of m = 0.35. In <ref type="figure" target="#fig_4">Fig. 5 (b)</ref>, Circle loss adopts a compromised setting of m = 0.325. The decision boundaries of (a) and (b) are tangent to each other, allowing an intuitive comparison. In <ref type="figure" target="#fig_4">Fig. 5 (c)</ref>, Circle loss adopts its optimal setting of m = 0.25. Comparing <ref type="figure" target="#fig_4">Fig. 5 (b)</ref> and (c) against <ref type="figure" target="#fig_4">Fig. 5 (a)</ref>, we find that Circle loss presents a relatively narrower passage on the decision boundary, as well as a more concentrated distribution for convergence (especially when m = 0.25). It indicates that Circle loss facilitates more consistent convergence for all the similarity pairs, compared with AM-Softmax loss. This phenomenon confirms that Circle loss has a more definite convergence target, which promotes the separability in the feature space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper provides two insights into the optimization process for deep feature learning. First, a majority of loss functions, including the triplet loss and popular classification losses, conduct optimization by embedding the between-class and within-class similarity into similarity pairs. Second, within a similarity pair under supervision, each similarity score favors different penalty strength, depending on its distance to the optimum. These insights result in Circle loss, which allows the similarity scores to learn at different paces. The Circle loss benefits deep feature learning with high flexibility in optimization and a more definite convergence target. It has a unified formula for two elemental learning approaches, i.e., learning with class-level labels and learning with pair-wise labels. On a variety of deep feature learning tasks, e.g., face recognition, person re-identification, and fine-grained image retrieval, the Circle loss achieves performance on par with the state of the art.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The gradients of the loss functions. (a) Triplet loss. (b) AM-Softmax loss. (c) The proposed Circle loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Impact of two hyper-parameters. In (a), Circle loss presents high robustness on various settings of scale factor γ. In (b), Circle loss surpasses the best performance of both AM-Softmax and ArcFace within a large range of relaxation factor m.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The change of s p and s n values during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>(a) AMSoftmax (m=0.35) (b) Circle loss (m=0.325) (c) Circle loss (m=0.25) Visualization of the similarity distribution after convergence. The blue dots mark the similarity pairs crossing the decision boundary during the whole training process. The green dots mark the similarity pairs after convergence. (a) AM-Softmax seeks to minimize (s n − s p ). During training, the similarity pairs cross the decision boundary through a wide passage. After convergence, the similarity pairs scatter in a relatively large region in the (s n , s p ) space. In (b) and (c), Circle loss has a circular decision boundary. The similarity pairs cross the decision boundary through a narrow passage and gather into a relatively concentrated region.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Face verification accuracy (%) on LFW, YTF and CFP-FP with ResNet34 backbone.</figDesc><table><row><cell>Loss function</cell><cell cols="3">LFW [10] YTF [37] CFP-FP [23]</cell></row><row><cell>Softmax</cell><cell>99.18</cell><cell>96.19</cell><cell>95.01</cell></row><row><cell>NormFace [30]</cell><cell>99.25</cell><cell>96.03</cell><cell>95.34</cell></row><row><cell>AM-Softmax [29, 32]</cell><cell>99.63</cell><cell>96.31</cell><cell>95.78</cell></row><row><cell>ArcFace [2]</cell><cell>99.68</cell><cell>96.34</cell><cell>95.84</cell></row><row><cell>CircleLoss(ours)</cell><cell>99.73</cell><cell>96.38</cell><cell>96.02</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison of TARs on the IJB-C 1:1 verification task.</figDesc><table><row><cell>Loss function</cell><cell cols="3">TAR@FAR (%)</cell></row><row><cell></cell><cell>1e-3</cell><cell>1e-4</cell><cell>1e-5</cell></row><row><cell>ResNet34, AM-Softmax [29, 32]</cell><cell cols="3">95.87 92.14 81.86</cell></row><row><cell>ResNet34, ArcFace [2]</cell><cell cols="3">95.94 92.28 84.23</cell></row><row><cell>ResNet34, CircleLoss(ours)</cell><cell cols="3">96.04 93.44 86.78</cell></row><row><cell>ResNet100, AM-Softmax [29, 32]</cell><cell cols="3">95.93 93.19 88.87</cell></row><row><cell>ResNet100, ArcFace [2]</cell><cell cols="3">96.01 93.25 89.10</cell></row><row><cell>ResNet100, CircleLoss(ours)</cell><cell cols="3">96.29 93.95 89.60</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison of R@K(%) on three fine-grained image retrieval datasets. Superscript denotes embedding size.</figDesc><table><row><cell>Loss function</cell><cell></cell><cell cols="2">CUB-200-2011 [28]</cell><cell></cell><cell></cell><cell cols="2">Cars196 [14]</cell><cell></cell><cell cols="4">Stanford Online Products [19]</cell></row><row><cell></cell><cell cols="4">R@1 R@2 R@4 R@8</cell><cell cols="4">R@1 R@2 R@4 R@8</cell><cell cols="4">R@1 R@10 R@10 2 R@10 3</cell></row><row><cell>LiftedStruct 64 [19]</cell><cell>43.6</cell><cell>56.6</cell><cell>68.6</cell><cell>79.6</cell><cell>53.0</cell><cell>65.7</cell><cell>76.0</cell><cell>84.3</cell><cell>62.5</cell><cell>80.8</cell><cell>91.9</cell><cell>97.4</cell></row><row><cell>HDC 384 [18]</cell><cell>53.6</cell><cell>65.7</cell><cell>77.0</cell><cell>85.6</cell><cell>73.7</cell><cell>83.2</cell><cell>89.5</cell><cell>93.8</cell><cell>69.5</cell><cell>84.4</cell><cell>92.8</cell><cell>97.7</cell></row><row><cell>HTL 512 [3]</cell><cell>57.1</cell><cell>68.8</cell><cell>78.7</cell><cell>86.5</cell><cell>81.4</cell><cell>88.0</cell><cell>92.7</cell><cell>95.7</cell><cell>74.8</cell><cell>88.3</cell><cell>94.8</cell><cell>98.4</cell></row><row><cell>ABIER 512 [20]</cell><cell>57.5</cell><cell>71.5</cell><cell>79.8</cell><cell>87.4</cell><cell>82.0</cell><cell>89.0</cell><cell>93.2</cell><cell>96.1</cell><cell>74.2</cell><cell>86.9</cell><cell>94.0</cell><cell>97.8</cell></row><row><cell>ABE 512 [13]</cell><cell>60.6</cell><cell>71.5</cell><cell>79.8</cell><cell>87.4</cell><cell>85.2</cell><cell>90.5</cell><cell>94.0</cell><cell>96.1</cell><cell>76.3</cell><cell>88.4</cell><cell>94.8</cell><cell>98.2</cell></row><row><cell>Multi-Simi 512 [34]</cell><cell>65.7</cell><cell>77.0</cell><cell>86.3</cell><cell>91.2</cell><cell>84.1</cell><cell>90.4</cell><cell>94.0</cell><cell>96.5</cell><cell>78.2</cell><cell>90.5</cell><cell>96.0</cell><cell>98.7</cell></row><row><cell>CircleLoss 512</cell><cell>66.7</cell><cell>77.4</cell><cell>86.2</cell><cell>91.2</cell><cell>83.4</cell><cell>89.8</cell><cell>94.1</cell><cell>96.5</cell><cell>78.3</cell><cell>90.5</cell><cell>96.1</cell><cell>98.6</cell></row><row><cell>Rank-1 accuracy (%) on MFC1</cell><cell></cell><cell>Rank-1 accuracy (%) on MFC1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(a) scale factor</cell><cell></cell><cell></cell><cell cols="2">(b) relaxation factor m</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep metric learning with hierarchical triplet loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ms-celeb-1m: A dataset and benchmark for large-scale face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Softmax dissection: Towards understanding intra-and inter-clas objective for embedding learning. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person re-identification</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep metric learning using triplet network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ailon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Similarity-Based Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="84" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007-10" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report 07-49</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The megaface benchmark: 1 million faces for recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4873" to="4882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Attention-based ensemble for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kwon</surname></persName>
		</author>
		<idno>Septem- ber 2018. 7</idno>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sphereface: Deep hypersphere embedding for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Large-margin softmax loss for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Iarpa janus benchmark-c: Face dataset and protocol</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Maze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Niggel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on Biometrics (ICB)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="158" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep metric learning via facility location</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="4004" to="4012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep metric learning with bier: Boosting independent embeddings robustly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Waltner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Possegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09507</idno>
		<title level="m">L2-constrained softmax loss for discriminative face verification</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Frontal to profile face verification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multi-class n-pair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep learning face representation from predicting 10,000 classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1891" to="1898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning deep embeddings with histogram loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Additive margin softmax for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="926" to="930" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Normface: L2 hypersphere embedding for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international conference on Multimedia</title>
		<meeting>the 25th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1041" to="1049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning discriminative features with multiple granularities for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia Conference on Multimedia Conference -MM 18</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cosface: Large margin cosine loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep metric learning with angular loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2612" to="2620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multi-similarity loss with general pair weighting for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Person transfer gan to bridge domain gap for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Face recognition in unconstrained videos with matched background similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Maoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Sampling matters in deep embedding learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2840" to="2848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Heated-up softmax embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<idno>abs/1809.04157</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Adacos: Adaptively scaling cosine logits for effectively learning deep face representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<idno>De- cember 2015. 5</idno>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Joint discriminative and generative learning for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
