<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ADVANCED DEEP NETWORKS FOR 3D MITOCHONDRIA INSTANCE SEGMENTATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueyi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Xiong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ADVANCED DEEP NETWORKS FOR 3D MITOCHONDRIA INSTANCE SEGMENTATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Electron microscopy</term>
					<term>mitochondria</term>
					<term>in- stance segmentation</term>
					<term>deep network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Mitochondria instance segmentation from electron microscopy (EM) images has seen notable progress since the introduction of deep learning methods. In this paper, we propose two advanced deep networks, named Res-UNet-R and Res-UNet-H, for 3D mitochondria instance segmentation from Rat and Human samples. Specifically, we design a simple yet effective anisotropic convolution block and deploy a multi-scale training strategy, which together boost the segmentation performance. Moreover, we enhance the generalizability of the trained models on the test set by adding a denoising operation as pre-processing. In the Large-scale 3D Mitochondria Instance Segmentation Challenge, our team ranks the 1st on the leaderboard at the end of the testing phase. Code is available at https://github.com/Limingxing00/ MitoEM2021-Challenge</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>As an important kind of organelle, mitochondria provide energy for cells and are of great value to the research of life science. Generally, electron microscopy (EM) images that contain recognizable mitochondria consume a huge storage, e.g., at the scale of Terabyte <ref type="bibr" target="#b0">[1]</ref>. Manual instance segmentation of mitochondria from such a large amount of data is impossible, and automatic segmentation algorithms are highly desired. As pioneer works, Lucci et al. <ref type="bibr" target="#b1">[2]</ref> propose a supervoxel-based method with learned shape features to recognize mitochondria. Seyedhosseini et al. <ref type="bibr" target="#b2">[3]</ref> use algebraic curves and a random forest classifier to segment mitochondria. Due to the limited generalizability, however, these traditional methods cannot be easily adapted to large-scale datasets such as Mi-toEM <ref type="bibr" target="#b3">[4]</ref> including both rat and human samples.</p><p>Recently, some methods based on convolutional neural networks (CNNs) have emerged for mitochondrial segmentation. For example, Oztel et al. <ref type="bibr" target="#b4">[5]</ref> propose a deep network to first segment 2D mitochondria slices and then integrate 3D information with median filtering in the axial dimension. Wei et al. <ref type="bibr" target="#b3">[4]</ref> summarize the CNN-based methods into two groups, top-down methods and bottom-up methods. Representative top-down methods use Mask-RCNN <ref type="bibr" target="#b5">[6]</ref> for instance segmen-tation. Due to the elongated and distorted shape of mitochondria, however, it is difficult to set a proper anchor size for Mask-RCNN in this task. The bottom-up methods usually predict a binary segmentation mask <ref type="bibr" target="#b6">[7]</ref>, an affinity map <ref type="bibr" target="#b7">[8]</ref>, or a binary mask with the instance boundary <ref type="bibr" target="#b8">[9]</ref>. Then a postprocessing algorithm is used to distinguish instances. Despite of the notable progress achieved, there is still a large room for improving the performance of mitochondria instance segmentation.</p><p>In this paper, we propose two advanced deep residual networks based on the 3D UNet backbone <ref type="bibr" target="#b9">[10]</ref>, named Res-UNet-R for the rat sample and Res-UNet-H for the human sample in the MitoEM dataset. Both networks generate the same form of outputs, including a semantic mask and an instance boundary. Since the human sample is more difficult (i.e., containing more noise) than the rat sample, we increase a decoder path for Res-UNet-H to predict the semantic mask and the instance boundary separately, while the decoder of Res-UNet-R has only one path. Obtaining the semantic mask and the instance boundary, we then synthesize a seed map. Finally, we adopt the connected component labeling to obtain the mitochondria instances.</p><p>To boost the segmentation performance of our networks, we design a simple yet effective anisotropic convolution block and deploy a multi-scale training strategy. The effectiveness of these two components is validated by comprehensive experiments. Moreover, we observe that noise caused by staining precipitates is sparsely distributed in the MitoEM dataset. Especially in the human sample, the noise level is subjectively stronger than that in the rat sample. To alleviate the influence of noise on segmentation, we utilize an interpolation network <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> to restore the noisy regions coarsely marked by labor. It is demonstrated that the generalizability of the trained models can be enhanced on the test set by adding this denoising operation as pre-processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Res-UNet-R and Res-UNet-H</head><p>We follow the bottom-up methods to extract the response map of mitochondria first. For the rat sample and the human sample, we propose two deep residual networks named Res-UNet-R and Res-UNet-H respectively. In the following description, we omit the exponential linear unit (ELU) after the convolutional layer for brevity.</p><p>Anisotropic Convolution Block. Since the MitoEM dataset has anisotropic resolution, we design an anisotropic convolution block (ACB) as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. After a 1 × 3 × 3 conventional layer, we cascade two 3 × 3 × 3 conventional layers to further enlarge the receptive field. At the same time, we insert the skip connection in the two 3×3×3 conventional layers.</p><p>Network Structure. The overall structure of Res-UNet-R is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. Inspired by 3D U-Net <ref type="bibr" target="#b9">[10]</ref>, we first embed the feature maps extracted from a 3D block with a 1 × 5 × 5 conventional layer. In each layer of the encoder, there is an ACB to extract the anisotropic information. Then we adopt a 1 × 3 × 3 conventional layer to downsample the feature maps in the lateral dimensions. In the decoder, we use the trilinear upsampling to restore the resolution of the feature maps and the ACB to reconstruct the detailed information. For Res-UNet-R, the decoder outputs a semantic mask and an instance boundary simultaneously. Since the human sample is of poorer imaging quality than the rat sample, we design two decoder paths for Res-UNet-H to predict the semantic mask and the instance boundary separately.</p><p>Loss Function. The binary cross entropy (BCE) is a common loss function used in biomedical image segmentation. To address the class imbalance problem, we adopt a weighted BCE (WBCE) loss as</p><formula xml:id="formula_0">L W BCE (X i , Y i ) = 1 DHW W i L BCE (X i , Y i ),<label>(1)</label></formula><p>where X i and Y i are the predicted response map and groundtruth of the i-th block, D, H, and W denote the depth, height, and width of the block, and the weight W i is defined as</p><formula xml:id="formula_1">W i = Y i + W f 1−W f (1 − Y i ) W f &gt; 0.5 1−W f W f Y i + (1 − Y i ) else<label>(2)</label></formula><p>Here W f is the foreground voxel ratio, i.e., W f = sum(Yi) DHW .</p><p>The overall loss function L is defined as</p><formula xml:id="formula_2">L = L W BCE (X M , Y M ) + L W BCE (X B , Y B ),<label>(3)</label></formula><p>where X M and X B are the predicted response maps of the semantic mask and the instance boundary respectively. Y M and Y B are the corresponding ground-truth of X M and X B .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Post-processing</head><p>Obtaining the semantic mask X M ∈ R D×H×W and the instance boundary X B ∈ R D×H×W , we can generate the seed map S j (j ∈ [1, D × H × W ]) as</p><formula xml:id="formula_3">S j = 1 X j M &gt; T 1 , X j B &lt; T 2 0 else<label>(4)</label></formula><p>where T 1 and T 2 are two thresholds. In our experiments, we set T 1 = 0.9 and T 2 = 0.8. Then we generate the seed map and adopt the connected component labeling to obtain the final mitochondria instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Denoising as Pre-processing</head><p>As mentioned above, we find that by adding a denoising operation as pre-processing on the test set, the influence of noisy regions on segmentation can be alleviated, especially for the human sample. To this end, we adopt the interpolation network initially proposed for video frame <ref type="bibr" target="#b10">[11]</ref> and also employed for EM image restoration in <ref type="bibr" target="#b11">[12]</ref>. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, the interpolation network takes the two adjacent frames of the noisy frame as input and predicts two kernels. The two adjacent frames are then convolved by the two kernels respectively, the sum of which contributes to the restored frame. <ref type="figure">Fig. 2</ref> gives visual examples of frames before and after denoising, which demonstrate the effectiveness of the denoising pre-processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Frame i Frame i-1 Frame i+1</head><p>Original After denoising <ref type="figure">Fig. 2</ref>. Visualized results before and after denoising preprocessing on the test set of MitoEM-H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset and Evaluation Metric</head><p>The MitoEM dataset <ref type="bibr" target="#b3">[4]</ref> consists of two (30 µm) <ref type="bibr" target="#b2">3</ref>  We adopt an efficient 3D metric, which evaluates AP-75 with the bounding boxes instead of voxel-wise calculation <ref type="bibr" target="#b3">[4]</ref>. In this case, at least 0.75 intersection over union (IoU) with the ground truth for a detection is required to be a true positive (TP). According to the number of mitochondrial voxels, mitochondria are divided into small, medium and large instances, with respective thresholds of 5K and 15K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Implementation Details</head><p>We adopt Pytorch (version 1.1) to implement the proposed method. Two TITAN Xp (12GB) are used for training and inference. During the training stage, we adopt the 11 data augmentation methods following <ref type="bibr" target="#b3">[4]</ref> and set the batch size as 2. The network is optimized by Adam with a fixed learning rate 0.0001. We train the network in two stages. First, we train the network in 20K iterations with the input size 32 × 256 × 256 to select the best model. Then we change the input size to 36 × 320 × 320 and fine-tune the network in 10K iterations. We call this two-stage training as multiscale training. The validation stage needs 45 GB of memory, which is mainly due to post-processing for the seed map of size 3 × 100 × 4096 × 4096.</p><p>We train the interpolation network with 3 × 256 × 256 patches in the training set. The interpolation network is responsible for predicting the middle slice of the patch supervised by the mean square error (MSE) loss. We manu-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Block unit</head><p>MitoEM-R Small Med Large ALL 3D ECA block 0.398 0.831 0.874 0.865 3D SE block 0.388 0.826 0.891 0.872 3D Res block 0.375 0.860 0.901 0.884 3D ACB 0.307 0.853 0.935 0.913 3D ACB+MT 0.277 0.850 0.949 0.917 <ref type="table">Table 1</ref>. Segmentation results on MitoEM-R validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>MitoEM-H Small Med Large ALL Res-UNet-R 0.470 0.791 0.790 0.783 Res-UNet-H 0.405 0.805 0.837 0.816 Res-UNet-H+MT 0.522 0.844 0.826 0.828 <ref type="table" target="#tab_1">Table 2</ref>. Segmentation results on MitoEM-H validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>MitoEM-R Small Med Large ALL Res-UNet-R 0.305 0.861 0.848 0.850 After denoising 0.151 0.832 0.854 0.851</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>MitoEM-H Small Med Large ALL Res-UNet-H 0.522 0.844 0.826 0.828 After denoising 0.531 0.834 0.827 0.829 <ref type="table">Table 3</ref>. Segmentation results on test sets of MitoEM-R and MitoEM-H . ally mark the coarse noisy regions on the test sets of both MitoEM-R and MitoEM-H and utilize the trained interpolation network to restore the noisy regions before inference of segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Ablation Study</head><p>We conduct comprehensive experiments to validate the effectiveness of main components in the proposed method.</p><p>Block Unit Selection. We test different block units in Table 1 on the validation set of MitoEM-R. Here 3D SE block <ref type="bibr" target="#b12">[13]</ref>, 3D ECA block <ref type="bibr" target="#b13">[14]</ref> and 3D Res block <ref type="bibr" target="#b14">[15]</ref> are modified from state-of-the-art methods for the image recognition task. In comparison with these more complex block units, our simply designed ACB alleviates overfitting and achieves the best results on the 3D mitochondria segmentation task, outperforming the second best method by 2.9%.</p><p>Network Structure. As shown in  It proves that both models need larger receptive field to avoid over-fitting. Denoising Pre-processing. As shown in <ref type="figure">Fig. 2</ref>, the noisy regions of the middle frame can be well restored by the interpolation network. We evaluate the effect of denoising preprocessing with the above trained segmentation models on the test sets of both MitoEM-R and MitoEM-H . As shown in Table 3, the AP-75 metric improves 0.1% on both test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Challenge Results</head><p>In the Large-scale 3D Mitochondria Instance Segmentation Challenge, our team ranks the 1st on the leaderboard at the end of the testing phase. As shown in <ref type="table">Table 4</ref>, the proposed method notably outperforms other competitors on both MitoEM-R and MitoEM-H test sets. We also show some visualized results from the validation set of MitoEM-R and MitoEM-H in <ref type="figure" target="#fig_1">Fig.3</ref>. It can be seen that the predicted results by the proposed method is very close to ground-truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION</head><p>In this paper, we present two advanced deep networks for 3D mitochondria instance segmentation, named Res-UNet-  <ref type="table">Table 4</ref>. MitoEM Challenge leaderboard at the end of the testing phase.</p><p>R for the rat sample and Res-UNet-H for the human sample. Specifically, we exploit a simple yet effective ACB and a multi-scale training strategy to boost the segmentation performance. Moreover, we enhance the generalizability of the trained models on the test set by adding a denoising operation as pre-processing. In the Large-scale 3D Mitochondria Instance Segmentation Challenge, our team ranks the 1st on the leaderboard at the end of the testing phase.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Network structure of Res-UNet-R. Note that the decoder of Res-UNet-H has two paths to generate the semantic mask and the instance boundary separately.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Visualized results from validation sets of MitoEM-R and MitoEM-H. models, especially for Res-UNet-H (AP-75 improves 1.2%).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>, if we train and test Res-UNet-R on MitoEM-H, the AP-75 result is 0.783. By introducing an extra decoder, Res-UNet-H improves the AP-75 result to 0.816 (3.3% increment). It verifies that Res-UNet-H can handle more difficult sample.Training Strategy. As shown inTable 1and 2, the multiscale training strategy (MT) we used is beneficial for both</figDesc><table><row><cell>Image</cell><cell>Prediction</cell><cell>Ground-truth</cell></row><row><cell>MitoEM-R</cell><cell></cell><cell></cell></row><row><cell>MitoEM-H</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dense connectomic reconstruction in layer 4 of the somatosensory cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Motta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">366</biblScope>
			<biblScope unit="issue">6469</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Supervoxel-based segmentation of mitochondria in em image stacks with learned shape features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurélien</forename><surname>Lucchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="474" to="486" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Segmentation of mitochondria in electron microscopy images using algebraic curves</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mojtaba</forename><surname>Seyedhosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Ellisman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tasdizen</surname></persName>
		</author>
		<editor>ISBI</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Mitoem dataset: Large-scale 3d mitochondria instance segmentation from em images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zudi</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>MICCAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mitochondria segmentation in electron microscopy volumes using deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ismail</forename><surname>Oztel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gozde</forename><surname>Yolcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilker</forename><surname>Ersoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filiz</forename><surname>Bunyak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BIBM</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>MICCAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kisuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Zung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viren</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H Sebastian</forename><surname>Seung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.00120</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Superhuman accuracy on the snemi3d connectomics challenge</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dcan: deep contour-aware networks for accurate gland segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Hao Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lequan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">3d u-net: learning dense volumetric segmentation from sparse annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ç</forename><surname>Özgün</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Içek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MICCAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Video frame interpolation via adaptive convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to restore sstem images from deformation and corruption</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Eca-net: Efficient channel attention for deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qilong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
