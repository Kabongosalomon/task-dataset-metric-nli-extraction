<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Trajectron++: Dynamically-Feasible Trajectory Forecasting With Heterogeneous Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salzmann</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Autonomous Systems Lab</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Ivanovic</surname></persName>
							<email>borisi@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Autonomous Systems Lab</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Punarjay</forename><surname>Chakravarty</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Ford Greenfield Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pavone</surname></persName>
							<email>pavone@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Autonomous Systems Lab</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Trajectron++: Dynamically-Feasible Trajectory Forecasting With Heterogeneous Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Trajectory Forecasting</term>
					<term>Spatiotemporal Graph Modeling</term>
					<term>Human-Robot Interaction</term>
					<term>Autonomous Driving</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Reasoning about human motion is an important prerequisite to safe and socially-aware robotic navigation. As a result, multi-agent behavior prediction has become a core component of modern human-robot interactive systems, such as self-driving cars. While there exist many methods for trajectory forecasting, most do not enforce dynamic constraints and do not account for environmental information (e.g., maps). Towards this end, we present Trajectron++, a modular, graph-structured recurrent model that forecasts the trajectories of a general number of diverse agents while incorporating agent dynamics and heterogeneous data (e.g., semantic maps). Trajectron++ is designed to be tightly integrated with robotic planning and control frameworks; for example, it can produce predictions that are optionally conditioned on ego-agent motion plans. We demonstrate its performance on several challenging real-world trajectory forecasting datasets, outperforming a wide array of state-ofthe-art deterministic and generative methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Predicting the future behavior of humans is a necessary part of developing safe human-interactive autonomous systems. Humans can naturally navigate through many social interaction scenarios because they have an intrinsic "theory of mind," which is the capacity to reason about other people's actions in terms of their mental states <ref type="bibr" target="#b12">[14]</ref>. As a result, imbuing autonomous systems with this capability could enable more informed decision making and proactive actions to be taken in the presence of other intelligent agents, e.g., in human-robot interaction scenarios. <ref type="figure" target="#fig_1">Figure 1</ref> illustrates a scenario where predicting the intent of other agents may inform an autonomous vehicle's path planning and decision making. Indeed, multi-agent behavior prediction has already become a core  component of modern robotic systems, especially in safety-critical applications like self-driving vehicles which are currently being tested in the real world and targeting widespread deployment in the near future <ref type="bibr" target="#b47">[49]</ref>.</p><p>There are many existing methods for multi-agent behavior prediction, ranging from deterministic regressors to generative, probabilistic models. However, many of them were developed without directly accounting for real-world robotic use cases; in particular, they ignore agents' dynamics constraints, the ego-agent's own motion (important to capture the interactive aspect in human-robot interaction), and a plethora of environmental information (e.g., camera images, lidar, maps) to which modern robotic systems have access. <ref type="table">Table 1</ref> provides a summary of recent state-of-the-art approaches and their consideration of such desiderata. Accordingly, in this work we are interested in developing a multi-agent behavior prediction model that <ref type="bibr" target="#b0">(1)</ref> accounts for the dynamics of the agents, and in particular of ground vehicles <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b32">34]</ref>; <ref type="bibr" target="#b1">(2)</ref> produces predictions possibly conditioned on potential future robot trajectories, useful for intelligent planning taking into account human responses; and (3) provides a generally-applicable, open, and extensible approach which can effectively use heterogeneous data about the surrounding environment. Importantly, making use of such data would allow for the incorporation of environmental information, e.g., maps, which would enable producing predictions that differ depending on the structure of the scene (e.g., interactions at an urban intersection are very different from those in an open sports field!). One method that comes close is the Trajectron <ref type="bibr" target="#b17">[19]</ref>, a multi-agent behavior model which can handle a time-varying number of agents, accounts for multimodality in human behavior (i.e., the potential for many high-level futures), and maintains a sense of interpretability in its outputs. However, the Trajectron only reasons about relatively simple vehicle models (i.e., cascaded <ref type="table">Table 1</ref>. A summary of recent state-of-the-art pedestrian (left) and vehicle (right) trajectory forecasting methods, indicating the desiderata addressed by each approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>GNA CD HD FCP OS DESIRE <ref type="bibr" target="#b28">[30]</ref> Trajectron <ref type="bibr" target="#b17">[19]</ref> S-BiGAT <ref type="bibr" target="#b25">[27]</ref> DRF-Net <ref type="bibr" target="#b20">[22]</ref> MATF <ref type="bibr" target="#b50">[53]</ref> Our Work</p><p>Method GNA CD HD FCP OS IntentNet <ref type="bibr" target="#b7">[8]</ref> PRECOG <ref type="bibr" target="#b36">[38]</ref> MFP <ref type="bibr" target="#b42">[44]</ref> NMP <ref type="bibr" target="#b48">[51]</ref> SpAGNN <ref type="bibr" target="#b6">[7]</ref> Our Work Legend: GNA = General Number of Agents, CD = Considers Dynamics, HD = Heterogeneous Data, FCP = Future-Conditional Predictions, OS = Open Source integrators) and past trajectory data (i.e., no considerations are made for added environmental information, if available).</p><p>In this work we present Trajectron++, an open and extensible approach built upon the Trajectron <ref type="bibr" target="#b17">[19]</ref> framework which produces dynamically-feasible trajectory forecasts from heterogeneous input data for multiple interacting agents of distinct semantic types. Our key contributions are twofold: First, we show how to effectively incorporate high-dimensional data through the lens of encoding semantic maps. Second, we propose a general method of incorporating dynamics constraints into learning-based methods for multi-agent trajectory forecasting. Trajectron++ is designed to be tightly integrated with downstream robotic modules, with the ability to produce trajectories that are optionally conditioned on future ego-agent motion plans. We present experimental results on a variety of datasets, which collectively demonstrate that Trajectron++ outperforms an extensive selection of state-of-the-art deterministic and generative trajectory prediction methods, in some cases achieving 60% lower average prediction error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Deterministic Regressors. Many earlier works in human trajectory forecasting were deterministic regression models. One of the earliest, the Social Forces model <ref type="bibr" target="#b14">[16]</ref>, models humans as physical objects affected by Newtonian forces (e.g., with attractors at goals and repulsors at other agents). Since then, many approaches have been applied to the problem of trajectory forecasting, formulating it as a time-series regression problem and applying methods like Gaussian Process Regression (GPR) <ref type="bibr" target="#b35">[37,</ref><ref type="bibr" target="#b46">48]</ref>, Inverse Reinforcement Learning (IRL) <ref type="bibr" target="#b29">[31]</ref>, and Recurrent Neural Networks (RNNs) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr" target="#b45">47]</ref> to good effect. An excellent review of such methods can be found in <ref type="bibr" target="#b37">[39]</ref>.</p><p>Generative, Probabilistic Approaches. Recently, generative approaches have emerged as state-of-the-art trajectory forecasting methods due to recent advancements in deep generative models <ref type="bibr" target="#b41">[43,</ref><ref type="bibr">12]</ref>. Notably, they have caused a shift from focusing on predicting the single best trajectory to producing a distribution of potential future trajectories. This is advantageous in autonomous systems as full distribution information is more useful for downstream tasks, e.g., motion planning and decision making where information such as variance can be used to make safer decisions. Most works in this category use a deep recurrent backbone architecture with a latent variable model, such as a Conditional Variational Autoencoder (CVAE) <ref type="bibr" target="#b41">[43]</ref>, to explicitly encode multimodality <ref type="bibr" target="#b28">[30,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b36">38]</ref>, or a Generative Adversarial Network (GAN) [12] to implicitly do so <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b50">53]</ref>. Common to both approach styles is the need to produce position distributions. GAN-based models can directly produce these and CVAE-based recurrent models usually rely on a bivariate Gaussian Mixture Model (GMM) to output position distributions. However, both of these output structures make it difficult to enforce dynamics constraints, e.g., non-holonomic constraints such as those arising from no side-slip conditions. Of these, the Trajectron <ref type="bibr" target="#b17">[19]</ref> and MATF <ref type="bibr" target="#b50">[53]</ref> are the best-performing CVAE-based and GAN-based models, respectively, on standard pedestrian trajectory forecasting benchmarks <ref type="bibr" target="#b34">[36,</ref><ref type="bibr" target="#b30">32]</ref>.</p><p>Accounting for Dynamics and Heterogeneous Data. There are few works that account for dynamics or make use of data modalities outside of prior trajectory information. This is mainly because standard trajectory forecasting benchmarks seldom include any other information, a fact that will surely change following the recent release of autonomous vehicle-based datasets with rich multi-sensor data [50, <ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b23">25]</ref>. As for dynamics, current methods almost exclusively reason about positional information. This does not capture dynamical constraints, however, which might lead to predictions in position space that are unrealizable by the underlying control variables (e.g., a car moving sideways). <ref type="table">Table 1</ref> provides a detailed breakdown of recent state-of-the-art approaches and their consideration of these desiderata.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Formulation</head><p>We aim to generate plausible trajectory distributions for a time-varying number N (t) of interacting agents A 1 , ..., A N (t) . Each agent A i has a semantic class S i , e.g., Car, Bus, or Pedestrian. At time t, given the state s ∈ R D of each agent and all of their histories for the previous H timesteps, which we denote as x, x = s (t−H:t) 1,...,N (t) ∈ R (H+1)×N (t)×D , as well as additional information available to each agent I (t) 1,...,N (t) , we seek a distribution over all agents' future states for the next T timesteps y = s (t+1:t+T ) 1,...,N (t) ∈ R T ×N (t)×D , which we denote as p(y | x, I).</p><p>We also assume that geometric semantic maps are available around A i 's position, M (t) i ∈ R C/r × C/r ×L , with context size C × C, spatial resolution r, and L semantic channels. Depending on the dataset, these maps can range in sophistication from simple obstacle occupancy grids to multiple layers of humanannotated semantic information (e.g., marking out sidewalks, road boundaries, and crosswalks).</p><p>We also consider the setting where we condition on an ego-agent's future motion plan, for example when evaluating responses to a set of motion primitives. In this setting, we additionally assume that we know the ego-agent's future motion plan for the next T timesteps, y R = s  </p><formula xml:id="formula_0">GRU F C + + Decoder ∫ ∫ x 1 (t-1) ŷ 1 (t+1) ŷ 1 (t+2) x 2,3 (t-1) x 4,R (t-1) x 1 (t+(T-1)) x 1 (t+T) x R (t+T) x R (t+(T-1)) M 1 (t) x 1 (t)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Trajectron++</head><p>Our approach 1 is visualized in <ref type="figure" target="#fig_3">Figure 2</ref>. At a high level, a spatiotemporal graph representation of the scene in question is created from its topology. Then, a similarly-structured deep learning architecture is generated that forecasts the evolution of node attributes, producing agent trajectories. Scene Representation. The current scene is abstracted as a spatiotemporal graph G = (V, E). Nodes represent agents and edges represent their interactions. As a result, in the rest of the paper we will use the terms "node" and "agent" interchangeably. Each node also has a semantic class matching the class of its agent (e.g., Car, Bus, Pedestrian). An edge (A i , A j ) is present in E if A i influences A j . In this work, the 2 distance is used as a proxy for whether agents are influencing each other or not. Formally, an edge is directed from A i to A j if p i − p j 2 ≤ d Sj where p i , p j ∈ R 2 are the 2D world positions of agents A i , A j , respectively, and d Sj is a distance that encodes the perception range of agents of semantic class S j . While more sophisticated methods can be used to construct edges (e.g., <ref type="bibr" target="#b45">[47]</ref>), they usually incur extra computational overhead by requiring a complete scene graph. <ref type="figure" target="#fig_3">Figure 2</ref> shows an example of this scene abstraction.</p><p>We specifically choose to model the scene as a directed graph, in contrast to an undirected one as in previous approaches <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b45">47,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b17">19]</ref>, because a directed graph can represent a more general set of scenes and interaction types, e.g., asymmetric influence. This provides the additional benefit of being able to simultaneously model agents with different perception ranges, e.g., the driver of a car looks much farther ahead on the road than a pedestrian does while walking on the sidewalk.</p><p>Modeling Agent History. Once a graph of the scene is constructed, the model needs to encode a node's current state, its history, and how it is influenced by its neighboring nodes. To encode the observed history of the modeled agent, their current and previous states are fed into a Long Short-Term Memory (LSTM) network <ref type="bibr" target="#b16">[18]</ref> with 32 hidden dimensions. Since we are interested in modeling trajectories, the inputs x = s Ideally, agent models should be chosen to best match their semantic class S i . For example, one would usually model vehicles on the road using a bicycle model <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b32">34]</ref>. However, estimating the bicycle model parameters of another vehicle from online observations is very difficult as it requires estimation of the vehicle's center of mass, wheelbase, and front wheel steer angle. As a result, in this work pedestrians are modeled as single integrators and wheeled vehicles are modeled as dynamically-extended unicycles <ref type="bibr" target="#b26">[28]</ref>, enabling us to account for key non-holonomic constraints (e.g., no side-slip constraints) <ref type="bibr" target="#b32">[34]</ref> without requiring complex online parameter estimation procedures -we will show through experiments that such a simplified model is already quite impactful on improving prediction accuracy. While the dynamically-extended unicycle model serves as an important representative example, we note that our approach can also be generalized to other dynamics models, provided its parameters can either be assumed or quickly estimated online.</p><p>Encoding Agent Interactions. To model neighboring agents' influence on the modeled agent, Trajectron++ encodes graph edges in two steps. First, edge information is aggregated from neighboring agents of the same semantic class. In this work, an element-wise sum is used as the aggregation operation. We choose to combine features in this way rather than with concatenation or an average to handle a variable number of neighboring nodes with a fixed architecture while preserving count information <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b19">21]</ref>. These aggregated states are then fed into an LSTM with 8 hidden dimensions whose weights are shared across all edge instances of the same type, e.g., all Pedestrian-Bus edge LSTMs share the same weights. Then, the encodings from all edge types that connect to the modeled node are aggregated to obtain one "influence" representation vector, representing the effect that all neighboring nodes have. For this, an additive attention module is used <ref type="bibr" target="#b1">[2]</ref>. Finally, the node history and edge influence encodings are concatenated to produce a single node representation vector, e x .</p><p>Incorporating Heterogeneous Data. Modern sensor suites are able to produce much more information than just tracked trajectories of other agents. Notably, HD maps are used by many real-world systems to aid localization as well as inform navigation. Depending on sensor availability and sophistication, maps can range in fidelity from simple binary obstacle maps, i.e., M ∈ {0, 1} H×W ×1 , to HD semantic maps, e.g., M ∈ {0, 1} H×W ×L where each layer 1 ≤ ≤ L corresponds to an area with semantic type (e.g., "driveable area," "road block," "walkway," "pedestrian crossing"). To make use of this information, for each modeled agent, Trajectron++ encodes a local map, rotated to match the agent's heading, with a Convolutional Neural Network (CNN). The CNN has 4 layers, with filters {5, 5, 5, 3} and respective strides of {2, 2, 1, 1}. These are followed by a dense layer with 32 hidden dimensions, the output of which is concatenated with the node history and edge influence representation vectors.</p><p>More generally, one can include further additional information (e.g., raw LIDAR data, camera images, pedestrian skeleton or gaze direction estimates) in this framework by encoding it as a vector and adding it to this backbone of representation vectors, e x .</p><p>Encoding Future Ego-Agent Motion Plans. Producing predictions which take into account future ego-agent motion is an important capability for robotic decision making and control. Specifically, it allows for the evaluation of a set of motion primitives with respect to possible responses from other agents. Tra-jectron++ can encode the future T timesteps of the ego-agent's motion plan y R using a bi-directional LSTM with 32 hidden dimensions. A bi-directional LSTM is used due to its strong performance on other sequence summarization tasks <ref type="bibr" target="#b4">[5]</ref>. The final hidden states are then concatenated into the backbone of representation vectors, e x .</p><p>Explicitly Accounting for Multimodality. Trajectron++ explicitly handles multimodality by leveraging the CVAE latent variable framework <ref type="bibr" target="#b41">[43]</ref>. It produces the target p(y | x) distribution by introducing a discrete Categorical latent variable z ∈ Z which encodes high-level latent behavior and allows for p(y | x) to be expressed as p(y | x) = z∈Z p ψ (y | x, z)p θ (z | x), where |Z| = 25 and ψ, θ are deep neural network weights that parameterize their respective distributions. z being discrete also aids in interpretability, as one can visualize which high-level behaviors belong to each z by sampling trajectories.</p><p>During training, a bi-directional LSTM with 32 hidden dimensions is used to encode a node's ground truth future trajectory, producing q φ (z | x, y) <ref type="bibr" target="#b41">[43]</ref>.</p><p>Producing Dynamically-Feasible Trajectories. After obtaining a latent variable z, it and the backbone representation vector e x are fed into the decoder, a 128-dimensional Gated Recurrent Unit (GRU) <ref type="bibr" target="#b9">[10]</ref>. Each GRU cell outputs the parameters of a bivariate Gaussian distribution over control actions u (t) (e.g., acceleration and steering rate). The agent's system dynamics are then integrated with the produced control actions u (t) to obtain trajectories in position space <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b44">46]</ref>. The only uncertainty at prediction time stems from Trajectron++'s output. Thus, in the case of linear dynamics (e.g., single integrators, used in this work to model pedestrians), the system dynamics are linear Gaussian. Explicitly, for a single integrator with control actions u (t) =ṗ (t) , the position mean at t + 1 is µ</p><formula xml:id="formula_1">(t+1) p = µ (t) p + µ (t) u ∆t, where µ (t)</formula><p>u is produced by Trajectron++. In the case of nonlinear dynamics (e.g., unicycle models, used in this work to model vehicles), one can still (approximately) use this uncertainty propagation scheme by linearizing the dynamics about the agent's current state and control. Full mean and covariance equations for the single integrator and dynamically-extended unicycle models are in the appendix. In contrast to exist-ing methods which directly output positions, our approach is uniquely able to guarantee that its trajectory samples are dynamically feasible by integrating an agent's dynamics with the predicted controls.</p><p>Output Configurations. Based on the desired use case, Trajectron++ can produce many different outputs. The main four are outlined below.</p><p>1. Most Likely (ML): The model's deterministic and most-likely single output. The high-level latent behavior mode and output trajectory are the modes of their respective distributions, where</p><formula xml:id="formula_2">z mode = arg max z p θ (z | x), y = arg max y p ψ (y | x, z mode ).<label>(1)</label></formula><p>2. z mode : Predictions from the model's most-likely high-level latent behavior mode, where</p><formula xml:id="formula_3">z mode = arg max z p θ (z | x), y ∼ p ψ (y | x, z mode ).<label>(2)</label></formula><p>3. Full : The model's full sampled output, where z and y are sampled sequentially according to</p><formula xml:id="formula_4">z ∼ p θ (z | x), y ∼ p ψ (y | x, z).<label>(3)</label></formula><p>4. Distribution: Due to the use of a discrete latent variable and Gaussian output structure, the model can provide an analytic output distribution by directly</p><formula xml:id="formula_5">computing p(y | x) = z∈Z p ψ (y | x, z)p θ (z | x).</formula><p>Training the Model. We adopt the InfoVAE <ref type="bibr" target="#b49">[52]</ref> objective function, and modify it to use discrete latent states in a conditional formulation (since the model uses a CVAE). Formally, we aim to solve</p><formula xml:id="formula_6">max φ,θ,ψ N i=1 E z∼q φ (·|xi,yi) log p ψ (y i | x i , z) − βD KL q φ (z | x i , y i ) p θ (z | x i ) + αI q (x; z),<label>(4)</label></formula><p>where I q is the mutual information between x and z under the distribution q φ (x, z). To compute I q , we follow <ref type="bibr" target="#b49">[52]</ref> and approximate q φ (z | x i , y i ) with p θ (z | x i ), obtaining the unconditioned latent distribution by summing out x i over the batch. Notably, the Gumbel-Softmax reparameterization <ref type="bibr" target="#b21">[23]</ref> is not used to backpropagate through the Categorical latent variable z because it is not sampled during training time. Instead, the first term of Equation <ref type="formula" target="#formula_6">(4)</ref> is directly computed since the latent space has only |Z| = 25 discrete elements. Additional training details can be found in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Our method is evaluated on three publicly-available datasets: The ETH <ref type="bibr" target="#b34">[36]</ref>, UCY <ref type="bibr" target="#b30">[32]</ref>, and nuScenes <ref type="bibr" target="#b5">[6]</ref> datasets. The ETH and UCY datasets consist of real pedestrian trajectories with rich multi-human interaction scenarios captured at 2.5 Hz (∆t = 0.4s). In total, there are 5 sets of data, 4 unique scenes, and 1536 unique pedestrians. They are a standard benchmark in the field, containing challenging behaviors such as couples walking together, groups crossing each other, and groups forming and dispersing. However, they only contain pedestrians, so we also evaluate on the recently-released nuScenes dataset. It is a large-scale dataset for autonomous driving with 1000 scenes in Boston and Singapore. Each scene is annotated at 2 Hz (∆t = 0.5s) and is 20s long, containing up to 23 semantic object classes as well as HD semantic maps with 11 annotated layers.</p><p>Trajectron++ was implemented in PyTorch <ref type="bibr" target="#b33">[35]</ref> on a desktop computer running Ubuntu 18.04 containing an AMD Ryzen 1800X CPU and two NVIDIA GTX 1080 Ti GPUs. We trained the model for 100 epochs (∼ 3 hours) on the pedestrian datasets and 12 epochs (∼ 8 hours) on the nuScenes dataset.</p><p>Evaluation Metrics. As in prior work <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b50">53]</ref>, our method for trajectory forecasting is evaluated with the following four error metrics:</p><p>1. Average Displacement Error (ADE): Mean 2 distance between the ground truth and predicted trajectories.</p><p>2. Final Displacement Error (FDE): 2 distance between the predicted final position and the ground truth final position at the prediction horizon T .</p><p>3. Kernel Density Estimate-based Negative Log Likelihood (KDE NLL): Mean NLL of the ground truth trajectory under a distribution created by fitting a kernel density estimate on trajectory samples <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b43">45]</ref>.</p><p>4. Best-of-N (BoN): The minimum ADE and FDE from N randomly-sampled trajectories. We compare our method to an exhaustive set of state-of-the art deterministic and generative approaches.</p><p>Deterministic Baselines. Our method is compared against the following deterministic baselines: (1) Linear : A linear regressor with parameters estimated by minimizing least square error. (2) LSTM : An LSTM network with only agent history information. (3) Social LSTM <ref type="bibr" target="#b0">[1]</ref>: Each agent is modeled with an LSTM and nearby agents' hidden states are pooled at each timestep using a proposed social pooling operation. (4) Social Attention <ref type="bibr" target="#b45">[47]</ref>: Same as <ref type="bibr" target="#b0">[1]</ref>, but all other agents' hidden states are incorporated via a proposed social attention operation.</p><p>Generative Baselines. On the ETH and UCY datasets, our method is compared against the following generative baselines: (1) S-GAN <ref type="bibr" target="#b11">[13]</ref>: Each agent is modeled with an LSTM-GAN, which is an LSTM encoder-decoder whose outputs are the generator of a GAN. The generated trajectories are then evaluated against the ground truth trajectories with a discriminator. : An LSTM-CVAE encoder-decoder which is explicitly constructed to match the spatiotemporal structure of the scene. Its scene abstraction is similar to ours, but uses undirected edges.</p><p>On the nuScenes dataset, the following methods are also compared against: (5) Convolutional Social Pooling (CSP) <ref type="bibr" target="#b10">[11]</ref>: An LSTM-based approach which explicitly considers a fixed number of movement classes and predicts which of Evaluation Methodology. For the ETH and UCY datasets, a leave-one-out strategy is used for evaluation, similar to previous works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b50">53]</ref>, where the model is trained on four datasets and evaluated on the held-out fifth. An observation length of 8 timesteps (3.2s) and a prediction horizon of 12 timesteps (4.8s) is used for evaluation. For the nuScenes dataset, we split off 15% of the train set for hyperparameter tuning and test on the provided validation set.</p><p>Throughout the following, we report the performance of Trajectron++ in multiple configurations. Specifically, Ours refers to the base model using only node and edge encoding, trained to predict agent velocities and Euler integrating velocity to produce positions; Ours+ is the base model with dynamics integration, trained to predict control actions and integrating the agent's dynamics with the control actions to produce positions; Ours+ , M additionally includes the map encoding CNN; and Ours+ , M, y R adds the robot future encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">ETH and UCY Datasets</head><p>Our approach is first evaluated on the ETH <ref type="bibr" target="#b34">[36]</ref> and UCY <ref type="bibr" target="#b30">[32]</ref> Pedestrian Datasets, against deterministic methods on standard trajectory forecasting met- rics. It is difficult to determine the current state-of-the-art in deterministic methods as there are contradictions between the results reported by the same authors in <ref type="bibr" target="#b11">[13]</ref> and <ref type="bibr" target="#b0">[1]</ref>. In <ref type="table">Table 1</ref> of <ref type="bibr" target="#b0">[1]</ref>, Social LSTM convincingly outperforms a baseline LSTM without pooling. However, in <ref type="table">Table 1</ref> of <ref type="bibr" target="#b11">[13]</ref>, Social LSTM is actually worse than the same baseline on average. Thus, when comparing against Social LSTM we report the results summarized in <ref type="table">Table 1</ref> of <ref type="bibr" target="#b11">[13]</ref> as it is the most recent work by the same authors. Further, the values reported by Social Attention in <ref type="bibr" target="#b45">[47]</ref> seem to have unusually high ratios of FDE to ADE. Nearly every other method (including ours) has FDE/ADE ratios around 2 − 3× whereas Social Attention's are around 3 − 12×. Social Attention's errors on the Univ dataset are especially striking, as its FDE of 3.92 is 12× its ADE of 0.33, meaning its prediction error on the other 11 timesteps is essentially zero. We still compare against the values reported in <ref type="bibr" target="#b45">[47]</ref> as there is no publicly-released code, but this raises doubts of their validity. To fairly compare against prior work, neither map encoding nor future motion plan encoding is used. Only the node history and edge encoders are used in the model's encoder. Additionally, the model's deterministic ML output scheme is employed, which produces the model's most likely single trajectory. <ref type="table" target="#tab_0">Table 2</ref> (a) summarizes these results and shows that our approach is competitive with state-of-the-art deterministic regressors on displacement error metrics (outperforming existing approaches by 33% on mean FDE), even though our method was not originally trained to minimize this. It makes sense that the model performs similarly with and without dynamics integration for pedestrians, since they are modeled as single integrators. Thus, their control actions are velocities which matches the base model's output structure.</p><p>To more concretely compare generative methods, we use the KDE-based NLL metric proposed in <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b43">45]</ref>, an approach that maintains full output distributions and compares the log-likelihood of the ground truth under different methods' outputs. <ref type="table" target="#tab_1">Table 3</ref> summarizes these results and shows that our method significantly outperforms others. This is also where the performance improvements brought by the dynamics integration scheme are clear. It yields the best performance because the model is now explicitly trained on the distribution it is seeking to output (the loss function term p ψ (y|x, z) is now directly over positions), whereas the base model is trained on velocity distributions, the in- tegration of which (with no accounting for system dynamics) introduces errors. Unfortunately, at this time there are no publicly-released models for SoPhie <ref type="bibr" target="#b38">[40]</ref> or MATF <ref type="bibr" target="#b50">[53]</ref>, so they cannot be evaluated with the KDE-based NLL metric. Instead, we evaluate Trajectron++ with the Best-of-N metric used in their works. <ref type="table" target="#tab_0">Table 2</ref> (b) summarizes these results, and shows that our method significantly ourperforms the state-of-the-art <ref type="bibr" target="#b50">[53]</ref>, achieving 55 − 60% lower average errors. Map Encoding. To evaluate the effect of incorporating heterogeneous data, we compare the performance of Trajectron++ with and without the map encoder. Specifically, we compare the frequency of obstacle violations in 2000 trajectory samples from the Full model output on the ETH -University scene, which provides a simple binary obstacle map. Overall, our approach generates colliding predictions 1.0% of the time with map encoding, compared to 4.6% without map encoding. We also study how much of a reduction there is for pedestrians that are especially close to an obstacle (i.e. they have at least one obstacle-violating trajectory in their Full output), an example of which is shown in the appendix. In this regime, our approach generates colliding predictions 4.9% of the time with map encoding, compared to 21.5% without map encoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">nuScenes Dataset</head><p>To further evaluate the model's ability to use heterogeneous data and simultaneously model multiple semantic classes of agents, we evaluate it on the nuScenes dataset <ref type="bibr" target="#b5">[6]</ref>. Again, the deterministic ML output scheme is used to fairly compare with other single-trajectory predictors. The trajectories of both Pedestrians and Cars are forecasted, two semantic object classes which account for most of the 23 possible object classes present in the dataset. To obtain an estimate of prediction quality degradation over time, we compute the model's FDE at t = {1, 2, 3, 4}s for all tracked objects with at least 4s of available future data. We also implement a constant velocity baseline, which simply maintains the agent's heading and speed for the prediction horizon. <ref type="table" target="#tab_2">Table 4</ref> (a) summarizes the model's performance in comparison with state-of-the-art vehicle trajectory prediction models. @1s @2s @3s @4s @1s @2s @3s @4s @1s @2s @3s @4s @1s @2s @3s @4s @1s @2s @3s @4s @1s @2s @3s @4s - Since other methods use a detection/tracking module (whereas ours does not), to establish a fair comparison we subtracted other methods' detection and tracking error from their reported values. The dynamics integration scheme and map encoding yield a noticeable improvement with vehicles, as their dynamicallyextended unicycle dynamics now differ from the single integrator assumption made by the base model. Note that our method was only trained to predict 3s into the future, thus its performance at 4s also provides a measure of its capability to generalize beyond its training configuration. Other methods do not report values at 2s and 4s. As can be seen, Trajectron++ outperforms existing approaches without facing a sharp degradation in performance after 3s. Our approach's performance on pedestrians is reported in <ref type="table" target="#tab_2">Table 4</ref> (b), where the inclusion of HD maps and dynamics integration similarly improve performance as in the pedestrian datasets. Ablation Study. To develop an understanding of which model components influence performance, a comprehensive ablation study is performed in <ref type="table" target="#tab_3">Table 5</ref>. As can be seen in the first row, even the base model's deterministic ML output performs strongly relative to current state-of-the-art approaches for vehicle trajectory forecasting <ref type="bibr" target="#b6">[7]</ref>. Adding the dynamics integration scheme yields a drastic reduction in NLL as well as FDE at all prediction horizons. There is also an associated slight increase in the frequency of road boundary-violating predictions. This is a consequence of training in position (as opposed to velocity) space, which yields more variability in the corresponding predictions. Additionally including map encoding maintains prediction accuracy while reducing the frequency of boundary-violating predictions.</p><p>The effect of conditioning on the ego-vehicle's future motion plan is also studied, with results summarized in <ref type="table" target="#tab_3">Table 5</ref> (b). As one would expect, providing the model with future motion plans of the ego-vehicle yields significant reductions in error and road boundary violations. This use-case is common throughout au- tonomous driving as the ego-vehicle repeatedly produces future motion plans at every timestep by evaluating motion primitives. Overall, dynamics integration is the dominant performance-improving module.</p><p>Qualitative Comparison. <ref type="figure" target="#fig_8">Figure 3</ref> shows trajectory predictions from the base model, with dynamics integration, and with dynamics integration + map encoding. In it, one can see that the base model (predicting in velocity space) undershoots the turn for the red car, predicting that it will end up in oncoming traffic. With the integration of dynamics, the model captures multimodality in the agent's action, predicting both the possibility of a right turn and continuing straight. With the addition of map encoding, the predictions are not only more accurate, but nearly all probability mass now lies within the correct side of the road. This is in contrast to versions of the model without map encoding which predict that the red car might move into oncoming traffic.</p><p>Online Runtime. A key consideration in robotics is runtime complexity. As a result, we evaluate the time it takes Trajectron++ to perform forward inference on commodity hardware. The results are summarized in the appendix, and confirm that our model scales well to scenes with many agents and interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we present Trajectron++, a generative multi-agent trajectory forecasting approach which uniquely addresses our desiderata for an open, generallyapplicable, and extensible framework. It can incorporate heterogeneous data beyond prior trajectory information and is able to produce future-conditional predictions that respect dynamics constraints, all while producing full probability distributions, which are especially useful in downstream robotic tasks such as motion planning, decision making, and control. It achieves state-of-the-art prediction performance in a variety of metrics on standard and new real-world multi-agent human behavior datasets.</p><p>Acknowledgment. Tim Salzmann is supported by a fellowship within the IFI programme of the German Academic Exchange Service (DAAD). We thank Matteo Zallio for his help in visually communicating our work and Amine Elhafsi for sharing his dynamics knowledge and proofreading. We also thank Brian Yao for improving our pedestrian dataset evaluation script, Osama Makansi for improving our result reporting, and Lu Zhang for correcting typos in our references. This work was supported in part by the Ford-Stanford Alliance. This article solely reflects the opinions and conclusions of its authors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Dynamically-Extended Unicycle Distribution Integration</head><p>Usually, unicycle models have velocity and heading rate as control inputs <ref type="bibr" target="#b27">[29,</ref><ref type="bibr" target="#b32">34]</ref>. However, vehicles in the real world are controlled by accelerator pedals and so we instead adopt the dynamically-extended unicycle model which instead uses acceleration a and heading rate ω as control inputs <ref type="bibr" target="#b26">[28]</ref>. The dynamicallyextended unicycle model has the following nonlinear continuous-time dynamics</p><formula xml:id="formula_7">   ẋ yφ v     =     v cos(φ) v sin(φ) ω a     ,<label>(8)</label></formula><p>where p = [x, y] T defines the position, v the speed, and φ the heading. As mentioned above, the control inputs are u = [ω, a] T . To discretize this, we assume a zero-order hold on the controls between each sampling step (i.e. control actions are piece-wise constant). This yields the following zero-order hold discrete equivalent dynamics</p><formula xml:id="formula_8">    x (t+1) y (t+1) φ (t+1) v (t+1)     =     x (t) y (t) φ (t) v (t)     +      v (t) · D (t) S + a (t) sin(φ (t) +ω (t) ∆t)∆t ω (t) + a (t) ω (t) · D (t) C −v (t) · D (t) C − a (t) cos(φ (t) +ω (t) ∆t)∆t ω (t) + a (t) ω (t) · D (t) S ω (t) ∆t a (t) ∆t      , where D (t) S = sin(φ (t) + ω (t) ∆t) − sin(φ (t) ) ω (t) D (t) C = cos(φ (t) + ω (t) ∆t) − cos(φ (t) ) ω (t) .<label>(9)</label></formula><p>We will refer to these dynamics in short with s (t+1) = f (s (t) , u (t) ). We adopt a slightly different set of dynamics when |ω| ≤ = 10 −3 to avoid singularities in Equation <ref type="bibr" target="#b8">(9)</ref>. With a small ω, we instead use the following dynamics, obtained by evaluating the limit as ω → 0.</p><formula xml:id="formula_9">    x (t+1) y (t+1) φ (t+1) v (t+1)     =     x (t) y (t) φ (t) v (t)     +     v (t) cos(φ (t) )∆t + 0.5a (t) cos(φ (t) )(∆t) 2 v (t) sin(φ (t) )∆t + 0.5a (t) sin(φ (t) )(∆t) 2 0 a (t) ∆t     .<label>(10)</label></formula><p>Thus, the full discrete-time dynamics are</p><formula xml:id="formula_10">    x (t+1) y (t+1) φ (t+1) v (t+1)     = Equation (9) if |ω| &gt; Equation (10) otherwise .<label>(11)</label></formula><p>At each timestep, and for a specific latent value z, Trajectron++ produces a Gaussian distribution over control actions N (µ u , Σ u ). Specifically, it outputs</p><formula xml:id="formula_11">µ u = µ ω µ a Σ u = σ 2 ω ρ ωa σ ω σ a ρ ωa σ ω σ a σ 2 a ,<label>(12)</label></formula><p>where µ ω is the mean rate of change of the agent's heading, µ a is the mean acceleration in the agent's heading direction, σ ω is the standard deviation of the heading rate of change, σ a is the acceleration standard deviation, and ρ ωa is the correlation between ω and a. The controls µ u and uncertainties Σ u are then integrated through the dynamics to obtain the following mean and covariance integration equations <ref type="bibr" target="#b44">[46]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Mean Derivation 3</head><p>The output mean positions are obtained by applying the mean control actions to Equation (11) <ref type="bibr" target="#b44">[46]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Covariance Derivation 3</head><p>Since Σ u is the only source of uncertainty in the prediction model, Equation <ref type="formula" target="#formula_2">(11)</ref> can be made a linear Gaussian system by linearizing about a specific state and control. For instance, the Jacobians F and G of the system dynamics in Equation (9) are Then, applying the equations for the covariance of a sum of Gaussian random variables <ref type="bibr" target="#b44">[46]</ref> yields</p><formula xml:id="formula_12">F (t) = ∂f ∂µ (t) s =      1 0 v (t) D (t) C − a (t) D (t) S ω (t) + a (t) cos(φ (t) +ω (t) ∆t)∆t ω (t) D (t) S 0 1 v (t) D (t) S + a (t) D (t) C ω (t) + a (t) sin(φ (t) +ω (t) ∆t)∆t ω (t) −D (t) C 0 0 1 0 0 0 0 1      G (t) = ∂f ∂µ (t) u =      G (t) 11 D (t) C ω (t) + sin(φ (t) +ω (t) ∆t)∆t ω (t) G (t) 21 D (t) S ω (t) − cos(φ (t) +ω (t) ∆t)∆t ω (t) ∆t 0 0 ∆t      , where G (t) 11 = v cos(φ + ω∆t)∆t ω − vD S ω − 2a sin(φ + ω∆t)∆t ω 2 − 2aD C ω 2 + a cos(φ + ω∆t)(∆t) 2 ω G (t) 21 = v sin(φ + ω∆t)∆t ω + vD C ω + 2a cos(φ + ω∆t)∆t ω 2 − 2aD S ω 2 + a sin(φ + ω∆t)(∆t) 2 ω .<label>(13)</label></formula><formula xml:id="formula_13">Σ (t+1) p,θ,v = F (t) Σ (t) p,θ,v F (t) T + G (t) Σ (t) u G (t) T .<label>(14)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Average and Final Displacement Error Evaluation</head><p>While ADE and FDE are important metrics for deterministic, single-trajectory methods, any deeper probabilistic information available from generative methods is destroyed when taking the mean over the dataset. Instead, in the main body of the paper we focus on evaluation methods which maintain such information. However, we can somewhat directly compare deterministic and generative methods using ADE and FDE by directly plotting the full error distributions for any generative methods, as in <ref type="bibr" target="#b17">[19]</ref>. This provides an idea as to how close and concentrated the predictions are around the ground truth. <ref type="figure">Figure 4</ref> shows both generative and deterministic methods' ADE and FDE performance. In both metrics, our method's error distribution is lower and more concentrated than other generative approaches, even outperforming state-of-the-art deterministic methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Additional Training Information</head><p>D.1 Choosing α, β in Equation <ref type="formula" target="#formula_6">(4)</ref> As shown in <ref type="bibr" target="#b15">[17]</ref>, the β parameter weighting the KL penalty term is important to disentangle the latent space and encourage multimodality. A good value for this hyperparameter varies with the the size of input y, condition x, and latent space z. Therefore, we adjust β depending on the size of the encoder's output e x . For example, we increase the value of β when encoding map information in the condition. Additionally, β is annealed following an increasing sigmoid <ref type="bibr" target="#b3">[4]</ref>. Thus, a low β factor is used during early training iterations so that the model learns to encode as much information in z as possible. As training continues, β is gradually increased to shift the role of information encoding from q φ (z | x, y) to p θ (z | x). For α, we found that a constant value of 1.0 works well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Separate Map Encoder Learning Rate</head><p>When used, we train the map encoding CNN with a smaller learning rate compared to the rest of the model, to prevent large gradients in early training iterations. We use leaky ReLU activation functions with α = 0.2 to prevent saturation during early training iterations (when the CNN does not provide useful encodings to the rest of the model). We found that regular ReLU, sigmoid, and tanh activation functions saturate during early training and fail to recover.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Data Augmentation</head><p>To avoid overfitting to environment-specific characteristics, such as the general directions that agents move, we augment the data from each scene. We rotate all trajectories in a scene around the scene's origin by γ, where γ varies from 0 • to 360 • (exclusive) in 15 • intervals. The benefits of dataset augmentation by trajectory rotation have already been studied for pedestrians <ref type="bibr" target="#b40">[42]</ref>. We apply this same augmentation to autonomous driving datasets as most of them are recorded in cities whose streets are roughly orthogonal and separated by blocks.  <ref type="figure">Figure 6</ref> illustrates how Trajectron++'s runtime scales with respect to problem size. In particular, a heatmap is used as there are two major factors that affect the model's runtime: number of agents and amount of interactions. For points with insufficient data, e.g., the rare case of 50 agents in a scene with only 3 interactions, we impute values using an optimization-based scheme <ref type="bibr" target="#b13">[15]</ref>. To achieve this real-time performance, we leverage the stateful representation that spatiotemporal graphs provide. Specifically, Trajectron++ is updated online with new information without fully executing a forward pass. This is possible due to our method's use of LSTMs, as only the last LSTM cells in the encoder need to be fed the newly-observed data. The rest of the model can then be executed using the updated encoder representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E ETH Pedestrians Map Encoding</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Online Runtime</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Equal contribution. † Work done as a visiting student in the Autonomous Systems Lab. arXiv:2001.03093v5 [cs.RO] 13 Jan 2021</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Exemplary road scene depicting pedestrians crossing a road in front of a vehicle which may continue straight or turn right. The graph representation of the scene is shown on the ground, where each agent and their interactions are represented as nodes and edges, visualized as white circles and dashed black lines, respectively. Arrows depict potential future agent velocities, with colors representing different high-level future behavior modes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>,M,y R ) q (z|x,y,M,y R ) [e x,R ;z;y (t) ] [e x,R ;z;ŷ (t+1) Left: Our approach represents a scene as a directed spatiotemporal graph. Nodes and edges represent agents and their interactions, respectively. Right: The corresponding network architecture for Node 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>..,N (t) ∈ R (H+1)×N (t)×D are the current and previous D-dimensional states of the modeled agents. These are typically positions and velocities, which can be easily estimated online.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>( 2 )</head><label>2</label><figDesc>SoPhie [40]: An LSTM-GAN with the addition of a proposed physical and social attention module. (3) MATF [53]: An LSTM-GAN model that leverages CNNs to fuse agent relationships and encode environmental information. (4) Trajectron [19]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>−4.26 −2.86 −1.76 −0.87 0.07 0.44 1.09 2.09 0.3 0.6 2.8 7.6 −3.90 −2.76 −1.75 −0.93 0.08 0.34 0.81 1.50 0.3 0.5 1.6 4.2 Legend: = Integration via Dynamics, M = Map Encoding, y R = Robot Future Encoding</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 3 .</head><label>3</label><figDesc>[nuScenes] The same scene as forecast by three versions of Trajectron++.(a) The base model tends to under-shoot turns, and makes overly-confident predictions. (b) Our approach better captures position uncertainty with dynamics integration, producing well-calibrated probabilities. (c) The model is able to leverage the additional information that a map provides, yielding accurate predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 5 .</head><label>5</label><figDesc>Left: When only using trajectory data, Trajectron++ does not know of obstacles and makes predictions into walls (in red). Right: Encoding a local map of the agent's surroundings significantly reduces the frequency of obstacle-violating predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 5 Fig. 6 .</head><label>56</label><figDesc>shows an example of the reduction in the number of obstacle violations for pedestrians in the ETH -University scene that are especially close to an obstacle (i.e. they have at least one obstacle-violating trajectory in their Full output). not possible as it would need more interactions than a complete graph of agents Model Runtime (s) Mean time for Trajectron++ to generate future trajectory distributions on a laptop with a 2.7 GHz Intel Core i5 (Broadwell) CPU and 8 GB of RAM. None of the runtimes exceed 1.2s (with most at or below 1s), enabling the model to run 3 − 5× per prediction horizon for all tested datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>(a) Our model's deterministic Most Likely output outperforms other deterministic methods on displacement error metrics, even if it was not originally trained to do so. (b) Our model's probabilistic Full output significantly outperforms other methods, yielding accurate predictions even in a small number of samples. Lower is better. Bold indicates best.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell></cell><cell cols="2">(a) ADE/FDE (m)</cell><cell></cell></row><row><cell></cell><cell>Linear</cell><cell>LSTM</cell><cell cols="4">S-LSTM [13] S-ATTN [47] Ours (ML) Ours+ (ML)</cell></row><row><cell>ETH</cell><cell cols="2">1.33/2.94 1.09/2.41</cell><cell>1.09/2.35</cell><cell cols="2">0.39/3.74 0.71/1.66</cell><cell>0.71/1.68</cell></row><row><cell>Hotel</cell><cell cols="2">0.39/0.72 0.86/1.91</cell><cell>0.79/1.76</cell><cell cols="2">0.29/2.64 0.22/0.46</cell><cell>0.22/0.46</cell></row><row><cell>Univ</cell><cell cols="2">0.82/1.59 0.61/1.31</cell><cell>0.67/1.40</cell><cell>0.33/3.92</cell><cell>0.44/1.17</cell><cell>0.41/1.07</cell></row><row><cell>Zara 1</cell><cell cols="2">0.62/1.21 0.41/0.88</cell><cell>0.47/1.00</cell><cell cols="2">0.20/0.52 0.30/0.79</cell><cell>0.30/0.77</cell></row><row><cell>Zara 2</cell><cell cols="2">0.77/1.48 0.52/1.11</cell><cell>0.56/1.17</cell><cell cols="2">0.30/2.13 0.23/0.59</cell><cell>0.23/0.59</cell></row><row><cell cols="3">Average 0.79/1.59 0.70/1.52</cell><cell>0.72/1.54</cell><cell>0.30/2.59</cell><cell>0.38/0.93</cell><cell>0.37/0.91</cell></row><row><cell>Dataset</cell><cell></cell><cell cols="4">(b) ADE/FDE, Best of 20 Samples (m)</cell></row><row><cell></cell><cell cols="6">S-GAN [13] SoPhie [40] Trajectron [19] MATF [53] Ours (Full) Ours+ (Full)</cell></row><row><cell>ETH</cell><cell cols="2">0.81/1.52 0.70/1.43</cell><cell>0.59/1.14</cell><cell cols="2">1.01/1.75 0.39/0.83</cell><cell>0.43/0.86</cell></row><row><cell>Hotel</cell><cell cols="2">0.72/1.61 0.76/1.67</cell><cell>0.35/0.66</cell><cell>0.43/0.80</cell><cell>0.12/0.21</cell><cell>0.12/0.19</cell></row><row><cell>Univ</cell><cell cols="2">0.60/1.26 0.54/1.24</cell><cell>0.54/1.13</cell><cell>0.44/0.91</cell><cell>0.20/0.44</cell><cell>0.22/0.43</cell></row><row><cell>Zara 1</cell><cell cols="2">0.34/0.69 0.30/0.63</cell><cell>0.43/0.83</cell><cell>0.26/0.45</cell><cell>0.15/0.33</cell><cell>0.17/0.32</cell></row><row><cell>Zara 2</cell><cell cols="2">0.42/0.84 0.38/0.78</cell><cell>0.43/0.85</cell><cell cols="2">0.26/0.57 0.11/0.25</cell><cell>0.12/0.25</cell></row><row><cell cols="3">Average 0.58/1.18 0.54/1.15</cell><cell>0.47/0.92</cell><cell cols="2">0.48/0.90 0.19/0.41</cell><cell>0.21/0.41</cell></row></table><note>Legend: = Integration via Dynamics, M = Map Encoding, y R = Robot Future Encoding those the modeled agent is likely to take. (6) CAR-Net [41]: An LSTM-based approach which encodes scene context with visual attention. (7) SpAGNN [7]: A CNN encodes raw LIDAR and semantic map data to produce object detections, from which a Graph Neural Network (GNN) produces probabilistic, interaction- aware trajectories.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Mean KDE-based NLL for each dataset. Lower is better. 2000 trajectories were sampled per model at each prediction timestep. Bold indicates the best values.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell cols="2">KDE NLL</cell><cell></cell></row><row><cell></cell><cell cols="4">S-GAN [13] Trajectron [19] Ours (Full) Ours+ (Full)</cell></row><row><cell>ETH</cell><cell>15.70</cell><cell>2.99</cell><cell>1.80</cell><cell>1.31</cell></row><row><cell>Hotel</cell><cell>8.10</cell><cell>2.26</cell><cell>−1.29</cell><cell>−1.94</cell></row><row><cell>Univ</cell><cell>2.88</cell><cell>1.05</cell><cell>−0.89</cell><cell>−1.13</cell></row><row><cell>Zara 1</cell><cell>1.36</cell><cell>1.86</cell><cell>−1.13</cell><cell>−1.41</cell></row><row><cell>Zara 2</cell><cell>0.96</cell><cell>0.81</cell><cell>−2.19</cell><cell>−2.53</cell></row><row><cell>Average</cell><cell>5.80</cell><cell>1.79</cell><cell>−0.74</cell><cell>−1.14</cell></row></table><note>Legend: = Integration via Dynamics, M = Map Encoding, y R = Robot Future Encoding</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>[nuScenes] (a): Vehicle-only FDE across time for Trajectron++ compared to that of other single-trajectory and probabilistic approaches. Bold indicates best. (b): Pedestrian-only FDE and KDE NLL across time for Trajectron++. We subtracted 22-24cm from these reported values (their detection/tracking error<ref type="bibr" target="#b6">[7]</ref>), as we do not use a detector/tracker. This is done to establish a fair comparison.</figDesc><table><row><cell cols="2">(a) Vehicle-only</cell><cell></cell></row><row><cell>Method</cell><cell>FDE (m) @1s @2s @3s @4s</cell><cell>(b) Pedestrian-only</cell></row><row><cell cols="2">Const. Velocity S-LSTM  *  [1,7] CSP  *  [11,7] CAR-Net  *  [41,7] 0.38 0.32 0.89 1.70 2.73 0.47 -1.61 -0.46 -1.50 --1.35 -SpAGNN  *  [7] 0.36 -1.23 -</cell><cell>KDE NLL @2s @3s −2.69 −2.46 −1.76 −1.09 0.03 0.17 0.37 0.60 FDE (m) @1s @4s @1s @2s @3s @4s Ours+ ,M (ML) −5.58 −3.96 −2.77 −1.89 0.01 0.17 0.37 0.62 Method Ours (ML)</cell></row><row><cell>Ours (ML)</cell><cell>0.18 0.57 1.25 2.24</cell><cell></cell></row><row><cell cols="2">Ours+ ,M (ML) 0.07 0.45 1.14 2.20</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell></row></table><note>*Legend: = Integration via Dynamics, M = Map Encoding, yR = Robot Future Encoding.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>[nuScenes] (a): Vehicle-only prediction performance for ablated versions of our model. (b): The same, but excluding the ego-robot from consideration (as it is being conditioned on). This shows that our model's robot future conditional performance does not arise from merely removing the ego-vehicle.</figDesc><table><row><cell></cell><cell cols="2">(a) Including the Ego-Vehicle</cell><cell></cell></row><row><cell>Ablation</cell><cell>KDE NLL</cell><cell>FDE ML (m)</cell><cell>B. Viol. (%)</cell></row><row><cell>M y R</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>ADE results of all methods per dataset, as well as their average performance. Boxplots are shown for all generative models since they produce distributions of trajectories. 2000 trajectories were sampled per model at each prediction timestep, with each sample's ADE included in the boxplots. Our approach with dynamics integration is compared here, specifically its z mode output configuration. X markers indicate the mean ADE. Mean ADE from deterministic baselines are visualized as horizontal lines.</figDesc><table><row><cell></cell><cell cols="2">Linear Social GAN</cell><cell>Vanilla LSTM Trajectron</cell><cell>Social LSTM Ours</cell><cell>Social Attention</cell><cell cols="2">Linear Social GAN</cell><cell>Vanilla LSTM Trajectron</cell><cell>Social LSTM Ours</cell><cell>Social Attention</cell></row><row><cell>Average Displacement Error (m)</cell><cell>1 2 3 4</cell><cell></cell><cell></cell><cell></cell><cell>Final Displacement Error (m)</cell><cell>8 2 4 6</cell><cell></cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell></row><row><cell></cell><cell>ETH -Univ</cell><cell cols="4">ETH -Hotel UCY -Univ UCY -Zara 1 UCY -Zara 2 Average</cell><cell>ETH -Univ</cell><cell cols="2">ETH -Hotel UCY -Univ UCY -Zara 1 UCY -Zara 2 Average</cell></row><row><cell cols="3">Fig. 4. Left:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Right: The same analysis for FDE.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">All of our source code, trained models, and data can be found online at https://github.com/StanfordASL/Trajectron-plus-plus.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">These equations are also found in the Kalman Filter prediction step<ref type="bibr" target="#b22">[24]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">These equations are also found in the Extended Kalman Filter prediction step<ref type="bibr" target="#b44">[46]</ref>.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Single Integrator Distribution Integration</head><p>For a single integrator, we define the state to be the position vector s = p = [x, y] T , the control to be the velocity vector u =ṗ = [ẋ,ẏ] T , and write the linear discrete-time dynamics as</p><p>At each timestep, and for a specific latent value z, Trajectron++ produces a Gaussian distribution over control actions N (µ u , Σ u ). Specifically, it outputs</p><p>where µẋ and µẏ are the respective mean velocities in the agent's longitudinal and lateral directions, σẋ and σẏ are the respective longitudinal and lateral velocity standard deviations, and ρẋẏ is the correlation betweenẋ andẏ. Since Σ u is the only source of uncertainty in the prediction model, Equation <ref type="formula">(5)</ref> is a linear Gaussian system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Mean Derivation 2</head><p>Following the sum of Gaussian random variables <ref type="bibr" target="#b22">[24]</ref>, the output mean positions are obtained by Equation <ref type="bibr" target="#b4">(5)</ref>. Thus, at test time, Trajectron++ produces µ </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Covariance Derivation 2</head><p>The position covariance is obtained via the covariance of a sum of Gaussian random variables <ref type="bibr" target="#b22">[24]</ref> Σ (t+1) p = I 2×2 Σ (t)</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Social LSTM: Human trajectory prediction in crowded spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Interaction networks for learning about objects, relations and physics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Massive exploration of neural machine translation architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Britz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goldie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Conf. on Empirical Methods in Natural Language Processing</title>
		<meeting>of Conf. on Empirical Methods in Natural Language essing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">E</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<title level="m">nuScenes: A multimodal dataset for autonomous driving</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<title level="m">SpAGNN: Spatially-aware graph neural networks for relational behavior forecasting from sensor data</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">IntentNet: Learning to predict intention from raw sensor data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. on Robot Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Argoverse: 3d tracking and forecasting with rich maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sangkloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hartnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Conf. on Empirical Methods in Natural Language Processing</title>
		<meeting>of Conf. on Empirical Methods in Natural Language essing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-modal trajectory prediction of surrounding vehicles with maneuver based lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Deo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Generative adversarial nets. In: Conf. on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note>IEEE Intelligent Vehicles Symposium</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Social GAN: Socially acceptable trajectories with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Developmental cognitive neuroscience of theory of mind</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gweon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Saxe</surname></persName>
		</author>
		<idno type="DOI">10.1016/B978-0-12-397267-5.00057-1</idno>
		<ptr target="http://www.sciencedirect.com/science/article/pii/B97801239726750005711" />
	</analytic>
	<monogr>
		<title level="m">Neural Circuit Development and Function in the Brain, chap. 20</title>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="367" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Network lasso: Clustering and optimization in large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hallac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Int. Conf. on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Social force model for pedestrian dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Helbing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molnár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">beta-VAE: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The Trajectron: Probabilistic multi-agent trajectory modeling with dynamic spatiotemporal graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ivanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pavone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generative modeling of multimodal multi-human behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ivanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schmerling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pavone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ Int. Conf. on Intelligent Robots &amp; Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Structural-RNN: Deep learning on spatio-temporal graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Discrete residual flow for probabilistic pedestrian behavior prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Segal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Conf. on Robot Learning</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Categorial reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A new approach to linear filtering and prediction problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Kalman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ASME Journal of Basic Engineering</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kesten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Usman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pandya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nadhamuni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Omari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kazakova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Platinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Shet</surname></persName>
		</author>
		<ptr target="https://level5.lyft.com/dataset/" />
		<title level="m">Lyft Level 5 AV Dataset</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Kinematic and dynamic vehicle models for autonomous driving control design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pfeifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Schildbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Borrelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Social-BiGAT: Multimodal trajectory forecasting using bicycle-GAN and graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Martín-Martín</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Better unicycle models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Lavalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Planning Algorithms</title>
		<imprint>
			<publisher>Cambridge Univ. Press</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A simple unicycle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Lavalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Planning Algorithms</title>
		<imprint>
			<publisher>Cambridge Univ. Press</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">DESIRE: distant future prediction in dynamic scenes with interacting agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vernaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Predicting wide receiver trajectories in American football</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conf. on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Crowds by example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chrysanthou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Analysis of recurrent neural networks for probabilistic modeling of driver behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Morton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Wheeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Kochenderfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A survey of motion planning and control techniques for self-driving urban vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Paden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Čáp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yershov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Frazzoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Vehicles</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. on Neural Information Processing Systems -Autodiff Workshop</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">You&apos;ll never walk alone: Modeling social behavior for multi-target tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pellegrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int. Conf. on Computer Vision</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<title level="m">Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning)</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>first edn.</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">PRECOG: Prediction conditioned on goals in visual multi-agent settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rhinehart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcallister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int. Conf. on Computer Vision</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Human motion trajectory prediction: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rudenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Palmieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Herman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Gavrila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">O</forename><surname>Arras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">SoPhie: An attentive GAN for predicting paths compliant to social and physical constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hirose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">CAR-Net: Clairvoyant attentive recurrent network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Legros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Voisin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vesel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Conf. on Computer Vision</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">What the constant velocity model can teach us about pedestrian motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schöller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Aravantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Knoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="page">22</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning structured output representation using deep conditional generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multiple futures prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Analyzing the variety loss in the context of probabilistic trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Thiede</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Brahma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The extended Kalman filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Probabilistic Robotics</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Social attention: Modeling attention in human crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vemula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Muelling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Robotics and Automation</title>
		<meeting>IEEE Conf. on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Gaussian process dynamical models for human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Waymo: Waymo Open Dataset: An autonomous driving dataset</title>
		<ptr target="https://waymo.com/open/" />
	</analytic>
	<monogr>
		<title level="j">Waymo: Safety report</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018-11-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">End-toend interpretable neural motion planner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">InfoVAE: Balancing learning and inference in variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. on Artificial Intelligence</title>
		<meeting>AAAI Conf. on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Multi-agent tensor fusion for contextual trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Monfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
