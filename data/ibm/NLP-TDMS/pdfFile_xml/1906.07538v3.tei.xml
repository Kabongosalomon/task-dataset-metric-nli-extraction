<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Locate, Size and Count: Accurately Resolving People in Dense Crowds via Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><forename type="middle">Babu</forename><surname>Sam</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Skand</roleName><forename type="first">Vishwanath</forename><surname>Peri</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Mukuntha</roleName><forename type="first">Narayanan</forename><surname>Sundararaman</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amogh</forename><surname>Kamath</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">R</forename><surname>Venkatesh Babu</surname></persName>
						</author>
						<title level="a" type="main">Locate, Size and Count: Accurately Resolving People in Dense Crowds via Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Crowd Counting</term>
					<term>Head Detection</term>
					<term>Deep Learning !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a detection framework for dense crowd counting and eliminate the need for the prevalent density regression paradigm. Typical counting models predict crowd density for an image as opposed to detecting every person. These regression methods, in general, fail to localize persons accurate enough for most applications other than counting. Hence, we adopt an architecture that locates every person in the crowd, sizes the spotted heads with bounding box and then counts them. Compared to normal object or face detectors, there exist certain unique challenges in designing such a detection system. Some of them are direct consequences of the huge diversity in dense crowds along with the need to predict boxes contiguously. We solve these issues and develop our LSC-CNN model, which can reliably detect heads of people across sparse to dense crowds. LSC-CNN employs a multi-column architecture with top-down feature modulation to better resolve persons and produce refined predictions at multiple resolutions. Interestingly, the proposed training regime requires only point head annotation, but can estimate approximate size information of heads. We show that LSC-CNN not only has superior localization than existing density regressors, but outperforms in counting as well. The code for our approach is available at https://github.com/val-iisc/lsc-cnn.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>A NALYZING large crowds quickly, is one of the highly sought-after capabilities nowadays. Especially in terms of public security and planning, this assumes prime importance. But automated reasoning of crowd images or videos is a challenging Computer Vision task. The difficulty is extreme in dense crowds that the task is typically narrowed down to estimating the number of people. Since the count or distribution of people in the scene itself can be very valuable information, this field of research has gained traction.</p><p>There exists a huge body of works on crowd counting. They range from initial detection based methods ( <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, etc.) to later models regressing crowd density <ref type="bibr">([4]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, etc.). The detection approaches, in general, seem to scale poorly across the entire spectrum of diversity evident in typical crowd scenes. Note the crucial difference between the normal face detection problem with crowd counting; faces may not be visible for people in all cases (see <ref type="figure">Figure 1</ref>). In fact, due to extreme pose, scale and view point variations , learning a consistent feature set to discriminate people seems difficult. Though faces might be largely visible in sparse assemblies, people become tiny blobs in highly dense crowds. This makes it cumbersome to put bounding boxes in dense crowds, not to mention the sheer number of people, in the order of thousands, that need to be annotated per image. Consequently, the problem is more conveniently reduced to that of density regression.</p><p>In density estimation, a model is trained to map an input image to its crowd density, where the spatial values indicate the number of people per unit pixel. To facilitate this, the heads of people are annotated, which is much easier than specifying bounding box for crowd images <ref type="bibr" target="#b8">[9]</ref>. These point annotations are converted to density map by convolving with a Gaussian kernel such that simple spatial summation gives out the crowd count. Though regression is the dominant paradigm in crowd analysis and delivers excellent count estimation, there are some serious limitations. The first being the inability to pinpoint persons as these models predict crowd density, which is a regional feature (see the density maps in <ref type="figure" target="#fig_7">Figure 7</ref>). Any simple post-processing of density maps to extract positions of people, does not seem to scale across the density ranges and results in poor counting performance (Section 4.2). Ideally, we expect the model to deliver accurate localization on every person in the scene possibly with bounding box. Such a system paves way for downstream applications other than predicting just the crowd distribution. With accurate bounding box for heads of people in dense crowds, one could do person recognition, tracking etc., which are practically more valuable. Hence, we try to go beyond the popular density regression framework and create a dense detection system for crowd counting.</p><p>Basically, our objective is to locate and predict bounding boxes on heads of people, irrespective of any kind of variations. Developing such a detection framework is a challenging task and cannot be easily achieved with trivial changes to existing detection frameworks ( <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, etc.). This is because of the following reasons:</p><p>• Diversity: Any counting model has to handle huge diversity in appearance of individual persons and their assemblies. There exist an interplay of multiple variables, including but not limited to pose, viewpoint and illumination variations within the same crowd as well as across crowd images. arXiv:1906.07538v3 [cs.CV] 15 Feb 2020 <ref type="figure">Fig. 1</ref>. Face detection vs. Crowd counting. Tiny Face detector <ref type="bibr" target="#b0">[1]</ref>, trained on face dataset <ref type="bibr" target="#b1">[2]</ref> with box annotations, is able to capture 731 out of the 1151 people in the first image <ref type="bibr" target="#b2">[3]</ref>, losing mainly in highly dense regions. In contrast, despite being trained on crowd dataset <ref type="bibr" target="#b3">[4]</ref> having only point head annotations, our LSC-CNN detects 999 persons (second image) consistently across density ranges and provides fairly accurate boxes.</p><p>• Scale: The extreme scale and density variations in crowd scenes pose unique challenges in formulating a dense detection framework. In normal detection scenarios, this could be mitigated using a multi-scale architecture, where images are fed to the model at different scales and trained. A large face in a sparse crowd is not simply a scaled up version of that of persons in dense regions. The pattern of appearance itself is changing across scales or density.</p><p>• Resolution: Usual detection models predict at a downsampled spatial resolution, typically one-sixteenth or one-eighth of the input dimensions. But this approach does not scale across density ranges. Especially, highly dense regions require fine grained detection of people, with the possibility of hundreds of instances being present in a small region, at a level difficult with the conventional frameworks.</p><p>• Extreme box sizes: Since the densities vary drastically, so should be the box sizes. The size of boxes must vary from as small as 1 pixel in highly dense crowds to more than 300 pixels in sparser regions, which is several folds beyond the setting under which normal detectors operate.</p><p>• Data imbalance: Another problem due to density variation is the imbalance in box sizes for people across dataset. The distribution is so skewed that the majority of samples are crowded to certain set of box sizes while only a few are available for the remaining.</p><p>• Only point annotation: Since only point head annotation is available with crowd datasets, bounding boxes are absent for training detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Local minima: Training the model to predict at higher resolutions causes the gradient updates to be averaged over a larger spatial area. This, especially with the diverse crowd data, increases the chances of optimization being stuck in local minimas, leading to suboptimal performance.</p><p>Hence, we try to tackle these challenges and develop a tailor-made detection framework for dense crowd counting. Our objective is to Locate every person in the scene, Size each detection with bounding box on the head and finally give the crowd Count. This LSC-CNN, at a functional view, is trained for pixel-wise classification task and detects the presence of persons along with the size of the heads. Cross entropy loss is used for training instead of the widely employed l 2 regression loss in density estimation. We devise novel solutions to each of the problems listed before, including a method to dynamically estimate bounding box sizes from point annotations. In summary, this work contributes:</p><p>• Dense detection as an alternative to the prevalent density regression paradigm for crowd counting.</p><p>• A novel CNN framework, different from conventional object detectors, that provides fine-grained localization of persons at very high resolution.</p><p>• A unique fusion configuration with top-down feature modulation that facilitates joint processing of multi-scale information to better resolve people.</p><p>• A practical training regime that only requires point annotations, but can estimate boxes for heads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>A new winner-take-all based loss formulation for better training at higher resolutions. • A benchmarked model that delivers impressive performance in localization, sizing and counting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PREVIOUS WORK</head><p>Person Detection: The topic of crowd counting broadly might have started with the detection of people in crowded scenes. These methods use appearance features from still images or motion vectors in videos to detect individual persons ( <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>). Idrees et al. <ref type="bibr" target="#b7">[8]</ref> leverage local scale prior and global occlusion reasoning to detect humans in crowds. With features extracted from a deep network, <ref type="bibr" target="#b18">[19]</ref> run a recurrent framework to sequentially detect and count people. In general, the person detection based methods are limited by their inability to operate faithfully in highly dense crowds and require bounding box annotations for training. Consequently, density regression becomes popular.</p><p>Density Regression: Idrees et al. <ref type="bibr" target="#b8">[9]</ref> introduce an approach where features from head detections, interest points and frequency analysis are used to regress the crowd density. A shallow CNN is employed as density regressor in <ref type="bibr" target="#b9">[10]</ref>, where the training is done by alternatively backpropagating the regression and direct count loss. There are works like <ref type="bibr" target="#b19">[20]</ref>, where the model directly regresses crowd count instead of density map. But such methods are shown to perform inferior due to the lack of spatial information in the loss.</p><p>Multiple and Multi-column CNNs: The next wave of methods focuses on addressing the huge diversity of appearance of people in crowd images through multiple networks. Walach et al. <ref type="bibr" target="#b20">[21]</ref> use a cascade of CNN regressors to sequentially correct the errors of previous networks. The outputs of multiple networks, each being trained with images of different scales, are fused in <ref type="bibr" target="#b10">[11]</ref> to generate the final density map. Extending the trend, architecture with multiple columns of CNN having different receptive fields starts to emerge. The receptive field determines the affinity towards certain density types. For example, the deep network in <ref type="bibr" target="#b21">[22]</ref> is supposed to capture sparse crowds, while the shallow network is for the blob like people in dense regions. The MCNN <ref type="bibr" target="#b3">[4]</ref> model leverages three networks with filter sizes tuned for different density types. The specialization acquired by individual columns in these approaches are improved through a differential training procedure by <ref type="bibr" target="#b11">[12]</ref>. On a similar theme, Sam et al. <ref type="bibr" target="#b22">[23]</ref> create a hierarchical tree of expert networks for fine-tuned regression. Going further, the multiple columns are combined into a single network, with parallel convolutional blocks of different filters by <ref type="bibr" target="#b23">[24]</ref> and is trained along with an additional consistency loss.</p><p>Leveraging context and other information: Improving density regression by incorporating additional information forms another line of thought. Works like ( <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b24">[25]</ref>) supply local or global level auxiliary information through a dedicated classifier. Sam et al. <ref type="bibr" target="#b25">[26]</ref> show a top-down feedback mechanism can effectively provide global context to iteratively improve density prediction made by a CNN regressor. A similar incremental density refinement is proposed in <ref type="bibr" target="#b26">[27]</ref>.</p><p>Better and easier architectures: Since density regression suits better for denser crowds, Decide-Net architecture from <ref type="bibr" target="#b27">[28]</ref> adaptively leverages predictions from Faster RCNN <ref type="bibr" target="#b14">[15]</ref> detector in sparse regions to improve performance. Though the predictions seems to be better in sparse crowds, the performance on dense datasets is not very evident. Also note that the focus of this work is to aid better regression with a detector and is not a person detection model. In fact, Decide-Net requires some bounding box annotation for training, which is infeasible for dense crowds. Striving for simpler architectures have always been a theme. Li et al. <ref type="bibr" target="#b13">[14]</ref> employ a VGG based model with additional dilated convolutional layers and obtain better count estimation. Further, a DenseNet <ref type="bibr" target="#b28">[29]</ref> model is trained in <ref type="bibr" target="#b29">[30]</ref> for density regression at different resolutions with composition loss.</p><p>Other counting works: An alternative set of works try to incorporate flavours of unsupervised learning and mitigate the issue of annotation difficulty. Liu et al. <ref type="bibr" target="#b30">[31]</ref> use count ranking as a self-supervision task on unlabeled data in a multitask framework along with regression supervision on labeled data. The Grid Winner-Take-All autoencoder, introduced in <ref type="bibr" target="#b31">[32]</ref>, trains almost 99% of the model parameters with unlabeled data and the acquired features are shown to be better for density regression. Other counting works employ Negative Correlation Learning <ref type="bibr" target="#b32">[33]</ref> and Adversarial training to improve regression <ref type="bibr" target="#b33">[34]</ref>. In contrast to all these regression approaches, we move to the paradigm of dense detection, where the objective is to predict bounding box on heads of people in crowd of any density type.</p><p>Object/face Detectors: Since our model is a detector tailormade for dense crowds, here we briefly compare with other detection works as well. Object detection has seen a shift from early methods relying on interest points (like SIFT <ref type="bibr" target="#b34">[35]</ref>) to CNNs. Early CNN based detectors operate on the philosophy of first extracting features from a deep CNN and then have a classifier on the region proposals ( <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>) or a Region Proposal Network (RPN) <ref type="bibr" target="#b14">[15]</ref> to jointly predict the presence and boxes for objects. But the current dominant methods ( <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>) have simpler end-to-end architecture without region proposals. They divide the input image in to a grid of cells and boxes are predicted with confidences for each cell. But these works generally suit for relatively large objects, with less number instances per image. Hence to capture multiple small objects (like faces), many models are proposed. Zhu et al. <ref type="bibr" target="#b38">[39]</ref> adapt Faster RCNN with multiscale ROI features to better detect small faces. On similar lines, a pyramid of images is leveraged in <ref type="bibr" target="#b0">[1]</ref>, with each scale being separately processed to detect faces of varied sizes. The SSH model <ref type="bibr" target="#b17">[18]</ref> detects faces from multiple scales in a single stage using features from different layers. More recently, Sindagi et al. <ref type="bibr" target="#b39">[40]</ref> improves small face detections by enriching features with density information. Our proposed architecture differs from these models in many aspects as described in Section 1. Though it has some similarity with the SSH model in terms of the single stage architecture, we output predictions at resolutions higher than any face detector. This is to handle extremely small heads (of few pixels size) occurring very close to each other, a typical characteristic of dense crowds. Moreover, bounding box annotation is not available per se from crowd datasets and has to rely on pseudo data. Due to this approximated box data, we prefer not to regress or adjust the template box sizes as the normal detectors do, instead just classifies every person to one of the predefined boxes. Above all, dense crowd analysis is generally considered a harder problem due to the large diversity.</p><p>A concurrent work: We note a recent paper <ref type="bibr" target="#b40">[41]</ref> which proposes a detection framework, PSDNN, for crowd counting. But this is a concurrent work which has appeared while this manuscript is under preparation. PSDNN uses a Faster RCNN model trained on crowd dataset with pseudo ground truth generated from point annotation. A locally constrained regression loss and an iterative ground truth box updation scheme is employed to improve performance. Though the idea of generating pseudo ground truth boxes is similar, we do not actually create (or store) the annotations, instead a box is chosen from head locations dynamically (Section 3.2). We do not regress box location or size as normal detectors and avoid any complicated ground truth updation schemes. Also, PSDNN employs Faster RCNN with minimal changes, but we use a custom completely end-to-end and single stage architecture tailor-made for the nuances of dense crowd detection and outperforms in almost all benchmarks.</p><p>WTA Architectures: Since LSC-CNN employs a winnertake-all (WTA) paradigm for training, here we briefly compare with similar WTA works. WTA is a biologically inspired widely used case of competitive learning in artificial neural networks. In the deep learning scenario, Makhzani et al. <ref type="bibr" target="#b41">[42]</ref> propose a WTA regularization for autoencoders, where the basic idea is to selectively update only the maximally activated 'winner' neurons. This introduces sparsity in weight updates and is shown to improve feature learning. The Grid WTA version from <ref type="bibr" target="#b31">[32]</ref> extends the methodology for large scale training and applies WTA sparsity on spatial cells in a convolutional output. We follow <ref type="bibr" target="#b31">[32]</ref> and repurpose the GWTA for supervised training, where the objective is to learn better features by restricting gradient updates to the highest loss making spatial region (see Section 3.2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OUR APPROACH</head><p>As motivated in Section 1, we drop the prevalent density regression paradigm and develop a dense detection model for dense crowd counting. Our model named, LSC-CNN, predicts accurately localized boxes on heads of people in crowd images. Though it seems like a multi-stage task of first locating and sizing the each person, we formulate it as an end-to-end single stage process. <ref type="figure" target="#fig_0">Figure 2</ref> depicts a high-level view of our architecture. LSC-CNN has three functional parts; the first is to extract features at multiple resolution by the Feature Extractor. These feature maps are fed to a set of Top-down Feature Modulator (TFM) networks, where information across the scales are fused and box predictions are made. Then Non-Maximum Suppression (NMS) selects valid detections from multiple resolutions and is combined to generate the final output. For training of the model, the last stage is replaced with the GWTA Loss module, where the winners-take-all (WTA) loss backpropagation and adaptive ground truth box selection are implemented. In the following sections, we elaborate on each part of LSC-CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Locate Heads</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Feature Extractor</head><p>Almost all existing CNN object detectors operate on a backbone deep feature extractor network. The quality of features seems to directly affect the detection performance. For crowd counting, VGG-16 <ref type="bibr" target="#b42">[43]</ref> based networks are widely used in a variety of ways ( <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b24">[25]</ref>) and delivers near state-of-the-art performance <ref type="bibr" target="#b13">[14]</ref>. In line with the trend, we also employ several of VGG-16 convolution layers for better crowd feature extraction. But, as shown in <ref type="figure" target="#fig_1">Figure  3</ref>, some blocks are replicated and manipulated to facilitate feature extraction at multiple resolutions. The first five 3 × 3 convolution blocks of VGG-16, initialized with ImageNet <ref type="bibr" target="#b43">[44]</ref> trained weights, form the backbone network. The input to the network is RGB crowd image of fixed size (224 × 224) with the output at each block being downsampled due to max-pooling. At every block, except for the last, the network branches with the next block being duplicated (weights are copied at initialization, not tied). We tap from these copied blocks to create feature maps at one-half, one-fourth, oneeighth and one-sixteenth of the input resolution. This is in slight contrast to typical hypercolumn features and helps to specialize each scale branches by sharing low-level features without any conflict. The low-level features with half the spatial size that of the input, could potentially capture and resolve highly dense crowds. The other lower resolution scale branches have progressively higher receptive field and are suitable for relatively less packed ones. In fact, people appearing large in very sparse crowds could be faithfully discriminated by the one-sixteenth features. The multi-scale architecture of feature extractor is motivated to solve many roadblocks in dense detection. It could simultaneously address the diversity, scale and resolution issues mentioned in Section 1. The diversity aspect is taken care by having multiple scale columns, so that each one can specialize to a different crowd type. Since the typical multiscale input paradigm is replaced with extraction of multiresolution features, the scale issue is mitigated to certain extent. Further, the increased resolution for branches helps to better resolve people appearing very close to each other. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Top-down Feature Modulator</head><p>One major issue with the multi-scale representations from the feature extractor is that the higher resolution feature maps have limited context to discriminate persons. More clearly, many patterns in the image formed by leaves of trees, structures of buildings, cluttered backgrounds etc. resemble formation of people in highly dense crowds <ref type="bibr" target="#b25">[26]</ref>. As a result, these crowd like patterns could be misclassified as people, especially at the higher resolution scales that have low receptive field for making predictions. We cannot avoid these low-level representations as it is crucial for resolving people in highly dense crowds. The problem mainly arises due to the absence of high-level context information about the crowd regions in the scene. Hence, we evaluate global context from scales with higher receptive fields and jointly process these top-down features to detect persons. As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, a set of Top-down Feature Modulator (TFM) modules feed on the output by crowd feature extractor. There is one TFM network for each scale branch and acts as a person detector at that scale. The TFM also have connections from all previous low resolution scale branches. For example, in the case of one-fourth branch TFM, it receives connections from one-eighth as well as one-sixteenth branches and generates features for one-half scale branch. If there are s feature connections from high-level branches, then it uniquely identifies an TFM network as TFM(s). s is also indicative of the scale and takes values from zero to n S − 1, where n S is the number of scale branches. For instance, TFM with s = 0 is for the lowest resolution scale (one-sixteenth) and takes no top-down features. Any TFM(s) with s &gt; 0 receives connections from all TFM(i) modules where 0 ≤ i &lt; s. At a functional view, the TFM predicts the presence of a person at every pixel for the given scale branch by coalescing all the scale features. This multi-scale feature processing helps to drive global context information to all the scale branches and suppress spurious detections, apart from aiding scale specialization. <ref type="figure" target="#fig_2">Figure 4</ref> illustrates the internal implementation of the TFM module. Terminal 1 of any TFM module takes one of the scale feature set from the feature extractor, which is then passed through a 3 × 3 convolution layer. We set the number of filters for this convolution, m, as one-half that of the incoming scale branch (f channels from terminal 1). To be specific, m = f 2 , where . denotes floor operation. This reduction in feature maps is to accommodate the topdown aggregrated multi-scale representations and decrease computational overhead for the final layers. Note that the output Terminal 2 is also drawn from this convolution layer and acts as the top-down features for next TFM module. Terminal 3 of TFM(s) takes s set of these top-down multiscale feature maps. For the top-down processing, each of the s feature inputs is operated by a set of two convolution layers. The first layer is a transposed convolution (also know as deconvolution) to upsample top-down feature maps to the same size as the scale branch. The upsampling is followed by a convolution with m filters. Each processed feature set has the same number of channels (m) as that of the scale input, which forces them to be weighed equally by the subsequent layers. All these feature maps are concatenated along the channel dimension and fed to a series of 3 × 3 TFM(s) ... convolutions with progressive reduction in number of filters to give the final prediction. These set of layers fuse the crowd features with top-down features from other scales to improve discrimination of people. Terminal 4 delivers the output, which basically classifies every pixel into either background or to one of the predefined bounding boxes for the detected head. Softmax nonlinearity is applied on these output maps to generate per-pixel confidences over the 1 + n B classes, where n B is the number of predefined boxes. n B is a hyper-parameter to control the fineness of the sizes and is typically set as 3, making a total of n S ×n B = 12 boxes for all the branches. The first channel of the prediction for scale s, D s 0 , is for background and remaining {D s 1 , D s 2 , . . . , D s n B } maps are for the boxes (see Section 3.2.1). The top-down feature processing architecture helps in fine-grained localization of persons spatially as well as in the scale pyramid. The diversity, scale and resolution bottlenecks (Section 1) are further mitigated by the top-down mechanism, which could selectively identify the appropriate scale branch for a person to resolve it more faithfully. This is further ensured through the training regime we employ (Section 3.2.2). Scaling across the extreme box sizes is also made possible to certain extent as each branch could focus on an appropriate subset of the box sizes.</p><formula xml:id="formula_0">T | t0,2 C | m T | t1,2 C | m T | ts-1,2 C | m t0 t1 ts-1 ... 3 s branches C | m 1 C | 256 C | 128 C | 64 C | 32 4 C | 4 2 f channels s-0 s-1 1 channels 3x3 DeConv with k filters &amp; stride r T | k,r 3x3 Conv with k filters &amp; stride 1 C | k Concat</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Size Heads</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Box classification</head><p>As described previously, LSC-CNN with the help of TFM modules locates people and has to put appropriate bounding boxes on the detected heads. For this sizing, we choose a per-pixel classification paradigm. Basically, a set of bounding boxes are fixed with predefined sizes and the model simply classifies each head to one of the boxes or as background. This is in contrast to the anchor box paradigm typically being employed in detectors ( <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref>), where the parameters of the boxes are regressed. Every scale branch (s) of the model outputs a set of maps, {D s b } n B b=0 , indicating the per-pixel confidence for the box classes (see <ref type="figure" target="#fig_2">Figure 4</ref>). Now we require the ground truth sizes of heads to train the model, which is not available and not convenient to annotate for typical dense crowd datasets. Hence, we devise a method to approximate the sizes of the heads.</p><p>For ground truth generation, we rely on the point annotations available with crowd datasets. These point annotations specify the locations of heads of people. The location is approximately at the center of head, but can vary significantly for sparse crowds (where the point could be any where on the large face or head). Apart from locating every person in the crowd, the point annotations also give some scale information. For instance, Zhang et al. <ref type="bibr" target="#b3">[4]</ref> use the mean of k nearest neighbours of any head annotation to estimate the adaptive Gaussian kernel for creating the ground truth density maps. Similarly, the distance between two adjacent persons could indicate the bounding box size for the heads, under the assumption of a smoothly varying crowd density. Note that we consider only square boxes. In short, the size for any head can simply be taken as the distance to the nearest neighbour. While this approach makes sense in medium to dense crowds, it might result in incorrect box sizes for people in sparse crowds, where the nearest neighbour is typically far. Nevertheless, empirically it is found to work fairly well over a wide range of densities.</p><p>Here we mathematically explain the pseudo ground truth creation. Let P be the set of all annotated (x, y) locations of people in the given image patch. Then for every point (x, y) in P, the box size is defined as,  falls to b = n B . Note that non-people locations are assigned b = 0 background class. A general philosophy is followed in choosing the box sizes β s b s for all the scales. The size of the first box (b = 1) at the highest resolution scale (s = n S −1) is always fixed to one, which improves the resolving capacity for highly dense crowds (Resolution issue in Section 1). We choose larger sizes for the remaining boxes in the same scale with a constant increment. This increment is fine-grained in higher resolution branches, but the coarseness progressively increases for low resolution scales. To be specific, if γ s represent the size increment for scale s, then box sizes are,</p><formula xml:id="formula_1">B[x, y] = min (x ,y ) P,(x ,y ) =(x,y) (x − x ) 2 + (y − y ) 2 ,<label>(1)</label></formula><formula xml:id="formula_2">β s b = β s+1 n B + bγ s if s &lt; n S − 1 1 + (b − 1)γ s otherwise.<label>(2)</label></formula><p>The typical values of the size increment for different scales are γ = {4, 2, 1, 1}. Note that the high resolution branches (one-half &amp; one-fourth) have boxes with finer sizes than the low resolution ones (one-sixteenth &amp; one-eighth), where coarse resolving capacity would suffice (see <ref type="figure" target="#fig_4">Figure 5</ref>). There are many reasons to discretize the head sizes and classify the boxes instead of regressing size values. The first is due to the use of pseudo ground truth. Since the size of heads itself is approximate, tight estimation of sizes proves to be difficult (see Section 5.2). Similar sized heads in two images could have different ground truths depending on the density. This might lead to some inconsistencies in training and could result in suboptimal performance. Moreover, the sizes of heads vary extremely across density ranges at a level not expected for normal detectors. This requires heavy normalization of value ranges along with complex data balancing schemes. But our per-pixel box classification paradigm effectively addresses these extreme box sizes and only point annotation issues (Section 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">GWTA Training</head><p>Loss: We train the LSC-CNN by back-propagating per-pixel cross entropy loss. The loss for a pixel is defined as,</p><formula xml:id="formula_3">l({d i } n B i=0 , {b i } n B i=0 , {α i } n B i=0 ) = − n B i=0 α i b i logd i ,<label>(3)</label></formula><p>where {d i } n B i=0 is the set of n B +1 probability values (softmax outputs) for the predefined box classes and {b i } n B i=0 refers to corresponding ground truth labels. All b i s take zero value except for the correct class. The α i s are weights to class balance the training. Now the loss for the entire prediction of scale branch s would be, b=0 (the set limits might be dropped for convenience). Note that (w s , h s ) are the spatial sizes of these prediction maps and the cross-entropy loss is averaged over it. The final loss for LSC-CNN after combining losses from all the branches is,</p><formula xml:id="formula_4">L comb = n S s=1 L({D s b }, {B s b }, {α s b }).<label>(4)</label></formula><p>Weighting: As mentioned in Section 1, the data imbalance issue is severe in the case of crowd datasets. Class wise weighting assumes prime importance for effective backpropagation of L comb (see Section 5.2). We follow a simple formulation to fix the α values. Once the box sizes are set, the number of data points available for each class is computed from the entire train set. Let c s b denote this frequency count for the box b in scale s. Then for every scale branch, we sum the box counts as c s sum = n B b=1 c s b and the scale with minimum number of samples is identified. This minimum value c min = min s c s sum , is used to balance the training data across the branches. Basically, we scale down the weights for the branches with higher counts such that the minimum count branch has weight one. Note that training points for all the branches as well as the classes within a branch need to be balanced. Usually the data samples would be highly skewed towards the background class (b = 0) in all the scales. To mitigate this, we scale up the weights of all box classes based on its ratio with background frequency of the same branch. Numerically, the balancing is done jointly as,</p><formula xml:id="formula_5">α s b = c min c s sum min( c s 0 c s b , 10).<label>(5)</label></formula><p>The term c s 0 /c s b can be large since the frequency of background to box is usually skewed. So we limit the value to 10 for better training stability. Further note that for some box size settings, α s b values itself could be very skewed, which depends on the distribution of dataset under consideration. Any difference in the values more than an order of magnitude is found to be affecting the proper training. Hence, the box size increments (γs) are chosen not only to roughly cover the density ranges in the dataset, but also such that the α s b s are close within an order of magnitude. GWTA: However, even after this balancing, training LSC-CNN by optimizing joint loss L comb does not achieve acceptable performance (see Section 5.2). This is because the model predicts at a high resolution than any typical crowd counting network and the loss is averaged over a relatively larger spatial area. The weighing scheme only makes sure that the averaged loss values across branches and classes is in similar range. But the scales with larger spatial area could have more instances of one particular class than others. For instance in dense regions, the one-half resolution scale (s = 3) would have more person instances and are typically very diverse. This causes the optimization to focus on all instances equally and might lead to a local minima solution. A strategy is needed to focus on a small region at a time, update the weights and repeat this for another region.</p><p>For solving this local minima issue (Section 1), we rely on the Grid Winner-Take-All (GWTA) approach introduced in <ref type="bibr" target="#b31">[32]</ref>. Though GWTA is originally used for unsupervised learning, we repurpose it to our loss formulation. The basic idea is to divide the prediction map into a grid of cells with fixed size and compute the loss only for one cell. Since only a small region is included in the loss, this acts as tie breaker and avoids the gradient averaging effect, reducing the chances of the optimization reaching a local minima. Now the question is how to select the cells. The 'winner' cell is chosen as the one which incurs the highest loss. At every iteration of training, we concentrate more on the high loss making regions in the image and learn better features. This has slight resemblance to hard mining approaches, where difficult instances are sampled more during training. In short, GWTA training selects 'hard' regions and try to improve the prediction (see Section 5.2 for ablations). <ref type="figure">Figure 6</ref> shows the implementation of GWTA training. For each scale, we apply GWTA non-linearity on the loss. The cell size for all branches is taken as the dimensions of the lowest resolution prediction map (w 0 , h 0 ). There is only one cell for scale s = 0 (one-sixteenth branch), but the grows by power of four (4 s ) for subsequent branches as the spatial dimensions consecutively doubles. Now we compute the cross-entropy loss for any cell at location (x, y) (top-left corner) in the grid as, where the summation of losses runs over for all pixels in the cell under consideration. Also note thatᾱ s b is computed using equation 5 with c s sum = 4 −s n B b=1 c s b , in order to account for the change in spatial size of the predictions. The winner cell is the one with the highest loss and the location is given by,</p><formula xml:id="formula_6">(x s wta , y s wta ) = argmax (x,y)=(w 0 i,h 0 j),i∈Z,j∈Z l s wta [x, y].<label>(6)</label></formula><p>Note that the argmax operator finds an (x, y) pair that identifies the top-left corner of the cell. The combined loss becomes,</p><formula xml:id="formula_7">Lwta = 1 w 0 h 0 n S s=1 l s wta [x s wta , y s wta ].<label>(7)</label></formula><p>We optimize the parameters of LSC-CNN by backpropagating Lwta using standard mini-batch gradient descent with momentum. Batch size is typically 4. Momentum parameter is set as 0.9 and a fixed learning rate schedule of 10 −3 is used. The training is continued till the counting performance (MAE in Section 4.2) on a validation set saturates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Count Heads</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Prediction Fusion</head><p>For testing the model, the GWTA training module is replaced with the prediction fusion operation as shown in <ref type="figure" target="#fig_0">Figure 2</ref>. The input image is evaluated by all the branches and results in predictions at multiple resolutions. Box locations are extracted from these prediction maps and are linearly scaled to the input resolution. Then standard Non-Maximum Suppression (NMS) is applied to remove boxes with overlap more than a threshold. The boxes after the NMS form the final prediction of the model and are enumerated to output the crowd count. Note that, in order to facilitate intermediate evaluations during training, the NMS threshold is set to 0.3 (30% area overlap). But for the best model after training, we run a threshold search to minimize the counting error (MAE, Section 4.2) over the validation set (typical value ranges from 0.2 to 0.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PERFORMANCE EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup and Datasets</head><p>We evaluate LSC-CNN for localization and counting performance on all major crowd datasets. Since these datasets have only point head annotations, sizing capability cannot be benchmarked. Hence, we use one face detection dataset where bounding box ground truth is available. Further, LSC-CNN is trained on vehicle counting dataset to show generalization. <ref type="figure" target="#fig_7">Figure 7</ref> displays some of the box detections by our model on all datasets. Note that unless otherwise specified, we use the same architecture and hyper-parameters given in Section 3. UCF-QNRF: Idrees et al. <ref type="bibr" target="#b29">[30]</ref> introduce UCF-QNRF dataset and by large the biggest, with 1201 images for training and 334 images for testing. The 1.2 million head annotations come from diverse crowd images with density varying from as small as 49 people per image to massive 12865. UCF CC 50: UCF CC 50 <ref type="bibr" target="#b8">[9]</ref> is a challenging dataset on multiple counts; the first is due to being a small set of 50 images and the second results from the extreme diversity, with crowd counts ranging in 50 to 4543. The small size poses a serious problem for training deep neural networks. Hence to reduce the number of parameters for training, we only use the one-eighth and one-fourth scale branches for this dataset. The prediction at one-sixteenth scale is avoided as sparse crowds are very less, but the top-down connections are kept as it is in <ref type="figure" target="#fig_0">Figure 2</ref>. The box increments are chosen as γ = {2, 2}. Following <ref type="bibr" target="#b8">[9]</ref>, we perform 5-fold cross validation for evaluation. WorldExpo'10: An array of 3980 frames collected from different video sequences of surveillance cameras forms the WorldExpo'10 dataset <ref type="bibr" target="#b9">[10]</ref>. It has sparse crowds with an average density of only 50 people per image. There are 3,380 images for training and 600 images from five different scenes form the test set. Region of Interest (RoI) masks are also provided for every scene. Since the dataset has mostly sparse crowds, only the low-resolution scales one-sixteenth and one-eighth are used with γ = {2, 2}. We use a higher batch size of 32 as there many no people images and follow training/testing protocols in <ref type="bibr" target="#b9">[10]</ref>.</p><p>TRANCOS: The vehicle counting dataset, TRANCOS <ref type="bibr" target="#b44">[45]</ref>, has 1244 images captured by various traffic surveillance cameras. In total, there are 46,796 vehicle point annotations. Also, RoIs are specified on every image for evaluation. We use the same architecture and box sizes as that of WorldExpo'10. WIDERFACE: WIDERFACE [2] is a face detection dataset with more than 0.3 million bounding box annotations, spanning 32,203 images. The images, in general, have sparse crowds having variations in pose and scale with some level of occlusions. We remove the one-half scale branch for this dataset as highly dense images are not present. To compare with existing methods on fitness of bounding box predictions, the fineness of the box sizes are increased by using five boxes per scale (nB = 5). The γ is set as {4, 2, 2} and learning rate is made lower to 10 −4 . Note that for fair comparison, we train LSC-CNN without using the actual ground truth bounding boxes. Instead, point face annotations are created by taking centers of the boxes, from which pseudo ground truth is generated as per the training regime of LSC-CNN. But the performance is evaluated with the actual ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation of Localization</head><p>The widely used metric for crowd counting is the Mean Absolute Error or MAE. MAE is simply the absolute difference between the predicted and actual crowd counts averaged over all the images in the test set. Mathematically,</p><formula xml:id="formula_8">MAE = 1 N N n=1 |Cn − C GT n |,</formula><p>where Cn is the count predicted for input image n for which the ground truth is C GT n . The counting performance of a model is directly evident from the MAE value. Further to estimate the variance and hence robustness of the count prediction, Mean Squared Error or MSE is used. It is given by MSE = 1 N N n=1 (Cn − C GT n ) 2 . Though these metrics measure the accuracy of overall count prediction, localization of the predictions is not very evident. Hence, apart from standard MAE, we evaluate the ability of LSC-CNN to accurately pinpoint individual persons. An existing metric called Grid Average Mean absolute Error or GAME <ref type="bibr" target="#b44">[45]</ref>, can roughly indicate coarse localization of count predictions. To compute GAME, the prediction map is divided into a grid of cells and Metric MLE ↓ GAME(0) ↓ GAME(1) ↓ GAME(2) ↓ GAME(3) ↓ Dataset ↓ / Method→ CSR-A-thr LSC-CNN CSR-A LSC-CNN CSR-A LSC-CNN CSR-A LSC-CNN CSR-A LSC-CNN ST  the absolute count errors within each cell are averaged over grid. <ref type="table">Table 1</ref> compares the GAME values of LSC-CNN against a regression baseline model for different grid sizes. Note that GAME with only one cell, GAME(0), is same as MAE. We take the baseline as CSRNet-A <ref type="bibr" target="#b13">[14]</ref> (labeled CSR-A) model as it has similarity to the Feature Extractor and delivers near state-of-theart results. Clearly, LSC-CNN has superior count localization than the density regression based CSR-A.</p><p>One could also measure localization in terms of how close the prediction matches with ground truth point annotation. For this, we define a metric named Mean Localization Error (MLE), which computes the distance in pixels between the predicted person location to its ground truth averaged over test set. The predictions are matched to head annotations in a one-to-one fashion and a fixed penalty of 16 pixels is added for absent or spurious detections. Since CSR-A or any other density regression based counting models do not individually locate persons, we apply threshold on the density map to get detections (CSR-A-thr). But it is difficult to threshold density maps without loss of counting accuracy. We choose a threshold such that the resultant MAE is minimum over validation set. For CSR-A, the best thresholded MAE comes to be 167.1, instead of the original 72.6. As expected, MLE scores for LSC-CNN is significantly better than CSR-A, indicating sharp localization capacity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation of Sizing</head><p>We follow other face detection works ( <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b17">[18]</ref>) and use the standard mean Average Precision or mAP metric to assess the sizing ability of our model. For this, LSC-CNN is trained on WIDERFACE face dataset without the actual box ground truth as mentioned in Section 4.1. <ref type="table" target="#tab_2">Table 2</ref> reports the comparison of mAP scores obtained by our model against other works. Despite using pseudo ground truth for training, LSC-CNN achieves a competitive performance, especially on Hard and Medium test sets, against the methods that use full box supervision. For baseline, we consider the CSR-A-thr model (Section 4.2) where the density outputs are processed to get head locations. These are subsequently converted to bounding boxes using the pseudo box algorithm of LSC-CNN and mAP scores are computed (CSR-A-thr (baseline)). LSC-CNN beats the baseline by a strong margin, evidencing the superiority of the proposed box classification training. We also compare with Method MAE MSE Idrees et al. <ref type="bibr" target="#b8">[9]</ref> 315 508 MCNN <ref type="bibr" target="#b3">[4]</ref> 277 426 CMTL <ref type="bibr" target="#b24">[25]</ref> 252 514 SCNN <ref type="bibr" target="#b11">[12]</ref> 228 445 Idrees et al. <ref type="bibr" target="#b29">[30]</ref> 132 191 LSC-CNN (Ours) 120.5 218.2  PSDNN model <ref type="bibr" target="#b40">[41]</ref> which trains on pseudo box ground truth similar to our model. Interestingly, LSC-CNN has higher mAP in the two difficult sets than that of PSDNN. Note that the images in Easy set are mostly of very sparse crowds with faces appearing large. We lose out in mAP mainly due to the high discretization of box sizes on large faces. This is not unexpected as LSC-CNN is designed for dense crowds without bounding box annotations. But the fact that it works well on the relatively denser other two test sets, clearly shows the effectiveness of our proposed framework. For completeness, we train LSC-CNN with boxes generated from actual box annotations instead of the head locations (LSC-CNN (Actual GT)). As expected LSC-CNN performance improved with the use of real box size data. We also compute the average classification accuracy of boxes with respect to the pseudo ground truth on test set. LSC-CNN has an accuracy of around 94.56% for ST PartA dataset and 93.97% for UCF QNRF, indicative of proper data fitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation of Counting</head><p>Here we compare LSC-CNN with other crowd counting models on the standard MAE and MSE metrics. <ref type="table" target="#tab_3">Table 3</ref> lists the evaluation results on UCF-QNRF dataset. Our model achieves an MAE of 120.5, which is lower than that of <ref type="bibr" target="#b29">[30]</ref> by a significant margin of 12.5. Evaluation on the next set of datasets is available in  difference being just 0.5. But note that PSDNN is trained with a curriculum learning strategy and the MAE without it seems to be significantly higher (above 80). This along with the fact that LSC-CNN has lower count error than PSDNN in all other datasets, indicates the strength of our proposed architecture. In fact, state-of-the-art performance is obtained in both Shanghaitech Part B and UCF CC 50 datasets. Despite having just 50 images with extreme diversity in the UCF CC 50, our model delivers a substantial decrease of 33 points in MAE. A similar trend is observed in WorldExpo dataset as well, with LSC-CNN acheiving lower MAE than existing methods ( <ref type="table">Table 5</ref>). Further to explore the generalization of LSC-CNN, we evaluate on a vehicle counting dataset TRANCOS. The results from <ref type="table" target="#tab_6">Table 6</ref> evidence a lower MAE than PSDNN, and is highly competitive with the best method. These experiments evidence the topnotch crowd counting ability of LSC-CNN compared to other density regressors, with all the merits of a detection model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">ANALYSIS AND ABLATIONS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Effect of Multi-Scale Box Classification</head><p>As mentioned in Section 3.2, in general, we use 3 box sizes (nB = 3) for each scale branch and employ 4 scales (nS = 4). Here we ablate over the choice of nB and nS . The results of the experiments are presented in <ref type="table">Table 7</ref>. It is intuitive to expect higher counting accuracy with more number of scale branches (from nS = 1 to nS = 4) as people at all the scales are resolved better. Although this is true in theory, as the number of scales increase, so do the number of trainable parameters for the same amount of data. This might be the cause for slight increase in counting error for nS = 5. Regarding the ablations on the number of boxes, we train LSC-CNN for nB = 1 to nB = 4 (maintaining the same size increments γ as specified in Section 3.2 for all). Initially, we observe a progressive gain in the counting accuracy till nB = 3, but seems to saturate after that. This could be attributed to the decrease in training samples per box class as nB increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Architectural Ablations</head><p>In this section, the advantage of various architectural choices made for our model is established through experiments. LSC-CNN employs multi-scale top-down modulation through the TFM modules (Section 3.1.2). We train LSC-CNN without these   <ref type="figure" target="#fig_2">Figure 4</ref> is removed for all TFM networks) and the resultant MAE is labeled as No TFM in <ref type="table" target="#tab_8">Table 8</ref>. We also ablate with a sequential TFM (Seq TFM), in which every branch gets only one top-down connection from its previous scale as opposed to features from all lower resolution scales in LSC-CNN. The results evidence that having top-down modulation is effective in leveraging high-level scene context and helps improve count accuracy. But the improvement is drastic with the proposed multiple top-down connections and seems to aid better extraction of context information. The topdown modulation can be incorporated in many ways, with LSC-CNN using concatenation of top-down features with that of bottom-up. Following <ref type="bibr" target="#b25">[26]</ref>, we generate features to gate the bottom-up feature maps (Mult TFM). Specifically, we modify the second convolution layer for top-down processing in <ref type="figure" target="#fig_2">Figure  4</ref> with Sigmoid activation. The Sigmoid output from each topdown connection is element-wise multiplied to the incoming scale feature maps. A slight performance drop is observed with this setup, but the MAE is close to that of LSC-CNN, stressing that top-down modulation in any form could be useful. Now we ablate the training regime of LSC-CNN. The experiment labeled No GWTA in <ref type="table" target="#tab_8">Table 8</ref> corresponds to LSC-CNN trained with just the L comb loss (equation 4). <ref type="figure" target="#fig_8">Figure  8</ref> clearly shows that without GWTA, LSC-CNN completely  <ref type="figure">Fig. 9</ref>. Comparison of predictions made by face detectors SSH <ref type="bibr" target="#b17">[18]</ref> and TinyFaces <ref type="bibr" target="#b0">[1]</ref> against LSC-CNN. Note that the Ground Truth shown for WIDERFACE dataset is the actual and not the pseudo box ground truth. Normal face detectors are seen to fail on dense crowds.  fails in the high resolution scale (one-half), where the gradient averaging effect is prominent. A significant drop in MAE is observed as well, validating the hypothesis that GWTA aids better optimization of the model. Another important aspect of the training is the class balancing scheme employed. LSC-CNN is trained with no weighting, essentially with all α s b s set to 1. As expected, the counting error reaches an unacceptable level, mainly due to the skewness in the distribution of persons across scales. We also validate the usefulness of replicating certain VGG blocks in the feature extractor (Section 3.1.1) through an experiment without it, labeled as No Replication. Lastly, instead of our per-pixel box classification framework, we train LSC-CNN to regress the box sizes. Box regression is done for all the branches by replicating the last five convolutional layers of the TFM <ref type="figure" target="#fig_2">(Figure 4)</ref> into two arms; one for the per-pixel binary classification to locate person and the other for estimating the corresponding head sizes (the sizes are normalized to 0-1 for all scales). However, this setting could not achieve good MAE, possibly due to class imbalance across box sizes (Section 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>top-down connections (terminal 3 in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ST Part</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison with Object/Face Detectors</head><p>To further demonstrate the utility of our framework beyond any doubt, we train existing detectors like FRCNN <ref type="bibr" target="#b14">[15]</ref>, SSH <ref type="bibr" target="#b17">[18]</ref> and TinyFaces <ref type="bibr" target="#b0">[1]</ref> on dense crowd datasets. The anchors for these models are adjusted to match the box sizes (βs) of LSC-CNN for fair comparison. The models are optimized with the pseudo box ground truth generated from point annotations. For these, we compute counting metrics MAE and MSE</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Inference Time (ms) Parameters (in millions) FRCNN <ref type="bibr" target="#b14">[15]</ref> 231.4 41.5 SSH <ref type="bibr" target="#b17">[18]</ref> 48. along with point localization measure MLE in <ref type="table" target="#tab_10">Table 9</ref>. Note that the SSH and TinyFaces face detectors are also trained with the default anchor box setting as specified by their authors (labeled as def ). The evaluation points to the poor counting performance of the detectors, which incur high MAE scores. This is mainly due to the inability to capture dense crowds as evident from <ref type="figure">Figure 9</ref>. LSC-CNN, on the other hand, works well across density ranges, with quite convincing detections even on sparse crowd images from WIDERFACE <ref type="bibr" target="#b1">[2]</ref>. In addition, we compare the detectors for per image inference time (averaged over ST Part A <ref type="bibr" target="#b3">[4]</ref> test set, evaluated on a NVIDIA V100 GPU) and model size in <ref type="table" target="#tab_11">Table 10</ref>. The results reiterate the suitability of LSC-CNN for practical applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>This paper introduces a dense detection framework for crowd counting and renders the prevalent paradigm of density regression obsolete. The proposed LSC-CNN model uses a multicolumn architecture with top-down modulation to resolve people in dense crowds. Though only point head annotations are available for training, LSC-CNN puts bounding box on every located person. Experiments indicate that the model achieves not only better crowd counting performance than existing regression methods, but also has superior localization with all the merits of a detection system. Given these, we hope that the community would switch from the current regression approach to more practical dense detection. Future research could address spurious detections and make sizing of heads further accurate.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>The architecture of the proposed LSC-CNN is shown. LSC-CNN jointly processes multi-scale information from the feature extractor and provides predictions at multiple resolutions, which are combined to form the final detections. The model is optimized for per-pixel classification of pseudo ground truth boxes generated in the GWTA training phase (indicated with dotted lines).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>The exact configuration of Feature Extractor, which is a modified version of VGG-16<ref type="bibr" target="#b42">[43]</ref> and outputs feature maps at multiple scales.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>The implementation of the TFM module is depicted. TFM(s) processes the features from scale s (terminal 1) along with s multiscale inputs from higher branches (terminal 3) to output head detections (terminal 4) and the features (terminal 2) for the next scale branch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>sb</head><label></label><figDesc>[x, y] denote a boolean value indicating whether the location (x, y) belongs to box b. Then a person annotation is assigned a box b (B s b [x, y] = 1) if its pseudo size B[x, y] is between β s b and β s b+1 . Box sizes less than β s 1 are given to b = 1 and those greater than β s n B</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Samples of generated pseudo box ground truth. Boxes with same color belong to one scale branch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>s b [x, y]}, {B s b [x, y]}, {α s b }) w s h s where the inputs are the set of predictions {D s b } n B b=0 and pseudo ground truths {B s b } n B</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>2 Fig. 6 .</head><label>26</label><figDesc>l s wta [x, y] = ( x w 0 , y h 0 )=(x,y) l({D s b [x , y ]}, {B s b [x , y ]}, {ᾱ s b }), Illustration of the operations in GWTA training. GWTA only selects the highest loss making cell in every scale. The per-pixel cross-entropy loss is computed between the prediction and pseudo ground truth maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Predictions made by LSC-CNN on images from Shanghaitech, UCF-QNRF and UCF-CC-50 datasets. The results emphasize the ability of our approach to pinpoint people consistently across crowds of different types than the baseline density regression method.The remaining part of this section introduces the datasets along with the hyper-parameters if there is any change.Shanghaitech: The Shanghaitech (ST) dataset<ref type="bibr" target="#b3">[4]</ref> consists of total 1,198 crowd images with more than 0.3 million head annotations. It is divided into two sets, namely, Part A and Part B. Part A has density variations ranging from 33 to 3139 people per image with average count being 501.4. In contrast, images in Part B are relatively less diverse and sparser with an average density of 123.6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Demonstrating the effectiveness of GWTA in proper training of high resolution scale branches (notice the highlighted region).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Comparison of LSC-CNN on localization metrics against the baseline regression method. Our model seems to pinpoint persons more accurately.</figDesc><table><row><cell>Part A</cell><cell>16.8</cell><cell></cell><cell>9.6</cell><cell>72.6</cell><cell>66.4</cell><cell>75.5</cell><cell>70.2</cell><cell>112.9</cell><cell>94.6</cell><cell>149.2</cell><cell>136.5</cell></row><row><cell>ST Part B</cell><cell>12.28</cell><cell></cell><cell>9.0</cell><cell>11.5</cell><cell>8.1</cell><cell>13.1</cell><cell>9.6</cell><cell>21.0</cell><cell>17.4</cell><cell>28.9</cell><cell>26.5</cell></row><row><cell>UCF QNRF</cell><cell>14.2</cell><cell></cell><cell>8.6</cell><cell>155.8</cell><cell>120.5</cell><cell>157.2</cell><cell>125.8</cell><cell>186.7</cell><cell>159.9</cell><cell>219.3</cell><cell>206.0</cell></row><row><cell>UCF CC 50</cell><cell>14.3</cell><cell></cell><cell>9.7</cell><cell>282.9</cell><cell>225.6</cell><cell>326.3</cell><cell>227.4</cell><cell>369.0</cell><cell>306.8</cell><cell>425.8</cell><cell>390.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>TABLE 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell>Easy</cell><cell cols="2">Medium Hard</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Faceness [46]</cell><cell></cell><cell>71.3</cell><cell>53.4</cell><cell>34.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Two Stage CNN [2]</cell><cell>68.1</cell><cell>61.4</cell><cell>32.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>TinyFace [1]</cell><cell></cell><cell>92.5</cell><cell>91.0</cell><cell>80.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SSH [18]</cell><cell></cell><cell>93.1</cell><cell>92.1</cell><cell>84.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">CSR-A-thr (baseline)</cell><cell>30.2</cell><cell>41.9</cell><cell>33.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PSDNN [41]</cell><cell></cell><cell>60.5</cell><cell>60.5</cell><cell>39.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">LSC-CNN (Pseudo GT)</cell><cell>40.5</cell><cell>62.1</cell><cell>46.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">LSC-CNN (Actual GT)</cell><cell>57.31</cell><cell>70.10</cell><cell>68.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2</head><label>2</label><figDesc>Evaluation of LSC-CNN box prediction on WIDERFACE [2]. Our model and PSDNN are trained on pseudo ground truths, while others use full supervision. LSC-CNN has impressive mAP in Medium and Hard sets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3</head><label>3</label><figDesc>Counting performance comparison of LSC-CNN on UCF-QNRF<ref type="bibr" target="#b8">[9]</ref>.</figDesc><table><row><cell></cell><cell cols="2">ST Part A</cell><cell cols="2">ST Part B</cell><cell cols="2">UCF CC 50</cell></row><row><cell>Models</cell><cell>MAE</cell><cell>MSE</cell><cell cols="3">MAE MSE MAE</cell><cell>MSE</cell></row><row><cell>Zhang et al. [10]</cell><cell>181.8</cell><cell>277.7</cell><cell>32.0</cell><cell>49.8</cell><cell>467.0</cell><cell>498.5</cell></row><row><cell>MCNN [4]</cell><cell>110.2</cell><cell>173.2</cell><cell>26.4</cell><cell>41.3</cell><cell>377.6</cell><cell>509.1</cell></row><row><cell>SCNN [12]</cell><cell>90.4</cell><cell>135.0</cell><cell>21.6</cell><cell>33.4</cell><cell>318.1</cell><cell>439.2</cell></row><row><cell>CP-CNN [13]</cell><cell>73.6</cell><cell>106.4</cell><cell>20.1</cell><cell>30.1</cell><cell>295.8</cell><cell>320.9</cell></row><row><cell>IG-CNN [23]</cell><cell>72.5</cell><cell>118.2</cell><cell>13.6</cell><cell>21.1</cell><cell>291.4</cell><cell>349.4</cell></row><row><cell>Liu et al. [31]</cell><cell>72.0</cell><cell>106.6</cell><cell>14.4</cell><cell>23.8</cell><cell>279.6</cell><cell>388.9</cell></row><row><cell>IC-CNN [27]</cell><cell>68.5</cell><cell>116.2</cell><cell>10.7</cell><cell>16.0</cell><cell>260.9</cell><cell>365.5</cell></row><row><cell>CSR-Net [14]</cell><cell>68.2</cell><cell>115.0</cell><cell>10.6</cell><cell>16.0</cell><cell>266.1</cell><cell>397.5</cell></row><row><cell>SA-Net [24]</cell><cell>67.0</cell><cell>104.5</cell><cell>8.4</cell><cell>13.6</cell><cell>258.4</cell><cell>334.9</cell></row><row><cell>PSDNN [41]</cell><cell>65.9</cell><cell>112.3</cell><cell>9.1</cell><cell>14.2</cell><cell>359.4</cell><cell>514.8</cell></row><row><cell>LSC-CNN</cell><cell>66.4</cell><cell>117.0</cell><cell>8.1</cell><cell>12.7</cell><cell>225.6</cell><cell>302.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 4</head><label>4</label><figDesc>Benchmarking LSC-CNN counting accuracy on Shanghaitech<ref type="bibr" target="#b3">[4]</ref> and UCF CC 50<ref type="bibr" target="#b8">[9]</ref> datasets. LSC-CNN stands state-of-the-art in both ST PartB and UCF CC 50, with very competitive MAE on ST PartA.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>On Part A of Shanghaitech, LSC-CNN performs better than all the other density regression methods and has very competitive MAE to that of PSDNN<ref type="bibr" target="#b40">[41]</ref>, with the</figDesc><table><row><cell>Method</cell><cell>S1</cell><cell>S2</cell><cell>S3</cell><cell>S4</cell><cell>S5</cell><cell>Avg.</cell></row><row><cell>Zhang et al. [10]</cell><cell>9.8</cell><cell cols="4">14.1 14.3 22.2 3.7</cell><cell>12.9</cell></row><row><cell>MCNN [4]</cell><cell>3.4</cell><cell cols="4">20.6 12.9 13.0 8.1</cell><cell>11.6</cell></row><row><cell>SCNN [12]</cell><cell>4.4</cell><cell cols="4">15.7 10.0 11.0 5.9</cell><cell>9.4</cell></row><row><cell>CP-CNN [13]</cell><cell>2.9</cell><cell cols="4">14.7 10.5 10.4 5.8</cell><cell>8.8</cell></row><row><cell>Liu et al. [31]</cell><cell>2.0</cell><cell>13.1</cell><cell>8.9</cell><cell cols="2">17.4 4.8</cell><cell>9.2</cell></row><row><cell>IC-CNN [27]</cell><cell cols="2">17.0 12.3</cell><cell>9.2</cell><cell>8.1</cell><cell>4.7</cell><cell>10.3</cell></row><row><cell>CSR-Net [14]</cell><cell>2.9</cell><cell>11.5</cell><cell>8.6</cell><cell cols="2">16.6 3.4</cell><cell>8.6</cell></row><row><cell>SA-Net [24]</cell><cell>2.6</cell><cell>13.2</cell><cell>9.0</cell><cell cols="2">13.3 3.0</cell><cell>8.2</cell></row><row><cell>LSC-CNN (Ours)</cell><cell>2.9</cell><cell>11.3</cell><cell>9.4</cell><cell cols="2">12.3 4.3</cell><cell>8.0</cell></row><row><cell></cell><cell cols="2">TABLE 5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">LSC-CNN on WorldExpo'10 [9] beats other methods in average MAE.</cell></row><row><cell>Method</cell><cell cols="6">GAME0 GAME1 GAME2 GAME3</cell></row><row><cell>Guerrero et al. [45]</cell><cell>14.0</cell><cell cols="2">18.1</cell><cell>23.7</cell><cell></cell><cell>28.4</cell></row><row><cell>Hydra CNN [11]</cell><cell>10.9</cell><cell cols="2">13.8</cell><cell>16.0</cell><cell></cell><cell>19.3</cell></row><row><cell>Li et al. [14]</cell><cell>3.7</cell><cell cols="2">5.5</cell><cell>8.6</cell><cell></cell><cell>15.0</cell></row><row><cell>PSDNN [41]</cell><cell>4.8</cell><cell cols="2">5.4</cell><cell>6.7</cell><cell></cell><cell>8.4</cell></row><row><cell>LSC-CNN (Ours)</cell><cell>4.6</cell><cell cols="2">5.4</cell><cell>6.9</cell><cell></cell><cell>8.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 6</head><label>6</label><figDesc>Evaluation of LSC-CNN on TRANCOS [45] vehicle counting dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 8</head><label>8</label><figDesc>Validating various architectural design choices of LSC-CNN.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 9</head><label>9</label><figDesc>LSC-CNN compared with existing detectors trained on crowd datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 10</head><label>10</label><figDesc>Efficiency of detectors in terms of inference speed and model size.</figDesc><table><row><cell></cell><cell>1</cell><cell>19.8</cell></row><row><cell>TinyFace [1]</cell><cell>348.6</cell><cell>30.0</cell></row><row><cell>LSC-CNN n S = 1</cell><cell>29.4</cell><cell>12.9</cell></row><row><cell>LSC-CNN n S = 2</cell><cell>32.3</cell><cell>18.3</cell></row><row><cell>LSC-CNN n S = 3</cell><cell>50.6</cell><cell>20.6</cell></row><row><cell>LSC-CNN n S = 4</cell><cell>69.0</cell><cell>21.9</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported by SERB, Dept. of Science and Technology, Govt. of India (Proj: SB/S3/EECE/0127/2015).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Finding tiny faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">WIDER FACE: A face detection benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">World&apos;s largest selfie</title>
		<ptr target="https://www.gsmarena.com/nokialumia730capturesworldslargestselfie-news-10285.php" />
		<imprint>
			<biblScope unit="page" from="2019" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Single-image crowd counting via multi-column convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Detection of multiple, partially occluded humans in a single image by bayesian combination of edgelet part detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Detecting pedestrians using patterns of motion and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Snow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic adaptation of a generic pedestrian detector to a specific traffic scene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Detecting humans in dense crowds using locally-consistent scale prior and global occlusion reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-source multiscale counting in extremely dense crowd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Saleemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Seibert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cross-scene crowd counting via deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Towards perspective-free object counting with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Onoro-Rubio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>López-Sastre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Switching convolutional neural network for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Surya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generating high-quality crowd density maps using contextual pyramid CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">CSRNet: Dilated convolutional neural networks for understanding the highly congested scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">YOLO9000: Better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">SSH: Single stage headless face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Samangouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">End-to-end people detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.04878</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep people counting in extremely dense crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia (ACMMM)</title>
		<meeting>the ACM International Conference on Multimedia (ACMMM)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to count with CNN boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Walach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Crowd-Net: A deep convolutional network for dense crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Boominathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Kruthiventi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia (ACMMM)</title>
		<meeting>the ACM International Conference on Multimedia (ACMMM)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Divide and grow: Capturing huge diversity in crowd images with incrementally growing CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">N</forename><surname>Sajjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Scale aggregation network for accurate and efficient crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">CNN-based cascaded multi-task learning of high-level prior and density estimation for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Advanced Video and Signal Based Surveillance</title>
		<meeting>the IEEE International Conference on Advanced Video and Signal Based Surveillance</meeting>
		<imprint>
			<publisher>AVSS</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Top-down feedback for crowd counting convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Iterative crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">DecideNet: Counting varying density crowds through attention guided detection and density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Composition loss for counting, density map estimation and localization in dense crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tayyab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Athrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Al-Maadeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rajpoot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Exploiting unlabeled data in CNNs by self-supervised learning to rank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Almost unsupervised learning for dense crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">N</forename><surname>Sajjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maurya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Crowd counting with deep negative correlation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Crowd counting via adversarial cross-scale consistency pursuit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (CVPR)</title>
		<meeting>the IEEE International Conference on Computer Vision (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">CMS-RCNN: Contextual multi-scale region-based CNN for unconstrained face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning for Biometrics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">DAFE-FD: Density aware feature enrichment for face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Point in, box out: Beyond counting persons in crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Winner-take-all autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Extremely overlapping vehicle counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guerrero-Gmez-Olmedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Torre-Jimnez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lpez-Sastre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Bascn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ooro-Rubio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Iberian Conference on Pattern Recognition and Image Analysis (IbPRIA)</title>
		<meeting>the Iberian Conference on Pattern Recognition and Image Analysis (IbPRIA)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">From facial parts responses to face detection: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
