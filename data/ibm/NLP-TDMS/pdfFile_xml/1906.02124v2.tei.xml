<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Patent Classification by Fine-Tuning BERT Language Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieh-Sheng</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Information Engineering</orgName>
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieh</forename><surname>Hsiang</surname></persName>
							<email>jhsiang@ntu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Information Engineering</orgName>
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Patent Classification by Fine-Tuning BERT Language Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work we focus on fine-tuning a pre-trained BERT model and applying it to patent classification. When applied to large datasets of over two millions patents, our approach outperforms the state of the art by an approach using CNN with word embeddings. In addition, we focus on patent claims without other parts in patent documents. Our contributions include: (1) a new state-of-the-art result based on pretrained BERT model and fine-tuning for patent classification, (2) a large dataset USPTO-3M at the CPC subclass level with SQL statements that can be used by future researchers, (3) showing that patent claims alone are sufficient for classification task, in contrast to conventional wisdom.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Patent classification is a multi-label classification task. It is challenging because the number of labels can be large, e.g. more than 630 at subclass level. We see this task from two aspects. From the perspective of Deep Learning, pre-training an unsupervised language model on large corpus and fine-tuning the model on downstream tasks have resulted in several state-of-the-art performances recently. Such pre-training models include ELMo (Embeddings from Language Models) <ref type="bibr" target="#b0">[1]</ref>, ULMFiT (Universal Language Model with Fine-tuning) <ref type="bibr" target="#b1">[2]</ref>, OpenAI GPT (Generative Pre-Training) <ref type="bibr" target="#b2">[3]</ref>, BERT (Bidirectional Encoder Representations from Transformers) <ref type="bibr" target="#b3">[4]</ref> and OpenAI GPT-2 <ref type="bibr" target="#b4">[5]</ref>. Among them, BERT is the most suitable for experiments if having the availability of source code and pre-trained models considered. Therefore, we set a goal to know how well BERT can perform on patent classification after finetuning.</p><p>From the perspective of patent research, it is time to have a new baseline with a large dataset based on the CPC (Cooperative Patent Classification) system. In general, Deep Learning outperforms other methods when the size of dataset is large. In the past, the sizes of datasets for patent research vary widely. Such variation made comparison difficult. Inference is also less valuable because sometimes the datasets were outdated. In this work, we prepared a new dataset based on the CPC with more than three millions US patents. Patent researchers can leverage the dataset or our approach to cover more tasks, since the entry barriers for data, algorithm and computation are all much lower than before.</p><p>The CPC system and the IPC (International Patent Classification) system are two of the most commonly used classification systems. The CPC is a more specific and detailed version of the IPC system. On 1st January 2013, the CPC system came into force and, with which, the United States Patent and Trademark Office (USPTO) replaced its original system. A growing number of national patent offices have decided to follow the CPC <ref type="bibr" target="#b5">[6]</ref>. Therefore, it is foreseeable that the CPC system will eventually replace the IPC system as the new standard. However, most of the papers in the field were based on the IPC because of the CLEF-IP competition <ref type="bibr" target="#b6">[7]</ref>. The CLEF-IP competition in 2011 was based on the IPC at the subclass level. The dataset consisted of patents filed between 1978 and 2009. Key performances were evaluated with P@1 (precision at top 1), P@5, R@5 (recall at top 5), F1@5 and other metrics. It is not clear to us why R@1 and F1@1 were omitted. This is critical for our work, because a precision value could be very high at the cost of a very low recall. Therefore, P@1 alone might not be a fair number to compare if R@1 and F@1 were not provided. In this work, our metric F1@1 is the best performance of patent classification. We use it to benchmark with the best F1 values in other works, regardless of whether the value is F1@1 or F1@5.</p><p>Moreover, our datasets are based on patent claims. The importance of patent claims was underappreciated in the past. When drafting a new patent application, it is a common practice for patent practitioners to draft the patent claims first. The rest of the patent document could be derived or extended from the claims. In patent law, the claims define the scope or the "metes and bounds" of the patented invention. It is a 'bedrock principle' of patent law that 'the claims of a patent define the invention to which the patentee is entitled the right to exclude' <ref type="bibr" target="#b7">[8]</ref>. One reason to use patent claims mainly is for our downstream task of patent claim generation in the future. To our knowledge, our work is the first to focus on patent claims and claims only, instead of using claims as supplementary data in the past. To keep our model simpler, we use only the first claim of each patent and leave the benefit of other independent and dependent claims to future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>We highlight the most relevant works in recent years. Li et al. <ref type="bibr" target="#b8">[9]</ref> proposed DeepPatent as a deep learning algorithm based on CNN (Convolutional Neural Network) and word vector embedding. They evaluated the algorithm on the CLEF-IP dataset, compared it with other algorithms in the CLEF-IP competition and claimed a precision of 83.98%, which outperformed all other algorithms. DeepPatent was further tested on USPTO-2M, a newly contributed dataset having 2,000,147 US utility patents in 637 categories at the IPC subclass level after data cleaning. DeepPatent achieved a precision of 73.88% @ Top 1, with no F1@ 1 disclosed. Further experiments by the authors using the same dataset showed that DeepPatent outperformed Random Forest, Decision Tree, BP Networks and Naive Bayes. The best F1 is about 43% @ Top 5. In this work, we use DeepPatent as the baseline to benchmark. We also assumed that the aforementioned methods benchmarked with DeepPatent are unlikely to perform better if the dataset is larger than USPTO-2M.</p><p>The idea of fine-tuning a pre-trained language model for patent classification was proposed in the Australasian Language Technology Association Workshop 2018 <ref type="bibr" target="#b9">[10]</ref>. The task is to classify Australian patents at the IPC section level (8 labels). The dataset has 75,250 patents (60/40 as training/testing data split). Hepburn <ref type="bibr" target="#b10">[11]</ref> used SVM and ULMFiT to achieve the best results in the student category. ULMFiT is a transfer learning technique and the fine-tuning idea is similar to fine-tuning a pre-trained BERT model. The major difference between Hepburn's work and ours is the pre-trained model itself. Another difference in this work is the size of the dataset. Our dataset has over three millions patents while the dataset for the workshop has only 75,250. Our F1 of 80.98% at the CPC section level is also better than the F1 of 78.4% at the IPC section level in Hepburn's work.</p><p>It is noted that most of patent classification tasks were done on IPC in the past. Tran and Kavuluru <ref type="bibr" target="#b11">[12]</ref> claimed being the first to reinitiate the patent classification task under the new CPC coding scheme. They used logistic regression as a base classifier and exploited extra data or method, such as the hierarchical taxonomy of CPC, the citation records of a test patent, various label ranking and cut-off methods. By experimenting on 436,993 U.S. patents (2010~2011 &amp; 70/30 as training/testing data split) at the subclasses level, their best method achieved 69.89% in micro-F1 score. In this work, we will skip benchmarking with their result since we target a larger dataset without ad-hoc feature engineering. Feature engineering is difficult to scale up in general.</p><p>By aiming at CPC and a larger dataset, we also skip any benchmark with the CLEF-IP results. It is uncertain whether fitting a big model like BERT to smaller datasets may make any sense. Conversely it should be more fruitful for other algorithms to benchmark with a larger CPC dataset in the future. Nevertheless, some of the recent works based the legacy CLEF-IP are still noteworthy, for knowing the highest F1 value in the past. For example, comparing with fastText, Yadrintsev et al. <ref type="bibr" target="#b12">[13]</ref> claimed that KNN is a viable alternative to traditional text classifiers. Their dataset has 699,000 patents (70/30 as training/testing data split). Their best result achieved 71.02% in micro-F1 score at the IPC subclass level.</p><p>Lim and Kwon <ref type="bibr" target="#b13">[14]</ref> showed 87.2% precision when using titles, abstracts, claims, technical fields and backgrounds of patents. However, no respective recall or F1 value was disclosed. A fair comparison is therefore not feasible. Their dataset has 564,793 Korean patents at the IPC subclass level. Their method combined multinomial naive Bayes with other tricks such as a Korean language morphological analyzer, using 1,860 stopwords removed and TF-ICF (a variation of the wellknown TF-IDF).</p><p>Hu et al. <ref type="bibr" target="#b14">[15]</ref> showed that a hierarchical feature extraction model can capture both local features as well as global semantics. An n-gram feature extractor based on CNN was designed to extract local features. A bidirectional long-short-term memory (BiLSTM) neural network model was proposed to capture sequential correlations from higher-level representations. The training, validation and test datasets contain 72,532, 18,133, and 2679 mechanical patents from the CLEF-IP dataset. The number of labels is 96 for mechanical patents only. The hierarchical model outperformed other models using CNN, LSTM or BiLSTM alone. Their best F1 is 63.97% @ Top 1. Back to the CLEF-IP competition itself, Verberne and D'hondt <ref type="bibr" target="#b15">[16]</ref> reached their best F1-value 70.59% in a series of classification experiments with the Linguistic Classification System (LCS). The training dataset has 905,458 patents and the testing has only 1,000 patents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data</head><p>Most of past patent datasets were from the CELF-IP or patent offices. We found it easier to leverage the Google Patents Public Datasets <ref type="bibr" target="#b16">[17]</ref> on BigQuery released in 2017. A dataset based on SQL lowers the entry barrier of data preparation significantly. We deem SQL statements as a better way than sharing conventional datasets for two reasons: (1) Separation of concerns. If a dataset contains pre-processing or post-processing already, it could be harder for other researchers to reuse when needing different manipulations. (2) Clarity and flexibility. An SQL statement is precise and easy to revise for different criteria. The SQL statement for our training dataset is listed in the Appendix A.</p><p>Our new dataset is called USPTO-3M (3,050,615 patents). Based on the SQL statements, it would be easy for other researchers to cover all patents if computing resource for training is not a constraint. When benchmarking with the DeepPatent, we use its dataset USPTO-2M to benchmark when feasible. If not feasible, we combine other data from USPTO-3M. For example, USPTO-2M does not have claims. In order to see how claims impact performance, we'd combine both datasets. We explain more details in <ref type="table">Table 1</ref> of the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Method &amp; Experimental Setup</head><p>In this work, we leverage the released BERT-Base pre-trained model (Uncased: 12-layer, 768hidden, 12-heads, 110M parameters) <ref type="bibr">[18]</ref>. We leave other models such as the BERT-Large (340M parameters) to the future, because the BERT-Base is already sufficient to outperform DeepPatent.</p><p>Our implementation follows the fine-tuning example released in the BERT project. For multilabel purpose, we use sigmoid cross entropy with logits function to replace the original softmax function which is suitable for one-hot classification only. We intentionally keep the code change as minimal as possible so as to make the BERT test a vanilla baseline for future experiments to compare against. All hyperparameters remain as default values, e.g. max_seq_length as 128. During our experiments we also observed that it might be sufficient for the max_seq_length to be shorter if having fewer labels, e.g. 9 labels at CPC section level. We leave testing different hyperparameters at different CPC levels to the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results: PatentBERT vs DeepPatent</head><p>In <ref type="table">Table 1</ref> we show the original DeepPatent performance in row (a) ~ (d) and our results in row (e) ~ (l). In row (a), DeepPatent achieved the precision of 83.98% @ Top 1 based on EPO and WIPO data. It was claimed that DeepPatent outperforms the state-of-the-art 82.1% @ Top 1 achieved by SVM with full content information of the patent and complicated human-designed features. In row (b), DeepPatent achieved the precision of 73.88% @ Top 1 based on USPTO-2M at the IPC subclass level. It is noted that no recall or F1 were provided in rows (a) and (b). In row (c), DeepPatent achieved the highest F1 of 55.09% @ Top 4 based on EPO and WIPO data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Patent Data <ref type="bibr" target="#b0">(1)</ref> Train <ref type="bibr" target="#b1">(2)</ref> Test <ref type="formula">(</ref> In row (d), we list the ranges of F1, precision and recall because no precise numbers were provided in their work. Based on USPTO-2M dataset, the highest F1 value achieved by DeepPatent is lower than 45% @Top 5.</p><p>PatentBERT results start from row (e). We compare row (e) with row (d) to show that PatentBERT reaches a higher F1 @ Top 5 as 46.85%. In row (f), the F1 value reaches even higher as 64.91% @ Top 1. In row (g), we use patent claims to replace title and abstract. The F1 value drops a little but the difference does not matter to our future downstream task. In row (h), we show that CPC is better than IPC at subclass level and the F1 value reaches 66.83%. It is noted that the precision value 84.26% achieved by PatentBERT outperforms DeepPatent in both row (a) and (b).</p><p>In row (i), we show that the F1 value remains stable when the size of test dataset triples. In row (j), we show that the F1 value is also stable after using the larger dataset USPTO-3M. We further observed from row (j), (k) and (l) that the F1, precision and recall values all dropped slightly as the date (year) of the test data moved further away from the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Patents might be an ideal data source for human to solve artificial innovation. However, patent classification as groundwork has been a challenging task with no satisfactory performance for decades. In this paper we present a new stateof-the-art approach based on fine-tuning a pretrained BERT model and it outperforms DeepPatent. Our results also show that using patent claims alone is sufficient for classification task. Most important of all, the recent success of the two-stage framework (pre-training &amp; fine-tuning) in Deep Learning is promising for patent researchers to explore more in the future. Patent classification in this work is just an example.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>,950,247 patents out of USPTO-2M for DeepPatent. 1,933,105 patents for PatentBERT. Minor discrepancy exists due to different data sources and probably preprocessing criteria.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>3)</cell><cell>F1</cell><cell>Precision</cell><cell>Recall</cell><cell>TREC</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell><cell>EVAL</cell></row><row><cell>(a)</cell><cell>DeepPatent</cell><cell>IPC+Title+Abstract</cell><cell>EPO+WIP</cell><cell>EPO</cell><cell>N/A</cell><cell>83.98</cell><cell>N/A</cell><cell>Top 1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>O</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(b)</cell><cell>DeepPatent</cell><cell>IPC+Title+Abstract</cell><cell>2006~2014</cell><cell>2015-A</cell><cell>N/A</cell><cell>73.88</cell><cell>N/A</cell><cell>Top 1</cell></row><row><cell>(c)</cell><cell>DeepPatent</cell><cell>IPC+Title+Abstract</cell><cell>EPO+WIP</cell><cell>EPO</cell><cell>55.09</cell><cell>45.79</cell><cell>75.46</cell><cell>Top 4</cell></row><row><cell></cell><cell></cell><cell></cell><cell>O</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(d)</cell><cell>DeepPatent</cell><cell>IPC+Title+Abstract</cell><cell>2006~2014</cell><cell>2015-A</cell><cell>&lt; 45</cell><cell>&lt; 35</cell><cell>&lt; 74</cell><cell>Top 5</cell></row><row><cell cols="3">(e) PatentBERT IPC+Title+Abstract</cell><cell>2006~2014</cell><cell cols="2">2015-A 46.85</cell><cell>32.19</cell><cell>86.06</cell><cell>Top 5</cell></row><row><cell>(f)</cell><cell>PatentBERT</cell><cell>IPC+Title+Abstract</cell><cell>2006~2014</cell><cell cols="2">2015-A 64.91</cell><cell>80.61</cell><cell>54.33</cell><cell>Top 1</cell></row><row><cell>(g)</cell><cell>PatentBERT</cell><cell>IPC+Claim</cell><cell>2006~2014</cell><cell cols="2">2015-A 63.74</cell><cell>79.14</cell><cell>53.36</cell><cell>Top 1</cell></row><row><cell>(h)</cell><cell>PatentBERT</cell><cell>CPC+Claim</cell><cell>2006~2014</cell><cell cols="2">2015-A 66.83</cell><cell>84.26</cell><cell>55.38</cell><cell>Top 1</cell></row><row><cell>(i)</cell><cell>PatentBERT</cell><cell>CPC+Claim</cell><cell>2006~2014</cell><cell cols="2">2015-B 66.80</cell><cell>84.24</cell><cell>55.35</cell><cell>Top 1</cell></row><row><cell>(j)</cell><cell>PatentBERT</cell><cell>CPC+Claim</cell><cell>2000~2014</cell><cell cols="2">2015-B 66.71</cell><cell>84.95</cell><cell>54.92</cell><cell>Top 1</cell></row><row><cell>(k)</cell><cell>PatentBERT</cell><cell>CPC+Claim</cell><cell>2000~2014</cell><cell>2016</cell><cell>65.89</cell><cell>84.89</cell><cell>53.84</cell><cell>Top 1</cell></row><row><cell>(l)</cell><cell>PatentBERT</cell><cell>CPC+Claim</cell><cell>2000~2014</cell><cell>2017</cell><cell>65.35</cell><cell>83.97</cell><cell>53.49</cell><cell>Top 1</cell></row><row><cell cols="5">(1) IPC subclass level: 632 labels. CPC subclass level: 656 labels</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">(2) Training dataset size:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4"> EPO: 580,546 patents. WIPO: 161,551 patents.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7"> USPTO-2M: 2,000,147 patents by the DeepPatent (2006~2015, from USPTO)</cell><cell></cell><cell></cell></row><row><cell cols="9"> USPTO-3M: 3,050,615 patents, our new dataset with SQL statements (2000~2015, from Google Patents</cell></row><row><cell></cell><cell cols="2">Public Datasets on BigQuery)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5"> 2006~2014: 1 2000~2014 : 2,900,615 patents out of USPTO-3M for PatentBERT</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">(3) Testing dataset:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3"> EPO: 1,350 patents</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="9"> 2015-A: 49,900 patents out of USPTO-2M for DeepPatent. 49,670 patents for PatentBERT (out of</cell></row><row><cell></cell><cell cols="4">USPTO-3M and based on DeepPatent's list of test patents)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5"> 2015-B: 150,000 of the 298,559 patents in 2015 (from USPTO-3M)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5"> 2016: 150,000 of the 298,559 patents in 2016 (from BigQuery)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6"> 2017: 150,000 of the 298,559 patents in 2017-01~2017-08 (from BigQuery)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">Table 1: Patent Classification Performance</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1802.05365" />
		<title level="m">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Universal Language Model Fine-tuning for Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 56th Annu</title>
		<meeting>56th Annu<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="328" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Improving Language Understanding by Generative Pre-Training (transformer in real world)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null<address><addrLine>Minneapolis, MN, USA; Long Short Pap</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-02" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Language Models are Unsupervised Multitask Learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radrof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Analysis of the patent documentation coverage of the CPC in comparison with the IPC with a focus on Asian documentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Degroote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Held</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.wpi.2017.10.001</idno>
	</analytic>
	<monogr>
		<title level="j">World Pat. Inf</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="78" to="84" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Clef-Ip</surname></persName>
		</author>
		<ptr target="http://ifs.tuwien.ac.at/~clef-ip/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Phillips V</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corp</surname></persName>
		</author>
		<title level="m">415 F.3d 1303, 1312 (Fed. Cir. 2005) (en banc)</title>
		<imprint/>
	</monogr>
	<note>n.d.</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">DeepPatent: patent classification with convolutional neural networks and word embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11192-018-2905-5</idno>
	</analytic>
	<monogr>
		<title level="j">Scientometrics</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page" from="721" to="744" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Australasian Language Technology Association</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Universal Language Model Finetuning for Patent Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hepburn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Australas. Lang. Technol. Assoc. Work. 2018</title>
		<imprint>
			<biblScope unit="page" from="93" to="96" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Supervised Approaches to Assign Cooperative Patent Classification (CPC) Codes to Patents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kavuluru</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-71928-3</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="22" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Fast and Accurate Patent Classification in Search Engines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Yadrintsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bakarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Suvorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sochenkov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">IPC Multi-label Classification Based on the Field Functionality of Patent Documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kwon</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-49586-6</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">10086</biblScope>
			<biblScope unit="page" from="677" to="691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A Hierarchical Feature Extraction Model for Multi-Label Mechanical Patent Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.3390/su10010219</idno>
	</analytic>
	<monogr>
		<title level="j">Sustainability</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">219</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Quantifying the Challenges in Parsing Patent Claims</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Verberne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Oostdijk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koster</surname></persName>
		</author>
		<idno>doi:970</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. 1st Int</title>
		<meeting>1st Int</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1609</biblScope>
			<biblScope unit="page" from="14" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Google Patents Public Datasets on BigQuery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Google</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
