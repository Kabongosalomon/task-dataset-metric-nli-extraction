<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Vector-quantized neural networks for acoustic unit discovery in the ZeroSpeech 2020 challenge</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Niekerk</surname></persName>
							<email>benjamin.l.van.niekerk@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">E&amp;E Engineering</orgName>
								<orgName type="institution">Stellenbosch University</orgName>
								<address>
									<country key="ZA">South Africa</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leanne</forename><surname>Nortje</surname></persName>
							<email>nortjeleanne@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">E&amp;E Engineering</orgName>
								<orgName type="institution">Stellenbosch University</orgName>
								<address>
									<country key="ZA">South Africa</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herman</forename><surname>Kamper</surname></persName>
							<email>kamperh@sun.ac.za</email>
							<affiliation key="aff0">
								<orgName type="department">E&amp;E Engineering</orgName>
								<orgName type="institution">Stellenbosch University</orgName>
								<address>
									<country key="ZA">South Africa</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Vector-quantized neural networks for acoustic unit discovery in the ZeroSpeech 2020 challenge</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: unsupervised speech processing</term>
					<term>acoustic unit discovery</term>
					<term>voice conversion</term>
					<term>representation learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we explore vector quantization for acoustic unit discovery. Leveraging unlabelled data, we aim to learn discrete representations of speech that separate phonetic content from speaker-specific details. We propose two neural models to tackle this challenge -both use vector quantization to map continuous features to a finite set of codes. The first model is a type of vector-quantized variational autoencoder (VQ-VAE). The VQ-VAE encodes speech into a sequence of discrete units before reconstructing the audio waveform. Our second model combines vector quantization with contrastive predictive coding (VQ-CPC). The idea is to learn a representation of speech by predicting future acoustic units. We evaluate the models on English and Indonesian data for the ZeroSpeech 2020 challenge. In ABX phone discrimination tests, both models outperform all submissions to the 2019 and 2020 challenges, with a relative improvement of more than 30%. The models also perform competitively on a downstream voice conversion task. Of the two, VQ-CPC performs slightly better in general and is simpler and faster to train. Finally, probing experiments show that vector quantization is an effective bottleneck, forcing the models to discard speaker information.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Modern speech and language technologies are developed with massive amounts of annotated data. However, large datasets of transcribed speech are not available for low-resource languages and building new corpora can be prohibitively expensive. As a result, tools like automatic speech recognition and text-to-speech are not available for many of the world's languages.</p><p>To address this problem, zero-resource speech processing aims to develop methods that can learn directly from speech without explicit supervision. The goal is to leverage unlabelled data to discover representations that capture meaningful phonetic contrasts while being invariant to background noise and speaker-specific details. These representations can then be used to bootstrap training in downstream speech systems and reduce requirements on labelled data. Additionally, since infants acquire language without explicit supervision, the discovered representations can be used in cognitive models of language learning <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>.</p><p>Over the last few years, progress in this area has been driven by the ZeroSpeech Challenges <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>. ZeroSpeech 2020 consolidates previous challenges, allowing submissions to both the 2017 and 2019 tracks. We focus on ZeroSpeech 2019: Text-to-Speech Without Text, which requires participants to discover discrete acoustic units from unlabelled data. From the discovered units, the task is then to synthesize speech in a target speaker's voice. Synthesized utterances are evaluated in terms of intelligibility, speaker-similarity, and naturalness. While similar to voice con-version <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>, an explicit goal of ZeroSpeech 2019 is to learn low-bitrate representations that perform well on phone discrimination tests. In contrast to work on continuous representation learning <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>, this encourages participants to find discrete units that correspond to distinct phones. <ref type="bibr" target="#b0">1</ref> Early approaches to acoustic unit discovery typically combined clustering methods with hidden Markov models <ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref>. More recent studies have explored neural networks with intermediate discretization <ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref>. In this paper, we investigate vector quantized (VQ) neural networks for acoustic unit discovery, and propose two models for the ZeroSpeech 2020 challenge.</p><p>The first model is a type of vector-quantized variational autoencoder (VQ-VAE) <ref type="bibr" target="#b23">[24]</ref>. The VQ-VAE maps speech into a discrete latent space before reconstructing the original waveform. Instead of using WaveNet <ref type="bibr" target="#b24">[25]</ref>, we opt for a lightweight recurrent network as the decoder. The result is a smaller, faster model that can be trained on a single GPU.</p><p>Inspired by vq-wav2vec <ref type="bibr" target="#b25">[26]</ref>, the second model combines vector quantization with contrastive predictive coding (VQ-CPC). Using a contrastive loss, the model is trained to distinguish future acoustic units from a set of negative examples. We compare across-speaker and within-speaker sampling for negative examples and show that the latter is important for speaker invariance.</p><p>In ABX phone discrimination tests on English and Indonesian data, the models outperform all other submissions to the ZeroSpeech 2019 and 2020 challenges. On the voice conversion task, both models are competitive with VQ-CPC achieving the best naturalness and speaker-similarity scores on the English dataset. Finally, in probing experiments, we analyze the effect of vector quantization, showing that it imposes an information bottleneck that separates phonetic and speaker content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Vector-quantized neural networks 2.1. Vector quantization</head><p>The VQ layer consists of a trainable codebook {e1, e2, . . . , eK } with K distinct codes. In the forward pass, a sequence of continuous feature vectors z := z1, z2, . . . , zT is discretized by mapping each zi to it's nearest neighbor in the codebook. Concretely, we find k := arg min j ||zi − ej|| 2 and replace zi with the code e k , resulting in the quantized sequenceẑ := ẑ1,ẑ2, . . . ,ẑT . Since the arg min operator is not differentiable, in the backward pass, gradients are approximated using the straight-through estimator <ref type="bibr" target="#b26">[27]</ref>. To train the codebook, we use an exponential moving average of the continuous features. Finally, a commitment cost is added to the loss to encourage each zi to commit to the selected code. For a more detailed explanation see <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Vector-quantized variational autoencoder</head><p>Our first model is a type of VQ-VAE inspired by the WaveNet autoencoders proposed in <ref type="bibr" target="#b21">[22]</ref>. To reduce computational costs, we replace the WaveNet decoder <ref type="bibr" target="#b24">[25]</ref> with a lightweight RNN-based vocoder <ref type="bibr" target="#b27">[28]</ref>. Together with automatic mixed precision <ref type="bibr" target="#b28">[29]</ref>, this allows us to train on a single GPU. Additionally, to learn a lowbitrate representation, we use a much smaller codebook. Finally, we release code and pretrained weights. <ref type="bibr" target="#b1">2</ref> Model description. The VQ-VAE can be divided into the three components shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The encoder takes a speech waveform sampled at 16 kHz as input and computes a log-Mel spectrogram. The spectrogram is processed by a stack of 5 convolutional layers, which downsamples the input by a factor of 2. In the bottleneck, the output of the encoder is projected into a sequence of continuous features. The representation is then discretized using a VQ layer with 512 codes. Finally, the decoder tries to reconstruct the original waveform. To predict the next sample, we condition an autoregressive model on the output of the bottleneck, the speaker identity, and past waveform samples.</p><p>For acoustic unit discovery, the VQ-VAE balances two opposing pressures. On the one hand, the encoder must preserve information from the input to accurately reconstruct the waveform. On the other hand, vector quantization imposes an information bottleneck, forcing a compressed representation that discards non-essential details. To encourage the bottleneck to specifi- The decoder (green) then tries to reconstruct the input waveform from the discrete representation using an RNN-based vocoder conditioned on a speaker embedding.</p><p>cally discard speaker information, we condition the decoder on speaker identity during training.</p><p>Training details. We train the model to maximize the loglikelihood of the waveform given the bottleneck, i.e. we minimize the sum of the reconstruction error and commitment cost:</p><formula xml:id="formula_0">L := − 1 N N i=1 log p(xi|ẑ) + β 1 T T i=1 ||zi − sg(ẑi)|| 2 ,</formula><p>where x1, x2, . . . , xN is a sequence of waveform samples, β is the commitment cost weight, and sg(·) denotes the stop-gradient operator. The model is trained on minibatches of 52 segments, each 320 ms long. We use the Adam optimizer <ref type="bibr" target="#b29">[30]</ref> with an initial learning rate of 4 · 10 −4 , which is halved after 300k and 400k steps. The network is trained for a total of 500k steps. Voice conversion. At test time, we can generate speech in a target voice by conditioning the decoder on a specific speaker. First, we encode a source utterance into a sequence of acoustic units. Since the bottleneck separates speaker details form phonetic information, we can replace the speaker while retaining the content of the utterance. Specifically, the output of the bottleneck is concatenated with the target speaker embedding and piped to the decoder.</p><p>Practical considerations. Since our goal is to discover phone-like representations, adjacent frames within the same phone should ideally be mapped to the same unit. To encourage consistency across frames, we apply time-jitter regularization <ref type="bibr" target="#b21">[22]</ref>: during training the code assigned to each frame may be replaced by one of its neighbors, forcing the discovered codes to be useful across multiple time steps. We apply jitter directly after the bottleneck, with a replacement probability of 0.5. Another common issue with vector quantization is codebook collapsewhere only a few codes are ever selected <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b30">31]</ref>. We found that batch normalization, coupled with large batch sizes, improved codebook utilization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Vector-quantized contrastive predictive coding</head><p>Contrastive predictive coding (CPC) is a recently proposed framework for unsupervised representation learning <ref type="bibr" target="#b31">[32]</ref>. Using a contrastive loss, models are trained to distinguish future observations from a set of negative examples. The idea is that to make accurate predictions, the model must infer global structure in speech (e.g. phone identity) while discarding low-level details.</p><p>Recent studies have shown that CPC learns representations that capture phonetic contrasts and transfer well across languages <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>. In this paper, we adapt CPC to the task of acoustic unit discovery by incorporate vector quantization. <ref type="bibr" target="#b2">3</ref> Previous work <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b34">35]</ref> has also explored the combination of vector quantization with contrastive learning. However, the goal in <ref type="bibr" target="#b25">[26]</ref> is to learn representations for a downstream automatic speech recognition. We show that some design choices (e.g. product codebooks) that work well in this context are not ideal for acoustic unit discovery.</p><p>Model description. The VQ-CPC model is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. First, the encoder maps input speech (parametrized as a log-Mel spectrogram) into a sequence of continuous features. The encoder consists of a strided convolutional layer (downsampling the input by a factor of 2), followed by a stack of 4 linear layers with ReLU activations. Layer normalization is applied after each layer. The bottleneck is identical to the one described in §2.2. The output of the encoder is projected into a sequence of continuous latent vectors which are discretized using a VQ layer with 512 codes. Finally, the autoregressive model summarizes </p><formula xml:id="formula_1">Lt := − 1 M M m=1 log exp(ẑ t+m Wmct) z∈N t,m exp(z Wmct)</formula><p>.</p><p>The loss is averaged over segments of 1.28 seconds and a VQ commitment cost is added. We set the prediction horizon to M = 6 steps and sample 17 negative examples per step. We use the Adam optimizer, with a batches size of 64, and a learning rate of 4 · 10 −4 . Each minibatch is divided into groups of 8 segments from which negative examples are sampled. To address codebook collapse, we use a warm-up phase where we linearly increase the learning rate from 1 · 10 −5 over the first 150 epochs.</p><p>Sampling negative examples. We investigate acrossspeaker and within-speaker sampling for negative examples. In across-speaker sampling, negatives are drawn from a mix of speakers, while within-speaker sampling uses the same speaker. We hypothesize that within-speaker sampling will encourage speaker-invariant representations since speaker information cannot be used to identify the positive example.</p><p>Voice conversion. VQ-CPC is not a generative model, so we train a separate vocoder on top of the discovered acoustic units for voice conversion. The vocoder is similar to the decoder in <ref type="figure" target="#fig_0">Figure 1</ref>, except the jitter layer is replaced with an embedding which reads in the code indices from the VQ-CPC bottleneck. Again, the target voice can be controlled by conditioning the vocoder on a specific speaker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental setup</head><p>Datasets. We evaluate our models on the English and Indonesian datasets from the ZeroSpeech 2019 Challenge. Indonesian is a low-resource Austronesian language widely used as a lingua franca <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>. Following the challenge guidelines, we use English as the development language. After tuning the models and hyperparameters on the English data, we apply the finalized training procedure to the Indonesian dataset. For both languages, training data consists of about 15 hours of speech from over 100 speakers. An additional hour is provided per target speaker for voice conversion. Finally, the test set contains approximately 30 minutes of speech from unseen speakers.</p><p>ABX evaluation. ABX phone discrimination tests are used to evaluate the discovered acoustic units <ref type="bibr" target="#b37">[38]</ref>. The tests ask whether triphone X is more similar to triphones A or B. Here, A and X are instances of the same triphone (e.g. "beg"), while B differs in the middle phone (e.g. "bag"). To measure speakerinvariance, A and B come from the same speaker, but X is taken from a different speaker. As a similarity metric, we use the average cosine distance along the dynamic time warping alignment path. ABX is reported as an aggregated error rate over all pairs of triphones in the test set.</p><p>Voice conversion. To assess voice conversion quality, human evaluators judge intelligibility, speaker-similarity, and naturalness. For intelligibility, the evaluators orthographically transcribe the synthesized speech. A character error rate (CER) is then calculated by comparing the transcriptions to the ground truth. Speaker-similarity and naturalness are scored on a scale from 1 to 5 (higher is better), with the latter reported as a mean opinion score (MOS).</p><p>Baselines. The challenge baseline system combines a Dirichlet process Gaussian mixture model (DPGMM) for acoustic unit discovery <ref type="bibr" target="#b18">[19]</ref> with a parametric speech synthesizer based on Merlin <ref type="bibr" target="#b38">[39]</ref>. The topline system feeds the output of a supervised speech recognition model to a text-to-speech system, both trained on ground-truth transcriptions. See <ref type="bibr" target="#b5">[6]</ref> for details.</p><p>We also include results for three other approaches. The first is the VQ-VAE-based system we submitted to the previous challenge <ref type="bibr" target="#b20">[21]</ref>, referred to here as VQ-VAE(spec). Instead of generating audio waveforms directly, VQ-VAE(spec) uses a two-stage approach. The model reconstructs log-Mel spectrograms, which are then fed to a separately trained FFTNet vocoder <ref type="bibr" target="#b39">[40]</ref> for synthesis. Secondly, we include results for the system of Chen and Hain <ref type="bibr" target="#b40">[41]</ref> -one of the other top-performing submissions to ZeroSpeech 2020. Their system is similar to the WaveNet autoencoder of <ref type="bibr" target="#b21">[22]</ref>, but uses instance-norm layers in the encoder and adaptive instance normalization for speaker conditioning. In contrast to our models, Chen and Hain also downsample by a factor of 4 and use a large product codebook with 2 16 codes. Finally, we compare against vq-wav2vec <ref type="bibr" target="#b25">[26]</ref>. <ref type="bibr" target="#b3">4</ref> Note that vq-wav2vec was trained on the 960h Librispeech dataset. <ref type="table" target="#tab_2">Table 1</ref> shows the evaluation results for the ZeroSpeech 2020 Challenge. <ref type="bibr" target="#b4">5</ref> On the ABX tests, our models outperform all other submissions to the 2019 and 2020 challenges. We improve ABX scores on the English and Indonesian datasets by more than 30% and 50%, respectively. On the English voice conversion task, VQ-CPC also achieves top naturalness and speaker-similarity scores, marginally beating the VQ-VAE. However on Indonesian, some of the other submissions perform better. This discrepancy may be explained by a mismatch in the volume of our synthesized speech and the source utterances. On the English dataset,  <ref type="table" target="#tab_2">306  163  34  213  213  144  118  267  310  358  348  31  466  329  329  504  395  282  479  479  479  252  252  252  252  433  433  433  85  HH  Y  UW  M</ref>   <ref type="table" target="#tab_2">381  364  144  144  371  118  371  431  287  348  348  441  466  391  100  84  18  479  479  479  509  509  252  252  433  433  433  HH  Y  UW  M  ER</ref> humor <ref type="figure">Figure 3</ref>: The log-Mel spectrograms of speech segments taken from two different speakers. Overlaid are the aligned transcriptions and acoustic units from VQ-CPC. Common units in the two code sequences are highlighted in yellow. the volume difference is moderate at around 6.1 LUFS 6 . But on Indonesian, a larger disparity of 9.4 LUFS may have negatively impacted the results. On intelligibility (CER), Chen and Hain <ref type="bibr" target="#b40">[41]</ref> perform the best across both languages. These results seem to indicate a trade-off between intelligibility and voice conversion quality. By using a larger codebook, Chen and Hain are able to improve CER at the cost of speaker-similarity. A second trade-off, along a different axis, is bitrate against CER and ABX score. While our models are able outperform the supervised topline, they operate at a much higher bitrate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental results</head><p>Comparing the two models, it is clear that the VQ-VAE and VQ-CPC perform similarly across all metrics. However, VQ-CPC is an order of magnitude faster to train and was more robust to codebook collapse in our experiments. A comparison to the VQ-VAE(spec) (from our previous submission <ref type="bibr" target="#b20">[21]</ref>), suggests that training an autoregressive decoder jointly with the encoder is beneficial. Finally, it is interesting to note that our models (trained exclusively on unlabelled speech) achieve comparable ABX scores to the visually grounded VQ model of <ref type="bibr" target="#b41">[42]</ref> -which is trained on paired images and unlabelled spoken captions. <ref type="bibr" target="#b5">6</ref> Loudness Units relative to Full Scale (LUFS), see the ITU-R BS.1770-4 standard. To show that the VQ bottleneck discards speaker information, we analyze the representations before and after quantization. At each probe point, we train a multilayer perceptron with 2048 hidden units to predict the speaker identity. We use mean-pooling after the non-linearity to aggregate features. <ref type="table" target="#tab_3">Table 2</ref> shows the results of the probing experiments on English data. Based on the drop in speaker classification accuracy across the probe points, the VQ layer clearly acts as an information bottleneck, forcing the models to discard speaker details. Interestingly, CPC without vector quantization performs well on ABX tests but does not explicitly discard speaker information. As a result, CPC alone was not capable of voice conversion in our experiments. <ref type="table" target="#tab_3">Table 2</ref> also compares within-speaker and across-speaker negative sampling for VQ-CPC (see §2.3). Within-speaker sampling results in better speaker invariance (lower speaker classification accuracy) and significantly lower ABX scores (13.4% vs. 36.2%).</p><p>To examine a few of the acoustic units discovered by VQ-CPC, <ref type="figure">Figure 3</ref> plots two utterances along with the extracted codes. The utterances are encoded to a similar sequence of units despite coming from different speakers. Additionally, adjacent frames within a phone are often mapped to the same code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and future work</head><p>We presented two neural models for acoustic unit discovery from unlabelled speech. Using vector quantization, both models learn discrete representations of speech that capture phonetic content but discard speaker information. They performed competitively on phone discrimination tests and a voice conversion task for the ZeroSpeech 2020 challenge. Despite these merits, the models operate at high bitrates compared to phonetic transcriptions and a supervised topline. In future work, we aim to lower bitrates and discover acoustic units that are more consistent across phones. This work is supported in part by the National Research Foundation of South Africa (grant number: 120409) and a Google Faculty Award.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2Figure 1 :</head><label>1</label><figDesc>https://github.com/bshall/ZeroSpeech conv 3 VQ-VAE: A convolutional encoder (purple) takes a speech waveform as input and outputs downsampled continuous features. These are discretized (blue) using vector quantization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>VQ-CPC: An encoder (purple) encodes speech (parametrized as a log-Mel spectrogram) to a sequence of continuous vectors z. Using a VQ bottleneck (blue) the z-vectors are quantized. The quantizedẑ-vectors are summarised by an autoregressive RNN (green) into context vectors c. Using this context, the model is trained to predict future codes.the discrete representations (up to time t) into a context vector ct. Using this context, the model is trained to discriminate future codes from negative examples drawn from other utterances.Training details. Given a prediction horizon of M steps, a trainable predictor matrix Wm, and a set Nt,m containing negative examples and the positive codeẑt+m, we minimize the InfoNCE loss<ref type="bibr" target="#b31">[32]</ref>:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Human and machine evaluations on the English and Indonesian test sets. For MOS and similarity scores, higher is better. For CER, ABX, and bitrate, lower is better. ABX scores for the discrete codes and auxiliary representations are shown under the "code" and "aux" columns respectively.</figDesc><table><row><cell></cell><cell cols="3">CER MOS Similarity</cell><cell cols="2">ABX (%)</cell><cell></cell></row><row><cell>Model</cell><cell cols="2">(%) [1, 5]</cell><cell cols="2">[1, 5] code</cell><cell>aux</cell><cell>Bitrate</cell></row><row><cell>English:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DPGMM-Merlin</cell><cell>77</cell><cell>2.14</cell><cell>2.98</cell><cell>35.6</cell><cell>-</cell><cell>72</cell></row><row><cell cols="2">VQ-VAE(spec) [21] 67</cell><cell>2.18</cell><cell>2.51</cell><cell cols="2">27.6 23.0</cell><cell>173</cell></row><row><cell cols="2">Chen and Hain [41] 18</cell><cell>3.61</cell><cell>2.57</cell><cell>20.2</cell><cell>-</cell><cell>386</cell></row><row><cell>vq-wav2vec [26]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">19.2 16.3</cell><cell>1205</cell></row><row><cell>VQ-VAE</cell><cell>39</cell><cell>3.62</cell><cell>3.49</cell><cell cols="2">14.0 13.2</cell><cell>412</cell></row><row><cell>VQ-CPC</cell><cell>38</cell><cell>3.64</cell><cell>3.80</cell><cell cols="2">13.4 12.5</cell><cell>421</cell></row><row><cell>Supervised</cell><cell>43</cell><cell>2.52</cell><cell>3.10</cell><cell>29.9</cell><cell>-</cell><cell>38</cell></row><row><cell>Indonesian:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DPGMM-Merlin</cell><cell>67</cell><cell>2.23</cell><cell>3.26</cell><cell>27.5</cell><cell>-</cell><cell>75</cell></row><row><cell cols="2">VQ-VAE(spec) [21] 60</cell><cell>1.96</cell><cell>1.76</cell><cell cols="2">19.8 14.5</cell><cell>140</cell></row><row><cell cols="2">Chen and Hain [41] 15</cell><cell>4.06</cell><cell>2.67</cell><cell>12.5</cell><cell>-</cell><cell>388</cell></row><row><cell>VQ-VAE</cell><cell>21</cell><cell>3.71</cell><cell>2.59</cell><cell>6.2</cell><cell>8.3</cell><cell>424</cell></row><row><cell>VQ-CPC</cell><cell>27</cell><cell>3.49</cell><cell>2.68</cell><cell>5.1</cell><cell>4.9</cell><cell>420</cell></row><row><cell>Supervised</cell><cell>33</cell><cell>3.49</cell><cell>3.77</cell><cell>16.1</cell><cell>-</cell><cell>35</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Speaker classification results at probe points before and after quantization (shown under the "pre-quant" and "code" columns respectively).</figDesc><table><row><cell></cell><cell cols="2">Spkr. class. accuracy (%)</cell><cell cols="2">ABX (%)</cell></row><row><cell>Model</cell><cell>code</cell><cell>pre-quant</cell><cell>code</cell><cell>aux</cell></row><row><cell>log-Mel spectrogram</cell><cell>98.9</cell><cell>-</cell><cell>27.0</cell><cell>-</cell></row><row><cell>vq-wav2vec [26]</cell><cell>76.4</cell><cell>98.7</cell><cell>19.2</cell><cell>16.3</cell></row><row><cell>VQ-VAE</cell><cell>65.8</cell><cell>98.8</cell><cell>14.0</cell><cell>13.2</cell></row><row><cell>VQ-CPC (within)</cell><cell>47.4</cell><cell>94.9</cell><cell>13.4</cell><cell>12.5</cell></row><row><cell>VQ-CPC (across)</cell><cell>80.3</cell><cell>98.5</cell><cell>36.2</cell><cell>31.7</cell></row><row><cell>CPC (within)</cell><cell>99.7</cell><cell>-</cell><cell>16.4</cell><cell>13.8</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">As a point of reference, phonetic transcriptions encode speech at a rate of about 39 bits per second<ref type="bibr" target="#b13">[14]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/bshall/VectorQuantizedCPC</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We use the pretrained weights available at https://github.com/ pytorch/fairseq.<ref type="bibr" target="#b4">5</ref> The leader-board can be viewed at https://zerospeech.com/ 2020/results.html. Voice conversion samples for our models can be found at https://bshall.github.io/ZeroSpeech/ and https: //bshall.github.io/VectorQuantizedCPC/, respectively.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Computational modeling of phonetic and lexical learning in early language acquisition: Existing models and future directions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">J</forename><surname>Räsänen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Commun</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="975" to="997" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural network vs. HMM speech recognition systems as models of human cross-linguistic phonetic perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schatz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">H</forename><surname>Feldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CCN</title>
		<meeting>CCN</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Measuring the perceptual availability of phonological features during language acquisition using unsupervised binary stochastic autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elsner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. HLT-NAACL</title>
		<meeting>HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The Zero Resource Speech Challenge 2015: Proposed approaches and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Versteegh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Anguera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dupoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SLTU</title>
		<meeting>SLTU</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The Zero Resource Speech Challenge 2017</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dunbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">N</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Benjumea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Karadayi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Besacier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Anguera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dupoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU</title>
		<meeting>ASRU</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The Zero Resource Speech Challenge 2019: TTS without T</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dunbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Algayres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Karadayi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Benjumea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-N</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Miskic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dugrain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Spectral voice conversion for text-tospeech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Macon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-target voice conversion without parallel data by adversarially learning disentangled audio representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Joint learning of speaker and phonetic similarities with Siamese networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dupoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning supervised feature transformations on zero resources for improved acoustic unit discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sakti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEICE T. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="205" to="214" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An unsupervised autoregressive model for speech representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-A</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-N</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised pre-training of bidirectional speech encoders via masked reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning for speech using correspondence and siamese networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-J</forename><surname>Last</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Engelbrecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kamper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Proc. Let</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="421" to="425" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Different languages, similar encoding efficiency: Comparable information rates across the human communicative niche</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Coupé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dediu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pellegrino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science advances</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised learning of acoustic sub-word units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dupoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A nonparametric Bayesian approach to acoustic model discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised training of an HMM-based self-organizing unit recognizer with applications to topic classification and keyword discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Siu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Belfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Speech Lang</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="210" to="223" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised lexicon discovery from acoustic input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>O&amp;apos;donnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. ACL</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="389" to="403" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Variational inference for acoustic unit discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Černockỳ</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Procedia Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="80" to="86" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Discovering discrete subword units with binarized autoencoders and hidden-Markov-model encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Badino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mereta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rosasco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised acoustic unit discovery for speech synthesis using discrete latent-variable neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Eloff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nortje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Van Niekerk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Govender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nortje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pretorius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Van Biljon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Van Der Westhuizen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Staden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kamper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised speech representation learning using WaveNet autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Language Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2041" to="2053" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">VQVAE unsupervised unit discovery and multi-scale code2spec inverter for Zerospeech Challenge 2019</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tjandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sisman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sakti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural discrete representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<title level="m">WaveNet: a generative model for raw audio</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">vq-wav2vec: Selfsupervised learning of discrete speech representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Estimating or propagating gradients through stochastic neurons for conditional computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Léonard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.3432</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Towards achieving robust universal neural vocoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lorenzo-Trueba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Drugman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Latorre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Merritt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Putrycz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barra-Chicote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moinet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mixed precision training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Alben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fast decoding in sequence models using discrete latent variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised pretraining transfers well across languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rivière</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-E</forename><surname>Mazaré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dupoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Librilight: A benchmark for asr with limited or no supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rivière</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kharitonov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-E</forename><surname>Mazaré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Karadayi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fuegen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Vector quantized contrastive predictive coding for template-based music generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hadjeres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Crestel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10120</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Development of HMM-based Indonesian speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sakti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Maia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. O-COCOSDA</title>
		<meeting>O-COCOSDA</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Development of Indonesian large vocabulary continuous speech recognition system within A-STAR project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sakti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kelana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Riza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Markov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. TCAST</title>
		<meeting>TCAST</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Evaluating speech features with the minimal-pair ABX task: Analysis of the classical MFC/PLP pipeline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schatz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dupoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Merlin: An open source neural network speech synthesis system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Watts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SSW</title>
		<meeting>SSW</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">FFTNet: A realtime speaker-dependent neural vocoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Mysore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Unsupervised acoustic unit representation learning for voice conversion using WaveNet auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>submitted to Interspeech</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning hierarchical discrete linguistic units from visually-grounded speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Harwath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-N</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
