<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EmoGraph: Capturing Emotion Correlations using Graph Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Artificial Intelligence Research (CAiRE) Department of Electronic and Computer Engineering</orgName>
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
								<address>
									<addrLine>Clear Water Bay</addrLine>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Artificial Intelligence Research (CAiRE) Department of Electronic and Computer Engineering</orgName>
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
								<address>
									<addrLine>Clear Water Bay</addrLine>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Genta</forename><surname>Indra Winata</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Artificial Intelligence Research (CAiRE) Department of Electronic and Computer Engineering</orgName>
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
								<address>
									<addrLine>Clear Water Bay</addrLine>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaojiang</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Artificial Intelligence Research (CAiRE) Department of Electronic and Computer Engineering</orgName>
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
								<address>
									<addrLine>Clear Water Bay</addrLine>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Fung</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Artificial Intelligence Research (CAiRE) Department of Electronic and Computer Engineering</orgName>
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
								<address>
									<addrLine>Clear Water Bay</addrLine>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">EmoGraph: Capturing Emotion Correlations using Graph Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most emotion recognition methods tackle the emotion understanding task by considering individual emotion independently while ignoring their fuzziness nature and the interconnections among them. In this paper, we explore how emotion correlations can be captured and help different classification tasks. We propose EmoGraph that captures the dependencies among different emotions through graph networks.</p><p>These graphs are constructed by leveraging the co-occurrence statistics among different emotion categories. Empirical results on two multi-label classification datasets demonstrate that EmoGraph outperforms strong baselines, especially for macro-F1. An additional experiment illustrates the captured emotion correlations can also benefit a single-label classification task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Understanding human emotions is considered as the key to building engaging dialogue systems <ref type="bibr" target="#b23">(Zhou et al., 2018)</ref>. However, most works on emotion understanding tasks treat individual emotions independently while ignoring the fuzziness nature and the interconnections among them. A psychoevolutionary theory proposed by <ref type="bibr" target="#b7">Plutchik (1984)</ref> shows that different emotions are actually correlated, and all emotions follow a circular structure. For example, "optimism" is close to "joy" and "anticipation" instead of "disgust" and "sadness". Without considering the fundamental intercorrelation between them, the understanding of emotions can be unilateral, leading to sub-optimal performance. These understanding can be particularly important for low resource emotions, such as "surprise" and "trust" whose training samples are hard to get. Therefore, the research question we * * Equal contributions. ask is, how can we obtain and incorporate the emotion correlation to improve emotion understanding tasks, such as classification?</p><p>To obtain emotion correlations, a possible way is to take advantage of a multi-label emotion dataset. Intuitively, emotions with high correlations will be labeled together, and therefore, emotion correlations can be extracted from the label cooccurrences. Recently, a multi-label emotion classification competition <ref type="bibr" target="#b5">(Mohammad et al., 2018)</ref> with 11 emotions has been introduced to promote research into emotional understanding. To tackle this challenge, the best team (Baziotis et al., 2018) first pre-trains on a large amount of external emotionrelated datasets and then performs transfer learning on this multi-label task. However, they still neglect the correlations between different emotions.</p><p>In this paper, we propose EmoGraph which leverages graph neural networks to model the dependencies between different emotions. We take each emotion as a node and first construct an emotion graph based on the co-occurrence statistics between every two emotion classes. Graph neural networks are then applied to extract the features from the neighbours of each emotion node. We conduct experiments on two multi-label emotion classification datasets. Empirical results show that our model outperforms strong baselines, especially for macro-F1 score. The analysis shows that low resource emotions, such as "trust", can particularly benefit from the emotion correlations. An additional experiment illustrates that the captured emotion correlations can also help the single-label emotion classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>For emotion classifications, <ref type="bibr" target="#b11">Tang et al. (2016)</ref> proposed sentiment embeddings that incorporate sentiment information into word vectors. Felbo et al.</p><p>(2017) trained a huge LSTM-based emotion representation by predicting emojis. Various methods have also been developed for automatic constructions of sentiment lexicons using both a supervised and unsupervised method <ref type="bibr" target="#b16">(Wang and Xia, 2017)</ref>. <ref type="bibr">Duppada et al. (2018)</ref> combined both pretrained representations and emotion lexicon features, which significantly improved the emotion understanding systems. ; Fung et al.;   Multi-label classification is an important yet challenging task in natural language processing. Binary relevance <ref type="bibr">(Boutell et al., 2004)</ref> transformed the multi-label problem into several independent classifiers. Other methods to model the dependencies have since been proposed by creating new labels <ref type="bibr">(Tsoumakas and Katakis, 2007)</ref>, using classifier chains <ref type="bibr" target="#b8">(Read et al., 2011</ref><ref type="bibr">), graphs (Li et al., 2015</ref>, and <ref type="bibr">RNN (Chen et al., 2017;</ref><ref type="bibr" target="#b20">Yang et al., 2018)</ref>. These models are either non-scalable or modeling the labels as a sequence.</p><p>Graph networks have been applied to model relations across different tasks such as image recognition <ref type="bibr">(Chen et al., 2019;</ref><ref type="bibr" target="#b9">Satorras and Estrach, 2018)</ref>, and text classification <ref type="bibr">(Ghosal et al., 2019;</ref><ref type="bibr" target="#b21">Yao et al., 2019)</ref> with different graph networks <ref type="bibr">(Kipf and Welling, 2017;</ref><ref type="bibr" target="#b15">Velikovi et al., 2018)</ref>.</p><p>Despite the growing interests in low-resource studies in machine translation <ref type="bibr" target="#b0">(Artetxe et al., 2017;</ref><ref type="bibr">Lample et al., 2017)</ref>, dialogue systems <ref type="bibr" target="#b1">(Bapna et al., 2017;</ref><ref type="bibr" target="#b2">Liu et al., 2019</ref>, speech recognition <ref type="bibr" target="#b4">(Miao et al., 2013;</ref><ref type="bibr" target="#b12">Thomas et al., 2013;</ref>, emotion recognition <ref type="bibr">(Haider et al., 2020)</ref>, and etc, emotion detection for low-resource emotions has been less studied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we first introduce the emotion graphs and then our emotion classification models. We denote the input sentence as x and the emotion classes as e = {e 1 , e 2 , · · · , e n }. The label for x is y, where y ∈ {0, 1} n and y j denotes the label for e j . The embedding matrix is E. The co-occurrence matrix of these emotions is M .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Emotion Graphs</head><p>We take each emotion class as a node in the emotion graph. To create the connections between emotion nodes, we use the co-occurrence statistics between emotions. The intuition is that if two emotions co-occurs frequently, they will have a high correlation. Directly using this co-occurrence matrix as our graph may be problematic because the cooccurrence matrix is symmetric while the emotion relation is not symmetric. For example, in our corpus, "anticipation" co-occurs with "optimism" 197 times, while "anticipation" appears 425 times and "optimism" appears 1143 times. Thus, knowing "anticipation" and "optimism" co-occur is notably more important for "anticipation" than "optimism". Thus, we calculate the co-occurrence matrix M from a given emotion corpus and then normalize M i,j with M i,i so that the graph encodes the asymmetric relation between different emotions.</p><formula xml:id="formula_0">G 1 i,j = M i,j M i,i .<label>(1)</label></formula><p>Due to the fuzziness nature of emotions, the graph matrix G 1 may contain some noise. Thus, we adopted the approach in Chen et al. (2019) to binarize G 1 with a threshold µ to reduce noise and tune another hyper-parameter w to mitigate the over-smoothing problem :</p><formula xml:id="formula_1">G 2 i,j = 1, if G 1 i,j ≥ µ 0, otherwise.</formula><p>(2)</p><formula xml:id="formula_2">G i,j =        G 2 i,j /( n j=1 G 2 i,j ), if i = j 1 − w, otherwise.</formula><p>(3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Emotion Classification Models</head><p>Our emotion classification model consists of an encoder and graph-based emotion classifiers following the framework of Chen et al. <ref type="bibr">(2019)</ref>. For the encoder, we choose the Transformer (TRS) <ref type="bibr" target="#b14">(Vaswani et al., 2017)</ref> and the pre-trained model BERT (Devlin et al., 2019) for their strong representation capacities on many natural language tasks. We denote the encoded representation of x as s. For graphbased emotion classifiers, we experiment with two types of graph networks: GCN (Kipf and Welling, 2017) and GAT <ref type="bibr" target="#b15">(Velikovi et al., 2018)</ref>. The GCN takes the features of each emotion node as inputs and applies a convolution over neighboring nodes to generate the classifier for e i :</p><formula xml:id="formula_3">C = ReLU(GE e W 1 ),<label>(4)</label></formula><p>whereG is the normalized matrix of G following <ref type="bibr">Kipf and Welling (2017)</ref>. E e is the embeddings for all emotions e, and W 1 is the trainable parameters. Our emotion classifiers are C = C 1 , C 2 , · · · , C n , where C i is the classifier for e i . Alternatively, GAT takes the features of each node as input and learns a multi-head self-attention over the emotion nodes to generate classifiers C. We then simply take the inner product between s and C i to computeŷ i , the logits of the emotion class e i for classification:</p><formula xml:id="formula_4">y i = s * C i .<label>(5)</label></formula><p>As our task is a multi-label prediction problem, we add a sigmoid activation toŷ i and use a crossentropy loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Evaluation Metrics</head><p>We choose two datasets for our multi-label classification training and evaluation. SemEval-2018 <ref type="bibr" target="#b5">(Mohammad et al., 2018)</ref> contains 10,983 tweets with 11 different emotion categories. It is divided into three splits: training set (6838 samples), validation set (886 samples), and testing set (3259 samples). Due to its small label quantities and small dataset size, we crawled another Twitter dataset where we use emojis as emotion labels. We follow the same 64 emoji types as in Felbo et al. <ref type="formula" target="#formula_0">(2017)</ref> and collect 4 million twitter data, where each tweet has at least two emojis. We split them into 2.8 million (70%) for training, 0.4 million (10%) for validation and 0.8 million (20%) for testing. Following the metrics in <ref type="bibr" target="#b5">Mohammad et al. (2018)</ref>, we use Jaccard accuracy, micro-average F1-score (micro-F1), and macro-average F1-score (macro-F1) as our evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">EmoGraph and Baselines</head><p>Our EmoGraph has four variants,TRS-GCN,TRS-GAT,BERT-GCN, and BERT-GAT, depending on the choice of sentence encoder (TRS/BERT) and graph networks (GCN/GAT). We compare our model to several strong baselines. NTUA-SLP (Baziotis et al., 2018) is the top-1 system of the SemEval-2018 competition, which pre-trains the model on large amounts of external emotionrelated datasets. DATN <ref type="bibr" target="#b22">(Yu et al., 2018)</ref> is the system that transfers sentiment information with dual attention transfer network. SGM <ref type="bibr" target="#b20">(Yang et al., 2018</ref>) models labels as a sequences and generates  the labels using beam search. TRS is the system that adds a linear layer on top of the Transformer <ref type="bibr" target="#b14">(Vaswani et al., 2017)</ref>. BERT is the system that adds a linear layer on top of the BERT (Devlin et al., 2019).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Analysis</head><p>Results on two emotion classification datasets The results on SemEval-2018 dataset are shown in <ref type="table">Table 1</ref>, which illustrates several points. Firstly, BERT-GCN/TRS-GCN consistently improves over the baseline of BERT/TRS, in terms of accuracy (+0.5%/+0.8%), micro-F1 (+0.6%/+0.4%) and macro-F1 (+2.5%/+2.5%). It shows the effectiveness of the emotion graph by giving the performance boost on the strong baseline BERT, especially on the macro-F1 score. Secondly, our BERT-GCN achieves a 58.9% accuracy score, 70.7% micro-F1 score, and 56.3% macro-F1 score, which beats the best system NTUA-SLP in terms of all metrics, without using any affect features or pretraining on emotional datasets. Thirdly, EmoGraph is particularly effective in macro-F1. For example, BERT-GAT is 2.5% better than DATN and 3.1% better than BERT. We notice that for both accuracy and micro-F1, the improvements of graph networks are very marginal on the SemEval-2018 dataset. This is because the small emotion label space (only 11 emotions) limits the effectiveness of emotion graphs. Thus, we train our models on another Twitter   dataset with 64 emoji labels. <ref type="table" target="#tab_1">Table 2</ref> shows both TRS-GCN and TRS-GAT consistently improves TRS with a large margin for all metrics, which shows that graph networks are better at dealing with rich emotion information. 1 Connection to the wheel of emotions Following the positioning of the wheel of emotion <ref type="bibr" target="#b7">(Plutchik, 1984)</ref>, we visualize the graph structure G in <ref type="figure" target="#fig_1">Figure 1</ref>. It shows that our emotion graph can be divided into three sub-graphs, 1) the nodes connected with "disgust", 2) "fear" 3) the nodes connected with "joy". Most nodes are locally connected, which means the positioning in the wheel of emotion does contain the correlation information. One exception is that "anticipation" is also close to "anger" in the wheel of emotions, which is not true in our case. Another exception we found is that "surprise" is close to "joy" in our emotion graph while they are quite far away from each other in the wheel of emotions. We believe it reflects the bias when people post tweets online, and the distance between neighboring emotions are not quantitatively the same as the wheel of emotions. <ref type="table" target="#tab_4">Table 4</ref> shows that the significant improvements in terms of the macro-F1 on the SemEval-2018 dataset mainly come from the low resource emotions, such as "surprise" and "trust", where only 5% of the labels are positive. From <ref type="figure" target="#fig_1">Figure 1</ref>, we observe that both "surprise" and "trust" are connected to "joy", which has 39.3% samples labeled as positive. We conjecture that low resource emotion classifiers learn effective inductive bias through connections with high-resource emotion classes, such as "joy" and "optimism", and achieve better performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis on low resource emotions</head><p>Generalization to single emotion classification task To verify the generalization ability of Emo-Graph, we conduct an extra experiment on IEMO-CAP dataset <ref type="bibr">(Busso et al., 2008)</ref>, which is a single label multi-class classification dataset. We only use the six emotions that overlap with the SemEval-2018 task, which are "anger", "sadness", "happiness", "disgust", "fear" and "surprise" with 2931 samples (the graph matrix G is obtained from the SemEval-2018 dataset). We then train our Emo-Graph using either a one-layer LSTM with attention or BERT as an encoder and GCN as the graph structure. To adapt the change from 11 emotions to 6 emotions, we only train the classifiers C i corresponding to those six emotions. The results with 10-fold cross-validation are reported in <ref type="table" target="#tab_3">Table 3</ref>. We didn't report "disgust" as there are less than five positive examples. The results confirm that with captured emotion correlations, classification performances can be improved by 2.8%/8.5% accuracy/average F1 score using LSTM encoder and 1.3%/3.4% accuracy/average F1 score using BERT encoder. Surprisingly, for both encoders with GCN improves more than 10% on "fear" (a low-resource emotion in the IEMOCAP dataset), although it doesn't have connections with other emotions in the graph. We conjecture that the improvements come from the shared representation of W 1 in Eq. 4 among all emotion categories, which helps our model optimize to a better local minimal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we proposed EmoGraph which leverages graph neural networks to model the dependencies among different emotions. We consider each emotion as a node and construct an emotion graph based on the co-occurrence statistics. Our model with EmoGraph outperforms the existing strong multi-label classification baselines. Our analysis shows EmoGraph is especially helpful for low resource emotions and large emotion space. An additional experiment shows that it can also help a single-label emotion classification task. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Preprocessing</head><p>To cope with the noise in the twitter data, we lowercase the tweets, remove the "#" punctuation and replace "URL link" with a special token &lt;url&gt;, "user mentions" with &lt;user&gt; and "number" with &lt;num&gt;. For example, one original tweet "@Jessi-caZ00 @ZRlondon ditto!! Such an amazing atmosphere! We have 10 people here. #LondonEvents #cheer" will be cleaned as "&lt;user&gt; &lt;user&gt; ditto! such an amazing atmosphere! we have &lt;num&gt; people here. londonevents cheer". By doing so, we can reduce the vocabulary size and make the model easier to learn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Training Details</head><p>To train the SemEval-2018, we calculate the cooccurrence matrix G 1 based on the training and development sets. We set the threshold µ as 0.4, and the weight w as 0.35 for GCN. We set the threshold µ as 0.5 for GAT. For both BERT-GCN and BERT-GAT, the emotion label embeddings are initialized with 300-dim GloVe vectors. For the BERT structure, we choose 12 layers BERT-base model, and the hidden size of the graph is set to 768. An Adam optimizer with the learning rate 1e-4 is used to train the graph networks. Another Adam optimizer is used to train the BERT model with the learning set as 2e-5 and dropout rate as 0.3. For TRS-GCN and TRS-GAT, the hidden size of the graph network is set to 200. The heads of the Transformer are 4, and the depth is 44. An Adam optimizer is used to train the full model with a 0.001 learning rate. For the Twitter dataset, we calculate the co-occurrence matrix G 1 based on the training set. We set the threshold µ as 0.1, and the weight w is set as 0.35 for both. We use the TRS as the sentence encoder. The hidden size of both graph networks and TRS are 200. The heads of TRS are 6, and the depth is 120. An Adam optimizer is used to train our model with a 0.001 learning rate.</p><p>To train IEMOCAP dataset, the parameters of both GCN and BERT follow the same setting as in SemEval 2018 task. For the LSTM based method, we set the hidden size of LSTM encoder as 200 and initialize the word embedding with GloVe. An Adam optimizer with a learning rate of 0.001 is used to optimize all parameters.    <ref type="table">Table 7</ref>: Percentage of data samples that are labeled with a given emotion in the IEMOCAP dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Data Statistics</head><p>The data statistics for the SemEval-2018, Twitter and IEMOCAP are shown in <ref type="table" target="#tab_7">Table 5</ref>, <ref type="table" target="#tab_8">Table 6</ref>, and <ref type="table">Table 7</ref>, respectively. From <ref type="table" target="#tab_7">Table 5</ref>, we can see that in the SemEval-2018 dataset, only 5.2% and 5.0% samples are labeled as positive for "surprise" and "trust", respectively. And from <ref type="table">Table 7</ref>, we can see that in the IEMOCAP dataset, only 1.4% samples are labeled as positive for "fear".</p><p>A.4 Effects of threshold µ and weight w As illustrated in <ref type="figure">Figure 2</ref> and <ref type="figure">Figure 3</ref>, we plot the diagram of accuracy, micro-F1 and macro-F1 on the test set by varying µ and w, respectively, for BERT-GCN. We also include another baseline BERT-GCN model which is trained by setting G = G 1 (w/o binarization), to see the effects of binarization (Eq. 2) and weighting (Eq. 3).  <ref type="figure">Figure 2</ref> shows that removing moderate amount of noisy connections by changing µ can improve the performance. For macro-F1, it clearly shows that small µ achieves much better results than large µ. If µ is large, useful emotion connections might be cut off, therefore leading to worse results. <ref type="figure">Figure 3</ref> shows that for accuracy and micro-F1, changing w can achieve better performance than the baseline without binarization and weighting.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>encoded emotion information into word representations and demonstrated improvements over emotion classification tasks. Shin et al. (2019); Lin et al. (2019); Xu et al. (2019) further introduced emotion supervision into generation tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>The graph visualization of G. The arrow denotes the conditional dependency from the source node to the target node.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Evaluation Scores for different threshold µ on the SemEval-2018 dataset. different weight w micro-F1 (w/o binarization) jaccard acc. (w/o binarization) macro-F1 (w/o binarization) micro-F1 jaccard acc. macro-F1 Evaluation Scores for different weight w on the SemEval-2018 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparisons among different systems on Twitter dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparisons of F1 scores and accuracy between EmoGraph w/ and w/o graph on IEMOCAP dataset.</figDesc><table><row><cell></cell><cell cols="2">Surprise Trust</cell></row><row><cell>BERT</cell><cell>18.7</cell><cell>6.9</cell></row><row><cell>BERT-GAT</cell><cell>31.9</cell><cell>14.8</cell></row><row><cell>BERT-GCN</cell><cell>27.8</cell><cell>24.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison of F1 scores between BERT-GAT, BERT-GCN and BERT for "surprise" and "trust".</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Aurelio Ranzato. 2017. Unsupervised machine translation using monolingual corpora only. arXiv preprint arXiv:1711.00043.</figDesc><table><row><cell>Guillaume Lample, Alexis Conneau, Ludovic Denoyer,</cell><cell>Narayanan. 2008. Iemocap: Interactive emotional</cell></row><row><cell>and Marc'Qimai Li, Zhichao Han, and Xiao-Ming Wu. 2018. Deeper insights into graph convolutional networks for semi-supervised learning. In Thirty-Second AAAI Conference on Artificial Intelligence.</cell><cell>dyadic motion capture database. Language re-sources and evaluation, 42(4):335. Guibin Chen, Deheng Ye, Zhenchang Xing, Jieshan Chen, and Erik Cambria. 2017. Ensemble appli-cation of convolutional and recurrent neural net-works for multi-label text categorization. In 2017 International Joint Conference on Neural Networks</cell></row><row><cell>Shoushan Li, Lei Huang, Rong Wang, and Guodong</cell><cell>(IJCNN), pages 2377-2383. IEEE.</cell></row><row><cell>Zhou. 2015. Sentence-level emotion classification</cell><cell></cell></row><row><cell>with label and context dependence. In Proceedings</cell><cell>Zhao-Min Chen, Xiu-Shen Wei, Peng Wang, and Yan-</cell></row><row><cell>of the 53rd Annual Meeting of the Association for</cell><cell>wen Guo. 2019. Multi-label image recognition with</cell></row><row><cell>Computational Linguistics and the 7th International</cell><cell>graph convolutional networks. In Proceedings of the</cell></row><row><cell>Joint Conference on Natural Language Processing</cell><cell>IEEE Conference on Computer Vision and Pattern</cell></row><row><cell>(Volume 1: Long Papers), pages 1045-1053.</cell><cell>Recognition, pages 5177-5186.</cell></row><row><cell>Zhaojiang Lin, Andrea Madotto, Jamin Shin, Peng Xu, and Pascale Fung. 2019. Moel: Mixture of empa-thetic listeners. In Proceedings of the 2019 Con-ference on Empirical Methods in Natural Language Processing and the 9th International Joint Confer-ence on Natural Language Processing (EMNLP-IJCNLP), pages 121-132.</cell><cell>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language under-standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-nologies, Volume 1 (Long and Short Papers), pages</cell></row><row><cell></cell><cell>4171-4186.</cell></row><row><cell></cell><cell>Venkatesh Duppada, Royal Jain, and Sushant Hiray.</cell></row><row><cell></cell><cell>2018. Seernet at semeval-2018 task 1: Domain adap-</cell></row><row><cell></cell><cell>tation for affect in tweets. In Proceedings of The</cell></row><row><cell></cell><cell>12th International Workshop on Semantic Evalua-</cell></row><row><cell></cell><cell>tion, pages 18-23.</cell></row><row><cell></cell><cell>Bjarke Felbo, Alan Mislove, Anders Søgaard, Iyad</cell></row><row><cell></cell><cell>Rahwan, and Sune Lehmann. 2017. Using millions</cell></row><row><cell></cell><cell>of emoji occurrences to learn any-domain represen-</cell></row><row><cell></cell><cell>tations for detecting sentiment, emotion and sarcasm.</cell></row><row><cell></cell><cell>In Proceedings of the 2017 Conference on Empiri-</cell></row><row><cell></cell><cell>cal Methods in Natural Language Processing, pages</cell></row><row><cell></cell><cell>1615-1625.</cell></row><row><cell></cell><cell>Pascale Fung, Dario Bertero, Peng Xu, Ji Ho Park,</cell></row><row><cell></cell><cell>Chien-Sheng Wu, and Andrea Madotto. Empathetic</cell></row><row><cell></cell><cell>dialog systems.</cell></row><row><cell></cell><cell>Deepanway Ghosal, Navonil Majumder, Soujanya Po-</cell></row><row><cell>Christos Baziotis, Athanasiou Nikolaos, Alexan-dra Chronopoulou, Athanasia Kolovou, Geor-gios Paraskevopoulos, Nikolaos Ellinas, Shrikanth Narayanan, and Alexandros Potamianos. 2018. Ntua-slp at semeval-2018 task 1: Predicting affec-tive content in tweets with deep attentive rnns and transfer learning. In Proceedings of The 12th Inter-</cell><cell>ria, Niyati Chhaya, and Alexander Gelbukh. 2019. Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation. In Proceed-ings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Inter-national Joint Conference on Natural Language Pro-cessing (EMNLP-IJCNLP), pages 154-164.</cell></row><row><cell>national Workshop on Semantic Evaluation, pages 245-255.</cell><cell>Fasih Haider, Senja Pollak, Pierre Albert, and Sat-urnino Luz. 2020. Emotion recognition in low-</cell></row><row><cell>Matthew R Boutell, Jiebo Luo, Xipeng Shen, and Christopher M Brown. 2004. Learning multi-label scene classification. Pattern recognition,</cell><cell>resource settings: An evaluation of automatic fea-ture selection methods. Computer Speech &amp; Lan-guage, page 101119.</cell></row><row><cell>37(9):1757-1771.</cell><cell></cell></row><row><cell></cell><cell>Thomas N Kipf and Max Welling. 2017. Semi-</cell></row><row><cell>Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe</cell><cell>supervised classification with graph convolutional</cell></row><row><cell>Kazemzadeh, Emily Mower, Samuel Kim, Jean-</cell><cell>networks. In International Conference on Learning</cell></row><row><cell>nette N Chang, Sungbok Lee, and Shrikanth S</cell><cell>Representations.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Percentage of tweets that are labeled with a given emotion in the SemEval-2018 Task 1 dataset.</figDesc><table><row><cell cols="4">33.37 21.86 38.13 16.63 0.41 15.40 3.71 4.35</cell></row><row><cell cols="2">32.95 0.95</cell><cell>3.78</cell><cell>1.43 5.02 0.21 3.13 10.13</cell></row><row><cell>2.59</cell><cell>0.26</cell><cell>9.67</cell><cell>4.40 2.18 2.83 4.86 2.23</cell></row><row><cell>1.39</cell><cell>1.58</cell><cell>3.50</cell><cell>3.03 1.98 3.35 1.14 3.73</cell></row><row><cell>2.92</cell><cell>3.21</cell><cell>9.91</cell><cell>0.51 2.16 4.00 3.30 2.97</cell></row><row><cell>5.14</cell><cell>3.66</cell><cell>1.69</cell><cell>1.89 2.43 6.95 2.10 2.56</cell></row><row><cell>0.19</cell><cell>3.83</cell><cell>1.35</cell><cell>3.49 1.55 2.12 2.83 0.78</cell></row><row><cell>1.48</cell><cell>2.03</cell><cell>0.94</cell><cell>0.25 1.28 0.53 0.56 0.28</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Percentage of tweets that are labeled with a given emoji in the twitter data. Note that our dataset is multi-labeled, the sum of all classes is not one.</figDesc><table><row><cell>F1 score</cell><cell cols="5">Anger Fear Happiness Surprise Sadness</cell></row><row><cell cols="2">Distribution(%) 37.7</cell><cell>1.4</cell><cell>20.1</cell><cell>3.5</cell><cell>37.2</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Data statistics, training details, and ablation studies for threshold µ (Eq. 2) and weight w (Eq. 3) are in the appendix.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is partially funded by ITF/319/16FP, HKUST 16248016 of Hong Kong Research Grants Council and MRP/055/18 of the Innovation Technology Commission, the Hong Kong SAR Government.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Unsupervised neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.11041</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards zero-shot frame semantic parsing for domain scaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gokhan</forename><surname>Tür</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tür</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2476" to="2480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Zero-shot cross-lingual dialogue systems with transferable latent variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamin</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Genta</forename><surname>Indra Winata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1297" to="1303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Coach: A coarse-to-fine approach for cross-domain slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Genta Indra Winata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="19" to="25" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep maxout networks for low-resource speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajie</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shourabh</forename><surname>Rawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Workshop on Automatic Speech Recognition and Understanding</title>
		<imprint>
			<biblScope unit="page" from="398" to="403" />
			<date type="published" when="2013" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semeval-2018 task 1: Affect in tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saif</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><surname>Bravo-Marquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Salameh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Kiritchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 12th International Workshop on Semantic Evaluation</title>
		<meeting>The 12th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Plusemo2vec at semeval-2018 task 1: Exploiting emotion knowledge from emoji and# hashtags</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji Ho</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 12th International Workshop on Semantic Evaluation</title>
		<meeting>The 12th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="264" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Emotions: A general psychoevolutionary theory. Approaches to emotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Plutchik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984" />
			<biblScope unit="page" from="197" to="219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Classifier chains for multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Read</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eibe</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">333</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Few-shot learning with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garcia</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><forename type="middle">Bruna</forename><surname>Satorras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Estrach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Happybot: Generating empathetic dialogue responses by improving user experience lookahead</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamin</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08487</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sentiment embeddings with applications to sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="496" to="509" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep neural network features and semi-supervised training for low resource speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hynek</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hermansky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE international conference on acoustics, speech and signal processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="6704" to="6708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Grigorios Tsoumakas and Ioannis Katakis</title>
	</analytic>
	<monogr>
		<title level="j">International Journal of Data Warehousing and Mining (IJDWM)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>Multi-label classification: An overview</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velikovi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sentiment lexicon construction with representation learning based on hierarchical sentiment supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="502" to="510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning fast adaptation on cross-accented speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Genta Indra Winata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Cahyawijaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaojiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.01901</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Emo2vec: Learning generalized emotion representation by multitask training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><forename type="middle">Ho</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</title>
		<meeting>the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="292" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Clickbait? sensational headline generation with auto-tuned reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3056" to="3066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sgm: Sequence generation model for multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3915" to="3926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengsheng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7370" to="7377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improving multilabel emotion classification via sentiment classification with dual attention transfer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luís</forename><surname>Marujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Karuturi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Brendel</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1137</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1097" to="1102" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The design and implementation of xiaoice, an empathetic social chatbot</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08989</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
