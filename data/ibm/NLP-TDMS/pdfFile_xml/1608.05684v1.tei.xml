<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Detecting Vanishing Points using Global Image Context in a Non-Manhattan World</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menghua</forename><surname>Zhai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="institution">University of Kentucky</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Workman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="institution">University of Kentucky</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Jacobs</surname></persName>
							<email>jacobs@cs.uky.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="institution">University of Kentucky</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Detecting Vanishing Points using Global Image Context in a Non-Manhattan World</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel method for detecting horizontal vanishing points and the zenith vanishing point in man-made environments. The dominant trend in existing methods is to first find candidate vanishing points, then remove outliers by enforcing mutual orthogonality. Our method reverses this process: we propose a set of horizon line candidates and score each based on the vanishing points it contains. A key element of our approach is the use of global image context, extracted with a deep convolutional network, to constrain the set of candidates under consideration. Our method does not make a Manhattan-world assumption and can operate effectively on scenes with only a single horizontal vanishing point. We evaluate our approach on three benchmark datasets and achieve state-ofthe-art performance on each. In addition, our approach is significantly faster than the previous best method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Automatic vanishing point (VP) and horizon line detection are two of the most fundamental problems in geometric computer vision <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b21">22]</ref>. Knowledge of these quantities is the foundation for many higher level tasks, including image mensuration <ref type="bibr" target="#b9">[10]</ref>, facade detection <ref type="bibr" target="#b19">[20]</ref>, geolocalization <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b30">31]</ref>, and camera calibration <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17]</ref>. Recent work in this area <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33]</ref> has explored novel problem formulations that significantly increase robustness to noise.</p><p>A vanishing point results from the intersection of projections of a set of parallel lines in the world. In man-made environments, such sets of lines are often caused by the edges of buildings, roads, and signs. VPs can typically be classified as either vertical, there is one such VP, and horizontal, there are often many such VPs. Given a set of horizontal VPs, there are numerous methods to estimate the horizon line. Therefore, previous approaches to this problem focus on first detecting the vanishing points, which is a challenging problem in many images due to line segment intersec- tions that are not true VPs.</p><p>Our approach is to propose candidate horizon lines, score them, and keep the best <ref type="figure" target="#fig_0">(Fig. 1</ref>). We use a deep convolutional neural network to extract global image context and guide the generation of a set of horizon line candidates. For each candidate, we identify vanishing points by solving a discrete-continuous optimization problem. The final score for each candidate line is based on the consistency of the lines in the image with the selected vanishing points.</p><p>This seemingly simple shift in approach leads to the need for novel algorithms and has excellent performance. We evaluated the proposed approach on two standard benchmark datasets, the Eurasian Cities Dataset <ref type="bibr" target="#b4">[5]</ref> and the York Urban Dataset <ref type="bibr" target="#b10">[11]</ref>. To our knowledge, our approach has the current best performance on both datasets. To evaluate our algorithm further, we also compare with the previous state-of-the-art method (Lezama et al. <ref type="bibr" target="#b18">[19]</ref>) on a recently introduced dataset <ref type="bibr" target="#b31">[32]</ref>; the results shows that our method is more accurate and much faster.</p><p>The main contributions of this work are: 1) a novel method for horizon line/vanishing point detection, which uses global image context to guide precise geometric analysis; 2) a strategy for quickly extracting this context, in  the form of constraints on possible horizon lines, using a deep convolutional neural network; 3) a discrete-continuous method for scoring horizon line candidates; and 4) an evaluation of the proposed approach on three benchmark datasets, which highlights that our method is both fast and accurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related Work</head><p>Vanishing points and the horizon line provide a strong characterization of geometric scene structure and as such have been intensely studied for decades <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b21">22]</ref>. For example, Hoiem et al. <ref type="bibr" target="#b12">[13]</ref> show how the horizon line improves the accuracy of object detection. A wide variety of methods have been introduced to estimate these quantities. We provide a brief overview of the main approaches, refer to <ref type="bibr" target="#b25">[26]</ref> for a comprehensive review.</p><p>Two distinct categories of methods exist, distinguished by the features they use. The first group of methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b24">25]</ref> operate directly on lower-level features, such as edge pixels or image gradients. The second group of methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33]</ref> build on top of the closely related problem of line segment detection. Our work is most closely related to the latter category, so we focus our discussion towards them.</p><p>The dominant approach to vanishing point detection from line segments is to cluster the line segments that pass through the same location. Various methods of clustering have been explored, including RANSAC <ref type="bibr" target="#b6">[7]</ref>, J-linkage <ref type="bibr" target="#b26">[27]</ref>, and the Hough transform <ref type="bibr" target="#b13">[14]</ref>. Once the line segments have been clustered, vanishing points can be estimated using one of many refinement procedures <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>These procedures typically minimize a nonlinear objective function. An important distinction between such methods is the choice of point and line representation and error metric. Collins and Weiss <ref type="bibr" target="#b7">[8]</ref> formulate vanishing point detection as a statistical estimation problem on the Gaussian Sphere, which is similar to the geometry we use. More recent work has explored the use of dual space <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b34">35]</ref> representations. Among the clustering-based approaches, Xu et al. <ref type="bibr" target="#b32">[33]</ref> improve this pipeline by introducing a new pointline consistency function that models errors in the line segment extraction step.</p><p>Alternatives to clustering-based approaches have been explored. For example, vanishing point detection from line segments has been modeled as an Uncapacitated Facility Location (UFL) problem <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b27">28]</ref>. To avoid error accumulation issues encountered by a step-by-step pipeline method, Barinova et al. <ref type="bibr" target="#b4">[5]</ref> solve the problem in a unified framework, where edges, lines, and vanishing points fit into a single graphical model. Our approach is motivated by the fact that properties of the scene, including objects, can provide additional cues for vanishing point and horizon line placement than line segments alone. Unlike existing methods that use Jlinkage <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b32">33]</ref> or similar techniques to find an initial set of VPs by clustering detected lines followed by a refinement step, our approach first proposes candidate horizon lines using global image context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Approach Overview</head><p>Our approach is motivated by two observations: 1) traditional purely geometric approaches to vanishing point detection often fail in seemingly nonsensical ways and 2) identifying the true vanishing points for many scenes is challenging and computationally expensive due to the large number of outlier line segments. Driven by these observations, we propose a two part strategy. First, we use global image context to estimate priors over the horizon line and the zenith vanishing point (Sec. 3). Using these priors, we introduce a novel VP detection method (Sec. 4) that samples horizon lines from the prior and performs a fast one-dimensional search for high-quality vanishing points in each. Both steps are essential for accurate results: the prior helps ensure a good initialization such that our horizon-first detection method may obtain very precise estimates that are necessary for many scene understanding tasks. See <ref type="figure" target="#fig_2">Fig. 2</ref> for an overview of our algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Problem Formulation</head><p>The goal of this work is to detect the horizon line, the zenith vanishing point, and any horizontal vanishing points from a single image. The remainder of this section defines the notation and basic geometric facts that we will use throughout. For clarity we use unbolded letters for points in world coordinates or the image plane and bolded letters for points or lines in homogeneous coordinates. We primarily follow the notation convention of Vedaldi and Zisserman <ref type="bibr" target="#b27">[28]</ref>.</p><p>Given a point (u, v) in the image plane, its homogeneous coordinate with respect to the calibrated image plane is denoted by:</p><formula xml:id="formula_0">p = [ρ(u − c u ), ρ(v − c v ), 1] T /Σ , where ρ is a scale constant, (c u , c v )</formula><p>is the camera principal point in the image frame, which we assume to be the center of the image, and Σ is the constant that makes p a unit vector.</p><p>In homogeneous coordinates, both lines and points are represented as three-dimensional vectors ( <ref type="figure">Fig. 3</ref>). Computing the line, l, that passes through two points, (p 1 , p 2 ), and the point, p, at the intersection of two lines, (l 1 , l 2 ), are defined as follows:</p><formula xml:id="formula_1">l = p 1 × p 2 p 1 × p 2 p = l 1 × l 2 l 1 × l 2 .<label>(1)</label></formula><p>We denote the smallest angle between two vectors x and y with Θ x,y = |cos −1 (x T y)|. We use this to define the consistency between a line, l, and a point, p, as: f c (p, l) = max(θ con − Θ p,l , 0). The maximum value of consistency between a vanishing point and a line segment is θ con . This will occur if it is possible to extend the line segment to contain the vanishing point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Horizon Priors from Global Image Context</head><p>Recent studies show that deep convolutional neural networks (CNNs) are adaptable for a wide variety of tasks <ref type="bibr" target="#b33">[34]</ref>, and are quite fast in practice. We propose to use a CNN to extract global image context from a single image.</p><p>We parameterize the horizon line by its slope angle, α ∈ [−π, π), and offset, o ∈ [0, inf), which is the shortest <ref type="figure">Figure 3</ref>: In homogeneous coordinates, lines (red lines) are defined by the normal (red arrow) of the plane (red triangle) they form with the origin (green dot). Two lines form a great circle (blue circle), whose normal (blue arrow) is their common point (blue dot) in homogeneous coordinates. distance between the horizon line and the principal point. In order to span the entire horizon line parameter space, we "squash" o from pixel coordinates to the interval [0, π/2), through a one-to-one function, w = tan −1 (o/κ), in which κ is a scaling factor that affects how dense the sampling is near the center of the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network Architecture</head><p>For our task, we adapt the popular AlexNet <ref type="bibr" target="#b17">[18]</ref> architecture, which was designed for object recognition as part of the ImageNet ILSVRC-2012 challenge <ref type="bibr" target="#b23">[24]</ref>. It consists of five convolutional layers, each followed by a non-linearity (rectified linear unit), and occasionally interspersed with pooling and local response normalization. This is followed by three fully connected layers (referred to as 'fc6', 'fc7', and 'fc8'). A softmax is applied to the final output layer to produce a categorical distribution over 1000 object classes. We use this as a foundation to create a CNN that simultaneously generates a categorical distribution for each horizonline parameter.</p><p>We modify the original AlexNet architecture in the following way: The first five convolutional layers are left unmodified. These layers are initialized with weights from a network trained for object detection and scene classification <ref type="bibr" target="#b35">[36]</ref>. We remove the original fully connected layers ('fc6'-'fc8') and add two disjoint sets of fully connected layers ('fc6α'-'fc8α' and 'fc6w'-'fc8w'), one for each target label, α and w. We convert the slope, α, and the squashed offset, w, into independent categorical labels by uniformly dividing their respective domains into 500 bins. We randomly initialize the weights for these new layers.</p><p>We train our network using stochastic gradient descent, with a multinomial logistic loss function. The learning rates for the convolutional layers are progressively increased such that the latter layers change more. The new fully con- nected layers are given full learning rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training Database</head><p>To support training our model of global image context, we construct a large dataset of images with known horizon lines. We make use of equirectangular panoramas downloaded from Google Street View in large metropolitan cities around the world. We identified a set of cities based on population and Street View coverage. From each city, we downloaded panoramas randomly sampled in a 5km × 5km region around the city center. This resulted in 11 001 panoramas from 93 cities. Example cities include New York, Rio de Janeiro, London, and Melbourne.</p><p>We Given the FOV, pitch, and roll of a generated perspective image, it is straightforward to compute the horizon line position in image space. In total, our training database contains 110 010 images with known horizon line. <ref type="figure" target="#fig_3">Fig. 4</ref> shows several example images from our dataset annotated with the ground-truth horizon line.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Making the Output Continuous</head><p>Given an image, I, the network outputs a categorical probability distribution for the slope, α, and squashed offset, w. We make these distributions continuous by approximating them with a Gaussian distribution. For each, we estimate the mean and variance from 5 000 samples generated from the categorical probability distribution. Since the relationship between w and o is one-to-one, this also results in a continuous distribution over o. The resulting distributions, p(α|I) and p(o|I), are used in the next step of our approach to aid in detecting the zenith VP and as a prior for sampling candidate horizon lines. To visualize this distribution we observe that the horizon line can be uniquely defined by the point on the line closest to the principal point. Therefore, we can visualize a horizon line distribution as a distribution over points in the image. <ref type="figure" target="#fig_5">Fig. 5</ref> shows this distribution for two images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Horizon-First Vanishing Point Detection</head><p>We propose an approach to obtain accurate estimates of the horizon line, the zenith vanishing point, and one or more horizontal vanishing points. Given an image, our approach makes use of the distributions estimated from global image context (Sec. 3) and line segments extracted with LSD <ref type="bibr" target="#b28">[29]</ref>. The algorithm consists of the following major steps: 3) The remainder of this section provides details for each of these steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Detecting the Zenith Vanishing Point</head><p>To detect the zenith vanishing point, we first select an initial set of line segments using the zenith direction, l z , from the global image context, then use the RANSAC <ref type="bibr" target="#b6">[7]</ref> algorithm to refine it. The zenith direction is the line con-necting the principal point and the zenith vanishing point, which is uniquely determined by the horizon line slope (see supplemental material for a proof).</p><p>We compute our initial estimate of l z using the global image context by choosing the value that maximizes the posterior:α = arg max α p(α|I). To handle the presence of outlier line segments, we first select a set of candidate vertical line segments as the RANSAC inputs by thresholding the angle between each line segment and the estimated zenith direction, Θ l,lz &lt; θ ver . For a randomly sampled pair of line segments with intersection, p, we compute the set of inlier line segments, {l | f c (p, l) &gt; 0}. If the largest set of inliers has a sufficient portion (more than 2% of candidate line segments), we obtain the final estimate of the zenith vanishing point, z, by minimizing the algebraic distance, l T p using singular value decomposition (SVD), and update the zenith direction, l z . Otherwise, we keep the zenith direction estimated from the global image context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Detecting Horizontal Vanishing Points</head><p>We start with sampling a set of horizon line candidates, {h i } S 1 , that are perpendicular to l z in the image space, under the distribution of horizon line offsets, p(o|I). See <ref type="figure">Fig. 6</ref> for examples of horizon line sampling with and without global context.</p><p>For each horizon line candidate, we identify a set of horizontal VPs by selecting points along the horizon line where many line segments intersect. We assume that for the true horizon line the identified horizontal VPs will be close to many intersection points and that these intersections will be more tightly clustered than for non-horizon lines. We use this intuition to define a scoring function for horizon line candidates.</p><p>As a preprocessing step, given the zenith direction, l z , and a horizon line candidate, h, we filter out nearly vertical line segments (Θ l,lz &lt; θ ver ), which are likely associated with the zenith vanishing point, and nearly horizontal line segments (Θ l,h &lt; θ hor ), which result in noisy horizon line intersection points. We remove such lines from consideration because they lead to spurious, or uninformative, vanishing points, which decreases accuracy.</p><p>Given a horizon line candidate, h, and the filtered line segments in homogeneous coordinates, L = {l i }, we select a set of horizontal VPs, P = {p i }, by minimizing the following objective function:</p><formula xml:id="formula_2">g(P|h, L) = − pi∈P lj ∈L f c (p i , l j )<label>(2)</label></formula><p>subject to: Θ pi,pj &gt; θ dist and p i , h = 0, ∀(i, j) .</p><p>The constraint prevents two vanishing points from being too close together, which eliminates the possibility of selecting multiple vanishing points in the same location. <ref type="figure">Figure 6</ref>: Our method samples more horizon line candidates (red) near the ground truth (green dash) with (middle) global image context than without (left). In the case of sampling with global image context, the offset PDF, p(o|I) (blue curve), is fit from the CNN categorical probability distribution outputs (hollow bins). For clarity, we only show a reduced number of horizon line candidates and bins.</p><p>We propose the following combinatorial optimization process for obtaining an initial set of vanishing points, followed by a constrained nonlinear optimization to refine the vanishing points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Initialization by Random Sampling and Discrete Optimization</head><p>To choose an initial set of candidate vanishing points, {p i } M 1 , we randomly select a subset of line segments, {l i } M 1 , and compute their intersection with the horizon line. We then construct a graph with a node for each vanishing point, p i , each with weight lj ∈L f c (p i , l j ), which is larger if there are many line segments in the image that are consistent with p i . Pairs of nodes, (i, j), are connected if the corresponding vanishing points, p i , p j , are sufficiently close in homogeneous space (Θ pi,pj ≤ θ dist ).</p><p>From this randomly sampled set, we select an optimal subset of VPs by maximizing the sum of weights, while ensuring no VPs in the final set are too close. Therefore, the problem of choosing the initial set of VPs reduces to a maximum weighted independent set problem, which is NPhard in general. Due to the nature of the constraints, the resulting graph has a ring-like structure which means that, in practice, the problem can be quickly solved. Our solver exploits this sparse ring-like structure by finding a set of VPs that when removed convert the ring-like graph into a set of nearly linear sub-graphs ( <ref type="figure">Fig. 7)</ref>. We solve each subproblem using dynamic programming. The set of VPs with maximum weight, {p i } opt , is used as initialization for local refinement. Usually, 2-4 such vanishing points are found near the horizon line ground truth.  <ref type="figure">Figure 7</ref>: A ring-like graph (left) is converted into three nearly linear subgraphs (right) by partitioning around a node with minimal degree. For the subgraphs, the red node is mandatory, the dashed nodes are excluded, and a subset of the solid nodes are selected using dynamic programming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Vanishing Points Refinement</head><p>Since they were randomly sampled, the set of vanishing points selected during initialization, {p i } opt , may not be at the optimal locations. We optimize their locations to further minimize the objective function <ref type="bibr" target="#b1">(2)</ref>. We perform an EM-like algorithm to refine the vanishing point locations, subject to the constraint that they lie on the horizon line:</p><p>• E-step: Given a vanishing point, p, assign line segments that have positive consistency with p: {l|f c (p, l) &gt; 0}. • M-step: Given the assigned line segments as a matrix, L = [l 1 , l 2 , . . . , l n ], and the horizon line, h, both represented in homogeneous coordinates, we solve for a refined vanishing point, p * , by minimizing the algebraic distance, L T p such that h T p = 0. We define a basis, B h , for the null space of h, and reformulate the problem as λ * = arg min L B h λ , which we solve using SVD. Given the optimal coefficients, λ * , we reconstruct the optimal vanishing point as: p * = B h λ * B h λ * . We run this refinement iteration until convergence. In practice, this converges quickly; we run at most three iterations for all the experiments. The final set of optimized VPs is then used to assign a score to the current horizon line candidate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Optimal Horizon Line Selection</head><p>For each horizon line candidate, we assign a score based on the total consistency of lines in the image with the VPs selected in the previous section. The score of a horizon line candidate, h, is defined as:</p><formula xml:id="formula_3">score(h) = {pi} lj ∈L f c (p i , l j ) .<label>(3)</label></formula><p>To reduce the impact of false positive vanishing points, we select from {p i } opt the two highest weighted vanishing points (or one if {p i } opt contains only one element), {p i }, for horizon line scoring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Evaluation</head><p>We perform an extensive evaluation of our methods, both quantitatively and qualitatively, on three benchmark datasets. The results show that our method achieves stateof-the-art performance based on horizon-line detection error, the standard criteria in recent work on VP detection <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33]</ref>. Horizon detection error is defined as the maximum distance from the detected horizon line to the ground-truth horizon line, normalized by the image height. Following tradition, we show the cumulative histogram of these errors and report the area under the curve (AUC).</p><p>Our method is implemented using MATLAB, with the exception of detecting line segments, which uses an existing C++ library <ref type="bibr" target="#b28">[29]</ref>, and extracting global image context, which we implemented using Caffe <ref type="bibr" target="#b15">[16]</ref>. We use the parameters defined in Tab. 1 for all experiments. This differs from other methods which usually use different parameters for different datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Quantitative Evaluation</head><p>The York Urban Dataset (YUD) <ref type="bibr" target="#b10">[11]</ref> is a commonly used dataset for evaluating horizon line estimation methods. It contains 102 images and ground-truth vanishing points. The scenes obey the Manhattan-world assumption, however we do not take advantage of this assumption. <ref type="figure">Fig. 8a</ref> shows the performance of our methods relative to previous work on YUD. These results demonstrate that our method achieves state-of-the-art AUC, improving upon the previous best of Lezama et al. <ref type="bibr" target="#b18">[19]</ref> by 0.28%, a relative improvement 1 of 5%. This is especially impressive given that our method only requires an average of 1 second per image, while Lezama et al. requires approximately 30 seconds per image.</p><p>The Eurasian Cities Dataset (ECD) <ref type="bibr" target="#b4">[5]</ref> is another commonly used benchmark dataset, which is considered challenging due to the large number of outlier line segments <ref type="bibr" target="#b0">1</ref> We define the relative improvement as AUCnew −AUC old (c) HLW <ref type="figure">Figure 8</ref>: For three benchmark datasets, the fraction of images (y-axis) with a horizon error less than a threshold (x-axis). The AUC for each curve is shown in the legend. For additional details see Sec. 5. and complex scene geometries. It contains 103 images captured in urban areas and, unlike the YUD dataset, not all images satisfy the Manhattan-world assumption. It provides reliable horizon line ground truth and is widely considered difficult for horizon line detection. To our knowledge, the previous state-of-the-art performance in terms of the AUC metric on this dataset was achieved by Lezama et al. <ref type="bibr" target="#b18">[19]</ref>. Our algorithm improves upon their performance, increasing the state of the art to 90.8%. This is a significant relative improvement of 14.8%, especially considering their improvement relative to the state of the art was 0.5%. On ECD, our method takes an average of 3 seconds per image, while Lezama et al. requires approximately 60 seconds per image. We present the performance comparison with other methods in <ref type="figure">Fig. 8b</ref>.</p><p>The Horizon Lines in the Wild (HLW) dataset <ref type="bibr" target="#b31">[32]</ref> is a new, very challenging benchmark dataset. We use the provided test set, which contains approximately 2 000 images from diverse locations, with many images not adhering to the Manhattan-world assumption. <ref type="figure">Fig. 8c</ref> compares our method with the method of Lezama et al. <ref type="bibr" target="#b18">[19]</ref> (the only publicly available implementation from a recent method). Our method is significantly better, achieving 58.24% versus 52.59% AUC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Component Error Analysis</head><p>Our method consists of two major components: global context extraction (Sec. 3) and horizon-first vanishing point detection (Sec. 4). This section provides an analysis of the impact each component has on accuracy.</p><p>To evaluate the impact of global context extraction, we considered three alternatives: our proposed approach (CNN), replacing the CNN with a random forest (using the Python "sklearn" library with 25 trees) applied to a GIST <ref type="bibr" target="#b22">[23]</ref> descriptor (GISTRF), and omitting context entirely (NONE). When omitting the global context, we assume no camera roll (horizon lines are horizontal in the image) and sample horizon lines uniformly between [−2H, 2H] (H is the image height). To evaluate the impact of vanishing point detection, we considered two alternatives: our proposed approach (FULL) and omitting the vanishing point detection step (EMPTY). When omitting vanishing point detection, we directly estimate the horizon line, (α, o), by maximizing the posterior estimated by our global-context CNN, p(α, o|I).</p><p>Quantitative results presented in Tab. 2 show that both components play important roles in the algorithm and that CNN provides better global context information than GISTRF. Though our vanishing point detection performs well by itself (see column NONE+FULL), global image context helps improve the accuracy further. <ref type="figure">Fig. 8c</ref> visualizes these results as a cumulative histogram of horizon error on HLW. To illustrate the impact of global image context, we present two examples in <ref type="figure" target="#fig_8">Fig. 9</ref> that compare horizon line estimates obtained using global context (CNN+FULL) and without (NONE+FULL). When using global context, the estimated horizon lines are very close to the ground truth. Without, the estimates obtained are implausible, even resulting in an estimate that is off the image.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Failure Cases</head><p>We highlight two representative failure cases in the last column of <ref type="figure" target="#fig_0">Fig. 10</ref>. The top image fails due to the propagation of measurement errors from the short line segments. The bottom image is challenging because the curved structures lead to indistinct VPs. Despite this, global context helps our method produce plausible results, while other methods (e.g., <ref type="bibr" target="#b4">[5]</ref>) fail dramatically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We presented a novel vanishing point detection algorithm that obtains state-of-the-art performance on three benchmark datasets. The main innovation in our method is the use of global image context to sample possible horizon lines, followed by a novel discrete-continuous procedure to score each horizon line by choosing the optimal vanishing points for the line. Our method is both more accurate and more efficient than the previous state-of-the-art algorithm, requiring no parameter tuning for a new testing dataset, which is common in other methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An example result of our method. (left) Horizon line candidates, colored by their scores (red means high score), and the true horizon line (green dash). (right) The horizon line (magenta) estimated by our algorithm is very close to the true horizon line (green dash). Line segments are color coded based on the most consistent detected vanishing point.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Algorithm overview: 1) use global image context to estimate a prior over horizon lines (Sec. 3); 2) extract line segments; 3) identify the zenith VP (Sec. 4.1); 4) sample horizon line candidates consistent with the zenith VP (Sec. 4.2); 5) find VPs on horizon line candidates (Sec. 4.2); and 6) select the best horizon line based on the VPs it contains (Sec. 4.3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Example images from our training dataset (Sec. 3.2), each overlaid with the ground-truth horizon line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>extracted 10 perspective images from each panorama with randomly sampled horizontal field-of-view (FOV), yaw, pitch, and roll. Here yaw is relative to the Google Street View capture vehicle. We sampled horizontal FOV from a normal distribution with µ = 60 • and σ = 10 • . Similarly, pitch and roll are sampled from normal distributions with µ = 0 • and σ = 10 • and σ = 5 • , respectively. Yaw is sampled uniformly. We truncate these distributions such that horizontal FOV ∈ [40 • , 80 • ], pitch ∈ [−30 • , 30 • ], and roll ∈ [−20 • , 20 • ]. These settings were selected empirically to match the distribution of images captured by casual photographers in the wild.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Global image context imposes a strong prior on horizon line location. The output of our CNN is visualized as an overlaid heatmap, with red indicating more likely locations. For each image, the ground-truth horizon line (dash green) and the line that maximizes the prior (red) are shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Example results produced by our method. (rows 1 and 3) Line segments color coded based on the most consistent VP, the ground-truth (green dash), and detected horizon lines (magenta). For clarity only the top two horizontal VPs are shown. (rows 2 and 4) The line segments (dots) and their VPs (rings) represented in homogeneous coordinates. (last column) Two failure cases of our method, caused by irregularly shaped objects (bottom) and short edges (top).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Two images where horizon line estimates are much better with global context (left) than without (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Algorithm parameters (given an H × W image). Sec. 4.2, Sec. 4.2.1 Θ pi,pj &gt; 33 •</figDesc><table><row><cell cols="2">Name Usage(s)</cell><cell>Value</cell></row><row><cell>θ con</cell><cell>Sec. 2</cell><cell>2 •</cell></row><row><cell>ρ</cell><cell>Sec. 2</cell><cell>2/ max(H, W )</cell></row><row><cell>κ</cell><cell>Sec. 3</cell><cell>1/5 × H</cell></row><row><cell>θ ver</cell><cell>Sec. 4.1, Sec. 4.2</cell><cell>Θ l,lz &lt; 10 •</cell></row><row><cell>θ hor</cell><cell>Sec. 4.2</cell><cell>Θ l,h &lt; 1.5 •</cell></row><row><cell>S</cell><cell>Sec. 4.2</cell><cell>300 candidates</cell></row><row><cell>M</cell><cell>Sec. 4.2.1</cell><cell>20 line segments</cell></row><row><cell>θ dist</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Component error analysis (AUC).</figDesc><table><row><cell>Method</cell><cell>YUD</cell><cell>ECD</cell><cell>HLW</cell></row><row><cell>Lezama et al. [19]</cell><cell cols="3">94.51% 89.20% 52.59%</cell></row><row><cell>NONE+FULL</cell><cell cols="3">93.87% 87.94% 53.04%</cell></row><row><cell>GISTRF+EMPTY</cell><cell cols="3">53.36% 32.69% 31.08%</cell></row><row><cell>GISTRF+FULL</cell><cell cols="3">94.66% 87.60% 54.95%</cell></row><row><cell>CNN+EMPTY</cell><cell cols="3">73.67% 67.64% 49.03%</cell></row><row><cell cols="4">CNN+FULL (Ours) 94.78% 90.80% 58.24%</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We gratefully acknowledge the support of DARPA (contract CSSG D11AP00255). The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA or the U.S. Government.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vanishing point detection without any a priori information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Almansa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desolneux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vamech</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="502" to="507" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automatic recovery of relative camera rotations for urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Antone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Teller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A global approach for the detection of vanishing points and mutually orthogonal vanishing directions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Antunes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Barreto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Handling urban location recognition as a 2d homothetic problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Baatz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Köser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grzeszczuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Geometric image parsing in man-made environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Barinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tretiak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Interpreting perspective images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Barnard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="435" to="462" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A ransac-based approach to model fitting and its application to finding cylinders in range data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="1981" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Vanishing point calculation as a statistical inference on the unit sphere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Manhattan world: Compass direction from a single image by bayesian inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Coughlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Single view metrology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="148" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient edge-based methods for estimating manhattan frames in urban imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Elder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Estrada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An automatic approach for camera calibration from vanishing points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Grammatikopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Petsa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="64" to="76" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Putting objects in perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">IJCV</biblScope>
			<biblScope unit="page" from="3" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Machine analysis of bubble chamber pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Hough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on High Energy Accelerators and Instrumentation</title>
		<imprint>
			<date type="published" when="1959" />
			<biblScope unit="volume">73</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cloud Motion as a Calibration Cue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Workman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Video compass</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Košecká</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Finding vanishing points via point alignments in image primal and dual domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lezama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Gioi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Randall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Local regularity-driven city-scale facade detection from aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Contribution to the determination of vanishing points using hough transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maitre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lopez-Krahe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="430" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Determining vanishing points from perspective images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Magee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, Graphics, and Image Processing</title>
		<imprint>
			<date type="published" when="1984" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="256" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Modeling the shape of the scene: A holistic representation of the spatial envelope</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Atlanta world: An expectation maximization framework for simultaneous low-level edge grouping and camera calibration in complex man-made environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dellaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Computer vision: algorithms and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Non-iterative approach for fast and accurate vanishing point detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Tardif</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Self-similar sketch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Lsd: A fast line segment detector with a false detection control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Von Gioi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jakubowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Morel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Randall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Robust camera selfcalibration from monocular images of manhattan worlds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wildenauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A Pot of Gold: Rainbows as a Calibration Cue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Workman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Mihail</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Workman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jacobs</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.02129</idno>
		<title level="m">Horizon lines in the wild</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A minimum error vanishing point detection approach for uncalibrated monocular images of man-made environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hoogs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Calculating vanishing points in dual space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-B</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intelligent Science and Intelligent Data Engineering</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="522" to="530" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning Deep Features for Scene Recognition using Places Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
