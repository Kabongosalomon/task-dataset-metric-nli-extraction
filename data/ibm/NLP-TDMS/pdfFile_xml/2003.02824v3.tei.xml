<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Action Segmentation with Joint Self-Supervised Temporal Domain Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Hung</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baopu</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>Baidu</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingze</forename><surname>Bao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghassan</forename><surname>Alregib</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<address>
									<settlement>Baidu</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Action Segmentation with Joint Self-Supervised Temporal Domain Adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite the recent progress of fully-supervised action segmentation techniques, the performance is still not fully satisfactory. One main challenge is the problem of spatiotemporal variations (e.g. different people may perform the same activity in various ways). Therefore, we exploit unlabeled videos to address this problem by reformulating the action segmentation task as a cross-domain problem with domain discrepancy caused by spatio-temporal variations. To reduce the discrepancy, we propose Self-Supervised Temporal Domain Adaptation (SSTDA), which contains two self-supervised auxiliary tasks (binary and sequential domain prediction) to jointly align cross-domain feature spaces embedded with local and global temporal dynamics, achieving better performance than other Domain Adaptation (DA) approaches. On three challenging benchmark datasets (GTEA, 50Salads, and Breakfast), SSTDA outperforms the current state-of-the-art method by large margins (e.g. for the F1@25 score, from 59.6% to 69.1% on Breakfast, from 73.4% to 81.5% on 50Salads, and from 83.6% to 89.1% on GTEA), and requires only 65% of the labeled training data for comparable performance, demonstrating the usefulness of adapting to unlabeled target videos across variations. The source code is available at https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The goal of action segmentation is to simultaneously segment videos by time and predict an action class for each segment, leading to various applications (e.g. human activity analyses). While action classification has shown great progress given the recent success of deep neural networks <ref type="bibr" target="#b39">[41,</ref><ref type="bibr" target="#b27">29,</ref><ref type="bibr" target="#b26">28]</ref>, temporally locating and recognizing action segments in long videos is still challenging. One main challenge is the problem of spatio-temporal variations of human actions across videos <ref type="bibr" target="#b16">[17]</ref>. For example, different people may make tea in different personalized styles even if the given recipe is the same. The intra-class variations * Work done during an internship at Baidu USA  <ref type="figure">Figure 1</ref>: An overview of the proposed Self-Supervised Temporal Domain Adaptation (SSTDA) for action segmentation. "Source" refers to the data with labels, and "Target" refers to the data without access to labels. SSTDA can effectively adapts the source model trained with standard fully-supervised learning to a target domain by diminishing the discrepancy of embedded feature spaces between the two domains caused by spatio-temporal variations. SSTDA only requires unlabeled videos from both domains with the standard transductive setting, which eliminates the need of additional labels to obtain the final target model. cause degraded performance by directly deploying a model trained with different groups of people. Despite significant progress made by recent methods based on temporal convolution with fully-supervised learning <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b8">9]</ref>, the performance is still not fully satisfactory (e.g. the best accuracy on the Breakfast dataset is still lower than 70%). One method to improve the performance is to exploit knowledge from larger-scale labeled data <ref type="bibr" target="#b1">[2]</ref>. However, manually annotating precise frame-by-frame actions is time-consuming and challenging. Another way is to design more complicated architectures but with higher costs of model complexity. Thus, we aim to address the spatiotemporal variation problem with unlabeled data, which are comparatively easy to obtain. To achieve this goal, we propose to diminish the distributional discrepancy caused by spatio-temporal variations by exploiting auxiliary unlabeled videos with the same types of human activities performed by different people. More specifically, to extend the framework of the main video task for exploiting auxiliary data <ref type="bibr" target="#b46">[48,</ref><ref type="bibr" target="#b19">20]</ref>, we reformulate our main task as an unsupervised domain adaptation (DA) problem with the transductive setting <ref type="bibr" target="#b30">[32,</ref><ref type="bibr" target="#b5">6]</ref>, which aims to reduce the discrepancy between source and target domains without access to the target labels.</p><p>Recently, adversarial-based DA approaches <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b45">47]</ref> show progress in reducing the discrepancy for images using a domain discriminator equipped with adversarial training. However, videos also suffer from domain discrepancy along the temporal direction <ref type="bibr" target="#b3">[4]</ref>, so using imagebased domain discriminators is not sufficient for action segmentation. Therefore, we propose Self-Supervised Temporal Domain Adaptation (SSTDA), containing two selfsupervised auxiliary tasks: 1) binary domain prediction, which predicts a single domain for each frame-level feature, and 2) sequential domain prediction, which predicts the permutation of domains for an untrimmed video. Through adversarial training with both auxiliary tasks, SSTDA can jointly align cross-domain feature spaces that embed local and global temporal dynamics, to address the spatiotemporal variation problem for action segmentation, as shown in <ref type="figure">Figure 1</ref>. To support our claims, we compare our method with other popular DA approaches and show better performance, demonstrating the effectiveness for aligning temporal dynamics by SSTDA. Finally, we evaluate our approaches on three datasets with high spatio-temporal variations: GTEA <ref type="bibr" target="#b9">[10]</ref>, 50Salads <ref type="bibr" target="#b35">[37]</ref>, and the Breakfast dataset <ref type="bibr" target="#b17">[18]</ref>. By exploiting unlabeled target videos with SSTDA, our approach outperforms the current state-of-theart methods by large margins and achieve comparable performance using only 65% of labeled training data.</p><p>In summary, our contributions are three-fold:</p><p>1. Self-Supervised Sequential Domain Prediction: We propose a novel self-supervised auxiliary task, which predicts the permutation of domains for long videos, to facilitate video domain adaptation. To the best of our knowledge, this is the first self-supervised method designed for cross-domain action segmentation.</p><p>2. Self-Supervised Temporal Domain Adaptation (SSTDA): By integrating two self-supervised auxiliary tasks, binary and sequential domain prediction, our proposed SSTDA can jointly align local and global embedded feature spaces across domains, outperforming other DA methods.</p><p>3. Action Segmentation with SSTDA: By integrating SSTDA for action segmentation, our approach outperforms the current state-of-the-art approach by large margins, and achieve comparable performance by using only 65% of labeled training data. Moreover, different design choices are analyzed to identify the key contributions of each component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Action Segmentation methods proposed recently are built upon temporal convolution networks (TCN) <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b8">9]</ref> because of their ability to capture long-range dependencies across frames and faster training compared to RNN-based methods. With the multi-stage pipeline, MS-TCN <ref type="bibr" target="#b8">[9]</ref> performs hierarchical temporal convolutions to effectively extract temporal features and achieve the state-of-the-art performance for action segmentation. In this work, we utilize MS-TCN as the baseline model and integrate the proposed self-supervised modules to further boost the performance without extra labeled data. Domain Adaptation (DA) has been popular recently especially with the integration of deep learning. With the twobranch (source and target) framework for most DA works, finding a common feature space between source and target domains is the ultimate goal, and the key is to design the domain loss to achieve this goal <ref type="bibr" target="#b5">[6]</ref>.</p><p>Discrepancy-based DA <ref type="bibr" target="#b23">[25,</ref><ref type="bibr" target="#b24">26,</ref><ref type="bibr" target="#b25">27</ref>] is one of the major classes of methods where the main goal is to reduce the distribution distance between the two domains. Adversarialbased DA <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> is also popular with similar concepts as GANs <ref type="bibr" target="#b12">[13]</ref> by using domain discriminators. With carefully designed adversarial objectives, the domain discriminator and the feature extractor are optimized through minmax training. Some works further improve the performance by assigning pseudo-labels to target data <ref type="bibr" target="#b32">[34,</ref><ref type="bibr" target="#b42">44]</ref>. Furthermore, Ensemble-based DA <ref type="bibr" target="#b34">[36,</ref><ref type="bibr">22]</ref> incorporates multiple target branches to build an ensemble model. Recently, Attention-based DA <ref type="bibr" target="#b40">[42,</ref><ref type="bibr" target="#b18">19]</ref> assigns attention weights to different regions of images for more effective DA.</p><p>Unlike images, video-based DA is still under-explored. Most works concentrate on small-scale video DA datasets <ref type="bibr" target="#b37">[39,</ref><ref type="bibr" target="#b44">46,</ref><ref type="bibr" target="#b14">15]</ref>. Recently, two larger-scale crossdomain video classification datasets along with the state-of-the-art approach are proposed <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. Moreover, some authors also proposed novel frameworks to utilize auxiliary data for other video tasks, including object detection <ref type="bibr" target="#b19">[20]</ref> and action localization <ref type="bibr" target="#b46">[48]</ref>. These works differ from our work by either different video tasks <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref> or access to the labels of auxiliary data <ref type="bibr" target="#b46">[48]</ref>. Self-Supervised Learning has become popular in recent years for images and videos given the ability to learn informative feature representations without human supervision. The key is to design an auxiliary task (or pretext task) that is related to the main task and the labels can be self-annotated. Most of the recent works for videos design auxiliary tasks based on spatio-temporal orders of videos <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b41">43,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b43">45]</ref>. Different from these works, our proposed auxiliary task predicts temporal permutation for cross-domain videos, aiming to address the problem of spatio-temporal variations for action segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SSTDA module</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single-Stage Temporal Convolution Network (SS-TCN)</head><p>. . . . . .</p><formula xml:id="formula_0">. . . . . . . . . . . .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-layer Temporal Convolution</head><formula xml:id="formula_1">ℒ ෝ ℒ . . . . . .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input frame-level features</head><p>Output frame-level features ℒ <ref type="figure">Figure 2</ref>: Illustration of the baseline model and the integration with our proposed SSTDA. The frame-level features f are obtained by applying the temporal convolution network G f to the inputs, and converted to the corresponding predictionsŷ using a fully-connected layer G y to calculate the prediction loss L y . The SSTDA module is integrated with f to calculate the local and global domain losses, L ld and L gd for optimizing f during training (see details in Section 3.2).</p><p>Here we only show one stage in our multi-stage model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Technical Approach</head><p>In this section, the baseline model which is the current state-of-the-art for action segmentation, MS-TCN <ref type="bibr" target="#b8">[9]</ref>, is reviewed first (Section 3.1). Then the novel temporal domain adaptation scheme consisting of two self-supervised auxiliary tasks, binary domain prediction (Section 3.2.1) and sequential domain prediction (Section 3.2.2), is proposed, followed by the final action segmentation model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Baseline Model</head><p>Our work is built on the current state-of-the-art model for action segmentation, multi-stage temporal convolutional network (MS-TCN) <ref type="bibr" target="#b8">[9]</ref>. For each stage, a single-stage TCN (SS-TCN) applies a multi-layer TCN, G f , to derive the frame-level features f = {f 1 , f 2 , ..., f T }, and makes the corresponding predictionsŷ = {ŷ 1 ,ŷ 2 , ...,ŷ T } using a fully-connected layer G y . By following <ref type="bibr" target="#b8">[9]</ref>, the prediction loss L y is calculated based on the predictionsŷ, as shown in the left part of <ref type="figure">Figure 2</ref>. Finally, multiple stages of SS-TCNs are stacked to enhance the temporal receptive fields, constructing the final baseline model, MS-TCN, where each stage takes the predictions from the previous stage as inputs, and makes predictions for the next stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Self-Supervised Temporal Domain Adaptation</head><p>Despite the promising performance of MS-TCN on action segmentation over previous methods, there is still a large room for improvement. One main challenge is the problem of spatio-temporal variations of human actions <ref type="bibr" target="#b16">[17]</ref>, causing the distributional discrepancy across domains <ref type="bibr" target="#b5">[6]</ref>. For example, different subjects may perform the same action completely differently due to personalized spatio-temporal styles. Moreover, collecting annotated data for action segmentation is challenging and time-consuming. Thus, such challenges motivate the need to learn domaininvariant feature representations without full supervision. Inspired by the recent progress of self-supervised learning, which learns informative features that can be transferred to the main target tasks without external supervision (e.g. human annotation), we propose Self-Supervised Temporal Domain Adaptation (SSTDA) to diminish cross-domain discrepancy by designing self-supervised auxiliary tasks using unlabeled videos.</p><p>To effectively transfer knowledge, the self-supervised auxiliary tasks should be closely related to the main task, which is cross-domain action segmentation in this paper. Recently, adversarial-based DA approaches <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> show progress in addressing cross-domain image problems using a domain discriminator with adversarial training where domain discrimination can be regarded as a self-supervised auxiliary task since domain labels are self-annotated. However, directly applying image-based DA for video tasks results in sub-optimal performance due to the temporal information being ignored <ref type="bibr" target="#b3">[4]</ref>. Therefore, the question becomes: How should we design the self-supervised auxiliary tasks to benefit cross-domain action segmentation? More specifically, the answer should address both cross-domain and action segmentation problems.</p><p>To address this question, we first apply an auxiliary task binary domain prediction to predict the domain for each frame where the frame-level features are embedded with local temporal dynamics, aiming to address the cross-domain problems for videos in local scales. Then we propose a novel auxiliary task sequential domain prediction to temporally segment domains for untrimmed videos where the video-level features are embedded with global temporal dynamics, aiming to fully address the above question. Finally, SSTDA is achieved locally and globally by jointly applying these two auxiliary tasks, as illustrated in <ref type="figure">Figure 3</ref>.</p><p>In practice, since the key for effective video DA is to simultaneously align and learn temporal dynamics, instead of separating the two processes <ref type="bibr" target="#b3">[4]</ref>, we integrate SSTDA modules to multiple stages instead of the last stage only, and the single-stage integration is illustrated in <ref type="figure">Figure 2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Local SSTDA</head><p>The main goal of action segmentation is to learn frame-level feature representations that encode spatio-temporal information so that the model can exploit information from multiple frames to predict the action for each frame. Therefore,  <ref type="figure">Figure 3</ref>: The two self-supervised auxiliary tasks in SSTDA: 1) binary domain prediction: discriminate single frame, 2) sequential domain prediction: predict a sequence of domains for an untrimmed video. These two tasks contribute to local and global SSTDA, respectively.</p><p>we first learn domain-invariant frame-level features with the auxiliary task binary domain prediction <ref type="figure">(Figure 3</ref> left). Binary Domain Prediction: For a single stage, we feed the frame-level features from source and target domains f S and f T , respectively, to an additional shallow binary domain classifier G ld , to discriminate which domain the features come from. Since temporal convolution from previous layers encodes information from multiple adjacent frames to each frame-level feature, those frames contribute to the binary domain prediction for each frame. Through adversarial training with a gradient reversal layer (GRL) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>, which reverses the gradient signs during back-propagation, G f will be optimized to gradually align the feature distributions between the two domains. Here we noteĜ ld as G ld equipped with GRL, as shown in <ref type="figure">Figure 4</ref>.</p><p>Since this work is built on MS-TCN, integratingĜ ld with proper stages is critical for effective DA. From our investigation, the best performance happens whenĜ ld s are integrated into middle stages. See Section 4.3 for details.</p><p>The overall loss function becomes a combination of the baseline prediction loss L y and the local domain loss L ld with reverse sign, which can be expressed as follows:</p><formula xml:id="formula_2">L = Ns Ly − Ns β l L ld (1) L ld = 1 T T j=1 L ld (G ld (fj), dj)<label>(2)</label></formula><p>where N s is the total stage number in MS-TCN, N s is the number of stages integrated withĜ ld , and T is the total frame number of a video. L ld is a binary cross-entropy loss function, and β l is the trade-off weight for local domain loss L ld , obtained by following the common strategy as <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Global SSTDA</head><p>Although frame-level features f is learned using the context and dependencies from neighbor frames, the temporal receptive fields of f are still limited, unable to represent full videos. Solely integrating DA into f cannot fully address spatio-temporal variations for untrimmed long videos. Therefore, in addition to binary domain prediction for frame-level features, we propose the second selfsupervised auxiliary task for video-level features: sequential domain prediction, which predicts a sequence of domains for video clips, as shown in the right part of <ref type="figure">Figure 3</ref>. This task is a temporal domain segmentation problem, aiming to predict the correct permutation of domains for long videos consisting of shuffled video clips from both source and target domains. Since this goal is related to both crossdomain and action segmentation problems, sequential domain prediction can effectively benefit our main task. More specifically, we first divide f S and f T into two sets of segments</p><formula xml:id="formula_3">F S = {f S a , f S b , ...} and F T = {f T a , f T b , .</formula><p>..}, respectively, and then learn the corresponding two sets of segment-level feature representations</p><formula xml:id="formula_4">V S = {v S a , v S b , ...} and V T = {v T a , v T b , .</formula><p>..} with Domain Attentive Temporal Pooling (DATP). All features v are then shuffled and combined in random order and fed to a sequential domain classifier G gd equipped with GRL (noted asĜ gd ) to predict the permutation of domains, as shown in <ref type="figure">Figure 4</ref>. Domain Attentive Temporal Pooling (DATP): The most straightforward method to obtain a video-level feature is to aggregate frame-level features using temporal pooling. However, not all the frame-level features contribute the same to the overall domain discrepancy, as mentioned in <ref type="bibr" target="#b3">[4]</ref>. Hence, we assign larger attention weights w j (calculated usingĜ gd in local SSTDA) to the features which have larger domain discrepancy so that we can focus more on aligning those features. Finally, the attended frame-level features are aggregated with temporal pooling to generate the video-level feature v, which can be expressed as:</p><formula xml:id="formula_5">v = 1 T T j=1 wj · fj<label>(3)</label></formula><p>where T is the number of frames in a video segment. For more details, please refer to the supplementary. Sequential Domain Prediction: By separately applying DATP to both source and target segments, respectively, a set of segment-level feature</p><formula xml:id="formula_6">representations V = {v S a , v S b , ..., v T a , v T b , ...} are obtained.</formula><p>We then shuffle all the features in V and concatenate them into a feature to represent a long and untrimmed video V , which contains video segments from both domains in random order. Finally, V is fed into a sequential domain classifier G gd to predict the permutation of domains for the video segments. For exam-  <ref type="figure">Figure 4</ref>: The overview of the proposed Self-Supervised Temporal Domain Adaptation (SSTDA). The inputs from the two domains are first encoded with local temporal dynamics using G f to obtain the frame-level features f S and f T , respectively. We apply local SSTDA on all f using binary domain predictionĜ ld . Besides, f S and f T are evenly divided into multiple segments to learn segment-level features V S and V T by DATP, respectively. Finally, the global SSTDA is applied on V , which is generated by concatenating shuffled V S and V T , using sequential domain predictionĜ gd . L ld and L gd are the domain losses fromĜ ld andĜ gd , respectively. w corresponds to the attention weights for DATP, which are calculated form the outputs ofĜ ld . Here we use 8-frame videos and 2 segments as an example for this figure. Best views in colors.</p><formula xml:id="formula_7">ple, if V = [v S a , v T a , v T b , v S b ],</formula><p>the permutation as [0, 1, 1, 0]. G gd is a multi-class classifier where the class number corresponds to the total number of all possible permutations of domains, and the complexity of G gd is determined by the segment number for each video (more analyses in Section 4.3). The outputs of G gd are used to calculate the global domain loss L gd as below:</p><formula xml:id="formula_8">L gd = L gd (G gd (V )), y d )<label>(4)</label></formula><p>where L gd is also a standard cross-entropy loss function where the class number is determined by the segment number. Through adversarial training with GRL, sequential domain prediction also contributes to optimizing G f to align the feature distributions between the two domains.</p><p>There are some self-supervised learning works also proposing the concepts of temporal shuffling <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b43">45]</ref>. However, they predict temporal orders within one domain, aiming to learn general temporal information for video features. Instead, our method predicts temporal permutation for cross-domain videos, which are shown with a dual-branch pipeline in <ref type="figure">Figure 4</ref>, and integrate with binary domain prediction to effectively address both cross-domain and action segmentation problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Local-Global Joint Training.</head><p>Finally, we also adopt a strategy from <ref type="bibr" target="#b40">[42]</ref> to minimize the class entropy for the frames that are similar across domains by adding a domain attentive entropy (DAE) loss L ae . Please refer to the supplementary for more details.</p><p>By adding the global domain loss L gd (Equation <ref type="formula" target="#formula_8">(4)</ref>) and the attentive entropy loss L ae into Equation <ref type="formula">(1)</ref>, the overall loss of our final proposed Self-Supervised Temporal Domain Adaptation (SSTDA) can be expressed as follows:</p><formula xml:id="formula_9">L = Ns Ly − Ns (β l L ld + βgL gd − µLae)<label>(5)</label></formula><p>where β g and µ are the weights for L gd and L ae , respectively. <ref type="table" target="#tab_6">subject #  4  25  52  class #  11  17  48  video #  28  50  1712  leave-#-subject-out  1  5</ref> 13 <ref type="table">Table 1</ref>: The statistics of action segmentation datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GTEA 50Salads Breakfast</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>To validate the effectiveness of the proposed methods in reducing spatial-temporal discrepancy for action segmentation, we choose three challenging datasets: GTEA <ref type="bibr" target="#b9">[10]</ref>, 50Salads <ref type="bibr" target="#b35">[37]</ref>, and Breakfast <ref type="bibr" target="#b17">[18]</ref>, which separate the training and validation sets by different people (noted as subjects) with leave-subjects-out cross-validation for evaluation, resulting in large domain shift problem due to spatiotemporal variations. Therefore, we regard the training set as Source domain, and the validation set as Target domain with the standard transductive unsupervised DA protocol <ref type="bibr" target="#b30">[32,</ref><ref type="bibr" target="#b5">6]</ref>. See the supplementary for more implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Evaluation Metrics</head><p>The overall statistics of the three datasets are listed in <ref type="table">Table 1</ref>. Three widely used evaluation metrics are chosen as follows <ref type="bibr" target="#b20">[21]</ref>: frame-wise accuracy (Acc), segmental edit score, and segmental F1 score at the IoU threshold k%, denoted as F1@k (k = {10, 25, 50}). While Acc is the most common metric, edit and F1 score both consider the temporal relation between predictions and ground truths, better reflecting the performance for action segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Results</head><p>We first investigate the effectiveness of our approaches in utilizing unlabeled target videos for action segmentation. We choose MS-TCN <ref type="bibr" target="#b8">[9]</ref> as the backbone model since it is the current state of the art for this task. "Source only" means the model is trained only with source labeled videos, i.e., the baseline model. And then our approach is compared to other methods with the same transductive protocol. Finally, we compare our method to the most recent action segmentation methods on all three datasets, and investigate how our method can reduce the reliance on source labeled data. Self-Supervised Temporal Domain Adaptation: First we investigate the performance of local SSTDA by integrating the auxiliary task binary domain prediction with the baseline model. The results on all three datasets are improved significantly, as shown in <ref type="table">Table 2</ref>. For example, on the GTEA dataset, our approach outperforms the baseline by 4.3% for F1@25, 3.2% for the edit score and 3.6% for the frame-wise accuracy. Although local SSTDA mainly works on the frame-level features, the temporal information is still encoded using the context from neighbor frames, helping  <ref type="table">Table 2</ref>: The experimental results for our approaches on three benchmark datasets. "SSTDA" refers to the full model while "Local SSTDA" only contains binary domain prediction. †We achieve higher performance than reported in <ref type="bibr" target="#b8">[9]</ref> when using the released code, so use that as the baseline performance for the whole paper. ‡Global SSTDA requires outputs from local SSTDA, so it is not evaluated alone.</p><p>address the variation problem for videos across domains. Despite the improvement from local SSTDA, integrating DA into frame-level features cannot fully address the problem of spatio-temporal variations for long videos. Therefore, we integrate our second proposed auxiliary task sequential domain prediction for untrimmed long videos. By jointly training with both auxiliary tasks, SSTDA can jointly align cross-domain feature spaces embedding with local and global temporal dynamics, and further improve over local SSTDA with significant margins. For example, on the 50Salads dataset, it outperforms local SSTDA by 3.8% for F1@10, 3.7% for F1@25, 3.5% for F1@50, and 3.8% for the edit score, as shown in <ref type="table">Table 2</ref>.</p><p>One interesting finding is that local SSTDA contributes to most of the frame-wise accuracy improvement for SSTDA because it focuses on aligning frame-level feature spaces. On the other hand, sequential domain prediction benefits aligning video-level feature spaces, contributing to further improvement for the other two metrics, which consider temporal relation for evaluation. Learning from Unlabeled Target Videos: We also compare SSTDA with other popular approaches <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr">22,</ref><ref type="bibr" target="#b43">45]</ref> to validate the effectiveness of reducing spatiotemporal discrepancy with the same amount of unlabeled target videos. For the fair comparison, we integrate all these methods with the same baseline model, MS-TCN. For more implementation details, please refer to the supplementary. <ref type="table" target="#tab_4">Table 3</ref> shows that our proposed SSTDA outperforms all the other investigated DA methods in terms of the two metrics that consider temporal relation. We conjecture the main reason is that all these DA approaches are designed for cross-domain image problems. Although they are in-  tegrated with frame-level features which encode local temporal dynamics, the limited temporal receptive fields prevent them from fully addressing temporal domain discrepancy. Instead, the sequential domain prediction in SSTDA is directly applied to the whole untrimmed video, helping to globally align the cross-domain feature spaces that embed longer temporal dynamics, so that spatio-temporal variations can be reduced more effectively.</p><p>We also compare with the most recent video-based selfsupervised learning method, <ref type="bibr" target="#b43">[45]</ref>, which can also learn temporal dynamics from unlabeled target videos. However, the performance is even worse than other DA methods, implying that temporal shuffling within single domain does not effectively benefit cross-domain action segmentation. Comparison with Action Segmentation Methods: Here we compare the recent methods to SSTDA trained with two settings: 1) fully source labels, and 2) weakly source labels.</p><p>The first setting means we have labels for all the frames in source videos, and SSTDA outperforms all the previous methods on the three datasets with respect to all evaluation metrics. For example, SSTDA outperforms currently the state-of-the-art fully-supervised method, MS-TCN <ref type="bibr" target="#b8">[9]</ref>, by large margins (e.g. 8.1% for F1@25, 8.6% for F1@50, and 6.9% for the edit score on 50Salads; 9.5% for F1@25, 8.0% for F1@50, and 8.0% for the edit score on Breakfast), as demonstrated in <ref type="table" target="#tab_6">Table 4</ref>. Since no additional labeled data is used, these results indicate how our proposed SSTDA address the spatio-temporal variation problem with unlabeled videos to improve the action segmentation performance.</p><p>Given the significant improvement by exploiting unlabeled target videos, it implies the potential to train with fewer number of labeled frames using SSTDA, which is our second setting. In this setting, we drop labeled frames from source domains with uniform sampling for training, and evaluate on the same length of validation data. Our experiment indicates that by integrating with SSTDA, only   65% of labeled training data are required to achieve comparable performance with MS-TCN, as shown in the "SSTDA (65%)" row in <ref type="table" target="#tab_6">Table 4</ref>. For the full experiments about labeled data reduction, please refer to the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study and Analysis</head><p>Design Choice for Local SSTDA: Since we develop our approaches upon MS-TCN <ref type="bibr" target="#b8">[9]</ref>, it raises the question: How to effectively integrate binary domain prediction to a multistage architecture? To answer this, we first integrateĜ ld into each stage and the results show that the best performance happens when theĜ ld is integrated into middle stages, such as S2 or S3, as shown in <ref type="table" target="#tab_7">Table 5</ref>. S1 is not a good choice for DA because it corresponds to low-level features with less discriminability where DA shows limited effects <ref type="bibr" target="#b23">[25]</ref>, and represents less temporal receptive fields for   <ref type="figure">Figure 5</ref>: The visualization of temporal action segmentation for our methods with color-coding (input example: make coffee). "MS-TCN" is the baseline model without any DA methods. We only highlight the action segments that are different from the ground truth for clear comparison.</p><p>videos. However, higher stages (e.g. S4) are not always better. We conjecture that it is because the model fits more to the source data, causing difficulty for DA. In our case, inte-gratingĜ ld into S2 provides the best overall performance.</p><p>We also integrate binary domain prediction with multiple stages. However, multi-stage DA does not always guarantee improved performance. For example, {S1, S2} has worse results than {S2} in terms of F1@{10, 25, 50}. Since {S2} and {S3} provide the best single-stage DA performance, we use {S2, S3}, which performs the best, as the final model for all our approaches in all the experiments. Design Choice for Global SSTDA: The most critical design decision for the sequential domain prediction is the segment number for each video. In our implementation, we divide one source video into m segments and do so for one target video, and then apply G gd to predict the permutation of domains for these 2m video segments. Therefore, the category number of G gd equals the number of all permutations (2m)!/(m!) 2 . In other words, the segment number m determine the complexity of the self-supervised auxiliary task. For example, m = 3 leads to a 20-way classifier, and m = 4 results in a 70-way classifier. Since a good self-supervised task should be neither naive nor over complicated <ref type="bibr" target="#b29">[31]</ref>, we choose m = 2 as our final decision, which is supported by our experiments as shown in <ref type="table" target="#tab_9">Table 6</ref>. Segmentation Visualization: It is also common to evaluate the qualitative performance to ensure that the prediction results are aligned with human vision. First, we compare our approaches with the baseline model MS-TCN <ref type="bibr" target="#b8">[9]</ref> and the ground truth, as shown in <ref type="figure">Figure 5</ref>. MS-TCN fails to detect some pour actions in the first half of the video, and falsely classify close as take in the latter part of the video. With local SSTDA, our approach can detect close in the latter part of the video. Finally, with full SSTDA, our proposed method also detects all pour action segments in the first half of video. We then compare SSTDA with other DA methods, and <ref type="figure" target="#fig_2">Figure 6</ref> shows that our result is the closest to the ground truth. The others either incorrectly detect some actions or make incorrect classification. For more qualitative results, please refer to the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Future Work</head><p>In this work, we propose a novel approach to effectively exploit unlabeled target videos to boost performance for action segmentation without target labels. To address the problem of spatio-temporal variations for videos across domains, we propose Self-Supervised Temporal Domain Adaptation (SSTDA) to jointly align cross-domain feature spaces embedded with local and global temporal dynamics by two self-supervised auxiliary tasks, binary and sequential domain prediction. Our experiments indicate that SSTDA outperforms other DA approaches by aligning temporal dynamics more effectively. We also validate the proposed SSTDA on three challenging datasets (GTEA, 50Salads, and Breakfast), and show that SSTDA outperforms the current state-of-the-art method by large margins and only requires 65% of the labeled training data to achieve the comparable performance, demonstrating the usefulness of adapting to unlabeled videos across variations. For the future work, we plan to apply SSTDA to more challenging video tasks (e.g. spatio-temporal action localization <ref type="bibr" target="#b13">[14]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Appendix</head><p>In the supplementary material, we would like to show more details about the technical approach, implementation, and experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Technical Approach Details</head><p>Domain Attentive Temporal Pooling (DATP): Temporal pooling is one of the most common methods to aggregate frame-level features into video-level features for each video. However, not all the frame-level features contribute the same to the overall domain discrepancy. Therefore, inspired by <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, we assign larger attention weights to the features which have larger domain discrepancy so that we can focus more on aligning those features, achieving more effective domain adaptation.</p><p>More specifically, we utilize the entropy criterion to generate the domain attention value for each frame-level feature f j as below:ŵ j = 1 − H(dj) <ref type="bibr" target="#b5">(6)</ref> whered j is the output from the learned domain classifier G ld used in local SSTDA. H(p) = − k p k · log(p k ) is the entropy function to measure uncertainty.ŵ j increases when H(d j ) decreases, which means the domains can be distinguished well. We also add a residual connection for more stable optimization. Finally, we aggregate the attended frame-level features with temporal pooling to generate the video-level feature v, which is noted as Domain Attentive Temporal Pooling (DATP), as illustrated in the left part of <ref type="figure" target="#fig_4">Figure 7</ref> and can be expressed as: (ŵj + 1) · fj <ref type="bibr" target="#b6">(7)</ref> where +1 refers to the residual connection, andŵ j + 1 is equal to w j in the main paper. T is the number of frames used to generate a video-level feature. Local SSTDA is necessary to calculate the attention weights for DATP. Without this mechanisms, frames will be aggregated in the same way as temporal pooling without cross-domain consideration, which is already demonstrated sub-optimal for cross-domain video tasks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. Domain Attentive Entropy (DAE): Minimum entropy regularization is a common strategy to perform more refined classifier adaptation. However, we only want to minimize class entropy for the frames that are similar across domains. Therefore, inspired by <ref type="bibr" target="#b40">[42]</ref>, we attend to the frames which have low domain discrepancy, corresponding to high domain entropy H(d j ). More specifically, we adopt the Domain Attentive Entropy (DAE) module to calculate the attentive entropy loss L ae , which can be expressed as follows: whered andŷ is the output ofĜ ld and G y , respectively. T is the total frame number of a video. We also apply the residual connection for stability, as shown in the right part of <ref type="figure" target="#fig_4">Figure 7</ref>. Full Architecture: Our method is built upon the stateof-the-art action segmentation model, MS-TCN <ref type="bibr" target="#b8">[9]</ref>, which takes input frame-level feature representations and generates the corresponding output frame-level class predictions by four stages of SS-TCN. In our implementation, we convert the second and third stages into Domain Adaptive TCN (DA-TCN) by integrating each SS-TCN with the following three parts: 1)Ĝ ld (for binary domain prediction), 2) DATP andĜ gd (for sequential domain prediction), and 3) DAE, bringing three corresponding loss functions, L ld , L gd and L ae , respectively, as illustrated in <ref type="figure" target="#fig_5">Figure 8</ref>. The final loss function can be formulated as below: <ref type="bibr" target="#b8">(9)</ref> where β l , β g and µ are the weights for L ld , L gd and L ae , respectively, obtained by the methods described in Section 6.2. s is the stage index in MS-TCN.</p><formula xml:id="formula_10">Lae = 1 T T j=1 (H(dj) + 1) · H(ŷj)<label>(8)</label></formula><formula xml:id="formula_11">L = 4 s=1 Ly (s) − 3 s=2 (β l L ld (s) + βgL gd (s) − µLae (s) )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Experiments</head><p>Datasets and Evaluation Metrics: The detailed statistics and the evaluation protocols of the three datasets are listed in <ref type="table" target="#tab_12">Table 7</ref>. We follow <ref type="bibr" target="#b20">[21]</ref> to use the following three metrics for evaluation:    long action classes have higher impact on this metric than shorter action classes, making it not able to reflect over-segmentation errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SS-TCN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SS-TCN SS-TCN</head><p>2. Segmental edit score (Edit): The edit score penalizes over-segmentation errors by measuring the ordering of predicted action segments independent of slight temporal shifts.</p><p>3. Segmental F1 score at the IoU threshold k% (F1@k):</p><p>F1@k also penalizes over-segmentation errors while ignoring minor temporal shifts between the predictions and ground truth. The scores are determined by the total number of actions but do not depend on the duration of each action instance, which is similar to mean average precision (mAP) with intersection-over-union (IoU) overlap criteria. F1@k becomes popular recently since it better reflects the qualitative results.</p><p>Implementation and Optimization: Our implementation is based on the PyTorch <ref type="bibr" target="#b31">[33,</ref><ref type="bibr" target="#b36">38]</ref> framework. We extract I3D <ref type="bibr" target="#b1">[2]</ref> features for the video frames and use these features as inputs to our model. The video frame rates are the same as <ref type="bibr" target="#b8">[9]</ref>. For GTEA and Breakfast datasets we use a video temporal resolution of 15 frames per second (fps), while for 50Salads we downsampled the features from 30 fps to 15 fps to be consistent with the other datasets. For fair comparison, we adopt the same architecture design choices of MS-TCN <ref type="bibr" target="#b8">[9]</ref> as our baseline model. The whole model consists of four stages where each stage contains ten dilated convolution layers. We set the number of filters to 64 in all the layers of the model and the filter size is 3. For optimization, we utilize the Adam optimizer and a batch size equal to 1, following the official implementation of MS-TCN <ref type="bibr" target="#b8">[9]</ref>. Since the target data size is smaller than the source data, each target data is loaded randomly multiple times in each  For the weighting of loss functions, we follow the common strategy as <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> to gradually increase β l and β g from 0 to 1. The weighting α for smoothness loss is 0.15 as in <ref type="bibr" target="#b8">[9]</ref> and µ is chosen as 1 × 10 −2 via the grid-search.</p><p>Less Training Labeled Data: To investigate the potential to train with a fewer number of labeled frames using SSTDA, we drop labeled frames from source domains with uniform sampling for training, and evaluate on the same length of validation data. Our experiment on the 50Salads dataset shows that by integrating with SSTDA, the performance does not drop significantly with the decrease in labeled training data, indicating the alleviation of reliance on labeled training data. Finally, only 65% of labeled training data are required to achieve comparable performance with MS-TCN, as shown in <ref type="table" target="#tab_14">Table 8</ref>. We then evaluate the proposed SSTDA on GTEA and Breakfast with the same percentage of labeled training data, and also get comparable or better performance. <ref type="table" target="#tab_14">Table 8</ref> also indicates the results without additional labeled training data, which contain discriminative information that can directly boost the performance for action segmentation. The additional trained data are all unlabeled, so they cannot be directly trained with standard prediction loss. Therefore, we propose SSTDA to exploit unlabeled data to: 1) further improve the strong baseline, MS-TCN, without additional training labels, and 2) achieve comparable performance with this strong baseline using only 65% of labels for training. Comparison with Other Approaches: We compare our proposed SSTDA with other approaches by integrating the same baseline architecture with other popular DA methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr">22</ref>] and a state-of-the-art video- the adversarial training procedure of Maximum Classifier Discrepancy to iteratively optimize the generator (G f in our case) and the classifier (G y ). The L1distance is used as the discrepancy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">SWD [22]:</head><p>The framework is similar to MCD, but we replace the L1-distance with the Wasserstein distance as the discrepancy loss.</p><p>7. VCOP <ref type="bibr" target="#b43">[45]</ref>: We divide f into three segments and compute the segment-level features with temporal pooling. After temporal shuffling the segment-level features, pairwise features are computed and concatenated into the final feature representing the video clip order. The final features are then fed into a shallow classifier to predict the order.</p><p>The experimental results on 50Salads and Breakfast both indicate that our proposed SSTDA outperforms all these methods, as shown in <ref type="table">Table 9</ref>.</p><p>The performance of the most recent video-based selfsupervised learning method <ref type="bibr" target="#b43">[45]</ref> on 50Salads and Breakfast also show that temporal shuffling within single domain without considering the relation across domains does not effectively benefit cross-domain action segmentation, resulting in even worse performance than other DA methods. Instead, our proposed self-supervised auxiliary tasks make predictions on cross-domain data, leading to cross-domain temporal relation reasoning instead of predicting withindomain temporal orders, achieving significant improvement in the performance of our main task, action segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Segmentation Visualization</head><p>Here we show more qualitative segmentation results from all three datasets to compare our methods with the baseline model, MS-TCN <ref type="bibr" target="#b8">[9]</ref>. All the results <ref type="figure">(Figure 9</ref> for GTEA, <ref type="figure">Figure 10</ref> for 50Salads, and <ref type="figure">Figure 11</ref> for Breakfast) demonstrate that the improvement over the baseline by only local SSTDA is sometimes limited. For example, local SSTDA falsely detects the pour action in <ref type="figure">Figure 9b</ref>, falsely classifies cheese-related actions as cucumber-related actions in <ref type="figure">Figure 10b</ref>, and falsely detects the stir milk action in <ref type="figure">Figure 11b</ref>. However, by jointly aligning local and global temporal dynamics with SSTDA, the model is effectively adapted to the target domain, reducing the above mentioned incorrect predictions and achieving better segmentation.  <ref type="figure">Figure 9</ref>: The visualization of temporal action segmentation for our methods with color-coding on GTEA. The video snapshots and the segmentation visualization are in the same temporal order (from left to right). We only highlight the action segments that are significantly different from the ground truth for clear comparison. "MS-TCN" represents the baseline model trained with only source data.  <ref type="figure">Figure 10</ref>: The visualization of temporal action segmentation for our methods with color-coding on 50Salads. The video snapshots and the segmentation visualization are in the same temporal order (from left to right). We only highlight the action segments that are different from the ground truth for clear comparison. Both examples correspond to the same activity Make salad, but they are performed by different subjects, i.e., people.  <ref type="figure">Figure 11</ref>: The visualization of temporal action segmentation for our methods with color-coding on Breakfast. The video snapshots and the segmentation visualization are in the same temporal order (from left to right). We only highlight the action segments that are different from the ground truth for clear comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground</head><p>Vision and Pattern Recognition (CVPR), 2019. 2, 6, 7, 11, 12</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>The visualization of temporal action segmentation for different DA methods (same input asFigure 5). "Source only" represents the baseline model, MS-TCN. Only the segments different from the ground truth are highlighted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>The details of DATP (left) and DAE (right). Both modules take the domain entropy H(d), which is calculated from the domain predictiond, to calculate the attention weights. With the residual connection, DATP attends to the frame-level features for aggregating into the final videolevel feature v (arrow thickness represents assigned attention values), and DAE attends to the class entropy H(ŷ) to obtain the attentive entropy loss L ae .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>The overall architecture of the proposed SSTDA. By equipping the network with a local adversarial domain classifierĜ ld , a global adversarial domain classifierĜ gd , a domain attentive temporal pooling (DATP) module, and a domain attentive entropy (DAE) module, we convert a SS-TCN into a DA-TCN, and stack multiple SS-TCNs and DA-TCNs to build the final architecture. L ld and L gd is the local and global domain loss, respectively. L y is the prediction loss and L ae is the attentive entropy loss. The domain entropy H(d) is used to calculate the attention weights for DATP and DAE. An adversarial domain classifierĜ refers to a domain classifier G equipped with a gradient reversal layer (GRL).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>the goal of G gd is to predict</figDesc><table><row><cell>Source</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Segment</cell><cell></cell><cell>DATP</cell><cell>Random Permutation</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>′</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Input frame-level features</cell><cell>GRL</cell><cell>Binary Domain Classifier</cell><cell>[0] [1]</cell><cell>ℒ</cell><cell>GRL</cell><cell>Sequential Domain Classifier</cell><cell>[0,0,1,1] [1,1,0,0] …</cell><cell>ℒ</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>[0,1,1,0]</cell><cell></cell></row><row><cell></cell><cell>Segment</cell><cell></cell><cell>DATP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Target</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>GTEA F1@{10, 25, 50} Edit Acc Source only (MS-TCN) † 86.5 83.6 71.9 81.3 76.5 Local SSTDA 89.6 87.9 74.4 84.5 80.1 SSTDA ‡ 90.0 89.1 78.0 86.2 79.8 50Salads F1@{10, 25, 50}</figDesc><table><row><cell></cell><cell></cell><cell>Edit Acc</cell></row><row><cell cols="3">Source only (MS-TCN) † 75.4 73.4 65.2 68.9 82.1</cell></row><row><cell>Local SSTDA</cell><cell cols="2">79.2 77.8 70.3 72.0 82.8</cell></row><row><cell>SSTDA ‡</cell><cell cols="2">83.0 81.5 73.8 75.8 83.2</cell></row><row><cell>Breakfast</cell><cell>F1@{10, 25, 50}</cell><cell>Edit Acc</cell></row><row><cell cols="3">Source only (MS-TCN) † 65.3 59.6 47.2 65.7 64.7</cell></row><row><cell>Local SSTDA</cell><cell cols="2">72.8 67.8 55.1 71.7 70.3</cell></row><row><cell>SSTDA ‡</cell><cell cols="2">75.0 69.1 55.2 73.7 70.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>The comparison of different methods that can learn information from unlabeled target videos (on GTEA). All the methods are integrated with the same baseline model MS-TCN for fair comparison. Please refer to the supplementary for the results on other datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell>F1@{10, 25, 50}</cell><cell>Edit Acc</cell></row><row><cell cols="3">Source only 86.5 83.6 71.9 81.3 76.5</cell></row><row><cell>{S1}</cell><cell cols="2">88.6 86.2 73.6 84.2 78.7</cell></row><row><cell>{S2}</cell><cell cols="2">89.1 87.2 74.4 84.3 79.1</cell></row><row><cell>{S3}</cell><cell cols="2">89.2 87.3 72.3 83.8 78.9</cell></row><row><cell>{S4}</cell><cell cols="2">88.1 86.4 73.0 83.0 78.8</cell></row><row><cell>{S1, S2}</cell><cell cols="2">89.0 85.8 73.5 84.8 79.5</cell></row><row><cell>{S2, S3}</cell><cell cols="2">89.6 87.9 74.4 84.5 80.1</cell></row><row><cell>{S3, S4}</cell><cell cols="2">88.3 86.8 73.9 83.6 78.6</cell></row></table><note>Comparison with the most recent action segmen- tation methods on all three datasets. SSTDA (65%) means training with 65% of total labeled training data. †Results from running the official code, as explained in Table 2.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>The experimental results of design choice for local SSTDA (on GTEA). {S n }: addĜ ld to the nth stage of MS-TCN, where smaller n implies closer to inputs.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>The experimental results for different segment numbers of sequential domain prediction (on GTEA).</figDesc><table><row><cell>Ground Truth</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MS-TCN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Local SSTDA</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SSTDA</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>background</cell><cell>take</cell><cell>open</cell><cell>scoop</cell><cell>pour</cell><cell>close</cell><cell>put</cell><cell>stir</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>The statistics of action segmentation datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>The comparison of SSTDA trained with less labeled training data. m in the first row indicates the percentage of labeled training data used to train a model.</figDesc><table /><note>epoch during training.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>Chen-Yu Lee, Tanmay Batra, Mohammad Haris Baig, and Daniel Ulbricht. Sliced wasserstein discrepancy for unsupervised domain adaptation. In IEEE conference on Computer</figDesc><table><row><cell>Ground Truth</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MS-TCN</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Local SSTDA</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SSTDA</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2, 6, 9 Ground Truth MS-TCN Local SSTDA SSTDA [22] background background</cell><cell>take bowl take cup</cell><cell>pour cereals (a) Make Cereal spoon powder (b) Make milk</cell><cell>pour milk pour milk</cell><cell>stir cereals stir milk</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="table">Table 9</ref><p>: The comparison of different methods that can learn information from unlabeled target videos (on 50Salads and Breakfast). All the methods are integrated with the same baseline model MS-TCN for fair comparison.</p><p>based self-supervised approach <ref type="bibr" target="#b43">[45]</ref>. For fair comparison, all the methods are integrated with the second and third stages, as our proposed SSTDA, where the single-stage integration methods are described as follows:</p><p>1. DANN <ref type="bibr" target="#b11">[12]</ref>: We add one discriminator, which is the same as G ld , equipped a gradient reversal layer (GRL) to the final frame-level features f .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">JAN [27]:</head><p>We integrate Joint Maximum Mean Discrepancy (JMMD) to the final frame-level features f and the class predictionŷ.</p><p>3. MADA <ref type="bibr" target="#b32">[34]</ref>: Instead of a single discriminator, we add multiple discriminators according to the class number to calculate the domain loss for each class. All the class-based domain losses are weighted with prediction probabilities and then summed up to obtain the final domain loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">MSTN [44]:</head><p>We utilize pseudo-labels to cluster the data from the source and target domains, and calculate the class centroids for the source and target domain separately. Then we compute the semantic loss by calculating mean squared error (MSE) between the source and target centroids. The final loss contains the prediction loss, the semantic loss, and the domain loss as DANN <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">MCD [36]:</head><p>We apply another classifier G y and follow</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Video jigsaw: Unsupervised learning of spatiotemporal context for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishi</forename><surname>Unaiza Ahsan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irfan</forename><surname>Madhok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Temporal attentive alignment for video domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Hung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghassan</forename><surname>Alregib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop on Learning from Unlabeled Videos</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Temporal attentive alignment for large-scale video domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Hung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghassan</forename><surname>Alregib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaekwon</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Action segmentation with mixed temporal domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Hung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baopu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingze</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghassan</forename><surname>Al-Regib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A comprehensive survey on domain adaptation for visual applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Domain Adaptation in Computer Vision Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Tricornet: A hybrid temporal convolutional and recurrent network for video action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07818</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Weakly-supervised action segmentation with iterative soft boundary assignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ms-tcn: Multi-stage temporal convolutional network for action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazan</forename><surname>Abu Farha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to recognize objects in egocentric activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2006" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A video dataset of spatio-temporally localized atomic visual actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhui</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeqing</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanna</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep domain adaptation in action space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arshad</forename><surname>Jamal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vinay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipti</forename><surname>Namboodiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Deodhare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Selfsupervised video representation learning with space-time cubic puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyeon</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Human action recognition and prediction: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.11230</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The language of actions: Recovering the syntax and semantics of goaldirected human activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Arslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Attending to discriminative certainty for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Kumar Kurmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanu</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vinay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Namboodiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised adversarial visual level domain adaptation for learning video object detectors from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avisek</forename><surname>Lahiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prabir</forename><surname>Sri Charan Ragireddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pabitra</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Temporal convolutional networks for action segmentation and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rene</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory D</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by sorting sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maneesh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Temporal deformable residual networks for action segmentation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with residual transfer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Ts-lstm and temporal-inception: Exploiting spatiotemporal dynamics for activity recognition. Signal Processing: Image Communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Hung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghassan</forename><surname>Alregib</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="76" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attend and interact: higher-order object interactions for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asim</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Melvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghassan</forename><surname>Alregib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><forename type="middle">Peter</forename><surname>Graf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning motion in feature space: Locally-consistent deformable convolution networks for fine-grained action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Khoi-Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhiraj</forename><surname>Mac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rogerio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><forename type="middle">N</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering (TKDE)</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<idno>2017. 10</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems Workshop</title>
		<imprint>
			<publisher>NeurIPSW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multi-adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyi</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Weakly supervised action learning with rnn based fine-to-coarse modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Maximum classifier discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kohei</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Combining embedded accelerometers with computer vision for recognizing food preparation activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mckenna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM international joint conference on Pervasive and ubiquitous computing (UbiComp)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Human action recognition across datasets by foreground-weighted histogram decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waqas</forename><surname>Sultani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imran</forename><surname>Saleemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Transferable attention for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ximei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weirui</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning and using the arrow of time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning semantic representations for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zibin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Self-supervised spatiotemporal learning via video clip order prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Dual many-to-one-encoder-based transfer learning for crossdataset human action recognition. Image and Vision Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiantian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="127" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Collaborative and adversarial network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning transferable selfattentive representations for action recognition in untrimmed videos with weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haichao</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
