<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Instance-aware Image and Sentence Matching with Selective Multimodal LSTM Matching score dog</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
							<email>yhuang@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Center for Research on Intelligent Perception and Computing National Laboratory of Pattern Recognition</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
							<email>wangwei@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Center for Research on Intelligent Perception and Computing National Laboratory of Pattern Recognition</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
							<email>wangliang@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Center for Research on Intelligent Perception and Computing National Laboratory of Pattern Recognition</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Center for Excellence in Brain Science and Intelligence Technology Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">LSTM LSTM LSTM LSTM LSTM LSTM</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Instance-aware Image and Sentence Matching with Selective Multimodal LSTM Matching score dog</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>An is sitting on the bench beside the road old and grass man with his bag and Multimodal context-modulated attention An old man with his bag and dog is sitting on the bench beside the road and grass.</p><p>Figure 1. The proposed selective multimodal Long Short-Term Memory network (sm-LSTM) (best viewed in colors).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Effective image and sentence matching depends on how to well measure their global visual-semantic similarity. Based on the observation that such a global similarity arises from a complex aggregation of multiple local similarities between pairwise instances of image (objects) and sentence (words), we propose a selective multimodal Long Short-Term Memory network (sm-LSTM) for instanceaware image and sentence matching. The sm-LSTM includes a multimodal context-modulated attention scheme at each timestep that can selectively attend to a pair of instances of image and sentence, by predicting pairwise instance-aware saliency maps for image and sentence. For selected pairwise instances, their representations are obtained based on the predicted saliency maps, and then compared to measure their local similarity. By similarly measuring multiple local similarities within a few timesteps, the sm-LSTM sequentially aggregates them with hidden states to obtain a final matching score as the desired global similarity. Extensive experiments show that our model can well match image and sentence with complex content, and achieve the state-of-the-art results on two public benchmark datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Matching image and sentence plays an important role in many applications, e.g., finding sentences given an image query for image annotation and caption, and retrieving images with a sentence query for image search. The key challenge of such a cross-modal matching task is how to accurately measure the image-sentence similarity. Recently, various methods have been proposed for this problem, which can be classified into two categories: 1) one-to-one matching and 2) many-to-many matching.</p><p>One-to-one matching methods usually extract global representations for image and sentence, and then associate them using either a structured objective <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b29">30]</ref> or a canonical correlation objective <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b16">17]</ref>. But they ignore the fact that the global similarity commonly arises from a complex aggregation of local similarities between imagesentence instances (objects in an image and words in a sentence). Accordingly, they fail to perform accurate instanceaware image and sentence matching.</p><p>Many-to-many matching methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">26]</ref> propose to compare many pairs of image-sentence instances, and aggregate their local similarities. However, it is not optimal to measure local similarities for all the possible pairs of instances without any selection, since only partial salient instance pairs describing the same semantic concept can actually be associated and contribute to the global similarity. Other redundant pairs are less useful which could act as noise that degenerates the final performance. In addition, it is not easy to obtain instances for either image or sentence, so these methods usually have to explicitly employ additional object detectors <ref type="bibr" target="#b5">[6]</ref> and dependency tree relations <ref type="bibr" target="#b10">[11]</ref>, or expensive human annotations.</p><p>To deal with these issues mentioned above, we propose a sequential model, named selective multimodal Long Short-Term Memory network (sm-LSTM), that can recurrently select salient pairs of image-sentence instances, and then measure and aggregate their local similarities within several timesteps. As shown in <ref type="figure">Figure 1</ref>, given a pair of image and sentence with complex content, the sm-LSTM first extracts their instance candidates, i.e., words of the sentence and regions of the image. Based on the extracted candidates, the model exploits a multimodal context-modulated attention scheme at each timestep to selectively attend to a pair of desired image and sentence instances (marked by circles and rectangles with the same color). In particular, the attention scheme first predicts pairwise instance-aware saliency maps for the image and sentence, and then combines saliencyweighted representations of candidates to represent the attended pairwise instances. Considering that each instance seldom occurs in isolation but co-varies with other ones as well as the particular context, the attention scheme uses multimodal global context as reference information to guide instance selection.</p><p>Then, the local similarity of the attended pairwise instances can be measured by comparing their obtained representations. During multiple timesteps, the sm-LSTM exploits hidden states to capture different local similarities of selected pairwise image-sentence instances, and sequentially accumulates them to predict the desired global similarity (i.e., the matching score) of image and sentence. Our model jointly performs pairwise instance selection, local similarity learning and aggregation in a single framework, which can be trained from scratch in an end-to-end manner with a structured objective. To demonstrate the effectiveness of the proposed sm-LSTM, we perform experiments of image annotation and retrieval on two publicly available datasets, and achieve the state-of-the-art results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">One-to-one Matching</head><p>Frome et al. <ref type="bibr" target="#b8">[9]</ref> propose a deep image-label embedding framework that uses Convolutional Neural Networks (CNN) <ref type="bibr" target="#b17">[18]</ref> and Skip-Gram <ref type="bibr" target="#b22">[23]</ref> to extract representations for image and label, respectively, and then associates them with a structured objective in which the matched imagesentence pairs have small distances. With a similar framework, Kiros et al. <ref type="bibr" target="#b14">[15]</ref> use Recurrent Neural Networks (RNN) <ref type="bibr" target="#b11">[12]</ref> for sentence representation learning, Vendrov et al. <ref type="bibr" target="#b29">[30]</ref> refine the objective to preserve the partial order structure of visual-semantic hierarchy, and Wang et al. <ref type="bibr" target="#b31">[32]</ref> combine cross-view and within-view constraints to learn structure-preserving embedding. Yan et al. <ref type="bibr" target="#b33">[34]</ref> associate representations of image and sentence using deep canonical correlation analysis where the matched image-sentence pairs have high correlation. Using a similar objective, Klein et al. <ref type="bibr" target="#b16">[17]</ref> propose to use Fisher Vectors (FV) <ref type="bibr" target="#b24">[25]</ref> to learn discriminative sentence representations, and Lev et al. <ref type="bibr" target="#b18">[19]</ref> exploit RNN to encode FV for further performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Many-to-many Matching</head><p>Karpathy et al. <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b12">13]</ref> make the first attempt to perform local similarity learning between fragments of images and sentences with a structured objective. Plummer et al. <ref type="bibr" target="#b25">[26]</ref> collect region-to-phrase correspondences for instance-level image and sentence matching. But they indistinctively use all pairwise instances for similarity measurement, which might not be optimal since there exist many matchingirrelevant instance pairs. In addition, obtaining image and sentence instances is not trial, since either additional object detectors or expensive human annotations need to be used. In contrast, our model can automatically select salient pairwise image-sentence instances, and sequentially aggregate their local similarities to obtain global similarity.</p><p>Other methods for image caption <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b3">4]</ref> can be extended to deal with image-sentence matching, by first generating the sentence given an image and then comparing the generated sentence and groundtruth one word-by-word in a many-to-many manner. But this kind of models are especially designed to predict a grammar-completed sentence close to the groundtruth sentence, rather than select salient pairwise sentence instances for similarity measurement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Deep Attention-based Models</head><p>Our proposed model is related to some attention-based models. Ba et al. <ref type="bibr" target="#b0">[1]</ref> present a recurrent attention model that can attend to some label-relevant image regions of an image for multiple objects recognition. Bahdanau et al. <ref type="bibr" target="#b1">[2]</ref> propose a neural machine translator which can search for relevant parts of a source sentence to predict a target word. Xu et al. <ref type="bibr" target="#b32">[33]</ref> develop an attention-based caption model which can automatically learn to fix gazes on salient objects in an image and generate the corresponding annotated words. Different from these models, our sm-LSTM focuses on joint multimodal instance selection and matching, which uses a multimodal context-modulated attention scheme to jointly predict instance-aware saliency maps for both image and sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Selective Multimodal LSTM</head><p>We will present the details of the proposed selective multimodal Long Short-Term Memory network (sm-LSTM) from the following three aspects: (a) instance candidate extraction for both image and sentence, (b) instanceaware saliency map prediction with a multimodal contextmodulated attention scheme, and (c) local similarity measurement and aggregation with a multimodal LSTM.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Instance Candidate Extraction</head><p>Sentence Instance Candidates. For a sentence, its underlying instances mostly exist in word-level or phraselevel, e.g., dog and man. So we simply tokenlize and split the sentence into words, and then obtain their representations by sequentially processing them with a bidirectional LSTM (BLSTM) <ref type="bibr" target="#b26">[27]</ref>, where two sequences of hidden states with different directions (forward and backward) are learnt. We concatenate the vectors of two directional hidden states at the same timestep as the representation for the corresponding input word.</p><p>Image Instance Candidates. For an image, directly obtaining its instances is very difficult, since the visual content is unorganized where the instances could appear in any location with various scales. To avoid the use of additional object detectors, we evenly divide the image into regions as instance candidates as shown in <ref type="figure" target="#fig_2">Figure 2</ref> (a), and represent them by extracting feature maps of the last convolutional layer in a CNN. We concatenate feature values at the same location across different feature maps as the feature vector for the corresponding convolved region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Instance-aware Saliency Map Prediction</head><p>Apparently, neither the split words nor evenly divided regions can precisely describe the desired sentence or image instances. It is attributed to the fact that: 1) not all instance candidates are necessary since both image and sentence consist of too much instance-irrelevant information, and 2) the desired instances usually exist as a combination of multiple candidates, e.g., the instance dog covers about 12 image regions. Therefore, we have to evaluate the instance-aware saliency of each instance candidates, with the aim to highlight those important and ignore those irrelevant.</p><p>To achieve this goal, we propose a multimodal contextmodulated attention scheme to predict pairwise instanceaware saliency maps for image and sentence. Different from <ref type="bibr" target="#b32">[33]</ref>, this attention scheme is designed for multimodal data rather than unimodal data, especially for the multimodal matching task. More importantly, we systematically study the importance of global context modulation in the attentional procedure. It results from an observation that each instance of image or sentence seldom occurs in isolation but co-varies with other ones as well as particular context. In particular, previous work <ref type="bibr" target="#b23">[24]</ref> has shown that the global image scene enables humans to quickly guide their attention to regions of interest. A recent study <ref type="bibr" target="#b9">[10]</ref> also demonstrates that the global sentence topic capturing long-range context can greatly facilitate inferring about the meaning of words.</p><p>As illustrated in <ref type="figure" target="#fig_2">Figure 2</ref> (b), we denote the previously obtained instance candidates of image and sentence as a i |a i ∈ R F i=1,··· ,I and w j |w j ∈ R G j=1,··· ,J , respectively. a i is the representation of the i-th divided region in the image and I is the total number of regions. w j describes the j-th split word in the sentence and J is the total number of words. F is the number of feature maps in the last convolutional layer of CNN while G is twice the dimension of hidden states in the BLSTM. We regard the output vector of the last fully-connected layer in the CNN as the global context m ∈ R D for the image, and the hidden state at the last timestep in a sentence-based LSTM as the global context n ∈ R E for the sentence. Based on these variables, we can perform instance-aware saliency map prediction at the t-th timestep as follows:</p><formula xml:id="formula_0">p t,i = ep t,i / I i=1 ep t,i ,p t,i = f p (m, a i , h t−1 ), q t,j = eq t,j / J j=1 eq t,j ,q t,j = f q (n, w j , h t−1 )<label>(1)</label></formula><p>where p t,i and q t,j are saliency values indicating probabilities that the i-th image instance candidate and the j-th sentence instance candidate will be attended to at the t-th timestep, respectively. f p (·) and f q (·) are two functions implementing the detailed context-modulation, where the input global context plays an essential role as reference information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Global Context as Reference Information</head><p>To illustrate the details of the context-modulated attention, we take an image for example in <ref type="figure" target="#fig_3">Figure 3</ref>, the case for sentence is similar. The global feature m provides a statistical summary of the image scene, including semantic instances and their relationship with each other. Such a summary can not only provide reference information about expected instances, e.g., man and dog, but also cause the perception of one instance to generate strong expectations about other instances <ref type="bibr" target="#b4">[5]</ref>. The local representations a i |a i ∈ R F i=1,··· ,I describe all the divided regions independently and are used to compute the initial saliency map. The hidden state at the previous timestep h t−1 indicates the already attended instances in the image, e.g., man.</p><p>To select which instance to attend to next, the attention scheme should first refer to the global context to find an instance, and then compare it with previous context to see if this instance has already been attended to. If yes (e.g., selecting the man), the scheme will refer to the global context again to find another instance. Otherwise (e.g., selecting the dog), regions in the initial saliency map corresponding to the instance will be highlighted. For efficient implementation, we simulate such a context-modulated attentional procedure using a simple three-way multilayer perceptrons (MLP) as follows:</p><formula xml:id="formula_1">f p (m, a i , h t−1 ) = w p (σ(mW m + b m ) + σ(a i W a + b a ) + σ(h t−1 W h + b h )) + b p</formula><p>(2) where σ denotes the sigmoid activation function. w p and b p are a weight vector and a scalar bias. Note that in this equation, the information in initial saliency map is additively modulated by global context and subtractively modulated by previous context, to finally produce the instance-aware saliency map.</p><p>The attention schemes in [33, 2, 1] consider only previous context without global context at each timestep, they have to alternatively use step-wise labels serving as expected instance information to guide the attentional procedure. But such strong supervision can only be available for limited tasks, e.g., the sequential words of sentence for image caption <ref type="bibr" target="#b32">[33]</ref>, and multiple class labels for multiobject recognition <ref type="bibr" target="#b0">[1]</ref>. For image and sentence matching, the words of sentence cannot be used as supervision information since we also have to select salient instances from the sentence to match image instances. In fact, we perform experiments without using global context in Section 4.7, but find that some instances like man and dog cannot be well attended to. It mainly results from the reason that without global context, the attention scheme can only refer to the initial saliency map to select which instance to attend to next, but the initial saliency map is computed from local representations that contain little instance information as well as relationship among instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Similarity Measurement and Aggregation</head><p>According to the predicted pairwise instance-aware saliency maps, we compute the weighted sum representations a t and w t to adaptively describe the attended image and sentence instances, respectively. We sum all the products of element-wise multiplication between each local representation (e.g., a i ) and its corresponding saliency value (e.g., p t,i ):</p><formula xml:id="formula_2">a t = I i=1 p t,i a i , w t = J j=1 q t,j w j<label>(3)</label></formula><p>where instance candidates with higher saliency values contribute more to the instance representations. Then, to measure the local similarity of the attended pairwise instances at the t-th timestep, we jointly feed their obtained representations a t and w t into a two-way MLP, and regard the output s t as the representation of the local similarity, as shown in <ref type="figure" target="#fig_2">Figure 2</ref> (c). From the 1-st to T -th timestep, we obtain a sequence of representations of local similarities {s t } t=1,··· ,T , where T is the total number of timesteps. To aggregate these local similarities for the global similarity, we use a LSTM network to sequentially take them as inputs, where the hidden states h t ∈ R H t=1,··· ,T dynamically propagate the captured local similarities until the end. The LSTM includes various gate mechanisms including memory state c t , hidden state h t , input gate i t , forget gate f t and output gate o t , which can well suit the complex nature of similarity aggregation:</p><formula xml:id="formula_3">i t = σ(W si s t + W hi h t−1 + b i ), f t = σ(W sf s t + W hf h t−1 + b f ), c t = f t c t−1 + i t tanh(W sc s t + W hc h t−1 + b c ), o t = σ(W so s t + W ho h t−1 + b o ), h t = o t tanh(c t )<label>(4)</label></formula><p>where denotes element-wise multiplication.</p><p>The hidden state at the last timestep h T can be regarded as the aggregated representation of all the local similarities. We use a MLP that takes h T as the input and produces the final matching score s as global similarity:</p><formula xml:id="formula_4">s = w hs (σ (W hh h t + b h )) + b s .<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Model Learning</head><p>The proposed sm-LSTM can be trained with a structured objective function that encourages the matching scores of matched images and sentences to be larger than those of mismatched ones:</p><formula xml:id="formula_5">ik max {0, m − s ii + s ik } + max {0, m − s ii + s ki } (6)</formula><p>where m is a tuning parameter, and s ii is the score of matched i-th image and i-th sentence. s ik is the score of mismatched i-th image and k-th sentence, and vice-versa with s ki . We empirically set the total number of mismatched pairs for each matched pair as 100 in our experiments. Since all modules of our model including the extraction of local representations and global contexts can constitute a whole deep network, our model can be trained in an end-to-end manner from raw image and sentence to matching score, without pre-/post-processing. For efficient optimization, we fix the weights of CNN and use pretrained weights as stated in Section 4.2.</p><p>In addition, we add a pairwise doubly stochastic regularization to the objective, by constraining the sum of saliency values of any instance candidates at all timesteps to be 1:</p><formula xml:id="formula_6">λ i (1 − t p t,i ) + j (1 − t q t,j )<label>(7)</label></formula><p>where λ is a balancing parameter. By adding this constraint, the loss will be large when our model attends to the same instance for multiple times. Therefore, it encourages the model to pay equal attention to every instance rather than a certain one for information maximization. In our experiments, we find that using this regularization can further improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>To demonstrate the effectiveness of the proposed sm-LSTM, we perform experiments in terms of image annotation and retrieval on two publicly available datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Protocols</head><p>The two evaluation datasets and their corresponding experimental protocols are described as follows. 1) Flickr30k <ref type="bibr" target="#b34">[35]</ref> consists of 31783 images collected from the Flickr website. Each image is accompanied with 5 human annotated sentences. We use the public training, validation and testing splits <ref type="bibr" target="#b14">[15]</ref>, which contain 28000, 1000 and 1000 images, respectively. 2) Microsoft COCO <ref type="bibr" target="#b19">[20]</ref> consists of 82783 training and 40504 validation images, each of which is associated with 5 sentences. We use the public training, validation and testing splits <ref type="bibr" target="#b14">[15]</ref>, with 82783, 4000 and 1000 images, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>The commonly used evaluation criterions for image annotation and retrieval are "R@1", "R@5" and "R@10", i.e., recall rates at the top 1, 5 and 10 results. Another one is "Med r" which is the median rank of the first ground truth result. We compute an additional criterion "Sum" to evaluate the overall performance for both image annotation and retrieval as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sum = R@1 + R@5 + R@10</head><p>Image annotation + R@1 + R@5 + R@10</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image retrieval</head><p>To systematically validate the effectiveness, we experiment with five variants of sm-LSTMs: (1) sm-LSTM-mean does not use the attention scheme to obtain weighted sum representations for selected instances but instead directly uses mean vectors, (2) sm-LSTM-att only uses the attention scheme but does not exploit global context, (3) sm-LSTM-ctx does not use the attention scheme but only exploits global context, (4) sm-LSTM is our full model that uses both the attention scheme and global context, and (5) sm-LSTM * is an ensemble of the described four models above, by summing their cross-modal similarity matrices together in a similar way as <ref type="bibr" target="#b20">[21]</ref>.</p><p>We use the 19-layer VGG network <ref type="bibr" target="#b27">[28]</ref> to initialize our CNN to extract 512 feature maps (with a size of 14×14) in "conv5-4" layer as representations for image instance candidates, and a feature vector in "fc7" layer as the image global context. We use MNLP <ref type="bibr" target="#b14">[15]</ref> to initialize our sentence-based LSTM and regard the hidden state at the last timestep as the sentence global context, while our BLSTM for representing sentence candidates are directly learned from raw sentences with a dimension of hidden state as 512. For image, the dimensions of local and global context features are F =512 and D=4096, respectively, and the total number of local regions is I=196 (14×14). For sentence, the dimensions of local and global context features are G=1024 and E=1024, respectively. We set the max length for all the sentences as 50, i.e., the number of split words J=50, and use zero-padding when a sentence is not </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with State-of-the-art Methods</head><p>We compare sm-LSTMs with several recent state-of-theart methods on the Flickr30k and Microsoft COCO datasets in Tables 1 and 2, respectively. We can find that sm-LSTM * can achieve much better performance than all the compared methods on both datasets. Our best single model sm-LSTM outperforms the state-of-the-art DSPE+FV † in image annotation, but performs slightly worse than it in image retrieval.</p><p>Different from DSPE+FV † that uses external text corpora to learn discriminative sentence features, our model learns them directly from scratch in an end-to-end manner. Beside DSPE+FV † , the sm-LSTM performs better than other compared methods by a large margin. These observations demonstrate that dynamically selecting image-sentence instances and aggregating their similarities is very effective for cross-modal retrieval.</p><p>When comparing among all the sm-LSTMs, we can conclude as follows. 1) Our attention scheme is effective, since sm-LSTM-att consistently outperforms sm-LSTM-mean on A airplane that is flying in the sky.   both datasets. When exploiting only context information without the attention scheme, sm-LSTM-ctx achieves much worse results than sm-LSTM. 2) Using global context to modulate the attentional procedure is very useful, since sm-LSTM greatly outperforms sm-LSTM-att with respect to all evaluation criterions.</p><p>3) The ensemble of four sm-LSTM variants as sm-LSTM * can further improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Analysis on Number of Timesteps</head><p>For a pair of image and sentence, we need to manually set the number of timesteps T in sm-LSTM. Ideally, T should be equal to the number of salient pairwise instances appearing in the image and sentence. Therefore, the sm-LSTM can separately attend to these pairwise instances within T steps to measure all the local similarities. To investigate what is the optimal number of timesteps, in the following, we gradually increase T from 1 to 5, and analyze the impact of different numbers of timesteps on the performance of sm-LSTM in <ref type="table" target="#tab_2">Table 3</ref>.</p><p>From the table we can observe that sm-LSTM can achieve its best performance when the number of timesteps is 3. It indicates that it can capture all the local similarity information by iteratively visiting both image and sentence for 3 times. Intuitively, most pairs of images and sentences usually contain approximately 3 associated instances. Note that when T becomes larger than 3, the performance slightly drops. It results from the fact that an overly complex network tends to overfit training data by paying attention to redundant instances at extra timesteps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Evaluation of Regularization Term</head><p>In our experiments, we find that the proposed sm-LSTM is inclined to focus on the same instance at all timesteps, which might result from the fact that always selecting most informative instances can largely avoid errors. But it is not good for our model to comprehensively perceive the entire content in the image and sentence. So we add the pairwise doubly stochastic regularization term (in Equation 7) to the structured objective, with the aim to force the model to pay equal attention to all the potential instances at different lo- cations. We vary the values of balancing parameter λ from 0 to 1000, and compare the corresponding performance in <ref type="table" target="#tab_3">Table 4</ref>. From the table, we can find that the performance improves when λ&gt;0, which demonstrates the usefulness of paying attention to more instances. In addition, when λ=100, the ms-LSTM can achieve the largest performance improvement, especially for the task of image annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Visualization of Instance-aware Saliency Maps</head><p>To verify whether the proposed model can selectively attend to salient pairwise instances of image and sentence at different timesteps, we visualize the predicted sequential instance-aware saliency maps by sm-LSTM, as shown in <ref type="figure" target="#fig_4">Figure 4</ref>. In particular for image, we resize the predicted saliency values at the t-th timestep {p t,i } to the same size as its corresponding original image, so that each value in the resized map measures the importance of an image pixel at the same location. We then perform element-wise multiplication between the resized saliency map and the original image to obtain the final saliency map, where lighter areas indicate attended instances. While for sentence, since different sentences have various lengths, we simply present two selected words at each timestep corresponding to the top-2 highest saliency values {q t,j }.</p><p>We can see that sm-LSTM can attend to different regions and words at different timesteps in the images and sentences, respectively. Most attended pairs of regions and words describe similar semantic concepts. Taking the last pair of image and sentence for example, sm-LSTM sequentially focuses on words: "giraffe", "children" and "park" at three different timesteps, as well as the corresponding image regions referring to similar meanings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Usefulness of Global Context</head><p>To qualitatively validate the effectiveness of using global context, we compare the resulting instance-aware saliency maps of images generated by sm-LSTM-att and sm-LSTM in <ref type="figure" target="#fig_5">Figure 5</ref>. Without the aid of global context, sm-LSTMatt cannot produce accurate dynamical saliency maps as those of sm-LSTM. In particular, it cannot well attend to semantically meaningful instances such as "dog", "cow" and "beach" in the first, second and third images, respectively. In addition, sm-LSTM-att always finishes attending to salient instances within the first two steps, and does not focus on meaningful instances at the third timestep any more. Different from it, sm-LSTM focuses on more salient instances at all three timesteps. These evidences show that global context modulation can be helpful for more accurate instance selection.</p><p>In <ref type="figure" target="#fig_6">Figure 6</ref>, we also compute the averaged saliency maps (rescaled to the same size of 500×500) for all the test images at three different timesteps by sm-LSTM. We can see that the proposed sm-LSTM statistically tends to focus on the central regions at the first timestep, which is in consistent with the observation of "center-bias" in human visual attention studies <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b2">3]</ref>. It is mainly attributed to the fact that salient instances mostly appear in the cental regions of images. Note that the model also attends to surrounding and lower regions at the other two timesteps, with the goal to find various instances at different locations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we have proposed the selective multimodal Long Short-Term Memory network (sm-LSTM) for instance-aware matching image and sentence. Our main contribution is proposing a multimodal context-modulated attention scheme to select salient pairwise instances from image and sentence, and a multimodal LSTM network for local similarity measurement and aggregation. We have systematically studied the global context modulation in the attentional procedure, and demonstrated its effectiveness with significant performance improvement. We have applied our model to the tasks of image annotation and retrieval, and achieved the state-of-the-art results. In the future, we will explore more advanced implementations of the context modulation (in Equation 2), and further verify our model on more datasets. We will also consider to jointly finetune the pretrained CNN for better performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Details of the proposed sm-LSTM, including (a) instance candidate extraction, (b) instance-aware saliency map prediction, and (c) similarity measurement and aggregation (best viewed in colors).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of context-modulated attention (the lighter areas indicate the attended instances, best viewed in colors).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Visualization of attended image and sentence instances at three different timesteps (best viewed in colors).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>(a) Input image (b) Without global context (by sm-LSTM-att) (c) With global context (by sm-LSTM) Attended image instances at three different timesteps, without or with global context, respectively (best viewed in colors).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>(a) 1-st timestep (b) 2-nd timestep (c) 3-rd timestep Averaged saliency maps at three different timesteps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Comparison results of image annotation and retrieval on the Flickr30K dataset. ( * indicates the ensemble or multi-model methods, and † indicates using external text corpora or manual annotations.) Comparison results of image annotation and retrieval on the Microsoft COCO dataset. ( * indicates the ensemble or multi-model methods, and † indicates using external text corpora or manual annotations.)</figDesc><table><row><cell>Method</cell><cell cols="8">Image Annotation R@1 R@5 R@10 Med r R@1 R@5 R@10 Med r Image Retrieval</cell><cell>Sum</cell></row><row><cell>RVP (T+I) [4]</cell><cell>12.1</cell><cell>27.8</cell><cell>47.8</cell><cell>11</cell><cell>12.7</cell><cell>33.1</cell><cell>44.9</cell><cell>12.5</cell><cell>178.4</cell></row><row><cell>Deep Fragment [13]</cell><cell>14.2</cell><cell>37.7</cell><cell>51.3</cell><cell>10</cell><cell>10.2</cell><cell>30.8</cell><cell>44.2</cell><cell>14</cell><cell>188.4</cell></row><row><cell>DCCA [34]</cell><cell>16.7</cell><cell>39.3</cell><cell>52.9</cell><cell>8</cell><cell>12.6</cell><cell>31.0</cell><cell>43.0</cell><cell>15</cell><cell>195.5</cell></row><row><cell>NIC [31]</cell><cell>17.0</cell><cell>-</cell><cell>56.0</cell><cell>7</cell><cell>17.0</cell><cell>-</cell><cell>57.0</cell><cell>7</cell><cell>-</cell></row><row><cell cols="2">DVSA (BRNN) [14] 22.2</cell><cell>48.2</cell><cell>61.4</cell><cell>4.8</cell><cell>15.2</cell><cell>37.7</cell><cell>50.5</cell><cell>9.2</cell><cell>235.2</cell></row><row><cell>MNLM [15]</cell><cell>23.0</cell><cell>50.7</cell><cell>62.9</cell><cell>5</cell><cell>16.8</cell><cell>42.0</cell><cell>56.5</cell><cell>8</cell><cell>251.9</cell></row><row><cell>LRCN [7]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>17.5</cell><cell>40.3</cell><cell>50.8</cell><cell>9</cell><cell>-</cell></row><row><cell>m-RNN [22]</cell><cell>35.4</cell><cell>63.8</cell><cell>73.7</cell><cell>3</cell><cell>22.8</cell><cell>50.7</cell><cell>63.1</cell><cell>5</cell><cell>309.5</cell></row><row><cell>FV  † *  [17]</cell><cell>35.0</cell><cell>62.0</cell><cell>73.8</cell><cell>3</cell><cell>25.0</cell><cell>52.7</cell><cell>66.0</cell><cell>5</cell><cell>314.5</cell></row><row><cell>m-CNN  *  [21]</cell><cell>33.6</cell><cell>64.1</cell><cell>74.9</cell><cell>3</cell><cell>26.2</cell><cell>56.3</cell><cell>69.6</cell><cell>4</cell><cell>324.7</cell></row><row><cell>RTP+FV  † *  [26]</cell><cell>37.4</cell><cell>63.1</cell><cell>74.3</cell><cell>-</cell><cell>26.0</cell><cell>56.0</cell><cell>69.3</cell><cell>-</cell><cell>326.1</cell></row><row><cell>RNN+FV  † [19]</cell><cell>34.7</cell><cell>62.7</cell><cell>72.6</cell><cell>3</cell><cell>26.2</cell><cell>55.1</cell><cell>69.2</cell><cell>4</cell><cell>320.5</cell></row><row><cell>DSPE+FV  † [32]</cell><cell>40.3</cell><cell>68.9</cell><cell>79.9</cell><cell>-</cell><cell>29.7</cell><cell>60.1</cell><cell>72.1</cell><cell>-</cell><cell>351.0</cell></row><row><cell>Ours:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>sm-LSTM-mean</cell><cell>25.9</cell><cell>53.1</cell><cell>65.4</cell><cell>5</cell><cell>18.1</cell><cell>43.3</cell><cell>55.7</cell><cell>8</cell><cell>261.5</cell></row><row><cell>sm-LSTM-att</cell><cell>27.0</cell><cell>53.6</cell><cell>65.6</cell><cell>5</cell><cell>20.4</cell><cell>46.4</cell><cell>58.1</cell><cell>7</cell><cell>271.1</cell></row><row><cell>sm-LSTM-ctx</cell><cell>33.5</cell><cell>60.6</cell><cell>70.8</cell><cell>3</cell><cell>23.6</cell><cell>50.4</cell><cell>61.3</cell><cell>5</cell><cell>300.1</cell></row><row><cell>sm-LSTM</cell><cell>42.4</cell><cell>67.5</cell><cell>79.9</cell><cell>2</cell><cell>28.2</cell><cell>57.0</cell><cell>68.4</cell><cell>4</cell><cell>343.4</cell></row><row><cell>sm-LSTM  *</cell><cell>42.5</cell><cell>71.9</cell><cell>81.5</cell><cell>2</cell><cell>30.2</cell><cell>60.4</cell><cell>72.3</cell><cell>3</cell><cell>358.7</cell></row><row><cell>Method</cell><cell cols="8">Image Annotation R@1 R@5 R@10 Med r R@1 R@5 R@10 Med r Image Retrieval</cell><cell>Sum</cell></row><row><cell>STD  † *  [16]</cell><cell>33.8</cell><cell>67.7</cell><cell>82.1</cell><cell>3</cell><cell>25.9</cell><cell>60.0</cell><cell>74.6</cell><cell>4</cell><cell>344.1</cell></row><row><cell>m-RNN [22]</cell><cell>41.0</cell><cell>73.0</cell><cell>83.5</cell><cell>2</cell><cell>29.0</cell><cell>42.2</cell><cell>77.0</cell><cell>3</cell><cell>345.7</cell></row><row><cell>FV  † *  [17]</cell><cell>39.4</cell><cell>67.9</cell><cell>80.9</cell><cell>2</cell><cell>25.1</cell><cell>59.8</cell><cell>76.6</cell><cell>4</cell><cell>349.7</cell></row><row><cell>DVSA [14]</cell><cell>38.4</cell><cell>69.9</cell><cell>80.5</cell><cell>1</cell><cell>27.4</cell><cell>60.2</cell><cell>74.8</cell><cell>3</cell><cell>351.2</cell></row><row><cell>MNLM [15]</cell><cell>43.4</cell><cell>75.7</cell><cell>85.8</cell><cell>2</cell><cell>31.0</cell><cell>66.7</cell><cell>79.9</cell><cell>3</cell><cell>382.5</cell></row><row><cell>m-CNN  *  [21]</cell><cell>42.8</cell><cell>73.1</cell><cell>84.1</cell><cell>2</cell><cell>32.6</cell><cell>68.6</cell><cell>82.8</cell><cell>3</cell><cell>384.0</cell></row><row><cell>RNN+FV  † [19]</cell><cell>40.8</cell><cell>71.9</cell><cell>83.2</cell><cell>2</cell><cell>29.6</cell><cell>64.8</cell><cell>80.5</cell><cell>3</cell><cell>370.8</cell></row><row><cell>OEM [30]</cell><cell>46.7</cell><cell>-</cell><cell>88.9</cell><cell>2</cell><cell>37.9</cell><cell>-</cell><cell>85.9</cell><cell>2</cell><cell>-</cell></row><row><cell>DSPE+FV  † [32]</cell><cell>50.1</cell><cell>79.7</cell><cell>89.2</cell><cell>-</cell><cell>39.6</cell><cell>75.2</cell><cell>86.9</cell><cell>-</cell><cell>420.7</cell></row><row><cell>Ours:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">sm-LSTM-mean 33.1</cell><cell>65.3</cell><cell>78.3</cell><cell>3</cell><cell>25.1</cell><cell>57.9</cell><cell>72.2</cell><cell>4</cell><cell>331.9</cell></row><row><cell>sm-LSTM-att</cell><cell>36.7</cell><cell>69.7</cell><cell>80.8</cell><cell>2</cell><cell>29.1</cell><cell>64.8</cell><cell>78.4</cell><cell>3</cell><cell>359.5</cell></row><row><cell>sm-LSTM-ctx</cell><cell>39.7</cell><cell>70.2</cell><cell>84.0</cell><cell>2</cell><cell>32.7</cell><cell>68.1</cell><cell>81.3</cell><cell>3</cell><cell>376.0</cell></row><row><cell>sm-LSTM</cell><cell>52.4</cell><cell>81.7</cell><cell>90.8</cell><cell>1</cell><cell>38.6</cell><cell>73.4</cell><cell>84.6</cell><cell>2</cell><cell>421.5</cell></row><row><cell>sm-LSTM  *</cell><cell>53.2</cell><cell>83.1</cell><cell>91.5</cell><cell>1</cell><cell>40.7</cell><cell>75.8</cell><cell>87.4</cell><cell>2</cell><cell>431.8</cell></row><row><cell cols="4">long enough. Other parameters are empirically set as fol-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">lows: H=1024, λ=100, T =3 and m=0.2.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>The impact of different numbers of timesteps on the Flick30k dataset. T : the number of timesteps in the sm-LSTM.</figDesc><table><row><cell cols="3">Image Annotation</cell><cell cols="3">Image Retrieval</cell></row><row><cell cols="6">R@1 R@5 R@10 R@1 R@5 R@10</cell></row><row><cell>T = 1 38.8</cell><cell>65.7</cell><cell>76.8</cell><cell>28.0</cell><cell>56.6</cell><cell>68.2</cell></row><row><cell>T = 2 38.0</cell><cell>68.9</cell><cell>77.9</cell><cell>28.1</cell><cell>56.5</cell><cell>68.1</cell></row><row><cell>T = 3 42.4</cell><cell>67.5</cell><cell>79.9</cell><cell>28.2</cell><cell>57.0</cell><cell>68.4</cell></row><row><cell>T = 4 38.2</cell><cell>67.6</cell><cell>78.5</cell><cell>27.5</cell><cell>56.6</cell><cell>68.0</cell></row><row><cell>T = 5 38.1</cell><cell>68.2</cell><cell>78.4</cell><cell>28.1</cell><cell>56.0</cell><cell>67.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>The impact of different values of the balancing parameter on the Flick30k dataset. λ: the balancing parameter between structured objective and regularization term.</figDesc><table><row><cell></cell><cell cols="2">Image Annotation</cell><cell cols="2">Image Retrieval</cell></row><row><cell></cell><cell cols="4">R@1 R@5 R@10 R@1 R@5 R@10</cell></row><row><cell>λ = 0</cell><cell>37.9 65.8</cell><cell>77.7</cell><cell>27.2 55.4</cell><cell>67.6</cell></row><row><cell>λ = 1</cell><cell>38.0 66.2</cell><cell>77.8</cell><cell>27.4 55.6</cell><cell>67.7</cell></row><row><cell>λ = 10</cell><cell>38.4 67.4</cell><cell>77.7</cell><cell>27.5 56.1</cell><cell>67.6</cell></row><row><cell>λ = 100</cell><cell>42.4 67.5</cell><cell>79.9</cell><cell>28.2 57.0</cell><cell>68.4</cell></row><row><cell cols="2">λ = 1000 40.2 67.1</cell><cell>78.6</cell><cell>27.8 56.9</cell><cell>67.9</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multiple object recognition with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scene and screen center bias early eye movements in scene viewing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bindemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mind&apos;s eye: A recurrent visual representation for image caption generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Top-down attentional guidance based on implicit learning of visual covariation. Psychological Science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generating typed dependency parses from phrase structure parses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-C</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">From captions to visual concepts and back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Strope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Heck</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.06291</idno>
		<title level="m">Contextual lstm (clstm) models for large scale nlp tasks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep fragment embeddings for bidirectional image sentence mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">TACL</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Associating neural word embeddings with deep image representations using fisher vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rnn fisher vectors for action recognition and image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multimodal convolutional neural networks for matching image and sentence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Explain images with multimodal recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The role of context in object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fisher kernels on visual vocabularies for image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Flickr30k entities: Collecting region-to-phrase correspondences for richer image-tosentence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Quantifying center bias of observers in free viewing of dynamic natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-H</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Carmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">G</forename><surname>Cameron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Orderembeddings of images and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vendrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning deep structurepreserving image-text embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep correlation for matching images and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>TACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
