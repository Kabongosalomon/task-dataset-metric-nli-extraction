<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Progressive Multi-Scale Residual Network for Single Image Super-Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Xinfeng</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshe</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Siwei</forename><surname>Ma</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Wen</forename><surname>Gao</surname></persName>
						</author>
						<title level="a" type="main">Progressive Multi-Scale Residual Network for Single Image Super-Resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>MANUSCRIPT 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Image super-resolution</term>
					<term>multi-scale network</term>
					<term>attention mechanism</term>
					<term>progressive design</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-scale convolutional neural networks (CNNs) achieve significant success in single image super-resolution (SISR), which considers the comprehensive information from different receptive fields. However, recent multi-scale networks usually aim to build the hierarchical exploration with different sizes of filters, which lead to high computation complexity costs, and seldom focus on the inherent correlations among different scales. This paper converts the multi-scale exploration into a sequential manner, and proposes a progressive multiscale residual network (PMRN) for SISR problem. Specifically, we devise a progressive multi-scale residual block (PMRB) to substitute the larger filters with small filter combinations, and gradually explore the hierarchical information. Furthermore, channel-and pixel-wise attention mechanism (CPA) is designed for finding the inherent correlations among image features with weighting and bias factors, which concentrates more on high-frequency information. Experimental results show that the proposed PMRN recovers structural textures more effectively with superior PSNR/SSIM results than other small networks. The extension model PMRN + with self-ensemble achieves competitive or better results than large networks with much fewer parameters and lower computation complexity.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>of the effective network patterns for image restoration, which considers the comprehensive information from different scales. Inspired by Laplacian pyramid, Lai et al. proposed Lap-SRN <ref type="bibr" target="#b9">[10]</ref> for graduate image super-resolution with different scaling factors. MSRN <ref type="bibr" target="#b10">[11]</ref> devised by Li et al. extracts the multi-scale features with different sizes of filters. However, the larger filters lead to more parameters and high computation costs. Existing works seldom concentrate on the inherent correlations among features from different scales. In practice, small scale information contains richer texture details, and may be helpful for larger scale structural information exploration.</p><p>Multi-scale exploration comprehensively considers the hierarchical information, which treat different feature maps equally. Attention mechanism aims to focus more on important information and textures. SENet <ref type="bibr" target="#b11">[12]</ref> proposed by Hu et al. introduced a channel-wise attention with global average pooling (GAP), which is widely considered in recent SISR works <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. Non-local attention <ref type="bibr" target="#b15">[16]</ref> proposed by Wang et al. and its derivatives also address amazing performance in image restoration <ref type="bibr" target="#b14">[15]</ref>. However, matrix multiplication in nonlocal attentions makes them hard for flexible applications. GAP-based methods only considers the channel-wise correlations without spatial attentions. Furthermore, existing attention methods mainly focus on the weighting factors for importance, but almost neglect the bias of different feature values. This paper proposes a progressive multi-scale residual network for SISR problem, which is termed as PMRN. Motivated by the calculation of convolution, we substitute the larger filters with combinations of several layers with small kernel size for efficiency, and devise a progressive multi-scale block (PMRB) for hierarchical feature exploration. PMRB processes the multi-scale features in a sequential manner, which aims to make full use of the relations among information from different scales. Furthermore, we design a channel-and pixelwise attention mechanism (CPA) to focus more on highfrequency and important information, which considers the weighting and bias factors jointly for different channels and pixels from the feature maps. With the elaborate designs, PMRN achieves competitive or better restoration performance than other works with much fewer parameters and lower computation complexity. <ref type="figure">Fig. 1</ref> demonstrates an example of qualitative comparison among different works. From the results, PMRN can restore more accurate textures than others with higher PSNR/SSIM values.</p><p>Our contributions can be concluded as follows:</p><p>• Motivated by the calculation of convolution, we design a progressive multi-scale residual block (PMRB) to se- quentially explore the hierarchical information with small filter combinations, which considers the relations among multi-scale features. <ref type="bibr">•</ref> We devise a channel-and pixel-wise attention (CPA) mechanism to focus more on high-frequency information, which considers the spatial features with weighting and bias factors. • We build the progressive multi-scale network (PMRN) for SISR with elaborate components. Experimental results show PMRN achieves competitive or better restoration performances with much fewer parameters and lower computation complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS A. Deep Learning for SISR</head><p>SISR has proved to be a challenging issue in image restoration area. Besides the complex degradation situations, different image acquisition circumstances also influence the restoration quality, such as night vision <ref type="bibr" target="#b17">[18]</ref>, inpainting <ref type="bibr" target="#b18">[19]</ref>, and moving blur <ref type="bibr" target="#b19">[20]</ref>. From this point of view, SISR is highly ill-posed with a large information loss.</p><p>CNN has shown its amazing performance on SISR with superior restoration capacity. SRCNN <ref type="bibr" target="#b20">[21]</ref> is the first CNNbased method for image SR with a three-layer network, which denotes a sparse coding like structure. After SRCNN, FSR-CNN <ref type="bibr" target="#b21">[22]</ref>, VDSR <ref type="bibr" target="#b6">[7]</ref>, and DRCN <ref type="bibr" target="#b22">[23]</ref> increased the network depth for better restoration performance. Besides building a deeper network, there are well-designed architectures with good restoration performances. CNF <ref type="bibr" target="#b23">[24]</ref>   <ref type="bibr" target="#b29">[30]</ref>. CARN <ref type="bibr" target="#b8">[9]</ref> proposed by Ahn et al. considered a cascading block for restoration. Wavelet <ref type="bibr" target="#b30">[31]</ref>, UNet <ref type="bibr" target="#b31">[32]</ref>, and optimization methods <ref type="bibr" target="#b32">[33]</ref> are also considered for better network architecture designs. In general, a deeper or wider network with higher computation complexity and more parameters can recover the textures more effectively.</p><p>Among these CNN-based networks, multi-scale architecture has proved to be an effective design for image restoration. LapSRN <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b24">[25]</ref> performs the image SR from different scales jointly. Li et al. investigated a multi-scale block in MSRN <ref type="bibr" target="#b10">[11]</ref> for feature exploration. Furthermore, MDCN <ref type="bibr" target="#b33">[34]</ref> proposed by Li et al. jointly considered residual learning, dense connection, and multi-scale features for SISR. MGHC-Net <ref type="bibr" target="#b34">[35]</ref>, proposed by Esmaeilzehi et al. also addressed amazing restoration capacity with a multi-scale granular and holistic channel feature generation network. These works concentrate on effective feature exploration, which almost neglect the diversity of spatial information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Attention Mechanism</head><p>High-frequency information and details act as a critical role for image restoration <ref type="bibr" target="#b35">[36]</ref>. Attention mechanism has proved to be a success component for computer vision issues, which concentrates more on the important information from features. SENet <ref type="bibr" target="#b11">[12]</ref> proposed by Hu et al. is one of the most famous attentions with GAP. IMDN <ref type="bibr" target="#b13">[14]</ref>, RCAN <ref type="bibr" target="#b12">[13]</ref>, SAN <ref type="bibr" target="#b14">[15]</ref> and other recent works utilize SENet or its derivations and achieve state-of-the-art performances. Non-local attention <ref type="bibr" target="#b15">[16]</ref> is another impressive design for global correlation consideration, which has been applied in recent SR works, such as SAN <ref type="bibr" target="#b14">[15]</ref>, CS-NL <ref type="bibr" target="#b36">[37]</ref>, and PFNL <ref type="bibr" target="#b37">[38]</ref>. However, nonlocal attention requires matrix multiplication to consider the spatial correlations, which require large memory cost and more parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Network Structure</head><p>As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, there are three modules in PMRN: feature extraction, non-linear feature exploration and restoration. Let's denote I LR , I HR as the input LR instances and restored HR outputs separately. Features from LR images will be extracted as,</p><formula xml:id="formula_0">H 0 = f F EM (I LR ),<label>(1)</label></formula><p>where f F EM (·) denotes the feature extraction module, and H 0 denotes the features. After feature extraction, non-linear feature exploration builds the mapping from LR to HR space, which is composed of several PMRBs and a padding structure. Suppose there are K PMRBs, for the k-th block, there is,</p><formula xml:id="formula_1">H k = f P M RB k (H k−1 ), k = 0, 1, ..., K,<label>(2)</label></formula><p>where f P M RB (·) denotes the PMRB, and H k denotes the output feature. After PMRBs, feature will pass the padding structure with residual learning, as,</p><formula xml:id="formula_2">H out = f P AD (H K ) + H 0 ,<label>(3)</label></formula><p>where f P AD (·) denotes the padding structure. Finally, HR images will be restored from features, which can be demonstrated as,</p><formula xml:id="formula_3">I HR = f RM (H out ),<label>(4)</label></formula><p>where f RM (·) denotes the restoration module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Progressive Multi-scale Residual Block</head><p>Fig <ref type="figure" target="#fig_0">. 2</ref> demonstrates the design of PMRB, which can be separated into three steps. First, the progressive multi-scale processing (PMP) step exploits the hierarchical features in a sequential manner. After PMP, the multi-scale features are aggregated in the multi-scale feature fusion (MFF) step. Finally, the local residual learning (LRL) step introduces the shortcut to preserve the information and accelerate the gradient transmission.</p><p>Progressive multi-scale processing step aims to sequentially exploit the hierarchical features. Let's denote Comb s (·), x s as the combination and features for scale s × s separately, then there is,  </p><formula xml:id="formula_4">x s+2 = Comb s (x s ), s = 3, x s+2 = Comb s (x s ) + x s , else.<label>(5)</label></formula><formula xml:id="formula_5">Comb s (x) = Conv(ReLU (Comb s−2 (x))). else,<label>(6)</label></formula><p>where Conv(·) and ReLU (·) denote the convolution and ReLU activation respectively. From Eqn. 5, there is no explicit residual when s = 3. On one hand, the identical information will be delivered by the local residual learning step in PMRB. On the other hand, there is no activation in Comb 3 (·), and the identical addition will be implied by the convolution operation.</p><p>Multi-scale feature fusion step concatenates and fuses the multi-scale features, which contains one point-wise convolution and a CPA block. The operation can be demonstrated as,</p><formula xml:id="formula_6">x M F F = f M F F ([x 3 , x 5 , x 7 , ..., x S ]),<label>(7)</label></formula><p>where f M F F (·) denotes the MFF step, and x M F F is the output feature.</p><p>Local residual learning is devised to preserve the information and improve the gradient flow. Finally, the output of PMRB is,</p><formula xml:id="formula_7">H k = x M F F + H k−1 .<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Channel-wise and Pixel-wise Attention</head><p>As shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, there are three parts in CPA. Firstly, space transformation (ST) step converts the input features into a specific space for attention exploration. After ST, factor extraction (FE) step exploits the weighting and bias factors jointly from two parallel paths, which considers channel-wise and pixel-wise features separately. Finally, attention allocation (AA) step distributes the learned adaptive attentions onto the features.</p><p>Space transformation step transforms the input feature into a specific space with one convolution. The operation of ST can be demonstrated as,</p><formula xml:id="formula_8">F ST = Conv(x F C in ),<label>(9)</label></formula><p>where F ST denotes the features after transformation, and x F C in is the input features.</p><p>Factor extraction step exploits the scale and bias factors after ST. In FE step, channel-wise and pixel-wise attentions are jointly considered. Channel-wise attentions are firstly explored by one point-wise convolutional layer (P-Conv), then the pixelwise attentions are considered by one depth-wise convolutional layer (D-Conv). The two layers process attentions from different perspectives orthogonally. One ReLU activation is utilized between the two convolutional layers for non-linearity. The operations of FE module can be demonstrated as,</p><formula xml:id="formula_9">F β = F E β (F ST ),<label>(10)</label></formula><formula xml:id="formula_10">F γ = σ(F E γ (F ST )),<label>(11)</label></formula><p>where F E(·) denotes the extraction layers, and σ denotes the sigmoid activation. F β , F γ are the bias and scale factors separately. Sigmoid activation after F E γ (·) introduces the non-negativity of learned scales. Attention allocation step allocates the attentions to features via learned scale and bias factors. The output of AA step is,</p><formula xml:id="formula_11">x F C out = (F γ + 1) * x F C in + F β .<label>(12)</label></formula><p>From Eq. <ref type="formula" target="#formula_0">(12)</ref>, there is a residual structure in CPA. (F γ + 1) contains the self-adaptive scale factors and an identical addition of input features, which is utilized to preserve the information and improve the gradient transmission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Discussion</head><p>a) Difference to MSRN <ref type="bibr" target="#b10">[11]</ref>: MSRN introduced a multiscale block termed as MSRB with 3×3 and 5×5 convolutional layers. In MSRB, features from two kinds of convoluitonal layers are crossly concatenated and explored, and an 1 × 1 convolutional layer is utilized to fuse the multi-scale features. Different from MSRB, there are features from four different scales extracted by PMRB, and concatenated with one convolutional layer for fusion. Features from different scales are explored sequentially, and residual connections are utilized for information preservation and better gradient flow. Multiscale information is extracted by layers with different kernel sizes in MSRB, while PMRB designs the multi-scale structure in a recursive way, which decreases the parameters and computation complexity. Besides multi-scale design, a novel attention mechanism CPA is designed in PMRB. Features from different MSRBs are collected and concatenated with an convolutional layer for global feature fusion. Different from the global feature fusion, blocks in PMRN are stacked with global residual learning. With the elaborated design, PMRN achieves better PSNR/SSIM results on all testing benchmarks than MSRN with fewer parameters and lower computation complexity.</p><p>b) Difference to Channel-wise Attention <ref type="bibr" target="#b11">[12]</ref>: There is an effective channel-wise attention design in SENet, which has been widely utilized for different image restoration problems. In channel-wise attentions, information from different channels is evaluated by global average pooling. Two full connection layers with a ReLU activation are designed to explore the attentions, and a Sigmoid activation is introduced for non-negativity. In PMRN, CPA is devised for joint channelwise and pixel-wise attentions. Different from channel-wise attentions, features are extracted and explored by convolutional layers, which concentrates more on complex textures and information. Squeezing step in SENet shrinks the channel number, which may cause information loss. In CPA, the numbers of filters are invariable for all convolutional layers.</p><p>Besides weighting factors, bias factors are also explored in CPA to shift the features and find a better attention representation. Finally, a shortcut is designed in CPA to maintain the origin information.</p><p>c) Difference to LapSRN <ref type="bibr" target="#b9">[10]</ref>: LapSRN is a progressive network for image super-resolution. In LapSRN, the progressive structure is designed for images restorations with multiple resolutions by using one network. Residual maps are learned from the network sequentially with the increase of resolutions. In PMRN, an end-to-end network is proposed for image super-resolution with a specific scaling factor. The progressive structure is mainly designed in PMRB to extract the multi-scale features. Information from multi-scale features is sequentially extracted with different layer combinations and fused with one convolutional layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In PMRN, all convolutional layers are with kernel size as 3 × 3 expect for MFF step in PMRB, which is designed with 1 × 1. The filter number of convolutional layers is set as c = 64. There are K = 8 PMRBs stacked in non-linear feature exploration module, and the padding structure is composed of two convolutional layers with a ReLU activation.</p><p>The proposed PMRN is trained with DIV2K <ref type="bibr" target="#b38">[39]</ref> dataset. DIV2K is a high-quality dataset with 2K resolution images from real world. There are 800 training images, 100 validation images and 100 test images in DIV2K dataset. In this paper, 800 images are chosen for training and 5 images for validation. For testing, five benchmarks widely used in image super-resolution works: Set5 <ref type="bibr" target="#b39">[40]</ref>, Set14 <ref type="bibr" target="#b40">[41]</ref>, B100 <ref type="bibr" target="#b41">[42]</ref>, Urban100 <ref type="bibr" target="#b16">[17]</ref>, and Manga109 <ref type="bibr" target="#b42">[43]</ref> are chosen. The training images are randomly flipped and rotated for data augmentation. Patch size of LR image for training is set as 48 × 48. PMRN are trained for 1000 iterations with 1 loss, and the parameters are updated with an Adam <ref type="bibr" target="#b43">[44]</ref> optimizer. The learning rate of optimizer is chosen as lr = 10 −4 , and halved for every 200 iterations. The degradation model is chosen as bicubic down (BI) with scaling factor ×2, ×3, and ×4. PSNR and SSIM are chosen as the indicators for quantitive comparison with other works. Self-ensemble strategy is used to improve the performance, and the extension model is termed as as PMRN + .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Model Analysis</head><p>Analysis on Network Settings. In PMRN, the largest scale of PMRB is chosen as S = 9 and the number of PMRB is chosen as K = 8. To show the effect of different S and K, models are trained with different scales and block numbers for 200 epochs. Quantitative comparisons are made on B100 with scaling factor ×4. The visualization results are shown in <ref type="figure" target="#fig_4">Fig. 5. From Fig. 5</ref>, both S and K will affect the network performance. In general, with the increase of S and K, the networks will achieve better results. Compared with K, S counts more for the performance. On one hand, when S is larger, the network will be deeper. On the other hand, with the increase of S, features from more scales will be considered.  Analysis on PMRB. There is multi-scale structure in PMRB, extracting information from different scales. To show the performance of multi-scale design, comparisons are conducted without different combinations of convolutional layers. All combinations are replaced by only one 3 × 3 convolutional layer. In other words, all the scales in PMRB are identical to 3 × 3. The results are shown in <ref type="table">Table.</ref> I on four benchmarks with scaling factor ×4. <ref type="table" target="#tab_2">From Table I</ref>, model with multi-scale design achieves better PSNR/SSIM results than the other one. There are two reasons for the performance improvement. On one hand, the features of different scales will contain more information, which helps to recover the complex structural textures. On the other hand, the multi-scale structures are built in a recursive way. With the combination of convolutional layers, the depth of PMRN will be increased, which may be helpful to improve the network representation.</p><p>Furthermore, we analyze the exploited features from different scales, which are shown in <ref type="figure" target="#fig_5">Fig. 6</ref>. The multi-scale features are exploited from different layer combinations. With the increasing of scale factors, the structural information will be sharper and more clear, and the tiny textures will be flat. This accords with the notion that multi-scale features contain different information.</p><p>In PMRB, residual connections are introduced to preserve the information from small scales. Feature fusion with 1 × 1 convolution is also used to concatenate information from different scales. To show the performance of information preservation and feature fusion, we perform the comparisons without residual and 1 × 1 convolution. The results are shown in <ref type="table">Table.</ref> II, where Res and Fuse denote the residual connection and concatenation separately. Three benchmarks covering different kinds of textures are used for testing with scaling  Analysis on Combination Substitution. In PMRN, recursive layer combinations are proposed to substitute convolutional layers with different kernel sizes. To show the performance of substitution, PSNR/SSIM comparisons are made on five benchmarks with scaling factor ×4. For ensuring the same receptive field, network without combinations is built with layers holding the kernel sizes as 5 × 5, 7 × 7 and 9 × 9 separately. The results are shown in <ref type="table" target="#tab_2">Table III</ref> Analysis on Attentions Mechanism. In PMRN, CPA is investigated for joint attention mechanism. To show the performance of proposed CPA, comparisons are designed on three testing benchmarks. We compare the models with CPA, channel-wise attention (CA) <ref type="bibr" target="#b11">[12]</ref>, and no attentions. The results are shown in <ref type="table" target="#tab_2">Table IV</ref>. From the table, the model with CPA achieves the best performance on all testing benchmarks. The model with channel-wise attentions achieves better PSNR/SSIM results than that without attentions. The results demonstrate that attention mechanism is efficient for image super-resolution.</p><p>To analyze the operation of CPA, attention factors F γ , F β and the feature maps before and after attention are visualized in <ref type="figure" target="#fig_6">Fig. 7</ref>. From the illustrations, learned attentions are more concentrated on structural textures. F γ and F β vary sharply on the area of edges and complex textures. After attentions, the features are more discriminative on structural textures, which is a convincing evidence of that the attention mechanism concentrates more on the important high-frequency information</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison with State-of-the-Arts</head><p>To make quantitive comparison, we compare the PSNR/SSIM results with several small works: bicubic, SRCNN <ref type="bibr" target="#b20">[21]</ref> FSRCNN <ref type="bibr" target="#b21">[22]</ref>, VDSR <ref type="bibr" target="#b6">[7]</ref>, DRCN <ref type="bibr" target="#b22">[23]</ref>, CNF <ref type="bibr" target="#b23">[24]</ref>, LapSRN <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b24">[25]</ref>, DRRN <ref type="bibr" target="#b26">[27]</ref>, BTSRN <ref type="bibr" target="#b25">[26]</ref>, MemNet <ref type="bibr" target="#b27">[28]</ref>, SelNet <ref type="bibr" target="#b44">[45]</ref>, CARN <ref type="bibr" target="#b8">[9]</ref>, MSRN <ref type="bibr" target="#b10">[11]</ref>, and OISR <ref type="bibr" target="#b45">[46]</ref>. For a fair comparison, extension model PMRN + is compared with large networks: EDSR <ref type="bibr" target="#b7">[8]</ref>, D-DBPN <ref type="bibr" target="#b47">[47]</ref>, and SRFBN <ref type="bibr" target="#b48">[48]</ref>. <ref type="table" target="#tab_7">Table V</ref> shows the PSNR/SSIM comparisons among several methods. From the results, PMRN achieves competitive or better performance than other small works on all five benchmarks. Compared with MSRN, PMRN gains 0.3dB increase on Urban100 with BI×2 degradation. Notice that PMRN achieves the best performances on B100, Urban100, and Manga109 with all degradation models. The three benchmarks contain plentiful structural information and edges, which consist of comic covers and real world photos. From this point of  view, PMRN can recover the high-frequency information more effectively than others. Meanwhile, we compare the computation complexity and parameters with other works. The total number of parameters is calculated as,</p><formula xml:id="formula_12">P aram = L l=1 ch i l · ch o l · f w l · f h l gs l + bs l ,<label>(13)</label></formula><p>where ch i l , ch o l denote the input and output number of filters in l-th convolutional layer, f w l and f h l denote the width and height of the kernel size, gs l denotes the number of groups, and bs l represents as the bias.</p><p>Computation complexity is modeled as the number of multiply-accumulate operations (MACs). Since it is a implementation independent factor, MACs can purely describe the computation complexity from the mathematical perspective. Comparisons of MACs are conducted by producing a 720P (1280 × 720) resolution image from corresponding LR image with different scaling factors.</p><p>From the results, PMRN achieves competitive or better PSNR/SSIM results than others with fewer parameters and MACs, which proves to be the efficient design. Compared with OISR, PMRN holds near half MACs and parameters with competitive or better PSNR/SSIM performances with BI × 4 degradation. Compared with larger networks, PMRN + achieves competitive performances with much fewer MACs and parameters. Specially, PMRN + holds near two thirds of the MACs and one tenth of the parameters than EDSR with BI × 4 degradation, and achieves superior PSNR results on Set5, Set14, B100, and Manga109 datasets.</p><p>Visualization comparisons on parameters and MACs are shown in <ref type="figure" target="#fig_7">Fig. 8 and Fig. 9</ref>. A running time comparison is investigated in <ref type="figure" target="#fig_9">Fig. 10</ref>. The time cost and performance are evaluated on Manga109 with BI×2 degradation.</p><p>Besides quantitative comparisons, we also analyze the qualitative restoration performance via visualization comparisons. Three images from Urban100 benchmark are chosen for comparison with BI ×4 degradation, which is shown in <ref type="figure">Fig. 11</ref>. These images are from real world with abundant high-frequency textures and competitive for restoration with large scaling factors. From the result, PMRN can recover the structural information effectively, and find more accurate textures than other works.</p><p>Besides Urban100, we also conduct the experiments on Manga109, which is composed of comic book covers with     <ref type="figure" target="#fig_0">Fig. 12</ref>. From the visualization comparison, PMRN recovers more lines and structural textures.</p><p>Since Manga109 is a normal textured benchmark, we also compare the methods in the very textured situation, which is shown in <ref type="figure" target="#fig_1">Fig. 13</ref>  model, we perform the PSNR significant tests among PMRN, MSRN <ref type="bibr" target="#b10">[11]</ref>, MS-LapSRN <ref type="bibr" target="#b24">[25]</ref>, VDSR <ref type="bibr" target="#b6">[7]</ref>, and bicubic on Urban100 dataset with BI×4 degradation, which is shown in <ref type="figure" target="#fig_3">Fig. 14.</ref> From the comparison, PMRN achieves higher average PSNR than other works. Compared with MSRN and MS-LapSRN, PMRN achieves higher median Q 1 and Q 3 values, which proves the effectiveness of the proposed method. We also perform the ANOVA (analysis of variance) between PMRN and MSRN, and find the P-value P = 0.7836. Although there is no statistical significant difference, PMRN requires near half parameters and MACs than MSRN, which proves to be an efficient network for restoration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we proposed a progressive multi-scale residual network (PMRN) with limited parameters and computation complexity for single image super-resolution (SISR) problem. Specifically, a novel progressive multi-scale residual block (PMRB) was introduced in PMRN for information exploration from various scales. Different layer combinations for multi-scale features extraction were designed in a recursive way to decrease the parameters and computation complexity, which progressively exploited the features. Besides PMRB, we also proposed a joint channel-wise and pixel-wise attention mechanism named CPA for inherent correlation consideration of features. Different from previous works, weighting and bias factors were explored in parallel for better representations. Experimental results shows PMRN could not only achieve competitive or better PSNR/SSIM results than other small works on five testing benchmarks, but also recover more complex structural textures. Meanwhile, the extension model PMRN + with much fewer parameters and lower computation complexity could achieve competitive or better PSNR/SSIM results than other large networks.</p><p>Wen Gao (M'92-SM'05-F'09) received the Ph.D. degree in electronics engineering from The University of Tokyo, Japan, in 1991. He was a Professor of computer science with the Harbin Institute of Technology, from 1991 to 1995, and a Professor with the Institute of Computing Technology, Chinese Academy of Sciences. He is currently a Professor of computer science with Peking University, China.. He has published extensively including five books and over 600 technical articles in refereed journals and conference proceedings in the areas of image processing, video coding and communication, pattern recognition, multimedia information retrieval, multimodal interface, and bioinformatics. He chaired a number of prestigious international conferences on multimedia and video signal processing, such as the IEEE ICME and the ACM Multimedia, and also served on the advisory and technical committees of numerous professional organizations. He served or serves on the Editorial Board for several journals, such as the IEEE Transactions on Circuits and Systems for Video Technology, the IEEE Transactions on Multimedia, the IEEE Transactions on Image Processing, the IEEE Transactions on Autonomous Mental Development, the EURASIP Journal of Image Communications, and the Journal of Visual Communication and Image Representation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Illustration of proposed PMRN. There are three modules in PMRN sequentially restore the resolution from corresponding LR images. In PMRB, there are layer combinations for feature exploration with different scales. CPA block is utilized for joint channel-wise and pixel-wise attentions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Illustration of different layer combination design. The combinations are defined in a recursive way for larger scales.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3</head><label>3</label><figDesc>shows the designs of different combinations, which are defined in a recursive fashion. For scaling factor s = 3, there is one convolutional layer for feature extraction. For other scales, the combinations are composed of an identical structure of previous scale combination and a convolutional layer with ReLU activation. With the accumulation of small convolutional layers, the combinations hold different larger receptive fields. The formulation of Comb(·) can be described as, Comb s (x) = Conv(x), s = 3,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Illustration of proposed CPA. Scale factor F γ and bias factor F β are adaptively learned from the attention mechanism. In CPA, point-wise (P-Conv) and depth-wise (D-Conv) convolutional layers exploit the channel-wise and pixel-wise relations separately.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Investigation on different S and K with scaling factor ×4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Illustrations of multi-scale features. (a) and (f) denote the input and output features. (b)-(e) denote the features with scale factor 3, 5, 7, and 9.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>Visualization attention factors and feature maps about CPA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 :</head><label>8</label><figDesc>An illustration comparison of performance and parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 :</head><label>9</label><figDesc>An illustration comparison of performance and MACs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 :</head><label>10</label><figDesc>An illustration comparison of performance and running time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 14 :</head><label>14</label><figDesc>PSNR significant tests on Urban100 dataset with BI×4 degradation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:2007.09552v3 [eess.IV] 17 Nov 2020 Visual quality comparisons for various image SR methods with scaling factor ×4.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>HR</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(PSNR/SSIM)</cell></row><row><cell cols="3">image 024 from Urban100 [17]</cell><cell>Bicubic</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(16.94/0.5539)</cell></row><row><cell>LapSRN [10]</cell><cell>CARN [9]</cell><cell>MSRN [11]</cell><cell>PMRN</cell></row><row><cell>(18.10/0.6714)</cell><cell>(18.84/0.7132)</cell><cell>(18.81/0.7224)</cell><cell>(19.09/0.7340)</cell></row><row><cell>Fig. 1:</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>LR and HR stages for SISR. DRRN [27] proposed by Tai et al. considered a recursive network for restoration. MemNet [28], proposed by Tai et al., also achieved good performances on several low-level tasks. Recently, elaborate components are proposed for better feature exploration. EDSR [8] removed the batch normalization and introduced residual blocks for SISR problem. SRDenseNet [29] proposed by Tong et al. utilized dense connection for better gradient transmission. Zhang et al. embedded residual and dense connection in RDN</figDesc><table /><note>proposed by Ren et al. introduced context-wise fusion for image ensemble. In- spired by Laplacian pyramid, Lai et al. investigated Lap- SRN [10] and MS-LapSRN [25] for progressive restoration. Fan et al. utilized BTSRN [26] to balance the</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I :</head><label>I</label><figDesc>Investigation on multi-scale mechanism in PMRB with scaling factor ×4 for different benchmarks.</figDesc><table><row><cell>Multi</cell><cell>Set5</cell><cell>Set14</cell><cell>B100</cell><cell>Urban100</cell></row><row><cell>w</cell><cell>32.34/0.8971</cell><cell>28.71/0.7850</cell><cell>27.66/0.7392</cell><cell>26.37/0.7953</cell></row><row><cell>w/o</cell><cell>32.03/0.8932</cell><cell>28.51/0.7799</cell><cell>27.53/0.7348</cell><cell>25.90/0.7803</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II :</head><label>II</label><figDesc>Investigation on different structures in PMRB with scaling factor ×4 for different benchmarks.</figDesc><table><row><cell>Res</cell><cell>Fuse</cell><cell>Set5</cell><cell>B100</cell><cell>Urban100</cell></row><row><cell></cell><cell></cell><cell>32.34/0.8971</cell><cell>27.66/0.7392</cell><cell>26.37/0.7953</cell></row><row><cell></cell><cell></cell><cell>32.35/0.8971</cell><cell>27.64/0.7384</cell><cell>26.34/0.7942</cell></row><row><cell></cell><cell></cell><cell>32.24/0.8963</cell><cell>27.65/0.7388</cell><cell>26.36/0.7955</cell></row></table><note>factor ×4. From the Table II, residual and feature fusion are both efficient for different benchmarks. For Set5, residual structure performs better than fusion, achieving around 0.1db improvement. For B100 and Urban100, feature fusion can recover the texture more effectively. Set5 contains less high- frequency information than the other benchmarks, while B100 and Urban100 are composed of abundant images from real world. From this perspective, residual connection is suitable for simple images, while feature fusion performs better on complex structural textures.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>. From Table III, model built with layer combinations achieves better PSNR/SSIM results on all five testing benchmarks, showing the performance of recursive design. Meanwhile, there are around 40.2% off on parameters and MACs when utilizing recursive combinations.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III :</head><label>III</label><figDesc>Investigation on recursive combination in PMRB with scaling factor ×4 on different benchmarks.</figDesc><table><row><cell>Comb</cell><cell>Param</cell><cell>MACs</cell><cell>Set5</cell><cell>Set14</cell><cell>B100</cell><cell>Urban100</cell><cell>Manga109</cell></row><row><cell>w</cell><cell>3,598K</cell><cell>207.2G</cell><cell>32.34/0.8971</cell><cell>28.71/0.7850</cell><cell>27.66/0.7392</cell><cell>26.37/0.7953</cell><cell>30.71/0.9107</cell></row><row><cell>w/o</cell><cell>6,020K</cell><cell>346.7G</cell><cell>32.07/0.8932</cell><cell>28.53/0.7804</cell><cell>27.53/0.7350</cell><cell>25.93/0.7819</cell><cell>30.16/0.9043</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV :</head><label>IV</label><figDesc>Investigation on different normalization methods with scaling factor ×4 for different benchmarks.</figDesc><table><row><cell>Method</cell><cell>Set5</cell><cell>Set14</cell><cell>Urban100</cell></row><row><cell>CPA</cell><cell>32.34/0.8971</cell><cell>28.71/0.7850</cell><cell>26.37/0.7953</cell></row><row><cell>CA [12]</cell><cell>32.31/0.8968</cell><cell>28.69/0.7844</cell><cell>26.34/0.7940</cell></row><row><cell>w/o</cell><cell>32.29/0.8965</cell><cell>28.68/0.7851</cell><cell>26.29/0.7940</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE V :</head><label>V</label><figDesc>Average PSNR/SSIM, parameters and MACs results with degradation model BI ×2, ×3, and ×4 on five benchmarks. The best and second performances are shown in bold and underline.</figDesc><table><row><cell>Scale</cell><cell>Model</cell><cell>Params</cell><cell>MACs</cell><cell>Set5 [40] PSNR/SSIM</cell><cell>Set14 [41] PSNR/SSIM</cell><cell>B100 [42] PSNR/SSIM</cell><cell>Urban100 [17] PSNR/SSIM</cell><cell>Manga109 [43] PSNR/SSIM</cell></row><row><cell></cell><cell>SRCNN [21]</cell><cell>57K</cell><cell>52.7G</cell><cell>36.66/0.9542</cell><cell>32.42/0.9063</cell><cell>31.36/0.8879</cell><cell>29.50/0.8946</cell><cell>35.74/0.9661</cell></row><row><cell></cell><cell>FSRCNN [22]</cell><cell>12K</cell><cell>6.0G</cell><cell>37.00/0.9558</cell><cell>32.63/0.9088</cell><cell>31.53/0.8920</cell><cell>29.88/0.9020</cell><cell>36.67/0.9694</cell></row><row><cell></cell><cell>VDSR [7]</cell><cell>665K</cell><cell>612.6G</cell><cell>37.53/0.9587</cell><cell>33.03/0.9124</cell><cell>31.90/0.8960</cell><cell>30.76/0.9140</cell><cell>37.22/0.9729</cell></row><row><cell></cell><cell>DRCN [23]</cell><cell>1,774K</cell><cell>17,974.3G</cell><cell>37.63/0.9588</cell><cell>33.04/0.9118</cell><cell>31.85/0.8942</cell><cell>30.75/0.9133</cell><cell>37.63/0.9723</cell></row><row><cell></cell><cell>CNF [24]</cell><cell>337K</cell><cell>311.0G</cell><cell>37.66/0.9590</cell><cell>33.38/0.9136</cell><cell>31.91/0.8962</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>LapSRN [10]</cell><cell>813K</cell><cell>29.9G</cell><cell>37.52/0.9590</cell><cell>33.08/0.9130</cell><cell>31.80/0.8950</cell><cell>30.41/0.9100</cell><cell>37.27/0.9740</cell></row><row><cell></cell><cell>DRRN [27]</cell><cell>297K</cell><cell>6,796.9G</cell><cell>37.74/0.9591</cell><cell>33.23/0.9136</cell><cell>32.05/0.8973</cell><cell>31.23/0.9188</cell><cell>37.92/0.9760</cell></row><row><cell></cell><cell>BTSRN [26]</cell><cell>410K</cell><cell>207.7G</cell><cell>37.75/-</cell><cell>33.20/-</cell><cell>32.05/-</cell><cell>31.63/-</cell><cell>-</cell></row><row><cell>×2</cell><cell>MemNet [28]</cell><cell>677K</cell><cell>2,662.4G</cell><cell>37.78/0.9597</cell><cell>33.28/0.9142</cell><cell>32.08/0.8978</cell><cell>31.31/0.9195</cell><cell>37.72/0.9740</cell></row><row><cell></cell><cell>SelNet [45]</cell><cell>974K</cell><cell>225.7G</cell><cell>37.89/0.9598</cell><cell>33.61/0.9160</cell><cell>32.08/0.8984</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>CARN [9]</cell><cell>1,592K</cell><cell>222.8G</cell><cell>37.76/0.9590</cell><cell>33.52/0.9166</cell><cell>32.09/0.8978</cell><cell>31.92/0.9256</cell><cell>38.36/0.9765</cell></row><row><cell></cell><cell>MSRN [11]</cell><cell>5,930K</cell><cell>1367.5G</cell><cell>38.08/0.9607</cell><cell>33.70/0.9186</cell><cell>32.23/0.9002</cell><cell>32.29/0.9303</cell><cell>38.69/0.9772</cell></row><row><cell></cell><cell>OISR-RK2 [46]</cell><cell>4,970K</cell><cell>1145.7G</cell><cell>38.12/0.9609</cell><cell>33.80/0.9193</cell><cell>32.26/0.9006</cell><cell>32.48/0.9317</cell><cell>-</cell></row><row><cell></cell><cell>PMRN</cell><cell>3,577K</cell><cell>824.2G</cell><cell>38.13/0.9609</cell><cell>33.85/0.9204</cell><cell>32.28/0.9010</cell><cell>32.59/0.9328</cell><cell>38.91/0.9775</cell></row><row><cell></cell><cell>EDSR [8]</cell><cell>40,729K</cell><cell>9,388.8G</cell><cell>38.11/0.9602</cell><cell>33.92/0.9195</cell><cell>32.32/0.9013</cell><cell>32.93/0.9351</cell><cell>39.10/0.9773</cell></row><row><cell></cell><cell>D-DBPN [47]</cell><cell>5,953K</cell><cell>3,746.2G</cell><cell>38.09/0.9600</cell><cell>33.85/0.9190</cell><cell>32.27/0.9000</cell><cell>32.55/0.9324</cell><cell>38.89/0.9775</cell></row><row><cell></cell><cell>SRFBN [48]</cell><cell>2,140K</cell><cell>5,043.5G</cell><cell>38.11/0.9609</cell><cell>33.82/0.9196</cell><cell>32.29/0.9010</cell><cell>32.62/0.9328</cell><cell>39.08/0.9779</cell></row><row><cell></cell><cell>PMRN +</cell><cell>3,577K</cell><cell>6,593.6G</cell><cell>38.22/0.9612</cell><cell>33.90/0.9205</cell><cell>32.34/0.9015</cell><cell>32.78/0.9342</cell><cell>39.15/0.9781</cell></row><row><cell></cell><cell>SRCNN [21]</cell><cell>57K</cell><cell>52.7G</cell><cell>32.75/0.9090</cell><cell>29.28/0.8209</cell><cell>28.41/0.7863</cell><cell>26.24/0.7989</cell><cell>30.59/0.9107</cell></row><row><cell></cell><cell>FSRCNN [22]</cell><cell>12K</cell><cell>5.0G</cell><cell>33.16/0.9140</cell><cell>29.43/0.8242</cell><cell>28.53/0.7910</cell><cell>26.43/0.8080</cell><cell>30.98/0.9212</cell></row><row><cell></cell><cell>VDSR [7]</cell><cell>665K</cell><cell>612.6G</cell><cell>33.66/0.9213</cell><cell>29.77/0.8314</cell><cell>28.82/0.7976</cell><cell>27.14/0.8279</cell><cell>32.01/0.9310</cell></row><row><cell></cell><cell>DRCN [23]</cell><cell>1,774K</cell><cell>17,974.3G</cell><cell>33.82/0.9226</cell><cell>29.76/0.8311</cell><cell>28.80/0.7963</cell><cell>27.15/0.8276</cell><cell>32.31/0.9328</cell></row><row><cell></cell><cell>CNF [24]</cell><cell>337K</cell><cell>311.0G</cell><cell>33.74/0.9226</cell><cell>29.90/0.8322</cell><cell>28.82/0.7980</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>DRRN [27]</cell><cell>297K</cell><cell>6,796.9G</cell><cell>34.03/0.9244</cell><cell>29.96/0.8349</cell><cell>28.95/0.8004</cell><cell>27.53/0.8378</cell><cell>32.74/0.9390</cell></row><row><cell></cell><cell>BTSRN [26]</cell><cell>410K</cell><cell>176.2G</cell><cell>34.03/-</cell><cell>29.90/-</cell><cell>28.97/-</cell><cell>27.75/-</cell><cell>-</cell></row><row><cell>×3</cell><cell>MemNet [28] SelNet [45]</cell><cell>677K 1,159K</cell><cell>2,662.4G 120.0G</cell><cell>34.09/0.9248 34.27/0.9257</cell><cell>30.00/0.8350 30.30/0.8399</cell><cell>28.96/0.8001 28.97/0.8025</cell><cell>27.56/0.8376 -</cell><cell>32.51/0.9369 -</cell></row><row><cell></cell><cell>CARN [9]</cell><cell>1,592K</cell><cell>118.8G</cell><cell>34.29/0.9255</cell><cell>30.29/0.8407</cell><cell>29.06/0.8034</cell><cell>28.06/0.8493</cell><cell>33.49/0.9440</cell></row><row><cell></cell><cell>MSRN [11]</cell><cell>6,114K</cell><cell>626.6G</cell><cell>34.46/0.9278</cell><cell>30.41/0.8437</cell><cell>29.15/0.8064</cell><cell>28.33/0.8561</cell><cell>33.67/0.9456</cell></row><row><cell></cell><cell>OISR-RK2 [46]</cell><cell>5,640K</cell><cell>578.6G</cell><cell>34.55/0.9282</cell><cell>30.46/0.8443</cell><cell>29.18/0.8075</cell><cell>28.50/0.8597</cell><cell>-</cell></row><row><cell></cell><cell>PMRN</cell><cell>3,586K</cell><cell>366.6G</cell><cell>34.57/0.9284</cell><cell>30.43/0.8444</cell><cell>29.19/0.8075</cell><cell>28.51/0.8601</cell><cell>33.85/0.9465</cell></row><row><cell></cell><cell>EDSR [8]</cell><cell>43,680K</cell><cell>4,471.5G</cell><cell>34.65/0.9280</cell><cell>30.52/0.8462</cell><cell>29.25/0.8093</cell><cell>28.80/0.8653</cell><cell>34.17/0.9476</cell></row><row><cell></cell><cell>SRFBN [48]</cell><cell>2,832K</cell><cell>6,023.8G</cell><cell>34.70/0.9292</cell><cell>30.51/0.8461</cell><cell>29.24/0.8084</cell><cell>28.73/0.8641</cell><cell>34.18/0.9481</cell></row><row><cell></cell><cell>PMRN +</cell><cell>3,586K</cell><cell>2,932.8G</cell><cell>34.65/0.9289</cell><cell>30.54/0.8461</cell><cell>29.24/0.8087</cell><cell>28.71/0.8630</cell><cell>34.10/0.9480</cell></row><row><cell></cell><cell>SRCNN [21]</cell><cell>57K</cell><cell>52.7G</cell><cell>30.48/0.8628</cell><cell>27.49/0.7503</cell><cell>26.90/0.7101</cell><cell>24.52/0.7221</cell><cell>27.66/0.8505</cell></row><row><cell></cell><cell>FSRCNN [22]</cell><cell>12K</cell><cell>4.6G</cell><cell>30.71/0.8657</cell><cell>27.59/0.7535</cell><cell>26.98/0.7150</cell><cell>24.62/0.7280</cell><cell>27.90/0.8517</cell></row><row><cell></cell><cell>VDSR [7]</cell><cell>665K</cell><cell>612.6G</cell><cell>31.35/0.8838</cell><cell>28.01/0.7674</cell><cell>27.29/0.7251</cell><cell>25.18/0.7524</cell><cell>28.83/0.8809</cell></row><row><cell></cell><cell>DRCN [23]</cell><cell>1,774K</cell><cell>17,974.3G</cell><cell>31.53/0.8854</cell><cell>28.02/0.7670</cell><cell>27.23/0.7233</cell><cell>25.14/0.7510</cell><cell>28.98/0.8816</cell></row><row><cell></cell><cell>CNF [24]</cell><cell>337K</cell><cell>311.0G</cell><cell>31.55/0.8856</cell><cell>28.15/0.7680</cell><cell>27.32/0.7253</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>LapSRN [10]</cell><cell>813K</cell><cell>149.4G</cell><cell>31.54/0.8850</cell><cell>28.19/0.7720</cell><cell>27.32/0.7280</cell><cell>25.21/0.7560</cell><cell>29.09/0.8845</cell></row><row><cell></cell><cell>DRRN [27]</cell><cell>297K</cell><cell>6,796.9G</cell><cell>31.68/0.8888</cell><cell>28.21/0.7720</cell><cell>27.38/0.7284</cell><cell>25.44/0.7638</cell><cell>29.46/0.8960</cell></row><row><cell></cell><cell>BTSRN [26]</cell><cell>410K</cell><cell>207.7G</cell><cell>31.85/-</cell><cell>28.20/-</cell><cell>27.47/-</cell><cell>25.74/-</cell><cell>-</cell></row><row><cell>×4</cell><cell>MemNet [28] SelNet [45]</cell><cell>677K 1,417K</cell><cell>2,662.4G 83.1G</cell><cell>31.74/0.8893 32.00/0.8931</cell><cell>28.26/0.7723 28.49/0.7783</cell><cell>27.40/0.7281 27.44/0.7325</cell><cell>25.50/0.7630 -</cell><cell>29.42/0.8942 -</cell></row><row><cell></cell><cell>SRDenseNet [29]</cell><cell>2,015K</cell><cell>389.9G</cell><cell>32.02/0.8934</cell><cell>28.50/0.7782</cell><cell>27.53/0.7337</cell><cell>26.05/0.7819</cell><cell>-</cell></row><row><cell></cell><cell>CARN [9]</cell><cell>1,592K</cell><cell>90.9G</cell><cell>32.13/0.8937</cell><cell>28.60/0.7806</cell><cell>27.58/0.7349</cell><cell>26.07/0.7837</cell><cell>30.40/0.9082</cell></row><row><cell></cell><cell>MSRN [11]</cell><cell>6,373K</cell><cell>368.6G</cell><cell>32.26/0.8960</cell><cell>28.63/0.7836</cell><cell>27.61/0.7380</cell><cell>26.22/0.7911</cell><cell>30.57/0.9103</cell></row><row><cell></cell><cell>OISR-RK2 [46]</cell><cell>5,500K</cell><cell>412.2G</cell><cell>32.32/0.8965</cell><cell>28.72/0.7843</cell><cell>27.66/0.7390</cell><cell>26.37/0.7953</cell><cell>-</cell></row><row><cell></cell><cell>PMRN</cell><cell>3,598K</cell><cell>207.2G</cell><cell>32.34/0.8971</cell><cell>28.71/0.7850</cell><cell>27.66/0.7392</cell><cell>26.37/0.7953</cell><cell>30.71/0.9107</cell></row><row><cell></cell><cell>EDSR [8]</cell><cell>43,089K</cell><cell>2,895.8G</cell><cell>32.46/0.8968</cell><cell>28.80/0.7876</cell><cell>27.71/0.7420</cell><cell>26.64/0.8033</cell><cell>31.02/0.9148</cell></row><row><cell></cell><cell>D-DBPN [47]</cell><cell>10,426K</cell><cell>5,213.0G</cell><cell>32.47/0.8980</cell><cell>28.82/0.7860</cell><cell>27.72/0.7400</cell><cell>26.38/0.7946</cell><cell>30.91/0.9137</cell></row><row><cell></cell><cell>SRFBN [48]</cell><cell>3,631K</cell><cell>7,466.1G</cell><cell>32.47/0.8983</cell><cell>28.81/0.7868</cell><cell>27.72/0.7409</cell><cell>26.60/0.8015</cell><cell>31.15/0.9160</cell></row><row><cell></cell><cell>PMRN +</cell><cell>3,598K</cell><cell>1,657.6G</cell><cell>32.47/0.8984</cell><cell>28.81/0.7870</cell><cell>27.72/0.7405</cell><cell>26.55/0.7995</cell><cell>31.07/0.9144</cell></row><row><cell cols="4">plentiful line structures. The result is shown in</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>. The feather contains plentiful small lines and textures which are hard for recovery. From the comparison, PMRN can restore the textured image more accurately than MSRN.Furthermore, we investigate the restoration capacity on blurred images. From the formulation of SISR problem, these works can naturally handle the blurry issue. We compare the proposed PMRN with recent works with blur and ×3 downsampling (BD×3) degradation, which is shown in Tab. VI. From the comparison, PMRN achieves competitive PSNR/SSIM performance with RDN, and the extension model PMRN + achieves superior performance than all other works. It should be noted that RDN holds 22.308K parameters and 2282.2G MACs, which are much more than PMRN. From this perspective, PMRN is an efficient design which can effectively restore the blurred images. Visualization comparisons on Urban100 with BI ×4 degradation.</figDesc><table><row><cell></cell><cell>HR</cell><cell>LR</cell><cell>Bicubic</cell><cell>VDSR [7]</cell></row><row><cell></cell><cell>(PSNR/SSIM)</cell><cell>(18.96/0.7246)</cell><cell>(19.21/0.7331)</cell><cell>(19.94/0.7910)</cell></row><row><cell></cell><cell>LapSRN [10]</cell><cell>CARN [9]</cell><cell>MSRN [11]</cell><cell>Ours</cell></row><row><cell>image 059 from Urban100</cell><cell>(19.92/0.7894)</cell><cell>(20.82/0.8234)</cell><cell>(21.11/0.8369)</cell><cell>(21.44/0.8447)</cell></row><row><cell></cell><cell>HR</cell><cell>LR</cell><cell>Bicubic</cell><cell>VDSR [7]</cell></row><row><cell></cell><cell>(PSNR/SSIM)</cell><cell>(14.95/0.7116)</cell><cell>(15.80/0.7490)</cell><cell>(17.30/0.8474)</cell></row><row><cell></cell><cell>LapSRN [10]</cell><cell>CARN [9]</cell><cell>MSRN [11]</cell><cell>Ours</cell></row><row><cell>image 067 from Urban100</cell><cell>(17.34/0.8577)</cell><cell>(18.12/0.8882)</cell><cell>(18.58/0.8950)</cell><cell>(18.84/0.9035)</cell></row><row><cell></cell><cell>HR</cell><cell>LR</cell><cell>Bicubic</cell><cell>VDSR [7]</cell></row><row><cell></cell><cell>(PSNR/SSIM)</cell><cell>(23.74/0.7624)</cell><cell>(24.49/0.7866)</cell><cell>(25.49/0.8401)</cell></row><row><cell></cell><cell>LapSRN [10]</cell><cell>CARN [9]</cell><cell>MSRN [11]</cell><cell>Ours</cell></row><row><cell>image 078 from Urban100</cell><cell>(25.41/0.8395)</cell><cell>(25.88/0.8536)</cell><cell>(26.12/0.8598)</cell><cell>(26.45/0.8658)</cell></row><row><cell>Fig. 11:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">To further investigate the performance of the proposed</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VI :</head><label>VI</label><figDesc>PSNR/SSIM comparisons on BD×3 degradation. Visualization comparisons on Set14 with BI ×4 degradation.</figDesc><table><row><cell></cell><cell cols="2">HR (PSNR/SSIM)</cell><cell cols="2">LR (20.16/0.8521)</cell><cell cols="3">MSRN [11] (26.31/0.9296)</cell><cell cols="2">PMRN (26.46/0.9637)</cell></row><row><cell></cell><cell cols="2">HR (PSNR/SSIM)</cell><cell cols="2">LR (21.34/0.8693)</cell><cell cols="3">MSRN [11] (27.49/0.9690)</cell><cell cols="2">PMRN (27.76/0.9637)</cell></row><row><cell></cell><cell></cell><cell cols="8">Fig. 12: Visualization comparisons on Manga109 with BI ×4 degradation.</cell></row><row><cell></cell><cell cols="2">HR (PSNR/SSIM)</cell><cell cols="2">LR (23.92/0.6380)</cell><cell cols="3">MSRN [11] (26.11/0.7529)</cell><cell cols="2">PMRN (26.18/0.7547)</cell></row><row><cell></cell><cell></cell><cell>Fig. 13:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Bicubic</cell><cell>SRCNN [21]</cell><cell>VDSR [7]</cell><cell cols="2">IRCNN G [49]</cell><cell>IRCNN C [49]</cell><cell cols="2">RDN [50]</cell><cell>PMRN</cell><cell>PMRN +</cell></row><row><cell>Set5</cell><cell>28.78/0.8308</cell><cell>32.05/0.8944</cell><cell>33.25/0.9150</cell><cell cols="2">33.38/0.9182</cell><cell>33.17/0.9157</cell><cell cols="2">34.58/0.9280</cell><cell>34.53/0.9274</cell><cell>34.66/0.9284</cell></row><row><cell>Set14</cell><cell>26.38/0.7271</cell><cell>28.80/0.8074</cell><cell>29.46/0.8244</cell><cell cols="2">29.63/0.8281</cell><cell>29.55/0.8271</cell><cell cols="2">30.53/0.8447</cell><cell>30.51/0.8442</cell><cell>30.60/0.8453</cell></row><row><cell>B100</cell><cell>26.33/0.6918</cell><cell>28.13/0.7736</cell><cell>28.57/0.7893</cell><cell cols="2">28.65/0.7922</cell><cell>28.49/0.7886</cell><cell cols="2">29.23/0.8079</cell><cell>29.22/0.8073</cell><cell>29.28/0.8083</cell></row><row><cell>Urban100</cell><cell>23.52/0.6862</cell><cell>25.70/0.7770</cell><cell>26.61/0.8136</cell><cell cols="2">26.77/0.8154</cell><cell>26.47/0.8081</cell><cell cols="2">28.46/0.8582</cell><cell>28.48/0.8580</cell><cell>28.63/0.8603</cell></row><row><cell>Manga109</cell><cell>25.46/0.8149</cell><cell>29.47/0.8924</cell><cell>31.06/0.9234</cell><cell cols="2">31.15/0.9245</cell><cell>31.13/0.9236</cell><cell cols="2">33.97/0.9465</cell><cell>34.05/0.9464</cell><cell>34.36/0.9480</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Meta-usr: A unified super-resolution network for multiple degradation parameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A real-time convolutional neural network for super-resolution on fpga with applications to 4k uhd 60 fps video services</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2521" to="2534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deeply supervised depth map superresolution as novel view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2323" to="2336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Modeling and optimizing of the multi-layer nearest neighbor network for face image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Super-resolution-guided progressive pansharpening based on a deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep learning based autonomous vehicle super resolution doa estimation for safety driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J P C</forename><surname>Rodrigues</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1132" to="1140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast, accurate, and lightweight super-resolution with cascading residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-A</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="256" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep laplacian pyramid networks for fast and accurate super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5835" to="5843" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-scale residual network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="527" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Image superresolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="294" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Lightweight image superresolution with information multi-distillation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2024" to="2032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Second-order attention network for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11057" to="11066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Single image super-resolution from transformed self-exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5197" to="5206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A combined multiple action recognition and summarization for surveillance video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Elharrouss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Almaadeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Al-Maadeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bouridane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beghdadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Intelligence</title>
		<imprint>
			<date type="published" when="2020-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Image inpainting: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Elharrouss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Almaadeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Al-Maadeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Akbari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Processing Letters</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="2007" to="2028" />
			<date type="published" when="2020-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A block-based background model for moving object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Elharrouss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abbad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Moujahid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Riffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tairi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ELCVIA Electronic Letters on Computer Vision and Image Analysis</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="17" to="31" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Accelerating the super-resolution convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="391" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deeply-recursive convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1637" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image super resolution based on fusing multiple convolution neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>El-Khamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1050" to="1057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fast and accurate image super-resolution with deep laplacian pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2599" to="2613" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Balanced two-stage residual networks for image superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1157" to="1164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Image super-resolution via deep recursive residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2790" to="2798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Memnet: A persistent memory network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4549" to="4557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Image super-resolution using dense skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4809" to="4817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2472" to="2481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Single image super-resolution reconstruction with wavelet based deep residual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 Chinese Control And Decision Conference (CCDC)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4270" to="4275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">s-lwsr: Super lightweight super-resolution network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="8368" to="8380" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Optimization of single image superresolution reconstruction algorithm based on residual dense network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 Chinese Control And Decision Conference (CCDC)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2497" to="2501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mdcn: Multi-scale dense cross network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mghcnet: A deep multi-scale granular and holistic channel feature generation network for image super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Esmaeilzehi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">O</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N S</forename><surname>Swamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep networks with detail enhancement for infrared image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="158690" to="158701" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Image super-resolution with cross-scale non-local attention and exhaustive selfexemplars mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5689" to="5698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Progressive fusion video super-resolution network via exploiting non-local spatio-temporal correlations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3106" to="3115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image super-resolution: Dataset and study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1122" to="1131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Lowcomplexity single-image super-resolution based on nonnegative neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Alberi-Morel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">On single image scale-up using sparse-representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on curves and surfaces</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="711" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Eighth IEEE International Conference on Computer Vision. ICCV</title>
		<meeting>Eighth IEEE International Conference on Computer Vision. ICCV</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Sketch-based manga retrieval using manga109 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aramaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="21811" to="21838" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A deep convolutional neural network with selection units for super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1150" to="1156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Odeinspired network design for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1732" to="1741" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep back-projection networks for super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1664" to="1673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Feedback network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3862" to="3871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning deep cnn denoiser prior for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2808" to="2817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Residual dense network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">He is currently pursuing the Ph.D. degree. His current research interests include video compression, processing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Yuqing Liu received the B.S. degree in software engineering from the Dalian University of Technology, China. and analysis</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
