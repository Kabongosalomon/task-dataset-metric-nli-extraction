<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Twofold Siamese Network for Real-Time Object Tracking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anfeng</forename><surname>He</surname></persName>
							<email>heanfeng@mail.ustc.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Luo</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
							<email>xinmei@ustc.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
							<email>wezeng@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Geo-Spatial Information Processing and Application System</orgName>
								<orgName type="institution" key="instit1">CAS Key Laboratory of Technology</orgName>
								<orgName type="institution" key="instit2">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<region>Anhui</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Twofold Siamese Network for Real-Time Object Tracking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Observing that Semantic features learned in an image classification task and Appearance features learned in a similarity matching task complement each other, we build a twofold Siamese network, named SA-Siam, for real-time object tracking. SA-Siam is composed of a semantic branch and an appearance branch. Each branch is a similaritylearning Siamese network. An important design choice in SA-Siam is to separately train the two branches to keep the heterogeneity of the two types of features. In addition, we propose a channel attention mechanism for the semantic branch. Channel-wise weights are computed according to the channel activations around the target position. While the inherited architecture from SiamFC [3] allows our tracker to operate beyond real-time, the twofold design and the attention mechanism significantly improve the tracking performance. The proposed SA-Siam outperforms all other real-time trackers by a large margin on OTB-2013/50/100 benchmarks. * This work is carried out while Anfeng He is an intern in MSRA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth</head><p>SiamFC ours</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Visual object tracking is one of the most fundamental and challenging tasks in computer vision. Given the bounding box of an unknown target in the first frame, the objective is to localize the target in all the following frames in a video sequence. While visual object tracking finds numerous applications in surveillance, autonomous systems, and augmented reality, it is a very challenging task. For one reason, with only a bounding box in the first frame, it is difficult to differentiate the unknown target from the cluttered background when the target itself moves, deforms, or has an appearance change due to various reasons. For another, most applications demand real-time tracking. It is even harder to design a real-time high-performance tracker.</p><p>The key to design a high-performance tracker is to find expressive features and corresponding classifiers that are simultaneously discriminative and generalized. Being discriminative allows the tracker to differentiate the true target from the cluttered or even deceptive background. Being generalized means that a tracker would tolerate the appearance changes of the tracked object, even when the object is not known a priori. Conventionally, both the discrimination and the generalization power need to be strengthened through online training process, which collects target information while tracking. However, online updating is time consuming, especially when a large number of parameters are involved. It is therefore very crucial to balance the tracking performance and the run-time speed.</p><p>In recent years, deep convolutional neural networks (CNNs) demonstrated their superior capabilities in various vision tasks. They have also significantly advanced the state-of-the-art of object tracking. Some trackers <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b5">6]</ref> integrate deep features into conventional tracking approaches and benefit from the expressive power of CNN features. Some others <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b32">33]</ref> directly use CNNs as classifiers and take full advantage of end-to-end training. Most of these approaches adopt online training to boost the tracking performance. However, due to the high volume of CNN features and the complexity of deep neural networks, it is computationally expensive to perform online training. As a result, most online CNN-based trackers have a far less operational speed than real-time.</p><p>Meanwhile, there emerge two real-time CNN-based trackers <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b2">3]</ref> which achieve high tracking speed by completely avoiding online training. While GOTURN <ref type="bibr" target="#b12">[13]</ref> treats object tracking as a box regression problem, SiamFC <ref type="bibr" target="#b2">[3]</ref> treats it as a similarity learning problem. It appears that SiamFC achieves a much better performance than GO-TURN. This owes to the fully convolutional network architecture, which allows SiamFC to make full use of the offline training data and make itself highly discriminative. However, the generalization capability remains quite poor and it encounters difficulties when the target has significant appearance change, as shown in <ref type="figure" target="#fig_0">Fig.1</ref>. As a result, SiamFC still has a performance gap to the best online tracker.</p><p>In this paper, we aim to improve the generalization capability of SiamFC. It is widely understood that, in a deep CNN trained for image classification task, features from deeper layers contain stronger semantic information and is more invariant to object appearance changes. These semantic features are an ideal complement to the appearance features trained in a similarity learning problem. Inspired by this observation, we design SA-Siam, which is a twofold Siamese network comprised of a semantic branch and an appearance branch. Each branch is a Siamese network computing the similarity scores between the target image and a search image. In order to maintain the heterogeneity of the two branches, they are separately trained and not combined until the similarity score is obtained by each branch.</p><p>For the semantic branch, we further propose a channel attention mechanism to achieve a minimum degree of target adaptation. The motivation is that different objects activate different sets of feature channels. We shall give higher weights to channels that play more important roles in tracking specific targets. This is realized by computing channel-wise weights based on the channel responses at the target object and in the surrounding context. This simplest form of target adaptation improves the discrimination power of the tracker. Evaluations show that our tracker outperforms all other real-time trackers by a large margin on OTB-2013/50/100 benchmarks, and achieves state-of-theart performance on VOT benchmarks. The rest of the paper is organized as follows. We first introduce related work in Section 2. Our approach is described in Section 3. The experimental results are presented in Section 4. Finally, Section 5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Siamese Network Based Trackers</head><p>Visual object tracking can be modeled as a similarity learning problem. By comparing the target image patch with the candidate patches in a search region, we can track the object to the location where the highest similarity score is obtained. A notable advantage of this method is that it needs no or little online training. Thus, real-time tracking can be easily achieved.</p><p>Similarity learning with deep CNNs is typically addressed using Siamese architectures <ref type="bibr" target="#b22">[23]</ref>. The pioneering work for object tracking is the fully convolutional Siamese network (SiamFC) <ref type="bibr" target="#b2">[3]</ref>. The advantage of a fullyconvolutional network is that, instead of a candidate patch of the same size of the target patch, one can provide as input to the network a much larger search image and it will compute the similarity at all translated sub-windows on a dense grid in a single evaluation <ref type="bibr" target="#b2">[3]</ref>. This advantage is also reflected at training time, where every sub-window effectively represents a useful sample with little extra cost.</p><p>There are a large number of follow-up work <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b10">11]</ref> of SiamFC. EAST <ref type="bibr" target="#b14">[15]</ref> attempts to speed up the tracker by early stopping the feature extractor if low-level features are sufficient to track the target. CFNet <ref type="bibr" target="#b36">[37]</ref> introduces correlation filters for low level CNNs features to speed up tracking without accuracy drop. SINT <ref type="bibr" target="#b31">[32]</ref> incorporates the optical flow information and achieves better performance. However, since computing optical flow is computationally expensive, SINT only operates at 4 frames per second (fps). DSiam <ref type="bibr" target="#b10">[11]</ref> attempts to online update the embeddings of tracked target. Significantly better performance is achieved without much speed drop.</p><p>SA-Siam inherits network architecture from SiamFC. We intend to improve SiamFC with an innovative way to utilize heterogeneous features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Ensemble Trackers</head><p>Our proposed SA-Siam is composed of two separately trained branches focusing on different types of CNN features. It shares some insights and design principles with ensemble trackers.</p><p>HDT <ref type="bibr" target="#b29">[30]</ref> is a typical ensemble tracker. It constructs trackers using correlation filters (CFs) with CNN features from each layer, and then uses an adaptive Hedge method to hedge these CNN trackers. TCNN <ref type="bibr" target="#b27">[28]</ref> maintains multiple CNNs in a tree structure to learn ensemble models and estimate target states. STCT <ref type="bibr" target="#b32">[33]</ref> is a sequential training  method for CNNs to effectively transfer pre-trained deep features for online applications. An ensemble CNN-based classifier is trained to reduce correlation across models. BranchOut <ref type="bibr" target="#b11">[12]</ref> employs a CNN for target representation, which has a common convolutional layers but has multiple branches of fully connected layers. It allows each branch to have a different number of layers so as to maintain variable abstraction levels of target appearances. PTAV <ref type="bibr" target="#b9">[10]</ref> keeps two classifiers, one acting as the tracker and the other acting as the verifier. The combination of an efficient tracker which runs for sequential frames and a powerful verifier which only runs when necessary strikes a good balance between speed and accuracy.</p><p>A common insight of these ensemble trackers is that it is possible to make a strong tracker by utilizing different layers of CNN features. Besides, the correlation across models should be weak. In SA-Siam design, the appearance branch and the semantic branch use features at very different abstraction levels. Besides, they are not jointly trained to avoid becoming homogeneous.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Adaptive Feature Selection</head><p>Different features have different impacts on different tracked target. Using all features available for a single object tracking is neither efficient nor effective. In SCT <ref type="bibr" target="#b3">[4]</ref> and ACFN <ref type="bibr" target="#b4">[5]</ref>, the authors build an attention network to select the best module among several feature extractors for the tracked target. HART <ref type="bibr" target="#b17">[18]</ref> and RATM <ref type="bibr" target="#b15">[16]</ref> use RNN with attention model to separate where and what processing pathways to actively suppress irrelevant visual features. Recently, SENet <ref type="bibr" target="#b13">[14]</ref> demonstrates the effectiveness of channel-wise attention on image recognition tasks.</p><p>In our SA-Siam network, we perform channel-wise attention based on the channel activations. It can be looked on as a type of target adaptation, which potentially improves the tracking performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head><p>We propose a twofold fully-convolutional siamese network for real-time object tracking. The fundamental idea behind this design is that the appearance features trained in a similarity learning problem and the semantic features trained in an image classification problem complement each other, and therefore should be jointly considered for robust visual tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">SA-Siam Network Architecture</head><p>The network architecture of the proposed SA-Siam network is depicted in <ref type="figure" target="#fig_2">Fig. 2</ref>. The input of the network is a pair of image patches cropped from the first (target) frame of the video sequence and the current frame for tracking. We use notations z, z s and X to denote the images of target, target with surrounding context and search region, respectively. Both z s and X have a size of</p><formula xml:id="formula_0">W s × H s × 3. The exact target z has a size of W t × H t × 3 (W t &lt; W s and H t &lt; H s ),</formula><p>locates in the center of z s . X can be looked on as a collection of candidate image patches x in the search region which have the same dimension as z.</p><p>SA-Siam is composed of the appearance branch (indicated by blue blocks in the figure) and the semantic branch (indicated by orange blocks). The output of each branch is a response map indicating the similarity between target z and candidate patch x within the search region X. The two branches are separately trained and not combined until testing time.</p><p>The appearance branch: The appearance branch takes (z, X) as input. It clones the SiamFC network <ref type="bibr" target="#b2">[3]</ref>. The convolutional network used to extract appearance features is called A-Net, and the features extracted are denoted by f a (·). The response map from the appearance branch can be written as:</p><formula xml:id="formula_1">h a (z, X) = corr(f a (z), f a (X)),<label>(1)</label></formula><p>where corr(·) is the correlation operation. All the parameters in the A-Net are trained from scratch in a similarity learning problem. In particular, with abundant pairs (z i , X i ) from training videos and corresponding groundtruth response map Y i of the search region, A-Net is optimized by minimizing the logistic loss function L(·) as follows:</p><formula xml:id="formula_2">arg min θa 1 N N i=1 {L (h a (z i , X i ; θ a ), Y i )},<label>(2)</label></formula><p>where θ a denotes the parameters in A-Net, N is the number of training samples. The semantic branch: The semantic branch takes (z s , X) as input. We directly use a CNN pretrained in the image classification task as S-Net and fix all its parameters during training and testing. We let S-Net to output features from the last two convolutional layers (conv4 and conv5 in our implementation), since they provide different levels of abstraction. The low-level features are not extracted.</p><p>Features from different convolutional layers have different spatial resolution. For simplicity of notation, we denote the concatenated multilevel features by f s (·). In order to make the semantic features suitable for the correlation operation, we insert a fusion module, implemented by 1 × 1 ConvNet, after feature extraction. The fusion is performed within features of the same layer. The feature vector for search region X after fusion can be written as g(f s (X)).</p><p>The target processing is slightly different. S-Net takes z s as the target input. z s has z in its center and contains the context information of the target. Since S-Net is fully convolutional, we can easily obtain f s (z) from f s (z s ) by a simple crop operation. The attention module takes f s (z s ) as input and outputs the channel weights ξ. The details of the attention module is included in the next subsection. The features are multiplied by channel weights before they are fused by 1 × 1 ConvNet. As such, the response map from the semantic branch can be written as:</p><formula xml:id="formula_3">h s (z s , X) = corr (g (ξ · f s (z)) , g (f s (X))) ,<label>(3)</label></formula><p>where ξ has the same dimension as the number of channels in f s (z) and · is element-wise multiplication.</p><p>In the semantic branch, we only train the fusion module and the channel attention module. With training pairs (z s i , X i ) and ground-truth response map Y i , the semantic branch is optimized by minimizing the following logistic arg min</p><formula xml:id="formula_4">θs 1 N N i=1 {L (h s (z s i , X i ; θ s ), Y i )},<label>(4)</label></formula><p>where θ s denotes the trainable parameters, N is the number of training samples. During testing time, the overall heat map is computed as the weighted average of the heat maps from the two branches:</p><formula xml:id="formula_5">h(z s , X) = λh a (z, X) + (1 − λ)h s (z s , X),<label>(5)</label></formula><p>where λ is the weighting parameter to balance the importance of the two branches. In practice, λ can be estimated from a validation set. The position of the largest value in h(z s , X) suggests the center location of the tracked target. Similar to SiamFC <ref type="bibr" target="#b2">[3]</ref>, we use multi-scale inputs to deal with scale changes. We find that using three scales strikes a good balance between performance and speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Channel Attention in Semantic Branch</head><p>High-level semantic features are robust to appearance changes of objects, and therefore make a tracker more generalized but less discriminative. In order to enhance the discriminative power of the semantic branch, we design a channel attention module.</p><p>Intuitively, different channels play different roles in tracking different targets. Some channels may be extremely important in tracking certain targets while being dispensable in tracking others. If we could adapt the channel importance to the tracking target, we achieve the minimum functionality of target adaptation. In order to do so, not only the target is relevant, the surrounding context of the target also matters. Therefore, in our proposed attention module, the input is the feature map of z s instead of z.</p><p>The attention module is realized by channel-wise operations. <ref type="figure" target="#fig_3">Fig. 3</ref> shows the attention process for the i th channel. Take a conv5 feature map as an example, the spatial dimension is 22 × 22 in our implementation. We divide the feature map into 3 × 3 grids, and the center grid with size 6 × 6 corresponds to the tracking target z. Max pooling is performed within each grid, and then a two-layer multilayer perceptron (MLP) is used to produce a coefficient for this channel. Finally, a Sigmoid function with bias is used to generate the final output weight ξ i . The MLP module shares weights across channels extracted from the same convolutional layer.</p><p>Note that attention is only added to the target processing. All the activations in channel i for the target patch are multiplied by ξ i . Therefore, this module is passed only once for the first frame of a tracking sequence. The computational overhead is negligible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Discussions of Design Choices</head><p>We separately train the two branches. We made this choice based on the following observations. For some training samples, tracking with semantic clues may be much easier than with appearance clues, and for some others, it could just be the opposite. Let us take the former case as an example. If the two branches are jointly trained, the overall loss could be small when the semantic branch has a discriminative heat map and the appearance branch has a noninformative heat map. Then, these training samples do not play their part in optimizing the appearance branch. As a result, both branches are less powerful when they are jointly trained than separately trained.</p><p>We do not fine-tune S-Net. A common practice in transfer learning is to fine-tune the pre-trained network in the new problem. We choose not to do so because fine-tuning S-Net using the same procedure as we train A-Net will make the two branches homogeneous. Although fine-tuning S-Net may improve the tracking performance of the semantic branch alone, the overall performance could be unfavorably impacted. We have carried out experiments to verify this design choice, although we do not include them in this paper due to space limitation.</p><p>We keep A-Net as it is in SiamFC. Using multilevel features and adding channel attention significantly improve the semantic branch, but we do not apply them to the appearance branch. This is because appearance features from different convolutional layers do not have significant difference in terms of expressiveness. We cannot apply the same attention module to the appearance branch because highlevel semantic features are very sparse while appearance features are quite dense. A simple max pooling operation could generate a descriptive summary for semantic features but not for appearance features. Therefore, a much more complicated attention module would be needed for A-Net, and the gain may not worth the cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Network structure: Both A-Net and S-Net use AlexNet <ref type="bibr" target="#b21">[22]</ref>-like network as base network. The A-Net has exactly the same structure as the SiamFC network <ref type="bibr" target="#b2">[3]</ref>. The S-Net is loaded from a pretrained AlexNet on ImageNet <ref type="bibr" target="#b30">[31]</ref>. A small modification is made to the stride so that the last layer output from S-Net has the same dimension as A-Net.</p><p>In the attention module, the pooled features of each channel are stacked into a 9-dimensional vector. The following MLP has 1 hidden layer with nine neurons. The non-linear function of the hidden layer is ReLU. The MLP is followed by a Sigmoid function with bias 0.5. This is to ensure that no channel will be suppressed to zero.</p><p>Data Dimensions: In our implementation, the target image patch z has a dimension of 127 × 127 × 3, and both z s and X have a dimension of 255 × 255 × 3. The output features of A-Net for z and X have a dimension of 6 × 6 × 256 and 22 × 22 × 256, respectively. The conv4 and conv5 features from S-Net have dimensions of 24 × 24 × 384 and 22×22×256 channels for z s and X. The 1×1 ConvNet for these two sets of features outputs 128 channels each (which adds up to 256), with the spatial resolution unchanged. The response maps have the same dimension of 17 × 17.</p><p>Training: Our approach is trained offline on the ILSVRC-2015 <ref type="bibr" target="#b30">[31]</ref> video dataset and we only use color images. Among a total of more than 4,000 sequences, there are around 1.3 million frames and about 2 million tracked objects with ground truth bounding boxes. For each tracklet, we randomly pick a pair of images and crop z s from one image with z in the center and crop X from the other image with the ground truth target in the center. Both branches are trained for 30 epochs with learning rate 0.01 in the first 25 epochs and learning rate 0.001 in the last five epochs. We implement our model in TensorFlow[1] 1.2.1 framework. Our experiments are performed on a PC with a Xeon E5 2.4GHz CPU and a GeForce GTX Titan X GPU. The average testing speed of SA-Siam is 50 fps.</p><p>Hyperparameters: The two branches are combined by a weight λ. This hyperparameter is estimated on a small validation set TC128 <ref type="bibr" target="#b23">[24]</ref> excluding sequences in OTB benchmarks. We perform a grid search from 0.1 to 0.9 with step 0.2. Evaluations suggest that the best performance is achieved when λ = 0.3. We use this value for all the test sequences. During evaluation and testing, three scales are searched to handle scale variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Datasets and Evaluation Metrics</head><p>OTB: The object tracking benchmarks (OTB) <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref> consist of three datasets, namely OTB-2013, OTB-50 and OTB-100. They have 51, 50 and 100 real-world target for tracking, respectively. There are eleven interference attributes to which all sequences belong.</p><p>The two standard evaluation metrics on OTB are success rate and precision. For each frame, we compute the IoU (intersection over union) between the tracked and groundtruth bounding boxes, as well as the distance of their center lo-  <ref type="table">Table 1</ref>. Ablation study of SA-Siam on OTB benchmarks. App. and Sem. denote appearance model and semantic model. ML means using multilevel feature and Att. denotes attention module. cations. A success plot can be obtained by evaluating the success rate at different IoU thresholds. Conventionally, the area-under-curve (AUC) of the success plot is reported. The precision plot can be acquired in a similar way, but usually the representative precision at the threshold of 20 pixels is reported. We use the standard OTB toolkit to obtain all the numbers.</p><p>VOT: The visual object tracking (VOT) benchmark has many versions, and we use the most recent ones: VOT2015 <ref type="bibr" target="#b18">[19]</ref>, VOT2016 <ref type="bibr" target="#b19">[20]</ref> and VOT2017 <ref type="bibr" target="#b20">[21]</ref>. VOT2015 and VOT2016 contain the same sequences, but the ground truth label in VOT2016 is more accurate than that in VOT2015. In VOT2017, ten sequences from VOT2016 are replaced by new ones.</p><p>The VOT benchmarks evaluate a tracker by applying a reset-based methodology. Whenever a tracker has no overlap with the ground truth, the tracker will be re-initialized after five frames. Major evaluation metrics of VOT benchmarks are accuracy (A), robustness (R) and expected average overlap (EAO). A good tracker has high A and EAO scores but low R scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Analysis</head><p>We use experiments to verify our claims and justify our design choices in SA-Siam. We use the OTB benchmark for the ablation analysis.</p><p>The semantic branch and the appearance branch complement each other. We evaluate the performances of the semantic model alone (model S 1 ) and the appearance model alone (model A 1 ). S 1 is a basic version with only S-Net and fusion module. Both S 1 and A 1 are trained from scratch with random initialization. The results are reported in the first two rows in <ref type="table">Table 1</ref>. The third row presents the results of an SA-Siam model which combines S 1 and A 1 . The improvement is huge and the combined model achieves state-of-the-art performance even without multilevel semantic features or channel attention.</p><p>In order to show the advantage of using heterogeneous features, we compare SA-Siam with two simple ensemble models. We train another semantic model S 2 using different initialization, and take the appearance model A 2 published by the SiamFC authors. Then we ensemble A 1 A 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>S2 A2 A1A2 S1S2 AUC 0.606 0.602 0.608 0.602 Prec.</p><p>0.822 0.806 0.813 0.811 <ref type="table">Table 2</ref>. Evaluation of separate and ensemble models on OTB-2013. A1A2 is an ensemble of appearance models and S1S2 is an ensemble of basic semantic models.  and S 1 S 2 . <ref type="table">Table 2</ref> shows the performance of the separate and the ensemble models. It is clear that the ensemble models A 1 A 2 and S 1 S 2 do not perform as well as the SA-Siam model. This confirms the importance of complementary features in designing a twofold Siamese network.</p><p>Using multilevel features and channel attention bring gain. The last three rows in <ref type="table">Table 1</ref> show how each component improves the tracking performance. Directly using multilevel features is slightly helpful, but there lacks a mechanism to balance the features of different abstraction levels. We find that the attention module plays an important role. It effectively balances the intra-layer and inter-layer channel importance and achieves significant gain. <ref type="figure" target="#fig_5">Fig.4</ref> visualizes the channel weights for the two convolutional layers of video sequence david and bolt. We have used a Sigmoid function with bias 0.5 so that the weights are in range [0.5, 1.5]. First, we observe that the weight distributions are quite different for conv4 and conv5. This provides hints why the attention module has a larger impact on models using multilevel features. Second, the weight distributions for conv4 are quite different for the two videos.  <ref type="figure">Figure 5</ref>. The precision plots and success plots over three OTB benchmarks. Curves and numbers are generated with OTB toolkit.</p><p>The attention module tends to suppress more channels from conv4 for video bolt.</p><p>Separate vs. joint training. We have claimed that the two branches in SA-Siam should be separately trained. In order to support this statement, we tried to jointly train the two branches (without multilevel features or attention). As we anticipated, the performance is not as good as the separate-training model. The AUC and precision of the joint training model on OTB-2013/50/100 benchmarks are (0.630, 0.831), (0.546,0.739), (0.620, 0.819), respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with State-of-the-Arts</head><p>We compare SA-Siam with the state-of-the-art real-time trackers on both OTB and VOT benchmarks. Convention- ally, a tracking speed beyond 25fps is considered real-time.</p><p>Our tracker runs at 50fps. OTB benchmarks: We compare SA-Siam with BACF <ref type="bibr" target="#b16">[17]</ref>, PTAV <ref type="bibr" target="#b9">[10]</ref>, DSiamM <ref type="bibr" target="#b10">[11]</ref>, EAST <ref type="bibr" target="#b14">[15]</ref>, Staple <ref type="bibr" target="#b1">[2]</ref> ,SiamFC <ref type="bibr" target="#b2">[3]</ref>, CFNet <ref type="bibr" target="#b36">[37]</ref>, LMCF <ref type="bibr" target="#b33">[34]</ref> and LCT <ref type="bibr" target="#b26">[27]</ref> on OTB 2013/50/100 benchmarks. The precision plots and success plots of one path evaluation (OPE) are shown in <ref type="figure">Fig.5</ref>. More results are summarized in <ref type="table" target="#tab_1">Table 3</ref>. The comparison shows that SA-Siam achieves the best performance among these real-time trackers on all three OTB benchmarks.</p><p>Note that the plots in <ref type="figure">Fig.5</ref> are obtained by running the OTB evaluation kit on the tracking result file released by authors. The numbers for BACF <ref type="bibr" target="#b16">[17]</ref> is different from the paper because the authors only evaluate 50 videos (no jogging-2) for OTB-2013. Also, they use per-video average instead of per-frame average to compute the numbers.</p><p>VOT2015 benchmark: VOT2015 is one of the most popular object tracking benchmarks.</p><p>Therefore, we also report the performance of some of the best nonreal-time trackers as a reference, including MDNet <ref type="bibr" target="#b28">[29]</ref>, DeepSRDCF <ref type="bibr" target="#b6">[7]</ref>, SRDCF <ref type="bibr" target="#b7">[8]</ref> and EBT <ref type="bibr" target="#b38">[39]</ref>. Also, SA-Siam is compared with BACF <ref type="bibr" target="#b16">[17]</ref>, EAST <ref type="bibr" target="#b14">[15]</ref>, Staple <ref type="bibr" target="#b1">[2]</ref> and SiamFC <ref type="bibr" target="#b2">[3]</ref>. <ref type="table" target="#tab_2">Table 4</ref> shows the raw scores generated from VOT toolkit and the speed (FPS) of each tracker. SA-Siam achieves the highest accuracy among all real-time trackers. While BACF has the same accuracy as SA-Siam, SA-Siam is more robust.</p><p>VOT2016 benchmark: We compare SA-Siam with the top real-time trackers in VOT2016 challenge <ref type="bibr" target="#b19">[20]</ref> and Staple <ref type="bibr" target="#b1">[2]</ref> , SiamFC <ref type="bibr" target="#b2">[3]</ref> and ECOhc <ref type="bibr" target="#b5">[6]</ref>. The results are shown in <ref type="table" target="#tab_3">Table 5</ref>. On this benchmark, SA-Siam appears to be the most robust real-time tracker. The A and EAO scores are also among the top three.</p><p>VOT2017 benchmark: <ref type="table">Table 6</ref> shows the comparison of SA-Siam with ECOhc <ref type="bibr" target="#b5">[6]</ref>, CSRDCF++ <ref type="bibr" target="#b24">[25]</ref>, UCT <ref type="bibr" target="#b39">[40]</ref>, SiamFC <ref type="bibr" target="#b2">[3]</ref> and Staple <ref type="bibr" target="#b1">[2]</ref> on the VOT2017 benchmark. Different trackers have different advantages, but SA-Siam is always among the top tier over all the evaluation metrics.</p><p>More qualitative results are given in <ref type="figure">Fig. 6</ref>. In the very challenging motorrolling and skiing sequence, SA-Siam is able to track correctly while most of others fail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have presented the design of a twofold Siamese network for real-time object tracking. We make use of the complementary semantic features and appearance features, but do not fuse them at early stage. The resulting  <ref type="table">Table 6</ref>. Comparisons between SA-Siam and state-of-the-art realtime trackers on VOT2017 benchmark. Accuracy, normalized weighted mean of robustness score, EAO and speed (FPS) are listed.</p><p>tracker greatly benefits from the heterogeneity of the two branches. In addition, we have designed a channel attention module to achieve target adaptation. As a result, the proposed SA-Siam outperforms other real-time trackers by a large margin on the OTB benchmarks. It also performs favorably on the series of VOT benchmarks. In the future, we plan to continue exploring the effective fusion of deep features in object tracking task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Comparing the tracking results of SiamFC and our tracker. Thanks to the semantic features, our tracker successfully follows the target object in case of shooting angle change or scale change, when SiamFC fails.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>The architecture of the proposed twofold SA-Siam network. A-Net indicates the appearance network. The network and data structures connected with dotted lines are exactly the same as SiamFC<ref type="bibr" target="#b2">[3]</ref>. S-Net indicates the semantic network. Features from the last two convolution layers are extracted. The channel attention module determines the weight for each feature channel based on both target and context information. The appearance branch and the semantic branch are separately trained and not combined until testing time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Channel-wise attention generates the weighing coefficient ξi for channel i through max pooling and multilayer perceptron (MLP). loss function L(·):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>0.811 0.520 0.718 0.585 0.790 0.607 0.821 0.517 0.715 0.588 0.791 0.649 0.862 0.583 0.792 0.637 0.843 0.656 0.865 0.581 0.778 0.641 0.841 0.650 0.861 0.591 0.803 0.642 0.849 0.676 0.894 0.610 0.823 0.656 0.864</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Visualizing the channel weights output by attention module for video david and bolt. Channels are sorted according to the weights. There is no correspondence between channel numbers for the two videos.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Comparison of state-of-the-art real-time trackers on OTB benchmarks. * The reported AUC scores of BACF on OTB-2013 (excluding jogging-2) and OTB-100 are 0.678 and 0.630, respectively. They have used a slightly different measure.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="11">Tracker SA-Siam BACF PTAV ECOhc DSiamM EAST Staple SiamFC CFNet LMCF</cell><cell>LCT</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(ours)</cell><cell></cell><cell>[17]</cell><cell>[10]</cell><cell>[6]</cell><cell></cell><cell>[11]</cell><cell>[15]</cell><cell>[2]</cell><cell>[3]</cell><cell>[37]</cell><cell>[34]</cell><cell>[27]</cell></row><row><cell cols="3">OTB-2013</cell><cell>AUC Prec.</cell><cell cols="3">0.677 0.896</cell><cell></cell><cell cols="2">0.656* 0.663 0.859 0.895</cell><cell cols="2">0.652 0.874</cell><cell>0.656 0.891</cell><cell>0.638 -</cell><cell>0.593 0.782</cell><cell>0.607 0.809</cell><cell>0.611 0.807</cell><cell>0.628 0.842</cell><cell>0.628 0.848</cell></row><row><cell cols="3">OTB-50</cell><cell>AUC Prec.</cell><cell cols="3">0.610 0.823</cell><cell></cell><cell>0.570 0.768</cell><cell>0.581 0.806</cell><cell cols="2">0.592 0.814</cell><cell>--</cell><cell>--</cell><cell>0.507 0.684</cell><cell>0.516 0.692</cell><cell>0.530 0.702</cell><cell>0.533 0.730</cell><cell>0.492 0.691</cell></row><row><cell cols="3">OTB-100</cell><cell>AUC Prec.</cell><cell cols="3">0.657 0.865</cell><cell></cell><cell cols="2">0.621* 0.635 0.822 0.849</cell><cell cols="2">0.643 0.856</cell><cell>--</cell><cell>0.629 -</cell><cell>0.578 0.784</cell><cell>0.582 0.771</cell><cell>0.568 0.748</cell><cell>0.580 0.789</cell><cell>0.562 0.762</cell></row><row><cell></cell><cell></cell><cell></cell><cell>FPS</cell><cell cols="2">50</cell><cell></cell><cell></cell><cell>35</cell><cell>25</cell><cell>60</cell><cell></cell><cell>25</cell><cell>159</cell><cell>80</cell><cell>86</cell><cell>75</cell><cell>85</cell><cell>27</cell></row><row><cell></cell><cell>1</cell><cell cols="3">Precision plots of OPE − OTB2013</cell><cell></cell><cell></cell><cell>1</cell><cell cols="3">Success plots of OPE − OTB2013</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Precision</cell><cell>0.4 0.6 0.2</cell><cell></cell><cell></cell><cell cols="2">SA−SIAM [0.896] PTAV [0.895] ECOhc [0.874] BACF [0.859] LCT [0.848] LMCF [0.842] SiamFC [0.809] CFNet [0.807]</cell><cell>Success rate</cell><cell>0.4 0.6 0.2</cell><cell cols="2">SA−SIAM [0.677] PTAV [0.663] BACF [0.656] ECOhc [0.652] LCT [0.628] LMCF [0.628] CFNet [0.611] SiamFC [0.607]</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0 0</cell><cell cols="3">10 Location error threshold 20 30 40 STAPLE [0.782]</cell><cell>50</cell><cell></cell><cell>0 0</cell><cell cols="2">Overlap threshold 0.4 0.6 STAPLE [0.593] 0.2</cell><cell>0.8</cell><cell>1</cell><cell></cell></row><row><cell></cell><cell>1</cell><cell cols="3">Precision plots of OPE − OTB50</cell><cell></cell><cell></cell><cell>1</cell><cell cols="3">Success plots of OPE − OTB50</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Precision</cell><cell>0.4 0.6 0.2</cell><cell></cell><cell></cell><cell cols="2">SA−SIAM [0.823] ECOhc [0.814] PTAV [0.806] BACF [0.768] LMCF [0.730] CFNet [0.702] SiamFC [0.692] LCT [0.691]</cell><cell>Success rate</cell><cell>0.4 0.6 0.2</cell><cell cols="2">SA−SIAM [0.610] ECOhc [0.592] PTAV [0.581] BACF [0.570] LMCF [0.533] CFNet [0.530] SiamFC [0.516] STAPLE [0.507]</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0 0</cell><cell cols="3">10 Location error threshold 20 30 40 STAPLE [0.684]</cell><cell>50</cell><cell></cell><cell>0 0</cell><cell cols="2">Overlap threshold 0.4 0.6 LCT [0.492] 0.2</cell><cell>0.8</cell><cell>1</cell><cell></cell></row><row><cell></cell><cell>1</cell><cell cols="3">Precision plots of OPE − OTB100</cell><cell></cell><cell></cell><cell>1</cell><cell cols="3">Success plots of OPE − OTB100</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Precision</cell><cell>0.4 0.6 0.2</cell><cell></cell><cell></cell><cell cols="2">SA−SIAM [0.865] ECOhc [0.856] PTAV [0.849] BACF [0.822] LMCF [0.789] STAPLE [0.784] SiamFC [0.771] LCT [0.762]</cell><cell>Success rate</cell><cell>0.4 0.6 0.2</cell><cell cols="2">SA−SIAM [0.657] ECOhc [0.643] PTAV [0.635] BACF [0.621] SiamFC [0.582] LMCF [0.580] STAPLE [0.578] CFNet [0.568]</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0 0</cell><cell cols="3">10 Location error threshold 20 30 40 CFNet [0.748]</cell><cell>50</cell><cell></cell><cell>0 0</cell><cell cols="2">Overlap threshold 0.4 0.6 LCT [0.562] 0.2</cell><cell>0.8</cell><cell>1</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Comparisons between SA-Siam and state-of-the-art realtime trackers on VOT2015 benchmark. Four top non-real-time trackers in the challenge are also included as a reference.</figDesc><table><row><cell>Tracker</cell><cell>A</cell><cell>R</cell><cell cols="2">EAO FPS</cell></row><row><cell>MDNet</cell><cell cols="3">0.60 0.69 0.38</cell><cell>1</cell></row><row><cell cols="5">DeepSRDCF 0.56 1.05 0.32 &lt; 1</cell></row><row><cell>EBT</cell><cell cols="3">0.47 1.02 0.31</cell><cell>4.4</cell></row><row><cell>SRDCF</cell><cell cols="3">0.56 1.24 0.29</cell><cell>5</cell></row><row><cell>BACF</cell><cell cols="2">0.59 1.56</cell><cell>-</cell><cell>35</cell></row><row><cell>EAST</cell><cell cols="3">0.57 1.03 0.34</cell><cell>159</cell></row><row><cell>Staple</cell><cell cols="3">0.57 1.39 0.30</cell><cell>80</cell></row><row><cell>SiamFC</cell><cell cols="3">0.55 1.58 0.29</cell><cell>86</cell></row><row><cell>SA-Siam</cell><cell cols="3">0.59 1.26 0.31</cell><cell>50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Qualitative results comparing SA-Siam with other trackers on sequences bolt, soccer, matrix, human3, skiing and motorrolling. SA-Siam tracks accurately and robustly in these hard cases. In the very challenging skiing and motorrolling sequence, SA-Siam can always track to the target when most of other trackers fail. Comparisons between SA-Siam and state-of-the-art realtime trackers on VOT2016 benchmark. Raw scores generated from VOT toolkit are listed.</figDesc><table><row><cell>Bolt</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>soccer</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Matrix</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Human3</cell><cell></cell><cell></cell></row><row><cell>Skiing</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MotorRolling</cell><cell></cell><cell></cell></row><row><cell>SA-Siam</cell><cell cols="2">SiamFC</cell><cell>Staple</cell><cell>BACF</cell><cell>LCT</cell><cell>LMCF</cell><cell>CFNet</cell><cell>ECOhc</cell><cell>PTAV</cell></row><row><cell>Figure 6. Tracker</cell><cell>A</cell><cell>R</cell><cell>EAO</cell><cell>FPS</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ECOhc</cell><cell cols="3">0.54 1.19 0.3221</cell><cell>60</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Staple</cell><cell cols="3">0.54 1.42 0.2952</cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">STAPLE+ 0.55 1.31 0.2862 &gt; 25</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SSKCF</cell><cell cols="4">0.54 1.43 0.2771 &gt; 25</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SiamRN</cell><cell cols="4">0.55 1.36 0.2766 &gt; 25</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DPT</cell><cell cols="4">0.49 1.85 0.2358 &gt; 25</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SiamAN</cell><cell cols="3">0.53 1.91 0.2352</cell><cell>86</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NSAMF</cell><cell>0.5</cell><cell cols="3">1.25 0.2269 &gt; 25</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CCCT</cell><cell cols="4">0.44 1.84 0.2230 &gt; 25</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GCF</cell><cell cols="4">0.51 1.57 0.2179 &gt; 25</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SA-Siam</cell><cell cols="3">0.54 1.08 0.2911</cell><cell>50</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Staple: Complementary learners for real-time tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Golodetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Miksik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1401" to="1409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fully-convolutional siamese networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision Workshop</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="850" to="865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Visual tracking using attention-modulated disintegration and integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Demiris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Young</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4321" to="4330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Attentional correlation filter network for adaptive visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Demiris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Young</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Eco: Efficient convolution operators for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Shahbaz</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional features for correlation filter based visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Shahbaz</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning spatially regularized correlation filters for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Shahbaz</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4310" to="4318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Beyond correlation filters: Learning continuous convolution operators for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="472" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Parallel tracking and verifying: A framework for real-time and high accuracy visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning dynamic siamese network for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Branchout: Regularization for online ensemble tracking with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to track at 100 fps with deep regression networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="749" to="765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01507</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning policies for adaptive tracking with deep feature cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.08660</idno>
		<title level="m">Ratm: recurrent attentive tracking model</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning background-aware correlation filters for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Galoogahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fagg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09262</idno>
		<title level="m">Hierarchical attentive recurrent tracking</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The visual object tracking vot2015 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision workshops</title>
		<meeting>the IEEE international conference on computer vision workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The visual object tracking vot2016 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision Workshop</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The visual object tracking vot2015 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision workshops</title>
		<meeting>the IEEE international conference on computer vision workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Classification and representation joint learning via deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 26th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2215" to="2221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Encoding color information for visual tracking: Algorithms and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Blasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5630" to="5644" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Discriminative correlation filter with channel and spatial reliability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lukezic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zajc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hierarchical convolutional features for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3074" to="3082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Long-term correlation tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5388" to="5396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Modeling and propagating cnns in a tree structure for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.07242</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning multi-domain convolutional neural networks for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hedged deep tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4303" to="4311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Siamese instance search for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1420" to="1429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Stct: Sequentially training convolutional networks for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Large margin object tracking with circulant feature maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Online object tracking: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2411" to="2418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<title level="m">Object tracking benchmark. IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1834" to="1848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">End-to-end learning of driving models from large-scale video datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Recurrent filter learning for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Beyond local search: Tracking objects everywhere with instance-specific proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="943" to="951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Uct: Learning unified convolutional networks for real-time visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
