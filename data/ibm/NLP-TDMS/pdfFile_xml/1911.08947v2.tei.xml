<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Real-time Scene Text Detection with Differentiable Binarization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Liao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<addrLine>2 Megvii</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyi</forename><surname>Wan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
							<email>kchen@sjtu.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Onlyou Tech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
							<email>xbai@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<addrLine>2 Megvii</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Real-time Scene Text Detection with Differentiable Binarization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, segmentation-based methods are quite popular in scene text detection, as the segmentation results can more accurately describe scene text of various shapes such as curve text. However, the post-processing of binarization is essential for segmentation-based detection, which converts probability maps produced by a segmentation method into bounding boxes/regions of text. In this paper, we propose a module named Differentiable Binarization (DB), which can perform the binarization process in a segmentation network. Optimized along with a DB module, a segmentation network can adaptively set the thresholds for binarization, which not only simplifies the post-processing but also enhances the performance of text detection. Based on a simple segmentation network, we validate the performance improvements of DB on five benchmark datasets, which consistently achieves stateof-the-art results, in terms of both detection accuracy and speed. In particular, with a light-weight backbone, the performance improvements by DB are significant so that we can look for an ideal tradeoff between detection accuracy and efficiency. Specifically, with a backbone of ResNet-18, our detector achieves an F-measure of 82.8, running at 62 FPS, on the MSRA-TD500 dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>In recent years, reading text in scene images has become an active research area, due to its wide practical applications such as image/video understanding, visual search, automatic driving, and blind auxiliary.</p><p>As a key component of scene text reading, scene text detection that aims to localize the bounding box or region of each text instance is still a challenging task, since scene text is often with various scales and shapes, including horizontal, multi-oriented and curved text. Segmentationbased scene text detection has attracted a lot of attention recently, as it can describe the text of various shapes, benefiting from its prediction results at the pixel-level. However, most segmentation-based methods require complex CRAFT <ref type="formula" target="#formula_1">(2019)</ref> TextSnake <ref type="formula" target="#formula_1">(2018)</ref> Corner <ref type="formula" target="#formula_1">(2018)</ref> RRD <ref type="formula" target="#formula_1">(2018)</ref> PixelLink <ref type="formula" target="#formula_1">(2018)</ref> SegLink <ref type="formula" target="#formula_1">(2017)</ref> DB-ResNet-50 DB-ResNet-18 <ref type="figure">Figure 1</ref>: The comparisons of several recent scene text detection methods on the MSRA-TD500 dataset, in terms of both accuracy and speed. Our method achieves the ideal tradeoff between effectiveness and efficiency.</p><p>post-processing for grouping the pixel-level prediction results into detected text instances, resulting in a considerable time cost in the inference procedure. Take two recent stateof-the-art methods for scene text detection as examples:</p><p>PSENet <ref type="bibr" target="#b30">(Wang et al. 2019a)</ref> proposed the post-processing of progressive scale expansion for improving the detection accuracies; Pixel embedding in <ref type="bibr" target="#b28">(Tian et al. 2019</ref>) is used for clustering the pixels based on the segmentation results, which has to calculate the feature distances among pixels. Most existing detection methods use the similar postprocessing pipeline as shown in <ref type="figure" target="#fig_1">Fig. 2</ref> (following the blue arrows): Firstly, they set a fixed threshold for converting the probability map produced by a segmentation network into a binary image; Then, some heuristic techniques like pixel clustering are used for grouping pixels into text instances. Alternatively, our pipeline (following the red arrows in <ref type="figure" target="#fig_1">Fig. 2)</ref> aims to insert the binarization operation into a segmentation network for joint optimization. In this manner, the threshold value at every place of an image can be adaptively predicted, which can fully distinguish the pixels from the foreground and background. However, the standard binarization function is not differentiable, we instead present an approximate function for binarization called Differentiable Binarization (DB), which is fully differentiable when train-  ing it along with a segmentation network.</p><p>The major contribution in this paper is the proposed DB module that is differentiable, which makes the process of binarization end-to-end trainable in a CNN. By combining a simple network for semantic segmentation and the proposed DB module, we proposed a robust and fast scene text detector. Observed from the performance evaluation of using the DB module, we discover that our detector has several prominent advantages over the previous state-of-the-art segmentation-based approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Our method achieves consistently better performances on</head><p>five benchmark datasets of scene text, including horizontal, multi-oriented and curved text.</p><p>2. Our method performs much faster than the previous leading methods, as DB can provide a highly robust binarization map, significantly simplifying the post-processing.</p><p>3. DB works quite well when using a light-weight backbone, which significantly enhances the detection performance with the backbone of ResNet-18.</p><p>4. As DB can be removed in the inference stage without sacrificing the performance, there is no extra memory/time cost for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Recent scene text detection methods can be roughly classified into two categories: Regression-based methods and segmentation-based methods.</p><p>Regression-based methods are a series of models which directly regress the bounding boxes of the text instances. TextBoxes <ref type="bibr" target="#b13">(Liao et al. 2017)</ref> modified the anchors and the scale of the convolutional kernels based on SSD ) for text detection. TextBoxes++  and DMPNet <ref type="bibr" target="#b17">(Liu and Jin 2017)</ref> applied quadrilaterals regression to detect multi-oriented text. SSTD <ref type="bibr" target="#b8">(He et al. 2017a)</ref> proposed an attention mechanism to roughly identifies text regions. RRD ) decoupled the classification and regression by using rotation-invariant features for classification and rotation-sensitive features for regression, for better effect on multi-oriented and long text instances. EAST <ref type="bibr" target="#b41">(Zhou et al. 2017)</ref> and DeepReg <ref type="bibr" target="#b9">(He et al. 2017b</ref>) are anchor-free methods, which applied pixel-level regression for multi-oriented text instances. SegLink <ref type="bibr" target="#b26">(Shi, Bai, and Belongie 2017)</ref> regressed the segment bounding boxes and predicted their links, to deal with long text instances. DeRPN <ref type="bibr" target="#b33">(Xie et al. 2019b</ref>) proposed a dimensiondecomposition region proposal network to handle the scale problem in scene text detection. Regression-based methods usually enjoy simple post-processing algorithms (e.g. nonmaximum suppression). However, most of them are limited to represent accurate bounding boxes for irregular shapes, such as curved shapes. Segmentation-based methods usually combine pixel-level prediction and post-processing algorithms to get the bounding boxes. <ref type="bibr" target="#b39">(Zhang et al. 2016</ref>) detected multi-oriented text by semantic segmentation and MSER-based algorithms.</p><p>Text border is used in <ref type="bibr" target="#b35">(Xue, Lu, and Zhan 2018)</ref> to split the text instances, Mask TextSpotter <ref type="bibr" target="#b23">(Lyu et al. 2018a;</ref><ref type="bibr" target="#b15">Liao et al. 2019</ref>) detected arbitrary-shape text instances in an instance segmentation manner based on Mask R-CNN. PSENet <ref type="bibr" target="#b30">(Wang et al. 2019a)</ref> proposed progressive scale expansion by segmenting the text instances with different scale kernel. Pixel embedding is proposed in <ref type="bibr" target="#b28">(Tian et al. 2019)</ref> to cluster the pixels from the segmentation results. PSENet <ref type="bibr" target="#b30">(Wang et al. 2019a</ref>) and SAE <ref type="bibr" target="#b28">(Tian et al. 2019)</ref> proposed new post-processing algorithms for the segmentation results, resulting in lower inference speed. Instead, our method focus on improving the segmentation results by including the binarization process into the training period, without the loss of the inference speed.</p><p>Fast scene text detection methods focus on both the accuracy and the inference speed. TextBoxes <ref type="bibr" target="#b13">(Liao et al. 2017)</ref>, TextBoxes++ , SegLink <ref type="bibr" target="#b26">(Shi, Bai, and Belongie 2017)</ref>, and RRD  achieved fast text detection by following the detection architecture of SSD . EAST <ref type="bibr" target="#b41">(Zhou et al. 2017)</ref> proposed to apply PVANet <ref type="bibr" target="#b12">(Kim et al. 2016)</ref> to improve its speed. Most of them can not deal with text instances of irregular shapes, such as curved shape. Compared to the previous fast scene text detectors, our method not only runs faster but also can detect text instances of arbitrary shapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology</head><p>The architecture of our proposed method is shown in <ref type="figure">Fig. 3</ref>. Firstly, the input image is fed into a feature-pyramid backbone. Secondly, the pyramid features are up-sampled to the same scale and cascaded to produce feature F . Then, feature F is used to predict both the probability map (P ) and the threshold map (T ). After that, the approximate binary map (B) is calculated by P and F . In the training period, the supervision is applied on the probability map, the threshold map, and the approximate binary map, where the probability map and the approximate binary map share the same supervision. In the inference period, the bounding boxes can be obtained easily from the approximate binary map or the probability map by a box formulation module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Binarization</head><p>Standard binarization Given a probability map P ∈ R H×W produced by a segmentation network, where H and W indicate the height and width of the map, it is essential to convert it to a binary map P ∈ R H×W , where pixels with  <ref type="figure">Figure 3</ref>: Architecture of our proposed method, where "pred" consists of a 3×3 convolutional operator and two de-convolutional operators with stride 2. The "1/2", "1/4", ... and "1/32" indicate the scale ratio compared to the input image. value 1 is considered as valid text areas. Usually, this binarization process can be described as follows:</p><formula xml:id="formula_0">+ + + CON-CAT conv conv,</formula><formula xml:id="formula_1">B i,j = 1 if P i,j &gt;= t, 0 otherwise.<label>(1)</label></formula><p>where t is the predefined threshold and (i, j) indicates the coordinate point in the map.</p><p>Differentiable binarization The standard binarization described in Eq. 1 is not differentiable. Thus, it can not be optimized along with the segmentation network in the training period. To solve this problem, we propose to perform binarization with an approximate step function:</p><formula xml:id="formula_2">B i,j = 1 1 + e −k(Pi,j −Ti,j )<label>(2)</label></formula><p>whereB is the approximate binary map; T is the adaptive threshold map learned from the network; k indicates the amplifying factor. k is set to 50 empirically. This approximate binarization function behaves similar to the standard binarization function (see <ref type="figure" target="#fig_2">Fig 4)</ref> but is differentiable thus can be optimized along with the segmentation network in the training period. The differentiable binarization with adaptive thresholds can not only help differentiate text regions from the background, but also separate text instances which are closely jointed. Some examples are illustrated in <ref type="figure" target="#fig_4">Fig.7</ref>.</p><p>The reasons that DB improves the performance can be explained by the backpropagation of the gradients. Lets take the binary cross-entropy loss as an example. Define f (x) = 1 1+e −kx as our DB function, where x = P i,j − T i,j . Then the losses l + for positive labels and l − for negative labels are:</p><formula xml:id="formula_3">l + = − log 1 1 + e −kx l − = − log(1 − 1 1 + e −kx )<label>(3)</label></formula><p>We can easily compute the differential of the losses with the chain rule:</p><formula xml:id="formula_4">∂l + ∂x = −kf (x)e −kx ∂l − ∂x = kf (x)<label>(4)</label></formula><p>The derivatives of l + and l − are also shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. We can perceive from the differential that (1) The gradient is augmented by the amplifying factor k;</p><p>(2) The amplification of gradient is significant for most of the wrongly predicted region (x &lt; 0 for L + ; x &gt; 0 for L − ), thus facilitating the optimization and helping to produce more distinctive predictions. Moreover, as x = P i,j − T i,j , the gradient of P is effected and rescaled between the foreground and the background by T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adaptive threshold</head><p>The threshold map in <ref type="figure">Fig. 1</ref> is similar to the text border map in (Xue, Lu, and Zhan 2018) from appearance. However, the motivation and usage of the threshold map are different from the text border map. The threshold map with/without supervision is visualized in <ref type="figure" target="#fig_3">Fig. 6</ref>. The threshold map would highlight the text border region even without supervision for the threshold map. This indicates that the border-like threshold map is beneficial to the final results. Thus, we apply borderlike supervision on the threshold map for better guidance.</p><p>An ablation study about the supervision is discussed in the Experiments section. For the usage, the text border map in <ref type="bibr" target="#b35">(Xue, Lu, and Zhan 2018)</ref> is used to split the text instances while our threshold map is served as thresholds for the binarization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deformable convolution</head><p>Deformable convolution <ref type="bibr">(Dai et al. 2017;</ref><ref type="bibr" target="#b42">Zhu et al. 2019)</ref> can provide a flexible receptive field for the model, which is especially beneficial to the text instances of extreme aspect ratios. Following <ref type="bibr" target="#b42">(Zhu et al. 2019)</ref>, modulated deformable convolutions are applied in all the 3 × 3 convolutional layers in stages conv3, conv4, and conv5 in the ResNet-18 or ResNet-50 backbone <ref type="bibr" target="#b6">(He et al. 2016a</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Label generation</head><p>probability map threshold map image polygon <ref type="figure">Figure 5</ref>: Label generation. The annotation of text polygon is visualized in red lines. The shrunk and dilated polygon are displayed in blue and green lines, respectively.</p><p>The label generation for the probability map is inspired by PSENet <ref type="bibr" target="#b30">(Wang et al. 2019a)</ref>. Given a text image, each polygon of its text regions is described by a set of segments:</p><formula xml:id="formula_5">G = {S k } n k=1 (5)</formula><p>n is the number of vertexes, which may be different in different datasets, e.g, 4 for the ICDAR 2015 dataset <ref type="bibr" target="#b11">(Karatzas et al. 2015)</ref> and 16 for the CTW1500 dataset <ref type="bibr" target="#b20">(Liu et al. 2019a</ref>). Then the positive area is generated by shrinking the polygon G to G s using the Vatti clipping algorithm <ref type="bibr" target="#b29">(Vati 1992)</ref>. The offset D of shrinking is computed from the perimeter L and area A of the original polygon:</p><formula xml:id="formula_6">D = A(1 − r 2 ) L<label>(6)</label></formula><p>where r is the shrink ratio, set to 0.4 empirically. With a similar procedure, we can generate labels for the threshold map. Firstly the text polygon G is dilated with the same offset D to G d . We consider the gap between G s and G d as the border of the text regions, where the label of the threshold map can be generated by computing the distance to the closest segment in G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optimization</head><p>The loss function L can be expressed as a weighted sum of the loss for the probability map L s , the loss for the binary map L b , and the loss for the threshold map L t :</p><formula xml:id="formula_7">L = L s + α × L b + β × L t<label>(7)</label></formula><p>where L s is the loss for the probability map and L b is the loss for the binary map. According to the numeric values of the losses, α and β are set to 1.0 and 10 respectively. We apply a binary cross-entropy (BCE) loss for both L s and L b . To overcome the unbalance of the number of positives and negatives, hard negative mining is used in the BCE loss by sampling the hard negatives.</p><formula xml:id="formula_8">L s = L b = i∈S l y i log x i + (1 − y i ) log (1 − x i )<label>(8)</label></formula><p>S l is the sampled set where the ratio of positives and negatives is 1 : 3. L t is computed as the sum of L1 distances between the prediction and label inside the dilated text polygon G d :</p><formula xml:id="formula_9">L t = i∈R d |y * i − x * i |<label>(9)</label></formula><p>where R d is a set of indexes of the pixels inside the dilated polygon G d ; y * is the label for the threshold map.</p><p>In the inference period, we can either use the probability map or the approximate binary map to generate text bounding boxes, which produces almost the same results. For better efficiency, we use the probability map so that the threshold branch can be removed. The box formation process consists of three steps: (1) the probability map/the approximate binary map is firstly binarized with a constant threshold (0.2), to get the binary map; (2)the connected regions (shrunk text regions) are obtained from the binary map; (3) the shrunk regions are dilated with an offset D the Vatti clipping algorithm <ref type="bibr" target="#b29">(Vati 1992)</ref>. D is calculated as</p><formula xml:id="formula_10">D = A × r L<label>(10)</label></formula><p>where A is the area of the shrunk polygon; L is the perimeter of the shrunk polygon; r is set to 1.5 empirically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Datasets</head><p>SynthText (Gupta, Vedaldi, and Zisserman 2016) is a synthetic dataset which consists of 800k images. These images are synthesized from 8k background images. This dataset is only used to pre-train our model. MLT-2017 dataset 1 is a multi-language dataset. It includes  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation details</head><p>For all the models, we first pre-train them with the SynthText dataset for 100k iterations. Then, we finetune the models on the corresponding real-world datasets for 1200 epochs. The training batch size is set to 16. We follow a poly learning rate policy where the learning rate at current iteration equals the initial learning rate multiplying (1 − iter max iter ) power , where the initial learning rate is set to 0.007 and power is 0.9. We use a weight decay of 0.0001 and a momentum of 0.9. The max iter means the maximum iterations, which depends on the maximum epochs.</p><p>The data augmentation for the training data includes: (1) Random rotation with an angle range of (−10 • , 10 • ); (2) Random cropping; (3) Random Flipping. All the processed images are re-sized to 640×640 for better training efficiency.</p><p>In the inference period, we keep the aspect ratio of the test images and re-size the input images by setting a suitable height for each dataset. The inference speed is tested with a batch size of 1, with a single 1080ti GPU in a single thread. The inference time cost consists of the model forward time cost and the post-processing time cost. The post-processing time cost is about 30% of the inference time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation study</head><p>We conduct an ablation study on the MSRA-TD500 dataset and the CTW1500 dataset to show the effectiveness of our proposed differentiable binarization, the deformable convolution, and different backbones. The detailed experimental results are shown in Tab. 1. Differentiable binarization In Tab. 1, we can see that our proposed DB improves the performance significantly for both ResNet-18 and ResNet-50 on the two datasets. For the ResNet-18 backbone, DB achieves 3.7% and 4.9% performance gain in terms of F-measure on the MSRA-TD500 dataset and the CTW1500 dataset. For the ResNet-50 backbone, DB brings 3.2% (on the MSRA-TD500 dataset) and 4.6% (on the CTW1500 dataset) improvements. Moreover, since DB can be removed in the inference period, the speed is the same as the one without DB. Deformable convolution As shown in Tab. 1, the deformable convolution can also brings 1.5 − 5.0 performance gain since it provides a flexible receptive field for the backbone, with small extra time costs. For the MSRA-TD500 dataset, the deformable convolution increase the F-measure by 1.5% (with ResNet-18) and 5.0% (with ResNet-50). For the CTW1500 dataset, 3.6% (with ResNet-18) and 4.9% (with ResNet-50) improvements are achieved by the deformable convolution. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparisons with previous methods</head><p>We compare our proposed method with previous methods on five standard benchmarks, including two benchmarks for curved text, one benchmark for multi-oriented text, and two multi-language benchmarks for long text lines. Some qualitative results are visualized in <ref type="figure" target="#fig_4">Fig. 7</ref>. Curved text detection We prove the shape robustness of our method on two curved text benchmarks (Total-Text and CTW1500). As shown in Tab. 3 and Tab. 4, our method achieves state-of-the-art performance both on accuracy and speed. Specifically, "DB-ResNet-50" outperforms the previous state-of-the-art method by 1.1% and 1.2% on the Total-Text and the CTW1500 dataset. "DB-ResNet-50" runs faster than all previous method and the speed can be further improved by using a ResNet-18 backbone, with a small performance drop. Compared to the recent segmentation-based detector <ref type="bibr" target="#b30">(Wang et al. 2019a)</ref>, which runs 3.9 FPS on Total-Text, "DB-ResNet-50 (800)" is 8.2 times faster and "DB-ResNet-18 (800)" is 12.8 times faster.</p><p>Multi-oriented text detection The ICDAR 2015 dataset is a multi-oriented text dataset that contains lots of small and low-resolution text instances. In Tab. 5, we can see that "DB-ResNet-50 (1152)" achieves the state-of-the-art performance on accuracy. Compared to the previous fastest method <ref type="bibr" target="#b41">(Zhou et al. 2017)</ref>, "DB-ResNet-50 (736)" outperforms it by 7.2% on accuracy and runs twice faster. For "DB-ResNet-18 (736)", the speed can be 48 fps when ResNet-18 is applied to the backbone, with an f-measure of 82.3. Multi-language text detection Our method is robust on multi-language text detection. As shown in Tab. 6 and Tab. 7, "DB-ResNet-50" is superior to previous methods on accuracy and speed. For the accuracy, "DB-ResNet-50" surpasses the previous state-of-the-art method by 1.9% and 3.8% on the MSRA-TD500 and MLT-2017 dataset respectively. For the speed, "DB-ResNet-50" is 3.2 times faster than the previous fastest method ) on the MSRA-TD500 dataset. With a light-weight backbone, "DB-ResNet-18 (736)" achieves a comparative accuracy compared to the previous state-of-the-art method ) (82.8 vs 83.0) and runs at 62 FPS, which is 6.2 times faster than the previous fastest method ), on the MSRA-TD500. The speed can be further accelerated to 82 FPS ("ResNet-18 (512)") by decreasing the input size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitation</head><p>One limitation of our method is that it can not deal with cases "text inside text", which means that a text instance is inside another text instance. Although the shrunk text regions are helpful to the cases that the text instance is not in the center region of another text instance, it fails when the text instance exactly locates in the center region of another text instance. This is a common limitation for segmentationbased scene text detectors.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we have presented a novel framework for detecting arbitrary-shape scene text, which includes the proposed differentiable binarization process (DB) in a segmentation network. The experiments have verified that our method (ResNet-50 backbone) consistently outperforms the state-the-the-art methods on five standard scene text benchmarks, in terms of speed and accuracy. In particular, even with a lightweight backbone (ResNet-18), our method can achieve competitive performance on all the testing datasets with real-time inference speed. In the future, we are interested in extending our method for end-to-end text spotting.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Traditional pipeline (blue flow) and our pipeline (red flow). Dashed arrows are the inference only operators; solid arrows indicate differentiable operators in both training and inference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Illustration of differentiable binarization and its derivative. (a) Numerical comparison of standard binarization (SB) and differentiable binarization (DB). (b) Derivative of l + . (c) Derivative of l − .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>The threshold map with/without supervision. (a) Input image. (b) Probability map. (c) Threshold map without supervision. (d) Threshold map with supervision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Some visualization results on text instances of various shapes, including curved text, multi-oriented text, vertical text, and long text lines. For each unit, the top right is the threshold map; the bottom right is the probability map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>Backbone DConv</cell><cell>DB</cell><cell>P</cell><cell>R</cell><cell cols="2">MSRA-TD500 F</cell><cell>FPS</cell><cell>P</cell><cell>R</cell><cell cols="2">CTW1500 F</cell><cell>FPS</cell></row><row><cell>ResNet-18 ×</cell><cell>×</cell><cell>85.5</cell><cell cols="2">70.8</cell><cell>77.4</cell><cell>66</cell><cell>76.3</cell><cell cols="2">72.8</cell><cell>74.5</cell><cell>59</cell></row><row><cell>ResNet-18</cell><cell>×</cell><cell>86.8</cell><cell cols="2">72.3</cell><cell>78.9</cell><cell>62</cell><cell>80.9</cell><cell cols="2">75.4</cell><cell>78.1</cell><cell>55</cell></row><row><cell>ResNet-18 ×</cell><cell></cell><cell>87.3</cell><cell cols="2">75.8</cell><cell>81.1</cell><cell>66</cell><cell>82.4</cell><cell cols="2">76.6</cell><cell>79.4</cell><cell>59</cell></row><row><cell>ResNet-18</cell><cell></cell><cell>90.4</cell><cell cols="2">76.3</cell><cell>82.8</cell><cell>62</cell><cell>84.8</cell><cell cols="2">77.5</cell><cell>81.0</cell><cell>55</cell></row><row><cell>ResNet-50 ×</cell><cell>×</cell><cell>84.6</cell><cell cols="2">73.5</cell><cell>78.7</cell><cell>40</cell><cell>81.6</cell><cell cols="2">72.9</cell><cell>77.0</cell><cell>27</cell></row><row><cell>ResNet-50</cell><cell>×</cell><cell>90.5</cell><cell cols="2">77.9</cell><cell>83.7</cell><cell>32</cell><cell>86.2</cell><cell cols="2">78.0</cell><cell>81.9</cell><cell>22</cell></row><row><cell>ResNet-50 ×</cell><cell></cell><cell>86.6</cell><cell cols="2">77.7</cell><cell>81.9</cell><cell>40</cell><cell>84.3</cell><cell cols="2">79.1</cell><cell>81.6</cell><cell>27</cell></row><row><cell>ResNet-50</cell><cell></cell><cell>91.5</cell><cell cols="2">79.2</cell><cell>84.9</cell><cell>32</cell><cell>86.9</cell><cell cols="2">80.2</cell><cell>83.4</cell><cell>22</cell></row><row><cell cols="5">9 languages representing 6 different scripts. There are 7,200</cell><cell></cell><cell cols="5">dataset that includes the text of various shapes, including</cell></row><row><cell cols="5">training images, 1,800 validation images and 9,000 testing</cell><cell></cell><cell cols="5">horizontal, multi-oriented, and curved. They are 1255 train-</cell></row><row><cell cols="5">images in this dataset. We use both the training set and the</cell><cell></cell><cell cols="5">ing images and 300 testing images. The text instances are</cell></row><row><cell cols="2">validation set in the finetune period.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">labeled at the word level.</cell><cell></cell><cell></cell></row><row><cell cols="5">ICDAR 2015 dataset (Karatzas et al. 2015) consists of 1000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">training images and 500 testing images, which are captured</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">by Google glasses with a resolution of 720 × 1280. The text</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">instances are labeled at the word level.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">MSRA-TD500 dataset (Yao et al. 2012) is a multi-language</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">dataset that includes English and Chinese. There are 300</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">training images and 200 testing images. The text instances</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">are labeled in the text-line level. Following the previous</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">methods (Zhou et al. 2017; Lyu et al. 2018b; Long et al.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">2018), we include extra 400 training images from HUST-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">TR400 (Yao, Bai, and Liu 2014).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">CTW1500 dataset CTW1500 (Liu et al. 2019a) is a dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">which focuses on the curved text. It consists of 1000 train-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">ing images and 500 testing images. The text instances are</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">annotated in the text-line level.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Total-Text dataset Total-Text (Chng and Chan 2017) is a</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Detection results with different settings. "DConv" indiates deformable convolution. "P", "R", and "F" indicate preci- sion, recall, and f-measure respectively.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Effect of supervising the threshold map on the MLT-2017 dataset. "Thr-Sup" denotes applying supervision on the threshold map.</figDesc><table><row><cell cols="2">Backbone Thr-Sup P</cell><cell>R</cell><cell>F</cell><cell>FPS</cell></row><row><cell>ResNet-18 ×</cell><cell>81.3</cell><cell>63.1</cell><cell>71.0</cell><cell>41</cell></row><row><cell>ResNet-18</cell><cell>81.9</cell><cell>63.8</cell><cell>71.7</cell><cell>41</cell></row><row><cell>ResNet-50 ×</cell><cell>81.5</cell><cell>64.6</cell><cell>72.1</cell><cell>19</cell></row><row><cell>ResNet-50</cell><cell>83.1</cell><cell>67.9</cell><cell>74.7</cell><cell>19</cell></row><row><cell cols="5">Supervision of threshold map Although the threshold</cell></row><row><cell cols="5">maps with/without supervision are similar in appearance,</cell></row><row><cell cols="5">the supervision can bring performance gain. As shown in</cell></row><row><cell cols="5">Tab. 2, the supervision improves 0.7% (ResNet-18) and</cell></row><row><cell cols="4">2.6% (ResNet-50) on the MLT-2017 dataset.</cell><cell></cell></row><row><cell cols="5">Backbone The proposed detector with ResNet-50 backbone</cell></row><row><cell cols="5">achieves better performance than the ResNet-18 but runs</cell></row><row><cell cols="5">slower. Specifically, The best ResNet-50 model outperforms</cell></row><row><cell cols="5">the best ResNet-18 model by 2.1% (on the MSRA-TD500</cell></row><row><cell cols="5">dataset) and 2.4% (on the CTW1500 dataset), with approxi-</cell></row><row><cell>mate double time cost.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="5">: Detection results on the Total-Text dataset. The val-</cell></row><row><cell cols="5">ues in the bracket mean the height of the input images. "*"</cell></row><row><cell cols="5">indicates testing with multiple scales. "MTS" and "PSE" are</cell></row><row><cell cols="3">short for Mask TextSpotter and PSENet.</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>FPS</cell></row><row><cell cols="5">TextSnake (Long et al. 2018) 82.7 74.5 78.4 -</cell></row><row><cell>ATRR (Wang et al. 2019b)</cell><cell cols="4">80.9 76.2 78.5 -</cell></row><row><cell>MTS (Lyu et al. 2018a)</cell><cell cols="4">82.5 75.6 78.6 -</cell></row><row><cell>TextField (Xu et al. 2019)</cell><cell cols="4">81.2 79.9 80.6 -</cell></row><row><cell cols="5">LOMO (Zhang et al. 2019)* 87.6 79.3 83.3 -</cell></row><row><cell>CRAFT (Baek et al. 2019)</cell><cell cols="4">87.6 79.9 83.6 -</cell></row><row><cell>CSE (Liu et al. 2019b)</cell><cell cols="4">81.4 79.1 80.2 -</cell></row><row><cell>PSE-1s (Wang et al. 2019a)</cell><cell cols="4">84.0 78.0 80.9 3.9</cell></row><row><cell>DB-ResNet-18 (800)</cell><cell cols="4">88.3 77.9 82.8 50</cell></row><row><cell>DB-ResNet-50 (800)</cell><cell cols="4">87.1 82.5 84.7 32</cell></row><row><cell cols="5">Table 4: Detection results on CTW1500. The methods with</cell></row><row><cell cols="5">"*" are collected from (Liu et al. 2019a). The values in the</cell></row><row><cell cols="3">bracket mean the height of the input images.</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>FPS</cell></row><row><cell>CTPN*</cell><cell cols="4">60.4 53.8 56.9 7.14</cell></row><row><cell>EAST*</cell><cell cols="4">78.7 49.1 60.4 21.2</cell></row><row><cell>SegLink*</cell><cell cols="4">42.3 40.0 40.8 10.7</cell></row><row><cell cols="5">TextSnake (Long et al. 2018) 67.9 85.3 75.6 1.1</cell></row><row><cell>TLOC (Liu et al. 2019a)</cell><cell cols="4">77.4 69.8 73.4 13.3</cell></row><row><cell>PSE-1s (Wang et al. 2019a)</cell><cell cols="4">84.8 79.7 82.2 3.9</cell></row><row><cell>SAE (Tian et al. 2019)</cell><cell cols="4">82.7 77.8 80.1 3</cell></row><row><cell>Ours-ResNet18 (1024)</cell><cell cols="4">84.8 77.5 81.0 55</cell></row><row><cell>Ours-ResNet50 (1024)</cell><cell cols="4">86.9 80.2 83.4 22</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Detection results on the ICDAR 2015 dataset. The values in the bracket mean the height of the input images. "TB" and "PSE" are short for TextBoxes++ and PSENet.</figDesc><table><row><cell>Method</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>FPS</cell></row><row><cell>CTPN (Tian et al. 2016)</cell><cell cols="4">74.2 51.6 60.9 7.1</cell></row><row><cell>EAST (Zhou et al. 2017)</cell><cell cols="4">83.6 73.5 78.2 13.2</cell></row><row><cell>SSTD (He et al. 2017a)</cell><cell cols="4">80.2 73.9 76.9 7.7</cell></row><row><cell>WordSup (Hu et al. 2017)</cell><cell cols="2">79.3 77</cell><cell cols="2">78.2 -</cell></row><row><cell>Corner (Lyu et al. 2018b)</cell><cell cols="4">94.1 70.7 80.7 3.6</cell></row><row><cell cols="5">TB (Liao, Shi, and Bai 2018) 87.2 76.7 81.7 11.6</cell></row><row><cell>RRD (Liao et al. 2018)</cell><cell cols="2">85.6 79</cell><cell cols="2">82.2 6.5</cell></row><row><cell>MCN (Liu et al. 2018)</cell><cell>72</cell><cell>80</cell><cell>76</cell><cell>-</cell></row><row><cell cols="5">TextSnake (Long et al. 2018) 84.9 80.4 82.6 1.1</cell></row><row><cell>PSE-1s (Wang et al. 2019a)</cell><cell cols="4">86.9 84.5 85.7 1.6</cell></row><row><cell>SPCNet (Xie et al. 2019a)</cell><cell cols="4">88.7 85.8 87.2 -</cell></row><row><cell>LOMO (Zhang et al. 2019)</cell><cell cols="4">91.3 83.5 87.2 -</cell></row><row><cell>CRAFT (Baek et al. 2019)</cell><cell cols="4">89.8 84.3 86.9 -</cell></row><row><cell>SAE(720) (Tian et al. 2019)</cell><cell cols="4">85.1 84.5 84.8 3</cell></row><row><cell>SAE(990) (Tian et al. 2019)</cell><cell cols="4">88.3 85.0 86.6 -</cell></row><row><cell>DB-ResNet-18 (736)</cell><cell cols="4">86.8 78.4 82.3 48</cell></row><row><cell>DB-ResNet-50 (736)</cell><cell cols="4">88.2 82.7 85.4 26</cell></row><row><cell>DB-ResNet-50 (1152)</cell><cell cols="4">91.8 83.2 87.3 12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Detection results on the MSRA-TD500 dataset. The values in the bracket mean the height of the input images.</figDesc><table><row><cell>Method</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>FPS</cell></row><row><cell>(He et al. 2016b)</cell><cell>71</cell><cell>61</cell><cell>69</cell><cell>-</cell></row><row><cell>DeepReg (He et al. 2017b)</cell><cell>77</cell><cell>70</cell><cell>74</cell><cell>1.1</cell></row><row><cell>RRPN (Ma et al. 2018)</cell><cell>82</cell><cell>68</cell><cell>74</cell><cell>-</cell></row><row><cell>RRD (Liao et al. 2018)</cell><cell>87</cell><cell>73</cell><cell>79</cell><cell>10</cell></row><row><cell>MCN (Liu et al. 2018)</cell><cell>88</cell><cell>79</cell><cell>83</cell><cell>-</cell></row><row><cell cols="2">PixelLink (Deng et al. 2018) 83</cell><cell cols="3">73.2 77.8 3</cell></row><row><cell>Corner (Lyu et al. 2018b)</cell><cell cols="4">87.6 76.2 81.5 5.7</cell></row><row><cell cols="5">TextSnake (Long et al. 2018) 83.2 73.9 78.3 1.1</cell></row><row><cell>(Xue, Lu, and Zhan 2018)</cell><cell cols="4">83.0 77.4 80.1 -</cell></row><row><cell>(Xue, Lu, and Zhang 2019)</cell><cell cols="4">87.4 76.7 81.7 -</cell></row><row><cell>CRAFT (Baek et al. 2019)</cell><cell cols="4">88.2 78.2 82.9 8.6</cell></row><row><cell>SAE (Tian et al. 2019)</cell><cell cols="4">84.2 81.7 82.9 -</cell></row><row><cell>DB-ResNet-18 (512)</cell><cell cols="4">85.7 73.2 79.0 82</cell></row><row><cell>DB-ResNet-18 (736)</cell><cell cols="4">90.4 76.3 82.8 62</cell></row><row><cell>DB-ResNet-50 (736)</cell><cell cols="4">91.5 79.2 84.9 32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Detection results on the MLT-2017 dataset. Methods with "*" are collected from<ref type="bibr" target="#b24">(Lyu et al. 2018b</ref>). The images in the MLT-2017 dataset are re-sized to 768 × 1024 in our method. "PSE" is short for PSENet.</figDesc><table><row><cell>Method</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>FPS</cell></row><row><cell>SARI FDU RRPN V1*</cell><cell>71.2</cell><cell>55.5</cell><cell>62.4</cell><cell>-</cell></row><row><cell>Sensetime OCR*</cell><cell>56.9</cell><cell>69.4</cell><cell>62.6</cell><cell>-</cell></row><row><cell>SCUT DLVlab1*</cell><cell>80.3</cell><cell>54.5</cell><cell>65.0</cell><cell>-</cell></row><row><cell>e2e ctc01 multi scale*</cell><cell>79.8</cell><cell>61.2</cell><cell>69.3</cell><cell>-</cell></row><row><cell cols="2">Corner (Lyu et al. 2018b) 83.8</cell><cell>55.6</cell><cell>66.8</cell><cell>-</cell></row><row><cell>PSE (Wang et al. 2019a)</cell><cell>73.8</cell><cell>68.2</cell><cell>70.9</cell><cell>-</cell></row><row><cell>DB-ResNet-18</cell><cell>81.9</cell><cell>63.8</cell><cell>71.7</cell><cell>41</cell></row><row><cell>DB-ResNet-50</cell><cell>83.1</cell><cell>67.9</cell><cell>74.7</cell><cell>19</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://rrc.cvc.uab.es/?ch=8</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Character region awareness for text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9365" to="9374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Total-text: A comprehensive dataset for scene text detection and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Chng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDAR</title>
		<meeting>ICDAR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="935" to="942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pixellink: Detecting scene text via instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Synthetic data for text localisation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Text-attentional convolutional neural network for scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2529" to="2541" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Single shot text detector with regional attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3047" to="3055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep direct regression for multi-oriented scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Wordsup: Exploiting word annotations for character based text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4940" to="4949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Valveny</surname></persName>
		</author>
		<title level="m">ICDAR 2015 competition on robust reading</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Proc. IC-DAR</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">PVANET: deep but lightweight neural networks for real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Roh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Park</surname></persName>
		</author>
		<idno>abs/1608.08021</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Textboxes: A fast text detector with a single deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rotationsensitive regression for oriented scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5909" to="5918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mask textspotter: An end-to-end trainable neural network for spotting text with arbitrary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Textboxes++: A singleshot oriented scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3676" to="3690" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep matching prior network: Toward tighter multi-oriented text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">SSD: single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning markov clustering networks for scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Goh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6936" to="6944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Curved scene text detection via transverse and longitudinal sequence connection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="337" to="345" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Towards robust curve text detection with conditional spatial expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Goh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7269" to="7278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Textsnake: A flexible representation for detecting text of arbitrary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mask textspotter: An end-to-end trainable neural network for spotting text with arbitrary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="67" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multioriented scene text detection via corner localization and region segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7553" to="7563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Arbitrary-oriented scene text detection via rotation proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3111" to="3122" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Detecting oriented text in natural images by linking segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Detecting text in natural image with connectionist text proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning shape-aware embedding for scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4234" to="4243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A generic solution to polygon clipping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Vati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="56" to="64" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Shape robust text detection with progressive scale expansion network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9336" to="9345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Arbitrary shape scene text detection with adaptive text region representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6449" to="6458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Scene text detection with supervised pyramid context network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9038" to="9045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Derpn: Taking a further step toward more general object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9046" to="9053" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Textfield: Learning a deep direction field for irregular scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5566" to="5579" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Accurate scene text detection through border semantics awareness and bootstrapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="355" to="372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">MSR: multi-scale shape regression for scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pro. IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="989" to="995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A unified framework for multioriented text detection and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4737" to="4749" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Detecting texts of arbitrary orientations in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multi-oriented text detection with fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Look more than once: An accurate detector for text of arbitrary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">EAST: an efficient and accurate scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9308" to="9316" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
