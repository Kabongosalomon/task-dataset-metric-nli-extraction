<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PaStaNet: Toward Human Activity Knowledge Engine</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
							<email>yongluli@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xu</surname></persName>
							<email>liangxu@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinpeng</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xijie</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyi</forename><surname>Wang</surname></persName>
							<email>shiywang@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
							<email>lucewu@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PaStaNet: Toward Human Activity Knowledge Engine</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing image-based activity understanding methods mainly adopt direct mapping, i.e. from image to activity concepts, which may encounter performance bottleneck since the huge gap. In light of this, we propose a new path: infer human part states first and then reason out the activities based on part-level semantics. Human Body Part States (PaSta) are fine-grained action semantic tokens, e.g. hand, hold, something , which can compose the activities and help us step toward human activity knowledge engine. To fully utilize the power of PaSta, we build a largescale knowledge base PaStaNet, which contains 7M+ PaSta annotations. And two corresponding models are proposed: first, we design a model named Activity2Vec to extract PaSta features, which aim to be general representations for various activities. Second, we use a PaSta-based Reasoning method to infer activities. Promoted by PaStaNet, our method achieves significant improvements, e.g. 6.4 and 13.9 mAP on full and one-shot sets of HICO in supervised learning, and 3.2 and 4.2 mAP on V-COCO and images-based AVA in transfer learning. Code and data are available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Understanding activity from images is crucial for building an intelligent system. Facilitated by deep learning, great advancements have been made in this field. Recent works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b39">40]</ref> mainly address this high-level cognition task in one-stage, i.e. from pixels to activity concept directly based on instance-level semantics ( <ref type="figure" target="#fig_0">Fig. 1(a)</ref>). This strategy faces performance bottleneck on large-scale benchmarks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">23]</ref>. Understanding activities is difficult for reasons, e.g. long-tail data distribution, complex visual patterns, etc. Moreover, action understanding expects a knowledge engine that can generally support activity related tasks. * Cewu Lu is the corresponding author, member of Qing Yuan Research Institute and MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, China. Thus, for data from another domain and unseen activities, much smaller effort is required for knowledge transfer and adaptation. Additionally, for most cases, we find that only a few key human parts are relevant to the existing actions, the other parts usually carry very few useful clues. Consider the example in <ref type="figure" target="#fig_0">Fig. 1</ref>, we argue that perception in human part-level semantics is a promising path but previously ignored. Our core idea is that human instance actions are composed of fine-grained atomic body part states. This lies in strong relationships with reductionism <ref type="bibr" target="#b9">[10]</ref>. Moreover, the part-level path can help us to pick up discriminative parts and disregard irrelevant ones. Therefore, encoding knowledge from human parts is a crucial step toward human activity knowledge engine. The generic object part states <ref type="bibr" target="#b37">[38]</ref> reveal that the semantic state of an object part is limited. For example, after exhaustively checking on 7M manually labeled body part state samples, we find that there are only about 12 states for "head" in daily life activities, such as "listen to", "eat", "talk to", "inspect", etc. Therefore, in this paper, we exhaustively collect and annotate the possible semantic meanings of human parts in activities to build a large-scale human part knowledge base PaStaNet (PaSta is the abbreviation of Body Part State). Now PaStaNet includes 118 K+ images, 285 K+ persons, 250 K+ interacted objects, 724 K+ activities and 7 M+ human part states. Extensive analysis verifies that PaStaNet can cover most of the the part-level knowledge in general. Using learned PaSta knowledge in transfer learning, we can achieve 3.2, 4.2 and 3.2 improvements on V-COCO <ref type="bibr" target="#b24">[25]</ref>, images-based AVA <ref type="bibr" target="#b22">[23]</ref> and HICO-DET <ref type="bibr" target="#b3">[4]</ref> (Sec. <ref type="bibr">5.4)</ref>.</p><p>Given PaStaNet, we propose two powerful tools to promote the image-based activity understanding: 1) Activ-ity2Vec: With PaStaNet, we convert a human instance into a vector consisting of PaSta representations. Activity2Vec extracts part-level semantic representation via PaSta recognition and combines its language representation. Since PaSta encodes common knowledge of activities, Activ-ity2Vec works as a general feature extractor for both seen and unseen activities. 2) PaSta-R: A Part State Based Reasoning method (PaSta-R) is further presented. We construct a Hierarchical Activity Graph consisting of human instance and part semantic representations, and infer the activities by combining both instance and part level sub-graph states.</p><p>The advantages of our method are two-fold: 1) Reusability and Transferability: PaSta are basic components of actions, their relationship can be in analogy with the amino acid and protein, letter and word, etc. Hence, PaSta are reusable, e.g., hand, hold, something is shared by various actions like "hold horse" and "eat apple". Therefore, we get the capacity to describe and differentiate plenty of activities with a much smaller set of PaSta, i.e. one-time labeling and transferability. For fewshot learning, reusability can greatly alleviate its learning difficulty. Thus our approach shows significant improvements, e.g. we boost 13.9 mAP on one-shot sets of HICO <ref type="bibr" target="#b2">[3]</ref>. 2) Interpretability: we obtain not only more powerful activity representations, but also better interpretation. When the model predicts what a person is doing, we can easily know the reasons: what the body parts are doing.</p><p>In conclusion, we believe PaStaNet will function as a human activity knowledge engine. Our main contributions are: 1) We construct PaStaNet, the first large-scale activity knowledge base with fine-grained PaSta annotations. 2) We propose a novel method to extract part-level activity representation named Activity2Vec and a PaSta-based reasoning method. 3) In supervised and transfer learning, our method achieves significant improvements on large-scale activity benchmarks, e.g. 6.4 (16%), 5.6 (33%) mAP improvements on HICO <ref type="bibr" target="#b2">[3]</ref> and HICO-DET <ref type="bibr" target="#b3">[4]</ref> respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Activity Understanding. Benefited by deep learning and large-scale datasets, image-based <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b0">1]</ref> or videobased <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b66">67]</ref> activity understanding has achieved huge improvements recently. Human activities have a hierarchical structure and include diverse verbs, so it is hard to define an explicit organization for their cate-gories. Existing datasets <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b24">25]</ref> often have a large difference in definition, thus transferring knowledge from one dataset to another is ineffective. Meanwhile, plenty of works have been proposed to address the activity understanding <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b53">54]</ref>. There are holistic body-level approaches <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b6">7]</ref>, body part-based methods <ref type="bibr" target="#b19">[20]</ref>, and skeleton-based methods <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b10">11]</ref>, etc. But compared with other tasks such as object detection <ref type="bibr" target="#b48">[49]</ref> or pose estimation <ref type="bibr" target="#b12">[13]</ref>, its performance is still limited. Human-Object Interaction. Human-Object Interaction (HOI) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> occupies the most of daily human activities. In terms of tasks, some works focus on image-based HOI recognition <ref type="bibr" target="#b2">[3]</ref>. Furthermore, instance-based HOI detection <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b24">25]</ref> needs to detect accurate positions of the humans and objects and classify interaction simultaneously. In terms of the information utilization, some works utilized holistic human body and pose <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b4">5]</ref>, and global context is also proved to be effective <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b6">7]</ref>. According to the learning paradigm, earlier works were often based on hand-crafted features <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b28">29]</ref>. Benefited from large scale HOI datasets, recent approaches <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b32">33]</ref> started to use deep neural networks to extract features and achieved great improvements. Body Part based Methods. Besides the instance pattern, some approaches studied to utilize part pattern <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b64">65]</ref>. Gkioxari et al. <ref type="bibr" target="#b19">[20]</ref> detects both the instance and parts and input them all into a classifier. Fang et al. <ref type="bibr" target="#b13">[14]</ref> defines part pairs and encodes pair features to improve HOI recognition. Yao et al. <ref type="bibr" target="#b61">[62]</ref> builds a graphical model and embed parts appearance as nodes, and use them with object feature and pose to predict the HOIs. Previous work mainly utilized the part appearance and location, but few studies tried to divide the instance actions into discrete part-level semantic tokens, and refer them as the basic components of activity concepts. In comparison, we aim at building human part semantics as reusable and transferable knowledge. Part States. Part state is proposed in <ref type="bibr" target="#b37">[38]</ref>. By tokenizing the semantic space as a discrete set of part states, <ref type="bibr" target="#b37">[38]</ref> constructs a sort of basic descriptors based on segmentation <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b59">60]</ref>. To exploit this cue, we divide the human body into natural parts and utilize their states as discretized part semantics to represent activities. In this paper, we focus on the part states of humans instead of daily objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Constructing PaStaNet</head><p>In this section, we introduce the construction of PaStaNet. PaStaNet seeks to explore the common knowledge of human PaSta as atomic elements to infer activities. PaSta Definition. We decompose human body into ten parts, namely head, two upper arms, two hands, hip, two thighs, two feet. Part states (PaSta) will be assigned to these parts. Each PaSta represents a description of the target part. For example, the PaSta of "hand" can be "hold something" or "push something", the PaSta of "head" can be "watch something", "eat something". After exhaustively reviewing collected 200K+ images, we found the descriptions of any human parts can be concluded into limited categories. That is, the PaSta category number of each part is limited. Especially, a person may have more than one action simultaneously, thus each part can have multiple PaSta, too. Data Collection. For generality, we collect human-centric activity images by crowdsourcing (30K images paired with rough activity label) as well as existing well-designed datasets <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b34">35]</ref> (185K images), which are structured around a rich semantic ontology, diversity, and variability of activities. All their annotated persons and objects are extracted for our construction. Finally, we collect more than 200K images of diverse activity categories. Activity Labeling. Activity categories of PaStaNet are chosen according to the most common human daily activities, interactions with object and person. Referred to the hierarchical activity structure <ref type="bibr" target="#b11">[12]</ref>, common activities in existing datasets <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b34">35]</ref> and crowdsourcing labels, we select 156 activities including human-object interactions and body motions from 118K images. According to them, we first clean and reorganize the annotated human and objects from existing datasets and crowdsourcing. Then, we annotate the active persons and the interacted objects in the rest of the images. Thus, PaStaNet includes all active human and object bounding boxes of 156 activities. Body Part Box. To locate the human parts, we use pose estimation <ref type="bibr" target="#b12">[13]</ref> to obtain the joints of all annotated persons. Then we generate ten body part boxes following <ref type="bibr" target="#b13">[14]</ref>. Estimation errors are addressed manually to ensure high-quality annotation. Each part box is centered with a joint, and the box size is pre-defined by scaling the distance between the joints of the neck and pelvis. A joint with confidence higher than 0.7 will be seen as visible. When not all joints can be detected, we use body knowledge-based rules. That is, if the neck or pelvis is invisible, we configure the part boxes according to other visible joint groups (head, main body, arms, legs), e.g., if only the upper body is visible, we set the size of the hand box to twice the pupil distance. PaSta Annotation. We carry out the annotation by crowdsourcing and receive 224,159 annotation uploads. The process is as follows: 1) First, we choose the PaSta categories considering the generalization. Based on the verbs of 156 activities, we choose 200 verbs from WordNet <ref type="bibr" target="#b43">[44]</ref> as the PaSta candidates, e.g., "hold", "pick" for hands, "eat", "talk to" for head, etc. If a part does not have any active states, we depict it as "no action". 2) Second, to find the most common PaSta that can work as the transferable activity knowledge, we invite 150 annotators from different backgrounds to annotate 10K images of 156 activities with PaSta candidates <ref type="figure">(Fig. 2</ref>). For example, given an activity "ride bicycle", they may describe  <ref type="figure">Figure 2</ref>. PaSta annotation. Based on instance activity labels, we add fine-grained body part boxes and corresponding part states PaSta labels. In PaSta, we use "something" <ref type="bibr" target="#b37">[38]</ref> to indicate the interacted object for generalization. The edge in Activity Parsing Tree indicates the statistical co-occurrence.</p><p>it as hip, sit on, something , hand, hold, something , f oot, step on, something , etc. 3) Based on their annotations, we use the Normalized Point-wise Mutual Information (NPMI) <ref type="bibr" target="#b5">[6]</ref> to calculate the co-occurrence between activities and PaSta candidates. Finally, we choose 76 candidates with the highest NPMI values as the final PaSta. 4) Using the annotations of 10K images as seeds, we automatically generate the initial PaSta labels for all of the rest images. Thus the other 210 annotators only need to revise the annotations. 5) Considering that a person may have multiple actions, for each action, we annotate its corresponding ten PaSta respectively. Then we combine all sets of PaSta from all actions. Thus, a part can also have multiple states, e.g., in "eating while talking", the head has PaSta head, eat, something , head, talk to, something and head, look at, something simultaneously. 6) To ensure quality, each image will be annotated twice and checked by automatic procedures and supervisors. We cluster all labels and discard the outliers to obtain robust agreements. Activity Parsing Tree. To illustrate the relationships between PaSta and activities, we use their statistical correlations to construct a graph ( <ref type="figure">Fig. 2</ref>): activities are root nodes, PaSta are son nodes and edges are co-occurrence.</p><p>Finally, PaStaNet includes 118K+ images, 285K+ persons, 250K+ interacted objects, 724K+ instance activities and 7M+ PaSta. Referred to well-designed datasets <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b2">3]</ref> and WordNet <ref type="bibr" target="#b43">[44]</ref>, PaSta can cover most part situations with good generalization. To verify that PaSta have encoded common part-level activity knowledge and can adapt to various activities, we adopt two experiments: Coverage Experiment. To verify that PaSta can cover most of the activities, we collect other 50K images out of PaStaNet. Those images contain diverse activities and many of them are unseen in PaStaNet. Another 100 volunteers from different backgrounds are invited to find human parts that can not be well described by our PaSta set. We found that only 2.3% cases cannot find appropriate descriptions. This verifies that PaStaNet is general to activities. Recognition Experiment. First, we find that PaSta can be well learned. A shallow model trained with a part of PaStaNet can easily achieve about 55 mAP on PaSta recognition. Meanwhile, a deeper model can only achieve about 40 mAP on activity recognition with the same data and metric (Sec. 5.2). Second, we argue that PaSta can be well transferred. To verify this, we conduct transfer learning experiments (Sec. 5.4), i.e. first trains a model to learn the knowledge from PaStaNet, then use it to infer the activities of unseen datasets, even unseen activities. Results show that PaSta can be well transferred and boost the performance (4.2 mAP on image-based AVA). Thus it can be considered as the general part-level activity knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Activity Representation by PaStaNet</head><p>In this section, we discuss the activity representation by PaStaNet. Conventional Paradigm Given an image I, conventional methods mainly use a direct mapping ( <ref type="figure" target="#fig_0">Fig. 1(a)</ref>):</p><formula xml:id="formula_0">S inst = F inst (I, b h , B o )<label>(1)</label></formula><p>to infer the action score S inst with instance-level seman-</p><formula xml:id="formula_1">tic representations f inst . b h is the human box and B o = {b i o } m i=1</formula><p>are the m interacted object boxes of this person. PaStaNet Paradigm. We propose a novel paradigm to utilize general part knowledge: 1) PaSta recognition and feature extraction for a person and an interacted object b o :</p><formula xml:id="formula_2">f P aSta = R A2V (I, B p , b o ),<label>(2)</label></formula><p>where B p = {b (i) p } 10 i are part boxes generated from the pose estimation <ref type="bibr" target="#b12">[13]</ref> automatically following <ref type="bibr" target="#b13">[14]</ref> (head, upper arms, hands, hip, thighs, feet). R A2V (·) indicates the Activity2Vec, which extracts ten PaSta representations</p><formula xml:id="formula_3">f P aSta = {f (i) P aSta } 10 i=1 . 2) PaSta-based Reasoning (PaSta- R), i.e.</formula><p>, from PaSta to activity semantics:</p><formula xml:id="formula_4">S part = F P aSta−R (f P aSta , f o ),<label>(3)</label></formula><p>where F P aSta−R (·) indicates the PaSta-R, f o is the object feature. S part is the action score of the part-level path. If the person does not interact with any objects, we use the ROI pooling feature of the whole image as f o . For multiple object case, i.e., a person interacts with several objects, we process each human-object pair (f P aSta , f</p><formula xml:id="formula_5">(i)</formula><p>o ) respectively and generate its Activity2Vec embedding.</p><p>Following, we introduce the PaSta recognition in Sec. 4.1. Then, we discuss how to map human instance to semantic vector via Activity2Vec in Sec. 4.2. We believe it can be a general activity representation extractor. In Sec. 4.3, a hierarchical activity graph is proposed to largely advance activity related tasks by leveraging PaStaNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Part State Recognition</head><p>With the object and body part boxes b o , B p , we operate the PaSta recognition as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. In detail, a COCO <ref type="bibr" target="#b33">[34]</ref> pre-trained Faster R-CNN <ref type="bibr" target="#b48">[49]</ref> is used as the feature extractor. For each part, we concatenate the part feature f</p><formula xml:id="formula_6">(i) p from b (i) p and object features f o from b o as in- puts.</formula><p>For body only motion, we input the whole image feature f c as f o . All features will be first input to a Part Relevance Predictor. Part relevance represents how important a body part is to the action. For example, feet usually have weak correlations with "drink with cup". And in "eat apple", only hands and head are essential. These relevance/attention labels can be converted from PaSta labels directly, i.e. the attention label will be one, unless its PaSta label is "no action", which means this part contributes nothing to the action inference. With the part attention labels as supervision, we use part relevance predictor consisting of FC layers and Sigmoids to infer the attentions {a i } 10 i=1 of each part. Formally, for a person and an interacted object:</p><formula xml:id="formula_7">a i = P pa (f (i) p , f o ),<label>(4)</label></formula><p>where P pa (·) is the part attention predictor. We compute cross-entropy loss L (i) att for each part and multiply f</p><formula xml:id="formula_8">(i) p with its scalar attention, i.e. f (i) * p = f (i) p × a i .</formula><p>Second, we operate the PaSta recognition. For each part, we concatenate the re-weighted f (i) * p with f o , and input them into a max pooling layer and two subsequent 512 sized FC layers, thus obtain the PaSta score S (i) P aSta for the ith part. Because a part can have multiple states, e.g. head performs "eat" and "watch" simultaneously. Hence we use multiple Sigmoids to do this multi-label classification. With PaSta labels, we construct cross-entropy loss L (i) P aSta . The total loss of PaSta recognition is:</p><formula xml:id="formula_9">L P aSta = 10 i (L (i) P aSta + L (i) att ).<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Activity2Vec</head><p>In Sec. 3, we define the PaSta according to the most common activities. That is, choosing the part-level verbs which are most often used to compose and describe the activities by a large number of annotators. Therefore PaSta can be seen as the fundamental components of instance activities. Meanwhile, PaSta recognition can be well learned. Thus, we can operate PaSta recognition on PaStaNet to learn the powerful PaSta representations, which have good transferability. They can be used to reason out the instance actions head-look_at-sth right_hand-hold-sth left_hand-hold-sth hip-sit_on-sth right_foot-step_on-sth left_foot-step_on-sth  in both supervised and transfer learning. Under such circumstance, PaStaNet works like the ImageNet <ref type="bibr" target="#b7">[8]</ref>. And PaStaNet pre-trained Activity2Vec functions as a knowledge engine and transfers the knowledge to other tasks. Visual PaSta feature. First, we extract visual PaSta representations from PaSta recognition. Specifically, we extract the feature from the last FC layer in PaSta classifier as the visual PaSta representation f</p><formula xml:id="formula_10">V (i) P aSta ∈ R 512 .</formula><p>Language PaSta feature. Our goal is to bridge the gap between PaSta and activity semantics. Language priors are useful in visual concept understanding <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b57">58]</ref>. Thus the combination of visual and language knowledge is a good choice for establishing this mapping. To further enhance the representation ability, we utilize the uncased BERT-Base pre-trained model <ref type="bibr" target="#b8">[9]</ref> as the language representation extractor. Bert <ref type="bibr" target="#b8">[9]</ref> is a language understanding model that considers the context of words and uses a deep bidirectional transformer to extract contextual representations. It is trained with large-scale corpus databases such as Wikipedia, hence the generated embedding contains helpful implicit semantic knowledge about the activity and PaSta. For example, the description of the entry "basketball" in Wikipedia: "drag one's foot without dribbling the ball, to carry it, or to hold the ball with both hands...placing his hand on the bottom of the ball;..known as carrying the ball".</p><p>In specific, for the i-th body part with n PaSta, we divide each PaSta into tokens {t </p><formula xml:id="formula_11">(i,k) p , t (i,k) v , t (i,k) o } n k=1 ,</formula><formula xml:id="formula_12">Bert = R Bert (t (i,k) p , t (i,k) v , t (i,k) o ). {f (i,k) Bert } n k=1</formula><p>will be concatenated as the f (i) Bert ∈ R 2304 * n for the ith part. Second, we multiply f</p><formula xml:id="formula_13">(i) Bert with predicted PaSta probabilities P (i) P aSta , i.e. f L(i) P aSta = f (i) Bert × P (i) P aSta , where P (i) P aSta = Sigmoid(S (i) P aSta ) ∈ R n , S (i) P aSta denotes the PaSta score of the i-th part, P P aSta = {P (i) P aSta } 10</formula><p>i=1 . This means a more possible PaSta will get larger attention. f L(i) P aSta ∈ R 2304 * n is the final language PaSta feature of the i-th part. We use the pre-converted and frozen f</p><formula xml:id="formula_14">(i,k)</formula><p>Bert in the whole process. Additionally, we also try to rewrite each PaSta into a sentence and convert it into a fixed-size vector as f</p><formula xml:id="formula_15">(i,k)</formula><p>Bert and the performance is slightly better (Sec. 5.5). PaSta Representation. At last, we pool and resize the f L(i) P aSta , and concatenate it with its corresponding visual PaSta feature f V (i) P aSta . Then we obtain the PaSta representation f (i) P aSta ∈ R m for each body part (e.g. m = 4096). This process is indicated as Activity2Vec <ref type="figure" target="#fig_2">(Fig. 3)</ref>. The output f P aSta = {f (i) P aSta } 10 i=1 is the part-level activity representation and can be used for various downstream tasks, e.g. activity detection, captioning, etc. From the experiments, we can find that Activity2Vec has a powerful representational capacity and can significantly improve the performance of activity related tasks. It works like a knowledge transformer with the fundamental PaSta to compose various activities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">PaSta-based Activity Reasoning</head><p>With part-level f P aSta , we construct a Hierarchical Activity Graph (HAG) to model the activities. Then we can extract the graph state to reason out the activities. Hierarchical Activity Graph. Hierarchical activity graph G = (V, E) is depicted in <ref type="figure">Fig. 4</ref>. For human-object interactions, V = {V p , V o }. For body only motions, V = V p . In instance level, a person is a node with instance representation from previous instance-level methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b22">23]</ref>   <ref type="figure">Figure 4</ref>. From PaSta to activities on hierarchical activity graph.</p><p>i.e. activities. In part-level, we use PaSta-based Activity Reasoning (PaSta-R) to infer the activities. That is, with the PaSta representation from Activity2Vec, we use S part = F P aSta−R (f P aSta , f o ) (Eq. 3) to infer the activity scores S part . For body motion only activities e.g. "dance", Eq. 3 is</p><formula xml:id="formula_16">S part = F P aSta−R (f P aSta , f c ), f c is the feature of image.</formula><p>We adopt different implementations of F P aSta−R (·). Linear Combination. The simplest implementation is to directly combine the part node features linearly. We concatenate the output of Activity2Vec f P aSta with f o and input them to a FC layer with Sigmoids. MLP. We can also operate nonlinear transformation on Ac-tivity2Vec output. We use two 1024 sized FC layers and an action category sized FC with Sigmoids. Graph Convolution Network. With part-level graph, we use Graph Convolution Network (GCN) <ref type="bibr" target="#b30">[31]</ref> to extract the global graph feature and use an MLP subsequently. Sequential Model. When watching an image in this way: watch body part and object patches with language description one by one, human can easily guess the actions. Inspired by this, we adopt an LSTM <ref type="bibr" target="#b27">[28]</ref> to take the part node features f (i) P aSta gradually, and use the output of the last time step to classify actions. We adopt two input orders: random and fixed (from head to foot), and fixed order is better. Tree-Structured Passing. Human body has a natural hierarchy. Thus we use a tree-structured graph passing. Specifically, we first combine the hand and upper arm nodes into an "arm" node, its feature is obtained by concatenating the features of three son nodes and passed a 512 sized FC layer. Similarly, we combine the foot and thigh nodes to an "leg" node. Head, arms, legs and feet nodes together form the second level. The third level contains the "upper body" (head, arms) and "lower-body" (hip, legs). Finally, the body node is generated. We input it and the object node into an MLP.</p><p>The instance-level graph inference can be operated by instance-based methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b22">23]</ref> using Eq. 1:</p><formula xml:id="formula_17">S inst = F inst (I, b h , B o ).</formula><p>To get the final result upon the whole graph, we can use either early or late fusion. In early fusion, we concatenate f inst with f P aSta , f o and input them to PaSta-R. In late fusion, we fuse the predictions of two levels, i.e. S = S inst + S part . In our test, late fusion outperforms early fusion in most cases. If not specified, we use late fusion in Sec. 5. We use L inst cls and L P aSta the cross-entropy losses of two levels. The total loss is:</p><formula xml:id="formula_18">L total = L P aSta + L P aSta cls + L inst cls .<label>(6)</label></formula><p>5. Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">An analogy: MNIST-Action</head><p>We design a simplified experiment to give an intuition ( <ref type="figure" target="#fig_3">Fig. 5</ref>). We randomly sample MNIST digits from 0 to 9 (28×28×1) and generate 128×128×1 images consists of 3 to 5 digits. Each image is given a label to indicate the sum of the two largest numbers within it (0 to 18). We assume that "PaSta-Activity" resembles the "Digits-Sum". Body parts can be seen as digits, thus human is the union box of all digits. To imitate the complex body movements, digits are randomly distributed, and Gaussian noise is added to the images. For comparison, we adopt two simple networks. For instance-level model, we input the ROI pooling feature of the digit union box into an MLP. For hierarchical model, we operate single-digit recognition, then concatenate the union box and digit features and input them to an MLP (early fusion), or use late fusion to combine scores of two levels. Early fusion achieves 43.7 accuracy and shows significant superiority over instance-level method (10.0). And late fusion achieves a preferable accuracy of 44.2. Moreover, the part-level method only without fusion also obtains an accuracy of 41.4. This supports our assumption about the effectiveness of part-level representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Image-based Activity Recognition</head><p>Usually, Human-Object Interactions (HOIs) often take up most of the activities, e.g., more than 70% activities in large-scale datasets <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b1">2]</ref> are HOIs. To evaluate PaStaNet, we perform image-based HOI recognition on HICO <ref type="bibr" target="#b2">[3]</ref>. HICO has 38,116 and 9,658 images in train and test sets and 600 HOIs composed of 117 verbs and 80 COCO objects <ref type="bibr" target="#b33">[34]</ref>. Each image has an image-level label which is the aggregation over all HOIs in an image and does not contain any instance boxes. Modes. We first pre-train Activity2Vec with PaSta labels, then fine-tune Activity2Vec and PaSta-R together on HICO train set. In pre-training and finetuning, we exclude the HICO testing data in PaStaNet to avoid data pollution. We Method mAP Few@1 Few@5 Few@10 R*CNN <ref type="bibr" target="#b20">[21]</ref> 28.  <ref type="bibr" target="#b13">[14]</ref>+PaStaNet" means the late fusion of <ref type="bibr" target="#b13">[14]</ref> and our part-level result. Few@i indicates the mAP on few-shot sets. @i means the number of training images is less than or equal to i. The HOI categories number of Few@1, 5, 10 are 49, 125 and 163. "PaStaNet-x" means different PaSta-R.</p><p>adopt different data mode to pre-train Activity2Vec: 1) "PaStaNet*" mode (38K images): we use the images in HICO train set and their PaSta labels. The only additional supervision here is the PaSta annotations compared to conventional way. 2) "GT-PaStaNet*" mode (38K images): the data used is same with "PaStaNet*". To verify the upper bound of our method, we use the ground truth PaSta (binary labels) as the predicted PaSta probabilities in Ac-tivity2Vec. This means we can recognize PaSta perfectly and reason out the activities from the best starting point.</p><p>3) "PaStaNet" mode (118K images): we use all PaStaNet images with PaSta labels except the HICO testing data. Settings. We use image-level PaSta labels to train Activ-ity2Vec. Each image-level PaSta label is the aggregation over all existing PaSta of all active persons in an image. For PaSta recognition, i.e., we compute the mAP for the PaSta categories of each part, and compute the mean mAP of all parts. To be fair, we use the person, body part and object boxes from <ref type="bibr" target="#b13">[14]</ref> and VGG-16 <ref type="bibr" target="#b51">[52]</ref> as the backbone. The batch size is 16 and the initial learning rate is 1e-5. We use SGD optimizer with momentum (0.9) and cosine decay restarts <ref type="bibr" target="#b35">[36]</ref> (the first decay step is 5000). The pretraining costs 80K iterations and fine-tuning costs 20K iterations. Image-level PaSta and HOI predictions are all generated via Multiple Instance Learning (MIL) <ref type="bibr" target="#b41">[42]</ref> of 3 persons and 4 objects. We choose previous methods <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b13">14]</ref> as the instance-level path in the hierarchical model, and uses late fusion. Particularly, <ref type="bibr" target="#b13">[14]</ref> uses part-pair appearance and location but not part-level semantics, thus we still consider it as a baseline to get a more abundant comparison.</p><p>Results. Results are reported in Tab. 1. PaStaNet* mode methods all outperform the instance-level method. The part-level method solely achieves 44.5 mAP and shows good complementarity to the instance-level. Their fusion can boost the performance to 45.9 mAP (6 mAP improvement). And the gap between <ref type="bibr" target="#b13">[14]</ref> and <ref type="bibr" target="#b40">[41]</ref> is largely narrowed from 3.8 to 0.9 mAP. Activity2Vec achieves 55.9 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Instance-based Activity Detection</head><p>We further conduct instance-based activity detection on HICO-DET <ref type="bibr" target="#b3">[4]</ref>, which needs to locate human and object and classify the actions simultaneously. HICO-DET <ref type="bibr" target="#b3">[4]</ref> is a benchmark built on HICO <ref type="bibr" target="#b2">[3]</ref> and add human and object bounding boxes. We choose several state-of-the-arts <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b32">33]</ref> to compare and cooperate. Settings. We use instance-level PaSta labels, i.e. each annotated person with the corresponding PaSta labels, to train Acitivty2Vec, and fine-tune Activity2Vec and PaSta-R together on HICO-DET. All testing data are excluded from pre-training and fine-tining. We follow the mAP metric of <ref type="bibr" target="#b3">[4]</ref>, i.e. true positive contains accurate human and object boxes (IoU &gt; 0.5 with reference to ground truth) and accurate action prediction. The metric for PaSta detection is similar, i.e., estimated part box and PaSta action prediction all have to be accurate. The mAP of each part and the mean mAP are calculated. For a fair comparison, we use the object detection from <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b32">33]</ref> and ResNet-50 <ref type="bibr" target="#b25">[26]</ref> as backbone. We use SGD with momentum (0.9) and cosine decay restart <ref type="bibr" target="#b35">[36]</ref> (the first decay step is 80K). The pre-training and fine-tuning take 1M and 2M iterations respectively. The learning rate is 1e-3 and the ratio of positive and negative samples is 1:4. A late fusion strategy is adopted. Three modes in Sec. 5.2 and different PaSta-R are also evaluated. Results. Results are shown in Tab. 2. All PaStaNet* mode methods significantly outperform the instance-level methods, which strongly prove the improvement from the learned PaSta information. In PaStaNet* mode, the PaSta detection performance are 30. Gupta et al. <ref type="bibr" target="#b24">[25]</ref> 31.8 -InteractNet <ref type="bibr" target="#b21">[22]</ref> 40.0 -GPNN <ref type="bibr" target="#b47">[48]</ref> 44.0 -iCAN <ref type="bibr" target="#b16">[17]</ref> 45  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Transfer Learning with Activity2Vec</head><p>To verify the transferability of PaStaNet, we design transfer learning experiments on large-scale benchmarks: V-COCO <ref type="bibr" target="#b24">[25]</ref>, HICO-DET <ref type="bibr" target="#b3">[4]</ref> and AVA <ref type="bibr" target="#b22">[23]</ref>. We first use PaStaNet to pre-train Activity2Vec and PaSta-R with 156 activities and PaSta labels. Then we change the last FC in PaSta-R to fit the activity categories of the target benchmark. Finally, we freeze Activity2Vec and fine-tune PaSta-R on the train set of the target dataset. Here, PaStaNet works like the ImageNet <ref type="bibr" target="#b7">[8]</ref> and Activity2Vec is used as a pretrained knowledge engine to promote other tasks. V-COCO. V-COCO contains 10,346 images and instance boxes. It has 29 action categories, COCO 80 objects <ref type="bibr" target="#b33">[34]</ref>. For a fair comparison, we exclude the images of V-COCO and corresponding PaSta labels in PaStaNet, and use remaining data (109K images) for pre-training. We use SGD with 0.9 momenta and cosine decay restarts <ref type="bibr" target="#b35">[36]</ref> (the first decay is 80K). The pre-training costs 300K iterations with the learning rate as 1e-3. The fine-tuning costs 80K iterations with the learning rate as 7e-4. We select state-of-thearts <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b32">33]</ref> as baselines and adopt the metric AP role <ref type="bibr" target="#b24">[25]</ref> (requires accurate human and object boxes and action prediction). Late fusion strategy is adopted. With the domain gap, PaStaNet still improves the performance by 3.2 mAP (Tab. 3.). Image-based AVA. AVA contains 430 video clips with spatio-temporal labels. It includes 80 atomic actions consists of body motions and HOIs. We utilize all PaStaNet data (118K images) for pre-training. Considering that PaStaNet is built upon still images, we use the frames per second as still images for image-based instance activity detection. We adopt ResNet-50 <ref type="bibr" target="#b25">[26]</ref> as backbone and SGD with momentum of 0.9. The initial learning rate is 1e-2 and the first decay of cosine decay restarts <ref type="bibr" target="#b35">[36]</ref> is 350K. For a fair comparison, we use the human box from <ref type="bibr" target="#b58">[59]</ref>. The pretraining costs 1.1M iterations and fine-tuning costs 710K iterations. We adopt the metric from <ref type="bibr" target="#b22">[23]</ref>, i.e. mAP of the top 60 most common action classes, using IoU threshold of 0.5 between detected human box and the ground truth and accurate action prediction. For comparison, we adopt a image-based baseline: Faster R-CNN detector <ref type="bibr" target="#b48">[49]</ref> with Method mAP AVA-TF <ref type="bibr" target="#b23">[24]</ref> 11.4 LFB-Res-50-baseline <ref type="bibr" target="#b58">[59]</ref> 22.2 LFB-Res-101-baseline <ref type="bibr" target="#b58">[59]</ref> 23.3 AVA-TF <ref type="bibr" target="#b23">[24]</ref>  <ref type="table">Table 4</ref>. Transfer learning results on image-based AVA <ref type="bibr" target="#b22">[23]</ref>.</p><p>ResNet-101 <ref type="bibr" target="#b25">[26]</ref> provided by the AVA website <ref type="bibr" target="#b23">[24]</ref>. Recent works mainly use a spatial-temporal model such as I3D <ref type="bibr" target="#b1">[2]</ref>. Although unfair, we still employ two video-based baselines <ref type="bibr" target="#b58">[59]</ref> as instance-level models to cooperate with the part-level method via late fusion. Results are listed in Tab. 4. Both image and video based methods cooperated with PaStaNet achieve impressive improvements, even our model is trained without temporal information. Considering the huge domain gap (films) and unseen activities, this result strongly proves its great generalization ability. HICO-DET. We exclude the images of HICO-DET and the corresponding PaSta labels, and use left data (71K images) for pre-training. The test setting in same with Sec. 5.3. The pre-training and fine-tuning cost 300K and 1.3M iterations. PaStaNet shows good transferability and achieve 3.25 mAP improvement on Default Full set (20.28 mAP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Ablation Study</head><p>We design ablation studies on HICO-DET with TIN [33]+PaSta*-Linear (22.12 mAP). 1) w/o Part Attention degrades the performance with 0.21 mAP. 2) Language Feature: We replace the PaSta Bert feature in Activity2Vec with: Gaussian noise, Word2Vec <ref type="bibr" target="#b42">[43]</ref> and GloVe <ref type="bibr" target="#b46">[47]</ref>. The results are all worse (20.80, 21.95, 22.01 mAP). If we change the PaSta triplet part, verb, sth into a sentence and convert it to Bert vector, this vector performs sightly better <ref type="bibr">(22.26 mAP)</ref>. This is probably because the sentence carries more contextual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, to make a step toward human activity knowledge engine, we construct PaStaNet to provide novel body part-level activity representation (PaSta). Meanwhile, a knowledge transformer Activity2Vec and a part-based reasoning method PaSta-R are proposed. PaStaNet brings in interpretability and new possibility for activity understanding. It can effectively bridge the semantic gap between pixels and activities. With PaStaNet, we significantly boost the performance in supervised and transfer learning tasks, especially under few-shot circumstances. In the future, we plan to enrich our PaStaNet with spatio-temporal PaSta.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset Details</head><p>In this section, we give a more detailed introduction of our knowledge base PaStaNet, covering characteristics and the annotator backgrounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Characteristics of PastaNet</head><p>Tab. 5 shows some characteristics of PaStaNet. Now PaStaNet contains about 118K+ images and the corresponding instance-level and human part-level annotations. To give an intuitive presentation of our PaSta, we use t-SNE <ref type="bibr" target="#b38">[39]</ref> to visualize the part-level features of some typical PaSta samples in <ref type="figure" target="#fig_5">Fig. 6</ref>. We use the human body part patches with different colored borders to replace the embedding points in <ref type="figure" target="#fig_5">Fig. 6</ref>, e.g. the embeddings of "hand holds something" and "foot kicks something".  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Annotator Backgrounds</head><p>There are about 360 annotators have participated in the construction of PastaNet. They have various backgrounds, thus we can ensure annotation diversity and reduce the bias. The specific information is shown in <ref type="figure" target="#fig_6">Fig. 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Selected Activities and PaSta</head><p>We list all selected 156 activities and 76 PaSta in Tab. 6 and Tab. 7. PaStaNet contains both person-object/person interactions and body only motions, which cover the vast majority of human daily life activities. And all the annotated interacted objects in interactions all belong to COCO 80 object categories <ref type="bibr" target="#b33">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Activity and PaSta</head><p>PaStaNet can provide abundant activity knowledge for both instance and part levels and help construct a large-scale activity parsing tree, as seen in <ref type="figure">Fig. 8</ref>. Moreover, we can represent the parsing tree as a co-occurrence matrix of the activities and PaSta. A part of the matrix is depicted in <ref type="figure" target="#fig_7">Fig. 9</ref>  C. Additional Details of PaSta-R <ref type="figure" target="#fig_0">Fig. 10</ref> gives more details about the different PaSta-R implementations, i.e. directly input the Activity2Vec output to the Linear Combination, MLP or GCN <ref type="bibr" target="#b30">[31]</ref>, sequential LSTM-based model and the tree-structured passing model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Additional Details of MNIST-Action</head><p>In this section, we provide some details of the MNIST-Action experiment. The instance-based and hierarchical models are shown in <ref type="figure" target="#fig_0">Fig. 11</ref>. The train and test set sizes are 5,000 and 800. In the instance-based model, we directly use the ROI  <ref type="figure" target="#fig_0">Figure 11</ref>. Instance-based and hierarchical models in MNIST-Action.</p><p>pooling feature of the digit union box to predict the target (summation of the largest and second largest numbers). We use optimizer RMSProp and the initial learning rate is 0.0001. The batch size is 16 and we train the model 1K epochs. In the hierarchical model, we first operate the single digit recognition and then use single digit features (part features) together with the instance feature to infer the sum (early fusion). If using late fusion, we directly fuse the scores of instance branch and part branch. We also use optimizer RMSProp and the initial learning rate is 0.001. The batch size is 32 and the training also costs 1K epochs. Two models are all implemented with 4 convolution layers with subsequent fully-connect layers.</p><p>Results are shown in <ref type="figure" target="#fig_0">Fig. 12</ref>. We can find that the hierarchical method largely outperforms the instance-based method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Effectiveness on Few-shot Problems</head><p>In this section, we show more detailed results on HICO <ref type="bibr" target="#b2">[3]</ref> to illustrate the effectiveness of Pasta on few-shot problems. We divide the 600 HOIs into different sets according to their training sample numbers. On HICO <ref type="bibr" target="#b2">[3]</ref>, there is an obvious positive correlation between performance and the number of training samples. From <ref type="figure" target="#fig_0">Fig. 13</ref>     <ref type="figure" target="#fig_0">Figure 13</ref>. Effectiveness on few-shot problems. The y-coordinate means the mAP of different sets. The x-coordinate indicates the activity sets. For example, i means the number of training images is equal to or less than i, if i is 1 then it means one-shot problem. Our approach can obviously improve the performance on few-shot problem, for the reason of reusability and transferability of PaSta. method outperforms the previous state-of-the-art <ref type="bibr" target="#b13">[14]</ref> on all sets, especially on the few-shot sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Additional Activity Detection Results</head><p>We report visualized PaSta and activity predictions of our method in <ref type="figure" target="#fig_0">Figure 14</ref>. The body part, part verb, object with the highest scores are visualized in blue, green and red boxes, and their corresponding PaSta descriptions are demonstrated under each image with colors consisted with boxes. The final activity predictions with the highest scores are also represented. We can find that our model is capable to detect various kinds of activities covering interactions with various objects.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Instance-level and hierarchical methods. Besides the instance-level path, we perform body part states recognition in part-level with the PaSta annotations. With the help of PaSta, we can significantly boost the performance of activity understanding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The overview of Part States (PaSta) recognition and Activity2Vec.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>An analogy to activity recognition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>2 mAP: 25.8 (head), 44.2 Method AProle(Scenario1) AProle(Scenario2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Visualized PaSta representations via t-SNE [39]. Sub-figure (a) depicts the PaSta features of foot and sub-figure (b) depicts PaSta features of hand. Our model can extract meaningful and reasonable features for various PaSta.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Annotator backgrounds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .</head><label>9</label><figDesc>Part of the Activity-Pasta Co-occurrence matrix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 .</head><label>10</label><figDesc>PaSta-R model: (a) Linear Combination, MLP and GCN; (b) Sequential Model; (c) Tree Structured Passing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 .</head><label>12</label><figDesc>Comparison of loss and accuracy in MNIST-Action.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>e.g., part, verb, object . The object comes from object detection. Each PaSta will be converted to a f</figDesc><table><row><cell></cell><cell>(i,k) Bert ∈ R 2304</cell></row><row><cell cols="2">(concatenating three 768 sized vectors of part, verb, ob-</cell></row><row><cell>ject), i.e. f</cell><cell>(i,k)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>as a node feature. Object node v o ∈ V o and has f o as node feature. In part level, each body part can be seen as a node v i</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Hierarchical Activity Graph</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Human</cell><cell>Object</cell><cell>Instance</cell></row><row><cell></cell><cell>Head</cell><cell>Node</cell><cell>Node</cell><cell>Level</cell></row><row><cell></cell><cell>RUArm LUArm</cell><cell></cell><cell></cell><cell></cell><cell>look_at-bicycle</cell></row><row><cell></cell><cell>LHand</cell><cell></cell><cell></cell><cell></cell><cell>ride-bicycle</cell></row><row><cell>Activity2Vec</cell><cell>RHand</cell><cell></cell><cell></cell><cell></cell><cell>hold-bicycle</cell></row><row><cell>Pre-trained on PaStaNet</cell><cell>RThigh LFeet Hip LThigh</cell><cell></cell><cell>Object Node</cell><cell>Level Part</cell><cell>…</cell></row><row><cell></cell><cell>RFeet</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Body Part Nodes</cell><cell></cell><cell></cell></row></table><note>p ∈ V p with PaSta representation f i P aSta as node feature. Edge between body parts and object is e po = (v i p , v o ) ∈ Vp × V o , and edge within parts is e ij pp = (v i p , v j p ) ∈ V p × V p . Our goal is to parse HAG and reason out the graph state,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell>5</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Girdhar et al. [18]</cell><cell>34.6</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Mallya et al. [41]</cell><cell>36.1</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Pairwise [14]</cell><cell>39.9</cell><cell>13.0</cell><cell>19.8</cell><cell>22.3</cell></row><row><cell cols="2">Mallya et al. [41]+PaStaNet*-Linear 45.0</cell><cell>26.5</cell><cell>29.1</cell><cell>30.3</cell></row><row><cell>Pairwise [14]+PaStaNet*-Linear</cell><cell>45.9</cell><cell>26.2</cell><cell>30.6</cell><cell>31.8</cell></row><row><cell>Pairwise [14]+PaStaNet*-MLP</cell><cell>45.6</cell><cell>26.0</cell><cell>30.8</cell><cell>31.9</cell></row><row><cell>Pairwise [14]+PaStaNet*-GCN</cell><cell>45.6</cell><cell>25.2</cell><cell>30.0</cell><cell>31.4</cell></row><row><cell>Pairwise [14]+PaStaNet*-Seq</cell><cell>45.9</cell><cell>25.3</cell><cell>30.2</cell><cell>31.6</cell></row><row><cell>Pairwise [14]+PaStaNet*-Tree</cell><cell>45.8</cell><cell>24.9</cell><cell>30.3</cell><cell>31.8</cell></row><row><cell>PaStaNet*-Linear</cell><cell>44.5</cell><cell>26.9</cell><cell>30.0</cell><cell>30.7</cell></row><row><cell cols="2">Pairwise [14]+GT-PaStaNet*-Linear 65.6</cell><cell>47.5</cell><cell>55.4</cell><cell>56.6</cell></row><row><cell>Pairwise [14]+PaStaNet-Linear</cell><cell>46.3</cell><cell>24.7</cell><cell>31.8</cell><cell>33.1</cell></row></table><note>Results on HICO. "Pairwise</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Results on HICO-DET. mAP on PaSta recognition in PaStaNet* mode: 46.3 (head), 66.8 (arms), 32.0 (hands), 68.6 (hip), 56.2 (thighs),65.8 (feet). This verifies that PaSta can be better learned than activities, thus they can be learned ahead as the basis for reasoning. In GT-PaStaNet* mode, hierarchical paradigm achieves 65.6 mAP. This is a powerful proof of the effectiveness of PaSta knowledge. Thus what remains to do is to improve the PaSta recognition and further promote the activity task performance. Moreover, in PaStaNet mode, we achieve relative 16% improvement. On few-shot sets, our best result significantly improves 13.9 mAP, which strongly proves the reusability and transferability of PaSta.</figDesc><table><row><cell></cell><cell></cell><cell>Default</cell><cell></cell><cell></cell><cell cols="2">Known Object</cell></row><row><cell>Method</cell><cell>Full</cell><cell cols="2">Rare Non-Rare</cell><cell>Full</cell><cell cols="2">Rare Non-Rare</cell></row><row><cell>InteractNet [22]</cell><cell>9.94</cell><cell>7.16</cell><cell>10.77</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>GPNN [48]</cell><cell cols="2">13.11 9.34</cell><cell>14.23</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>iCAN [17]</cell><cell cols="2">14.84 10.45</cell><cell>16.15</cell><cell cols="2">16.26 11.33</cell><cell>17.73</cell></row><row><cell>TIN [33]</cell><cell cols="2">17.03 13.42</cell><cell>18.11</cell><cell cols="2">19.17 15.51</cell><cell>20.26</cell></row><row><cell>iCAN [17]+PaStaNet*-Linear</cell><cell cols="2">19.61 17.29</cell><cell>20.30</cell><cell cols="2">22.10 20.46</cell><cell>22.59</cell></row><row><cell>TIN [33]+PaStaNet*-Linear</cell><cell cols="2">22.12 20.19</cell><cell>22.69</cell><cell cols="2">24.06 22.19</cell><cell>24.62</cell></row><row><cell>TIN [33]+PaStaNet*-MLP</cell><cell cols="2">21.59 18.97</cell><cell>22.37</cell><cell cols="2">23.84 21.66</cell><cell>24.49</cell></row><row><cell>TIN [33]+PaStaNet*-GCN</cell><cell cols="2">21.73 19.55</cell><cell>22.38</cell><cell cols="2">23.95 22.14</cell><cell>24.49</cell></row><row><cell>TIN [33]+PaStaNet*-Seq</cell><cell cols="2">21.64 19.10</cell><cell>22.40</cell><cell cols="2">23.82 21.65</cell><cell>24.47</cell></row><row><cell>TIN [33]+PaStaNet*-Tree</cell><cell cols="2">21.36 18.83</cell><cell>22.11</cell><cell cols="2">23.68 21.75</cell><cell>24.25</cell></row><row><cell>PaStaNet*-Linear</cell><cell cols="2">19.52 17.29</cell><cell>20.19</cell><cell cols="2">21.99 20.47</cell><cell>22.45</cell></row><row><cell cols="3">TIN [33]+GT-PaStaNet*-Linear 34.86 42.83</cell><cell>32.48</cell><cell cols="2">35.59 42.94</cell><cell>33.40</cell></row><row><cell>TIN [33]+PaStaNet-Linear</cell><cell cols="2">22.65 21.17</cell><cell>23.09</cell><cell cols="2">24.53 23.00</cell><cell>24.99</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 .</head><label>3</label><figDesc>Transfer learning results on V-COCO [25].(arms), 17.5 (hands), 41.8 (hip), 22.2 (thighs), 29.9 (feet). This again verifies that PaSta can be well learned. And GT-PaStaNet* (upper bound) and PaStaNet (more PaSta labels) modes both greatly boosts the performance. On Rare sets, our method obtains 7.7 mAP improvement.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>LFB-Res-101-baseline<ref type="bibr" target="#b58">[59]</ref>+PaStaNet-Linear 24.3</figDesc><table><row><cell>+PaStaNet-Linear</cell><cell>15.6</cell></row><row><cell>LFB-Res-50-baseline [59]+PaStaNet-Linear</cell><cell>23.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 .</head><label>5</label><figDesc>Characteristics of PastaNet. "Existing-Datasets" and "Crowdsourcing" indicate the image sources of PastaNet.</figDesc><table><row><cell>28 24 26 18 20 22</cell><cell>29 25 27 19 21 23</cell><cell>6% 9%</cell><cell>1% 12% 1% 1% 1% 6% 4%</cell><cell>25%</cell><cell>Law Education Literature Engineering Natural Science Medicine Economics Agriculture</cell><cell>19%</cell><cell>2% 1% 1% 1% 1% 6%</cell><cell>High school degree Bachelor Degree Master Degree</cell><cell>26%</cell><cell>1%</cell></row><row><cell></cell><cell></cell><cell>10%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>69%</cell><cell></cell><cell></cell><cell>73%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>24%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>(a) Age</cell><cell></cell><cell></cell><cell></cell><cell>(b) Major</cell><cell cols="3">(c) Education Degree</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>HOIs adjust, assemble, block, blow, board, break, brush with, board gaming, buy, carry, catch, chase, check, chop, clean, clink glass, close, control, cook, cut, cut with, dig, direct, drag, dribble, drink with, drive, dry, eat, eat at, enter, exit, extract, feed, fill, flip, flush, fly, fight, fishing, give sth to sb, grab, greet, grind, groom, hand shake, herd, hit, hold, hop on, hose, hug, hunt, inspect, install, jump, kick, kiss, lasso, launch, lick, lie on, lift, light, listen to sth, listen to a person, load, lose, make, milk, move, open, operate, pack, paint, park, pay, peel, pet, play musical instrument, play with sb, play with pets, pick, pick up, point, pour, press, pull, push, put down, put on, race, read, release, repair, ride, row, run, sail, scratch, serve, set, shear, shoot, shovel, sign, sing to sb, sip, sit at, sit on, slide, smell, smoke, spin, squeeze, stab, stand on, stand under, stick, stir, stop at, straddle, swing, tag, take a photo, take sth from sb, talk on, talk to, teach, text on, throw, tie, toast, touch, train, turn, type on, walk, wash, watch, wave, wear, wield, work on laptop, write, zip Body Motions bow, clap, climb, crawl, dance, fall, get up, kneel, physical exercise, swim</figDesc><table><row><cell>Objects</cell><cell>airplane, apple, backpack, banana, baseball bat, baseball glove, bear, bed, bench, bicycle, bird, boat,</cell></row><row><cell></cell><cell>book, bottle, bowl, broccoli, bus, cake, car, carrot, cat, cell phone, chair, clock, couch, cow, cup, dining</cell></row><row><cell></cell><cell>table, dog, donut, elephant, fire hydrant, fork, frisbee, giraffe, hair drier, handbag, horse, hot dog,</cell></row><row><cell></cell><cell>keyboard, kite, knife, laptop, microwave, motorcycle, mouse, orange, oven, parking meter, person,</cell></row><row><cell></cell><cell>pizza, potted plant, refrigerator, remote, sandwich, scissors, sheep, sink, skateboard, skis, snowboard,</cell></row><row><cell></cell><cell>spoon, sports ball, stop sign, suitcase, surfboard, teddy bear, tennis racket, tie, toaster, toilet, toothbrush,</cell></row><row><cell></cell><cell>traffic light, train, truck, tv, umbrella, vase, wine glass, zebra</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 .Table 7 .</head><label>67</label><figDesc>Activities and related objects in our PaStaNet. HOIs indicate the interactions between person and object/person. Head eat, inspect, talk with sth, talk to, close with, kiss, raise up, lick, blow, drink with, smell, wear, no action Arm carry, close to, hug, swing, no action Hand hold, carry, reach for, touch, put on, twist, wear, throw, throw out, write on, point with, point to, use sth to point to, press, squeeze, scratch, pinch, gesture to, push, pull, pull with sth, wash, wash with sth, hold in both hands, lift, raise, feed, cut with sth, catch with sth, pour into, no action Human body part states (PaSta) in our PaStaNet.</figDesc><table><row><cell>Hip</cell><cell>sit on, sit in, sit beside, close with, no action</cell></row><row><cell>Thigh</cell><cell>walk with, walk to, run with, run to, jump with, close with, straddle, jump down, walk away, no action</cell></row><row><cell>Foot</cell><cell>stand on, step on, walk with, walk to, run with, run to, dribble, kick, jump down, jump with, walk away,</cell></row><row><cell></cell><cell>no action</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>.Figure 8. Part of the Activity Parsing Tree.</figDesc><table><row><cell>hug</cell><cell></cell><cell>wash</cell><cell></cell><cell cols="2">inspect</cell><cell></cell><cell>feed</cell><cell>feed</cell></row><row><cell>elephant</cell><cell></cell><cell>bowl</cell><cell></cell><cell>ball</cell><cell></cell><cell></cell><cell>sheep</cell><cell>bear</cell></row><row><cell>0.469</cell><cell>0.571</cell><cell></cell><cell>0.286</cell><cell>0.006</cell><cell>0.991</cell><cell>0.501</cell><cell>0.500</cell><cell>0.386</cell></row><row><cell>0.531</cell><cell></cell><cell cols="2">0.143</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.614</cell></row><row><cell>arm hug</cell><cell>hand wash</cell><cell></cell><cell>hip sit on</cell><cell>hand hold</cell><cell>hand touch</cell><cell>head inspect</cell><cell>hand feed</cell><cell>foot stand on</cell></row><row><cell>0.695</cell><cell>0.304</cell><cell>0.345</cell><cell>0.481</cell><cell>0.155</cell><cell>0.333</cell><cell></cell><cell>0.230</cell><cell>0.016</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.667</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.702</cell></row><row><cell cols="2">hug cow</cell><cell></cell><cell>drive bus</cell><cell></cell><cell>clean bed</cell><cell></cell><cell cols="2">sit on boat</cell></row><row><cell cols="9">board_airplane direct_airplane exit_airplane fly_airplane inspect_airplane load_airplane ride_airplane sit_on_airplane wash_airplane no_interaction_airplane carry_bicycle hold_bicycle inspect_bicycle jump_bicycle hop_on_bicycle park_bicycle push_bicycle repair_bicycle ride_bicycle sit_on_bicycle straddle_bicycle walk_bicycle wash_bicycle no_interaction_bicycle chase_bird feed_bird hold_bird pet_bird release_bird watch_bird no_interaction_bird board_boat drive_boat exit_boat inspect_boat jump_boat launch_boat repair_boat ride_boat row_boat sail_boat sit_on_boat stand_on_boat tie_boat wash_boat no_interaction_boat carry_bottle drink_with_bottle hold_bottle inspect_bottle lick_bottle open_bottle pour_bottle no_interaction_bottle board_bus direct_bus drive_bus exit_bus inspect_bus load_bus ride_bus sit_on_bus wash_bus wave_bus no_interaction_bus board_car direct_car drive_car hose_car inspect_car jump_car load_car park_car ride_car wash_car no_interaction_car dry_cat feed_cat hold_cat hug_cat kiss_cat pet_cat scratch_cat wash_cat chase_cat no_interaction_cat carry_chair hold_chair lie_on_chair sit_on_chair stand_on_chair no_interaction_chair carry_couch lie_on_couch sit_on_couch no_interaction_couch feed_cow herd_cow hold_cow hug_cow kiss_cow lasso_cow milk_cow pet_cow ride_cow walk_cow no_interaction_cow clean_dining_table eat_at_dining_table sit_at_dining_table no_interaction_dining_table carry_dog dry_dog feed_dog groom_dog hold_dog hose_dog hug_dog inspect_dog kiss_dog pet_dog run_dog scratch_dog straddle_dog train_dog walk_dog wash_dog chase_dog no_interaction_dog feed_horse groom_horse hold_horse hug_horse jump_horse kiss_horse load_horse hop_on_horse pet_horse race_horse ride_horse run_horse straddle_horse train_horse walk_horse wash_horse no_interaction_horse hold_motorcycle inspect_motorcycle jump_motorcycle</cell></row><row><cell>shank foot,stand on,object shank foot,tread step on,object shank foot,walk with,object shank foot,walk towards,object shank foot,run with,object shank foot,run towards,object shank foot,dribble,object shank foot,kick,object shank foot,jump down,object shank foot,jump with,object shank foot,walk away,object shank foot,no interaction,object thigh knee,walk with,object thigh knee,walk towards,object thigh knee,run with,object thigh knee,run towards,object thigh knee,jump with,object thigh knee,close with,object thigh knee,straddle bestride,object thigh knee,jump down,object thigh knee,walk away,object thigh knee,no interaction,object hip crotch,sit on,object hip crotch,sit in,object hip crotch,sit beside,object hip crotch,close with,object hip crotch,no interaction,object forearm hand,hold,object forearm hand,carry,object forearm hand,reach for,object forearm hand,touch,object forearm hand,put on,object forearm hand,twist,object forearm hand,wear,object forearm hand,throw,object forearm hand,throw out,object forearm hand,write on,object forearm hand,point with,object forearm hand,point towards,object forearm hand,use something point towards,object forearm hand,press,object forearm hand,squeeze,object forearm hand,scratch,object forearm hand,pinch,object forearm hand,gesture towards,object forearm hand,push,object forearm hand,pull,object forearm hand,pull with something,object forearm hand,wash,object forearm hand,wash with something,object forearm hand,hold in both hands,object forearm hand,lift,object forearm hand,raise,object forearm hand,feed,object forearm hand,cut with something,object forearm hand,catch with something,object forearm hand,pour into,object forearm hand,no interaction,object shoulder arm,shoulder carry,object shoulder arm,close towards,object shoulder arm,hug,object shoulder arm,swing,object shoulder arm,no interaction,object head,eat,object head,inspect,object head,talk with something,object head,talk towards,object head,close with,object head,kiss,object head,put something over head,object head,lick,object head,blow,object head,drink with,object head,smell,object head,wear,object</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Data usage</head><p>The data usages of pre-training and finetuning are clarified in Tab. 8. We have carefully excluded the testing data in all pre-training and finetuning to avoid data pollution.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hico: A benchmark for recognizing human-object interactions in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><forename type="middle">Wei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yugeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to detect human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xieyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huayi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Potion: Pose motion representation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jérôme</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Word association norms, mutual information, and lexicography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><forename type="middle">Ward</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Hanks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational linguistics</title>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning person-object interactions for action recognition in still images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Delaitre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wendy</forename><surname>Doniger</surname></persName>
		</author>
		<title level="m">Reductionism</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Bernard Ghanem Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">RMPE: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuqin</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pairwise body-part attention for recognizing human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinkun</forename><surname>Hao Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><forename type="middle">Wing</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Instaboost: Boosting instance segmentation via probability map guided copypasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhua</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runzhong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">ican: Instancecentric attention network for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.10437</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attentional pooling for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">R-cnns for pose estimation and action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Science</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Actions and attributes from wholes and parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Contextual action recognition with r* cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Detecting and recognizing human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A video dataset of spatio-temporally localized atomic visual actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhui</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeqing</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanna</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhui</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeqing</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanna</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sukthankar</surname></persName>
		</author>
		<ptr target="https://research.google.com/ava/download.html" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Visual semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04474</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Recognising human-object interaction via exemplar based modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian Fang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shi Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhuang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TPAMI</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Openimages: A public dataset for large-scale multi-label and multi-class image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<ptr target="https://github.com/openimages" />
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Transferable interactiveness knowledge for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xijie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tsung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Pic-person in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lejian</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanghui</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<ptr target="http://picdataset.com/challenge/index/" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Visual relationship detection with language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Beyond holistic object recognition: Enriching image understanding with part states</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongyi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JMLR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Action recognition from a distributed representation of pose and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning models for actions and person-object interactions with transfer to question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A framework for multiple-instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oded</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toms</forename><surname>Lozano-Prez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep rnn framework for visual sequential applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwen</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Further understanding videos through adverbs: A new video task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning human-object interactions by graph parsing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxiong</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Find and focus: Retrieve and localize video events with natural language queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dian</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingqiu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Human action recognition using factorized spatio-temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertram</forename><forename type="middle">E</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Pose primitive based human action recognition in videos or still images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Thurau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaclav</forename><surname>Hlavac</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Human action recognition by representing 3d skeletons as points in a lie group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raviteja</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><surname>Arrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Long-term feature banks for detailed video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Srda: Generating instance segmentation annotation via scanning, reasoning and domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Recognizing human actions from still images with latent poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Modeling mutual context of object and human pose in human-object interaction activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangpeng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Action recognition with exemplar based 2.5d graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangpeng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Human action recognition by learning bases of action attributes and parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangpeng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoye</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><forename type="middle">Lai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Single image action recognition using semantic body part actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaodi</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Ian Reid, and Anton van den Hengel. Care about you: towards largescale human-centric visual relationship detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.09892</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Towards understanding action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
