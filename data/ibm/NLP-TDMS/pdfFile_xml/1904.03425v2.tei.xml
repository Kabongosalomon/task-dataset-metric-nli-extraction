<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Novel Unsupervised Camera-aware Domain Adaptation Framework for Person Re-identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Qi</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computing and Information Technology</orgName>
								<orgName type="institution">University of Wollongong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huo</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luping</forename><surname>Zhou</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Electrical and Information Engineering</orgName>
								<orgName type="institution">The University of Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghuan</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Novel Unsupervised Camera-aware Domain Adaptation Framework for Person Re-identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised cross-domain person re-identification (Re-ID) faces two key issues. One is the data distribution discrepancy between source and target domains, and the other is the lack of label information in target domain. They are addressed in this paper from the perspective of representation learning. For the first issue, we highlight the presence of camera-level sub-domains as a unique characteristic of person Re-ID, and develop "camera-aware" domain adaptation to reduce the discrepancy not only between source and target domains but also across these sub-domains. For the second issue, we exploit the temporal continuity in each camera of target domain to create discriminative information. This is implemented by dynamically generating online triplets within each batch, in order to maximally take advantage of the steadily improved feature representation in training process. Together, the above two methods give rise to a novel unsupervised deep domain adaptation framework for person Re-ID. Experiments and ablation studies on benchmark datasets demonstrate its superiority and interesting properties.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Person re-identification (Re-ID) matches images of the same identity captured by different cameras of nonoverlapping views <ref type="bibr" target="#b3">[4]</ref>. In unsupervised cross-domain person Re-ID, labeled data is only available in source domain, while all data in target domain is unlabeled <ref type="bibr" target="#b19">[20]</ref>. It aims to learn an effective model to conduct Re-ID in target domain.</p><p>Unsupervised cross-domain person Re-ID faces two key issues. One is the data distribution discrepancy of source and target domains, caused by the variations such as body pose, view angle, illumination, image resolution, occlusion and background. The other is the lack of label information in target domain, due to time-consuming or even infeasible manual annotation in real applications. Tangible progress has been made to address them <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr">31,</ref><ref type="bibr" target="#b1">2]</ref>, as reviewed in next section. However, unsupervised crossdomain person Re-ID is still far from satisfactory, especially when compared with the supervised counterpart <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">29,</ref><ref type="bibr" target="#b10">11]</ref>. This paper aims to better address the above two issues from the perspective of representation learning. We try to learn a deep feature representation (or equally, a shared subspace) in which the discrepancy of source and target domains is more effectively reduced and the discriminative information in target domain is more effectively reflected.</p><p>First, in most practical applications, the camera ID of a frame can be readily obtained <ref type="bibr" target="#b0">1</ref> . The frames from the same camera exhibit a common visual style (e.g., in terms of background, image resolution, viewing angle, and so on). This camera-specific style has recently started attracting attention in person Re-ID (e.g., image-image translation) <ref type="bibr">[31]</ref>. The presence of camera-level sub-domains is a unique characteristic of cross-domain person Re-ID. Nevertheless, it has not been well exploited from the perspective of representation learning. To utilize this characteristic, we propose a novel camera-aware domain adaptation. It emphasizes that with the learned representation, the distribution discrepancy across camera-level sub-domains shall also be sufficiently reduced, dealing with the discrepancy of source and target domains at a finer level. This idea is realized by adversarial learning with a novel criterion called crossdomain camera equiprobability.</p><p>Second, temporal information is usually available for the images from a camera (e.g., via frame ID) in person Re-ID. Temporally close frames more likely correspond to the same subject, which has long been used as an important cue for video analysis. Nevertheless, it has not been well exploited for unsupervised cross-domain person Re-ID to handle the lack of label information in target domain. In this paper, temporal information is jointly used with image distances to generate, in an unsupervised manner, triplets from each camera in target domain. A smart scheme is proposed to better seek true positive and true negative images. More importantly, instead of generating triplets offline, we dynamically generate triplets online in each batch during training. This allows us to fully take advantage of the steadily improved feature representation to produce better triplets. In turn, these triplets help networks to learn better feature representations, forming a positive loop to exploit discriminative information from target domain. Together, the above two improvements give rise to a novel domain adaptation framework for unsupervised crossdomain person Re-ID. As will be shown, both improvements are essential and it is their joint effort that makes the proposed framework really function. For clarity, the contributions in this work are summarized as follows.</p><p>First, a camera-aware domain adaptation method is developed by considering the unique presence of camera-level sub-domains in person Re-ID. To the best of our survey, our adversarial learning based method is the first one, among those aiming to learn better feature representation, to integrate source and target domains at this fine level.</p><p>Second, an unsupervised online in-batch triplet generation method is proposed to explore the underlying discriminative information in unlabeled target domain. Through high-quality triplets, it provides important information to boost the performance of the whole framework.</p><p>Last, both theoretical analysis and experimental studies are conducted to illustrate the proposed camera-aware domain adaptation. The results and ablation studies demonstrate the superiority of the proposed framework and its interesting properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Unsupervised domain adaptation.</p><p>Unsupervised cross-domain person Re-ID is related to unsupervised domain adaptation, a more general technique handling unlabeled target domain with the help of labeled source domain. In the literature, most unsupervised domain adaptation methods learn a common mapping between source and target distributions. Several methods based on the maximum mean discrepancy (MMD) have been proposed <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b21">22]</ref>. Long et al. <ref type="bibr" target="#b13">[14]</ref> use a new deep adaptation network, where hidden representations of all task-specific layers are embedded in a Reproducing Kernel Hilbert space. To transfer a classifier from source domain to target do-main, the work in <ref type="bibr" target="#b14">[15]</ref> jointly learns adaptive classifiers between the two domains by a residual function. In <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b2">3]</ref>, autoencode-based methods are investigated to explore the discriminative information in target domain. Recently, adversarial learning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b20">21]</ref> has been applied to domain adaptation. <ref type="bibr">Ganin et al. [7]</ref> propose the gradient reversal layer (GRL) to pursue the same distribution between source and target domains. Inspired by generative adversarial networks (GANs), Tzeng et al. <ref type="bibr" target="#b20">[21]</ref> leverage a GAN loss to match the data distributions of source and target domains.</p><p>Nevertheless, for a person Re-ID task, the distribution discrepancy also exists at the camera level. It will not be effectively reduced when only the overall domain-level discrepancy is concerned. In this sense, directly applying existing unsupervised domain adaptation methods to a person Re-ID task may not be the best option.</p><p>Unsupervised cross-domain person Re-ID. As previously mentioned, most existing methods on this topic address two issues, i.e., reducing data distribution discrepancy between two domains and generating discriminative information for target domain. In the literature, methods have been developed to learn a shared subspace or dictionary across domains <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20]</ref>. However, these methods are not based on deep learning and thus do not fully explore the high-level semantics in images. Recently, several deep-learning-based methods <ref type="bibr">[31,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b15">16]</ref> have been seen. In <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b22">23]</ref>, generating pseudo labels for target images is investigated. Lv et al. <ref type="bibr" target="#b15">[16]</ref> propose an unsupervised incremental learning algorithm, aided by the transfer learning of spatio-temporal patterns of pedestrians in target domain. In <ref type="bibr" target="#b22">[23]</ref>, the proposed approach simultaneously learns an attribute-semantic and identity-discriminative feature representation in target domain. However, when generating discriminative information for target domain, the above methods do not utilize temporal continuity of images in each camera. Moreover, the generation of information is usually conducted offline and separately, instead of on the fly during training. All of these will be improved in our framework.</p><p>Recently, generating extra training images for target domain has become popular <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr">31,</ref><ref type="bibr" target="#b1">2]</ref>. Wei et al. <ref type="bibr" target="#b24">[25]</ref> impose constraints to maintain the identity in image generation. The approach in <ref type="bibr" target="#b5">[6]</ref> enforces the self-similarity of an image before and after translation and the domaindissimilarity of a translated source image and a target image. Zhong et al. <ref type="bibr">[31]</ref> propose to seek camera invariance by using unlabeled target images and their camera-style transferred counterparts as positive matching pairs. Besides, it views source and target images as negative pairs for the domain connectedness. Note that these methods have attempted to deal with the distribution discrepancy at the camera level. Differently, they reduce the discrepancy through the approach of image generation, rather than learning better representation as in this paper. As will be demonstrated in the experiment, our approach can produce better person Re-ID performance in target domain than these methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Proposed Framework</head><p>Our framework consists of three objectives, including i) classification of the labeled images in source domain; ii) camera-aware domain adaptation via adversarial learning; and iii) enforcing discrimination information in target domain. The first objective is not our focus and is implemented by following the literature <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b13">14]</ref>. The second and third objectives are detailed in Sections 3.1 and 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Camera-aware domain adaptation</head><p>In person Re-ID, images of different cameras form a set of unique units. This is also reflected in cross-domain discrepancy. Merely reducing the overall discrepancy of source and target domains will not effectively handle the camera-level discrepancy, and this could adversely affect the quality of learned feature representation. We put forward a camera-aware domain adaptation to map the images of different cameras into a shared subspace. To achieve this, a camera-aware adversarial learning (CAL in short) method is developed. Unlike conventional adversarial learning dealing with two domains <ref type="bibr" target="#b6">[7]</ref>, CAL deals with multiple subdomains (i.e., cameras in source and target domains).</p><p>Let X s and X t be the training images in source and target domains and X = [X s , X t ]. The camera IDs (i.e., the label of each camera class) of the images in X are denoted by Y c . Let C s and C t denote the number of the cameras in source and target domains, respectively, and C = C s + C t . Adversarial learning involves the optimization of discriminator and generator. As commonly seen, the discriminator in this work is optimized by a cross-entropy loss defined on the C camera classes in source and target domains as</p><formula xml:id="formula_0">min D L CAL−D (X, Y c , B) = min D E (x,yc)∼(X,Yc) − C k=1 δ(y c − k) log D(B(x), k) ,<label>(1)</label></formula><p>where x denotes an image, y c the camera class label of x, and δ(·) the Dirac delta function. B denotes the backbone network, and B(x) is the feature representation of x. D denotes the discriminator and D(B(x), k) denotes the prediction score for x with respect to the k-th camera class.</p><p>The generator in this work is the backbone network B. Due to the special need to deal with multiple camera classes, the optimization of B becomes tricky. We first investigate the gradient reversal layer (GRL) technique <ref type="bibr" target="#b6">[7]</ref>. By showing its limitations for this task, we propose a new criterion "cross-domain camera equiprobability" (CCE) for our task.</p><p>GRL-based adaptation scheme. The gradient reversal layer (GRL) <ref type="bibr" target="#b6">[7]</ref> is commonly used to reduce distribution discrepancy of two domains by maximizing the domain discrimination loss (i.e., Eq.(1)). A direct extension of GRL to our task leads to optimizing the generator B as</p><formula xml:id="formula_1">min B L CAL−B (X, Y c , D) max B L CAL−D (X, Y c , D) = min B E (x,yc)∼(X,Yc) C k=1 δ(y c − k) log D(B(x), k) ,<label>(2)</label></formula><p>where for consistency it is written as minimizing the negative discriminator loss.</p><p>To train the backbone network B with Eq.(2), we insert GRL between B and D as in the literature <ref type="bibr" target="#b6">[7]</ref>. During forward propagation, GRL is simply an identity transform. During backpropagation, GRL reverses (i.e., multiplying by a negative constant) the gradients of the domain discriminator loss with respect to the network parameters in feature extraction layers and pass them backwards. This GRLbased adaptation scheme can somehow work to reduce distribution discrepancy across different cameras (i.e., subdomains), as will be experimentally demonstrated shortly.</p><p>However, this scheme has a drawback. Maximizing the discriminator loss only enforces an image not to be classified into its true camera class. It will appear to be "equivalently good" for this optimization as long as an image is classified into any wrong camera classes, including those from the same domain. In this case, this scheme will not be able to effectively pull source and target domains together. The larger the discrepancy between source and target domains is, the more pronounced this issue could be.</p><p>CCE-based adaptation scheme. In this scheme, we enforce "preferred misclassification patterns" to maximize the discriminator loss. Noting that the primary goal of crossdomain person Re-ID is to reduce the distribution discrep-ancy of source and target domains, we require that with the learned feature representation, an image from a camera class in source domain shall be wrongly classified into a camera class in target domain and vice versa. As we do not have any bias on the camera classes in the opposite domain, it is required that the image shall be misclassified into them with equal probability, as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. This is where the name "cross-domain camera equiprobability" (CCE) comes from. This scheme effectively avoids considering the misclassification among the camera classes of the same domain, which is unfavourably done in the GRL scheme, and makes specific efforts to pull source and target domains together. Also, the CCE-based scheme can practically lead to alleviating the discrepancy across all cameras in target domain, as demonstrated in the experiment.</p><p>Let S and T denote source and target domains. Formally, the CCE loss on an image x can be expressed as</p><formula xml:id="formula_2">L CCE (x) = − 1 Cs Cs i=1 log(D(B(x), i)), x ∈ T − 1 Ct Ct j=1 log(D(B(x), j)), x ∈ S (3)</formula><p>For x in target domain, D(B(x), i) denotes the predicted probability that x belongs to the ith camera class in source domain. Similar definition applies to D(B(x), j) for x in source domain. In this way, the optimization for training the backbone network B (as a generator) is defined as</p><formula xml:id="formula_3">min B L CAL−B (X, D) = min B E x∼X L CCE (x, D).<label>(4)</label></formula><p>During adversarial training, D and B are trained in an alternate way. Each iteration consists of two steps: i) Temporarily fixing the weights of B, D is trained by Eq.(1) to predict the camera ID of each image; ii) Temporarily fixing the weights of D, B is trained by Eq.(4) to learn feature representation. This is repeated till convergence. Note that traditional two-domain adversarial learning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b20">21]</ref> is just a special case of this CCE-based scheme when there is exactly one camera class in each of source and target domains.</p><p>Remarks. A question may arise: why do we prefer the CCE criterion rather than simply requiring an image from each camera class to be equiprobably misclassified into all the other (i.e., C s + C t − 1) camera classes? This is because in person Re-ID the cross-domain discrepancy is usually more significant than the within-domain counterpart and affects the performance more. By enforcing the cross-domain camera equiprobability, the CCE criterion puts higher priority on reducing the former discrepancy and therefore is a better option. Its effectiveness and advantage will be validated in the experimental study.</p><p>Theoretical analysis of CCE. At last, we provide theoretical analysis to gain more insight on this new criterion. The full analysis is provided in the supplement material. Proposition. Let S and T denote source and target domains. x s and x t are the images from the two domains; p s (x) and p t (x) are their probability density functions; and C s and C t are the number of camera classes in these two domains. Let p(x|C s i ) and p(x|C t i ) be the class-conditional density functions of the ith camera class in source and target domains, respectively. It can be proved (see supplement material) that ideally, minimizing the CCE loss will lead to</p><formula xml:id="formula_4">p(x s |C t i ) = p s (x s ), ∀x s ∈ S; i = 1, · · · , C t . (5) p(x t |C s i ) = p t (x t ), ∀x t ∈ T ; i = 1, · · · , C s . p s (x) = p t (x), ∀x ∈ S ∪ T .</formula><p>This result indicates that in the learned shared space: i) For any image in source domain, it will not feel the discrepancy among the C t camera classes in target domain. Its class-conditional density function value for these camera classes (e.g., p(x s |C t i )) just equals its density function value in its own domain (e.g., p s (x s )). The above conclusion also applies to any image in target domain in a similar but reverse way.</p><p>ii) The data distributions of source and target domains, p s (x) and p t (x), will become identical and the overall cross-domain distribution discrepancy can be removed.</p><p>Meanwhile, it is worth mentioning that minimizing the CCE loss does not theoretically guarantee that an image (in either source or target domain) will not feel the distribution discrepancy among the camera classes in its own domain. Nevertheless, jointly considering the above three proved equalities, it can be reasonably expected (see the supplement) that the above situation could be observed in practice. This will be experimentally demonstrated shortly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Unsupervised online triplet generation</head><p>Only reducing cross-domain distribution discrepancy is insufficient, even though the above camera-aware domain adaptation is deployed. Rather, maintaining the intrinsic properties of target domain is equally essential. Otherwise, the distribution of target domain could be arbitrarily altered just for reducing the distribution discrepancy, significantly degrading the Re-ID performance in target domain. To avoid this, this framework explores the underlying discriminative information in target domain.</p><p>This information is explored in the form of image triplet, consisting of an anchor image, a positive image (i.e., sharing the same identity as the anchor) and a negative image (i.e., having a different identity). When selecting positive and negative images, we not only consider the distance between images but also jointly utilize the temporal information among images, which can often be obtained via the frame ID in person Re-ID. More importantly, we generate triplets online in each batch during training. This allows triplet generation to effectively take advantage of the steadily improved feature representation to produce better triplets. Note that the triplet generation is carried out in an unsupervised manner and only needed in training process.</p><p>Given a camera in target domain, all of its images are sorted temporally into a list. From this list, p nonoverlapping fragments are randomly selected to construct a batch. Each fragment consists of q images, and the batch therefore contains n (= p×q) images in total. Each of the n images is used as an anchor image to create triplets in turn. To begin with, a pairwise distance matrix M ∈ R n×n is computed for the n images, with the feature representation learned by the network so far. To generate triplets for an anchor image I a , we develop the following rules.</p><p>Above all, according to M, sort all the (n − 1) images (excluding I a ) in the batch in ascending order of the distance from I a . The obtained list is denoted by S(I a ).</p><p>Positive image selection. To be selected as positive, an image must meet both the requirements: i) it is within the top-k positions of S(I a ), and ii) it is from the same fragment as I a . The first requirement ensures that this image is indeed similar to I a in terms of feature representation, while the second one further increases its likelihood of positiveness with temporal information. Jointly using these two requirements helps us to select highly likely (not guaranteed though) true positive images. In implementation, k is empirically set. The total number of selected positive images is denoted by k p . Note that k p could be zero, meaning that this anchor cannot find any positive images by the above rule. In this case, this anchor will not be taken into account.</p><p>Negative image selection. Starting from the head of the list S(I a ), each image I is checked in turn against the following conditions: 1) I is not from the same fragment of I a , and 2) no image in the fragment of I has previously been selected as negative. That is, the negative images are selected as the nearest neighbours of the anchor from the fragments other than the anchor's, with the condition that each of these negative images shall be, respectively, collected from a different fragment. Such a rule is designed to deal with the case that the same person may reappear in two or more fragments. Requiring each negative sample to reside in different fragments well reduces (although cannot entirely avoid) the chance to mis-select a truly positive sample as negative. The total number of selected negative images is denoted by k n .</p><p>Once triplets are generated in a training batch, we can train the backbone network via a triplet loss defined as</p><formula xml:id="formula_5">L Triplet = n a=1 w a [d p (I a ) −d n (I a ) + m] + ,<label>(6)</label></formula><p>where w a is zero if I a has no positive images and one otherwise. in case any positive or negative sample is wrongly selected. d(·, ·) is just the distance used for the distance matrix M.</p><p>Finally, although M can be simply calculated by Euclidean distance, more advanced measures can be readily used. This work uses a re-ranking algorithm [30] to improve M, in order to generate even better triplets and further improve the person Re-ID performance in target domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">The overall proposed framework</head><p>Recall that this framework consists of an adversarial task across the cameras (sub-domains) in source and target domains, a discrimination task for target domain, and a classification task for source domain, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. The overall loss function in a training batch is expressed as</p><formula xml:id="formula_6">min D L CAL−D (X, Y c , B) = min D E (x,yc)∼(X,Y C ) − C k=1 δ(y c − k) log D(B(x), k) , min B L(X, Z s , D) = min B (L Cross (X s , Z s ) + λ 1 L Triplet (X t ) + λ 2 L CAL−B (X, D)) ,<label>(7)</label></formula><p>where L Cross , L Triplet and L CAL−B are the cross-entropy loss for source domain, the triplet loss for target domain, and the adversarial loss for B. λ 1 and λ 2 are the trade-off parameters. Z s is the person IDs of X s in source domain. To caclulate L Triplet , one camera in target domain is randomly chosen to construct the training batch and generate triplets at each iteration.</p><p>In this framework, ResNet-50 <ref type="bibr" target="#b8">[9]</ref> is used as backbone network. Global average pooling (GAP) is used to obtain the 2048-d feature representation. To do person Re-ID in target domain, the 2048-d feature representation is extracted for each query and gallery images, and an L 2 normalization is applied. Euclidean distance is calculated to rank the gallery images for a query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and settings</head><p>We evaluate our approach on two large-scale benchmark datasets: Market1501 [28], DukeMTMC-reID (Duke in short) <ref type="bibr">[29]</ref>. Market1501 contains 1,501 persons with 32,668 images from six cameras. Among them, 12,936 images of 751 identities are used for training. For evaluation, there are 3,368 and 19,732 images in the query and gallery sets. DukeMTMC-reID has 1,404 persons from eight cameras, with 16,522 training images of 702 identities, 2,228 queries, and 17,661 gallery images. Both camera ID and Frame ID information are available on Market1501 and DukeMTMC-reID. To evaluate person Re-ID performance, we use Rank-1 accuracy and mAP <ref type="bibr">[28]</ref>. On Mar-ket1501, there are single-and multi-query evaluation protocols. We use the more challenging single-query protocol.</p><p>For training CAL, we randomly select 64 images from each of source and target domains in a batch. The 64 images from source domain are also used to train the classification component. To generate triplets, we set p (i.e., number of temporal fragments) and q (i.e., number of images per fragment) to 12 and 10. k and k n used in the selection of positive and negative samples are set as 5 and 2. The margin of triplet loss, m, is 0.3. λ 1 and λ 2 in Eq. <ref type="formula" target="#formula_6">(7)</ref> are set as 1.</p><p>The proposed model is trained with the SGD optimizer in a total of 100 epochs. The initial learning rates of the finetuned parameters (those in the pre-trained ResNet-50 on ImageNet <ref type="bibr" target="#b4">[5]</ref>) and the new parameters (those in the newly added layers) are 0.1 and 0.01, respectively. When the number of epochs exceeds 80, we decrease the learning rates by a factor of 0.1. The size of the input images is 256 × 128.</p><p>Note that the baseline model (denoted by BL) in this experiment represents the ResNet-50 <ref type="bibr" target="#b8">[9]</ref> with additional 1024-d fully connected (FC) layer and cross-entropy loss. UOT and UOT(eud) denote two unsupervised online triplet generation variants in which the aforementioned re-ranking algorithm [30] (our default setting) and Euclidean distance are used to compute the distance matrix M, respectively. CAL-GRL and CAL-CCE denote the proposed cameraaware adversarial learning (CAL) implemented by the GRL and CCE schemes, respectively. For clarity, we use UCDA-GRL 2 and UCDA-CCE to represent BL+UOT+CAL-GRL and BL+UOT+CAL-CCE, respectively. Note that for the proposed framework, camera IDs and frame IDs are only needed in training, but not in the test stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with the state-of-the-art methods</head><p>We compare our approach with seven state-of-theart unsupervised cross-domain person Re-ID approaches. Among them, there are two non-deep-learning-based methods (UMDL <ref type="bibr" target="#b18">[19]</ref> and UJSDL <ref type="bibr" target="#b19">[20]</ref>) and five deep-learning-2 UCDA is short for Unsupervised Camera-aware Domain Adaptation. based methods. The latter includes two recent pseudolabel-generation-based methods (TFusion <ref type="bibr" target="#b15">[16]</ref> and TJ-AIDL <ref type="bibr" target="#b22">[23]</ref>) and three recent image-generation-based approaches (PTGAN <ref type="bibr" target="#b24">[25]</ref>, SPGAN <ref type="bibr" target="#b5">[6]</ref> and HHL [31]). Mar-ket1501 and Duke are in turn used as source and target domains to compare these methods. <ref type="table" target="#tab_0">Table 1</ref> reports the result. As seen, our approach (i.e., UCDA-CCE) consistently achieves the best results in both settings. For the setting of "Duke→Market1501", it obtains 34.5% and 64.3% in mAP and Rank-1 accuracy, outperforming all the methods in comparison. In the setting of "Market1501→Duke", UCDA-CCE still excels. Particularly, compared with HHL [31], the state-of-the-art by using image generation to reduce the camera-level discrepancy in target domain, UCDA-CCE gains 9.5% (36.7 vs. 27.2) in mAP and 8.5% (55.4 vs. 46.9) in Rank-1 accuracy.</p><p>This result shows the advantage of our approach over the pseudo-label-generation-based and image-generation-based methods. It effectively alleviates the data distribution discrepancy at the camera level through representation learning. We will demonstrate this property in Section 4.4. Our approach can be easily extended to semi-supervised person Re-ID. That is, when within each camera in target domain, the labels of person identity become available for each frame (e.g., obtained by tracking algorithms or manual annotation), our approach can utilize this information by simply changing our unsupervised online triplet generation (UOT) to traditional triplet generation <ref type="bibr" target="#b9">[10]</ref>. We name this setting SOT where "S" stands for semi-supervised. <ref type="bibr" target="#b2">3</ref> Our approach is compared with TAUDL <ref type="bibr" target="#b11">[12]</ref>, a recent stateof-the-art method for video-based person Re-ID. Note that TAUDL utilizes the person identity labels within each camera in target domain but does not employ cross-domain adaptation from a source domain. This comparison aims to show that via CAL, our approach can effectively utilize source domain to produce better Re-ID features in target domain than TAUDL. This is validated in <ref type="table" target="#tab_1">Table 2</ref>. Compared with TAUDL, our approach (BL+SOT+CAL-GRL/CAL-CCE) achieves considerable improvement in both tasks of "Market1501→Duke" and "Duke→Market1501". To validate the effectiveness of the proposed cameraaware adversarial learning (CAL), we compare our approach with the state-of-the-art domain adaptation methods. They use domain-aware adversarial learning (DAL) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b20">21]</ref> to conduct adversarial learning between source and target domains only. We implement DAL by ourselves to ensure a fair comparison. As in <ref type="table">Table 3</ref>, DAL (BL+UOT+DAL and BL+SOT+DAL) is inferior to the proposed CAL in both unsupervised and semi-supervised settings. There is a large gap between them in both experiments. This result further demonstrates the advantage of CAL by considering the discrepancy across all camera-level sub-domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">On the effectiveness of CAL and UOT</head><p>In the following ablation studies, we provide more details on the effectiveness of the two proposed components: camera-aware adversarial learning (CAL) and unsupervised online triplet generation (UOT) in <ref type="table" target="#tab_2">Tables 4 and 5</ref>.</p><p>Effectiveness of CAL. First, BL+UOT+CAL-CCE consistently outperforms BL+UOT+CAL-GRL in <ref type="table" target="#tab_2">Table 4</ref>. The former improves 3.9% (64.3 vs. 60.4) and 7.7% (55.4 vs. 47.7) on Rank-1 accuracy in "Duke→Market1501" and "Market1501→Duke". This shows that the proposed CCEbased scheme can overcome the drawback of the GRLbased one, as analyzed in Section 3.1. Second, incorporating the proposed CAL (via either BL+UOT+CAL-CCE or BL+UOT+CAL-GRL in <ref type="table" target="#tab_2">Table 4</ref>) greatly improves over BL+UOT. This validates the effectiveness of CAL in helping reduce camera-level distribution discrepancy to learn better feature representation. Also, we validate the effectiveness of CAL in the semi-supervised setting. As seen in <ref type="table" target="#tab_3">Table 5</ref>, BL+SOT+CAL-CCE improves BL+SOT by 10.4% (49.6 vs. 39.2) and 6.4% (45.6 vs. 39.2) on mAP in "Duke→Market1501" and "Market1501→Duke".</p><p>Effectiveness of UOT. Adding the proposed unsupervised online triplets into the baseline (i.e., BL+UOT) clearly improves the baseline (BL), as seen in <ref type="table" target="#tab_2">Table 4</ref>. This confirms the benefit of exploiting discriminative information from target domain via the proposed UOT. In addition, we test two schemes to compute the distance matrix M in Section 3.2, via Euclidean distance (BL+UOT(eud)) and the default re-ranking algorithm (BL+UOT), respectively. As seen, BL+UOT does achieve better performance.</p><p>In addition, we are interested in what if CAL is used alone without the UOT component. Both BL+CAL-GRL and BL+CAL-CCE are investigated at the bottom of Table 4. As seen, merely using CAL is insufficient. They do not show sufficient improvement over BL+UOT or even BL+UOT(eud). BL+CAL-CCE even fails in the case of "Duke→Market1501", although showing some improvement on "Market1501→Duke". This result is explained as follows. Duke has more cameras than Market1501. Also, in the literature, when the same Re-ID model is applied to them, the performance on Duke is usually inferior to that on Market1501 <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr">29,</ref><ref type="bibr" target="#b10">11]</ref>. This shows that Market1501 is a relatively easier dataset than Duke. When Market1501 is target domain and the more challenging Duke is source domain, its distribution could be altered significantly in order to fit the Duke's, if no discriminative information from target domain is used as a regularizer. Meanwhile, the situation will become less critical in the other way round. This is because Duke's data distribution is more complicated and therefore has sufficient "capacity" to fit Market1501's.</p><p>The above result clearly shows the necessity of UOT in our framework. Working together, the two proposed components produce the best performance in <ref type="table" target="#tab_2">Table 4</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Further evaluation of the proposed framework</head><p>We examine the inter-domain (between source and target domains) and inter-camera (across all cameras in target domain) discrepancy to validate the effectiveness of CAL, as reported in <ref type="table" target="#tab_4">Table 6</ref>. Since our goal is to obtain better feature representation for target domain, we focus on target domain when examining the discrepancy across all cameras. In this experiment, we measure the inter-domain discrepancy by the distance d inter−domain = X s − X t 2 , where X s and X t denote the sample mean of source and target domains, respectively. To measure the inter-camera discrepancy, we use d inter−camera = 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ct</head><p>Ct c=1 X t,c − X t 2 , where X t,c is the sample mean of the cth camera class in target domain and C t is the total number of cameras in target domain. These distances are calculated in <ref type="table" target="#tab_4">Table 6</ref>. First, for the interdomain discrepancy, DAL, CAL-GRL and CAL-CCE are all smaller than BL. This validates that they are all able to reduce the discrepancy between source and target domains. Particularly, since DAL specifically focuses on the overall domain-level discrepancy, its distance is smaller than those of CAL-GRL and CAL-CCE. In addition, CAL-GRL has a slightly larger value than CAL-CCE. This is consistent with the analysis on the drawback of CAL-GRL in Section 3.1. Second, for the inter-camera discrepancy, both CAL-GRL and CAL-CCE achieve smaller distances than DAL because DAL does not consider camera-level discrepancy. Besides, this experiment shows that CAL-CCE achieves the smallest distance, showing its best capability in reducing the discrepancy across the cameras in target domain. Additionally, we visualize the data distributions obtained by the feature representation from BL and CAL-CCE in <ref type="figure">Fig. 3</ref>. The result further illustrates the effectiveness of CAL-CCE. As analyzed in the Remarks of Section 3.1, why do we prefer the CCE-based scheme rather than equiprobably mixing a camera class with all the other (C s + C t − 1) ones? In this experiment, we name this setting AOE, standing for "all others equiprobability." We compare AOE and CCE in <ref type="table" target="#tab_5">Table 7</ref>. As shown, the CCE-based scheme outperforms the AOE-based scheme in both tasks. This is consistent with our previous analysis. It indicates that during reducing camera-level discrepancy, giving the reduction of crossdomain discrepancy higher priority is beneficial because it is usually more significant than the within-domain counterpart and affects the performance more.</p><p>Finally, we validate the advantage of the unsupervised online triplet (UOT) generation by comparing it with the <ref type="figure">Figure 3</ref>. Visualization of data distributions at the domain-level and camera-level via t-SNE <ref type="bibr" target="#b17">[18]</ref>. The features of each image are extracted by the baseline (BL) and CAL-CCE in the task of "DukeMTMC-reID→Market1501", respectively. The top shows the distributions of source and target domains (i.e., interdomain).The bottom illustrates the distribution of each camera class in target domain (i.e., inter-camera on Market1501), where different colors denote different camera classes. As seen, the proposed CAL-CCE effectively "mixes" the two domains and the camera classes as expected. Illustration of the methods of DAL and CAL-GRL is in the supplement material.</p><p>offline way. To generate offline triplets, we use the same method in Section 3.2 with the mere difference that they are generated by the feature from the baseline model before training starts. In total, 88,719 and 100,742 triplets are generated on Market1501 and Duke. We randomly select 40 triplets in each batch (i.e., 120 samples, giving the same batch size as the online method) to train our model. As seen in <ref type="table" target="#tab_6">Table 8</ref>, offline method is clearly inferior. It confirms the advantage of online method by utilizing the steadily improved features in training, as discussed in Section 3.2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper proposes a novel deep domain adaptation framework to address two key issues in unsupervised crossdomain person Re-ID. It clearly shows that when pursuing better feature representation for person Re-ID, considering camera-level domain discrepancy is beneficial. Also, exploring discriminative information from unlabeled target domain is equally, if not more, important. Only when these two components are adequately resolved, unsupervised cross-domain person Re-ID can become promising.</p><p>In the future work, we will further develop more flexible and adaptive camera-aware domain adaptation for this task. Supplementary material A1. Theoretical analysis of the CCE-based scheme</p><p>The following conducts theoretical analysis for the proposed CCE-(i.e., cross-domain camera equiprobability) based scheme.</p><p>Proposition. Let S and T denote source and target domains. x s and x t are the images from the two domains; p s (x) and p t (x) are their probability density functions; and C s and C t are the number of camera classes in these two domains. Let p(x|C s i ) and p(x|C t i ) be the class-conditional density functions of the ith camera class in source and target domains, respectively. It can be proved that ideally, minimizing the CCE loss will lead to p(x s |C t i ) = p s (x s ), ∀x s ∈ S; i = 1, · · · , C t .</p><formula xml:id="formula_7">(8) p(x t |C s i ) = p t (x t ), ∀x t ∈ T ; i = 1, · · · , C s . p s (x) = p t (x), ∀x ∈ S ∪ T .</formula><p>Proof. All the following analysis is conducted in the context of the learned feature representation (or equally, the learned shared subspace). Given an image x s from source domain, its posteriori probability with respect to the ith camera class in target domain (denoted by C t i (i = 1, · · · , C t )) can be expressed via the Bayes' rule as</p><formula xml:id="formula_8">P (C t i |x s ) = p(x s |C t i )P (C t i ) p s (x s )</formula><p>, ∀x s ∈ S; i = 1, · · · , C t ,</p><formula xml:id="formula_9">(9) where p(x s |C t i )</formula><p>is the class-conditional probability density function of the ith camera class in target domain, p s (x s ) denotes the probability density function of the images in source domain, and P (C t i ) is the priori probability of the ith camera class in target domain. Referring to Eq.(3) in our paper, P (C t i |x s ) is just D(B(x), j) in the CCE loss. Let us investigate the CCE loss in Eq.(3) of our paper to gain understanding on the optimal value of D(B(x), j) when this loss is minimized. Since the CCE loss is defined for each individual image x independently, it will be sufficient to investigate the minimization of the loss for any given image x. Without loss of generality, it is assumed that x is from source domain. For clarity, D(B(x), j) is compactly denoted by D j . With respect to D j (j = 1, · · · , C t ), the minimization of the CCE loss can be expressed as a constrained optimization</p><formula xml:id="formula_10">min {D1,··· ,D C t }   − 1 C t Ct j=1 log(D j )  <label>(10)</label></formula><p>with the constraints of D j ≥ 0 and Ct j=1 D j = 1, considering that D j represents the posteriori probability. Due to the symmetry of the objective function with respect to the variables D 1 , · · · , D Ct , it is not difficult to see that the optimal value of D j is 1/C t for j = 1, · · · , C t . A rigorous proof can be readily obtained by applying the Karush-Kuhn-Tucker conditions to this optimization, which is omitted here. This indicates that P (C t i |x s ) will equal 1/C t when the CCE loss is minimized for this given image x. Now we assume the ideal case that this CCE loss is minimized for any given image x in source domain <ref type="bibr" target="#b3">4</ref> .</p><p>Let us turn to Eq.(9) and rearrange it as</p><formula xml:id="formula_11">p(x s |C t i ) = P (C t i |x s ) P (C t i ) p s (x s ), ∀x s ∈ S; i = 1, · · · , C t .</formula><p>(11) Without loss of generality, equal priori probability can be set for the C t camera classes in target domain, that is, P (C t i ) is constant 1/C t . Further, note that by optimizing D j in Eq.(10) above, it can be known that</p><formula xml:id="formula_12">P (C t i |x s ) = 1 C t , ∀x s ∈ S; i = 1, · · · , C t .<label>(12)</label></formula><p>Combining the above results, Eq.(11) becomes</p><formula xml:id="formula_13">p(x s |C t i ) = 1/C t 1/C t p s (x s ) = p s (x s ), ∀x s ∈ S; i = 1, · · · , C t .</formula><p>(13) Note that the right hand side of this equation does not depend on the index i. This indicates that in the learned shared subspace, the class-conditional density function for each camera class in target domain becomes same for any given x s ∈ S. Applying the same argument to the images in target domain can similarly obtain</p><formula xml:id="formula_14">p(x t |C s i ) = p t (x t ), ∀x t ∈ T ; i = 1, · · · , C s ,<label>(14)</label></formula><p>where p(x t |C s i ) and p t (x t ) are defined in the similar way as the above. This result indicates that in the learned shared subspace, the class-conditional density function for each camera class in source domain becomes same for any given x t ∈ T .</p><p>The above results indicate that for an image in source domain, it will not feel the distribution discrepancy among the camera classes in target domain. Furthermore, its classconditional density function value (e.g., p(x s |C t i )) for those camera classes just equals its density function value in source domain (e.g., p s (x s )). The similar remark can be made for an image in target domain.</p><p>Upon the above results, the following further proves that in the learned shared subspace, the data distributions of source and target domains will become identical and this removes the domain-level distribution discrepancy.</p><p>For any image x s from source domain, its value evaluated by the probability density function of target domain can be obtained as</p><formula xml:id="formula_15">p t (x s ) = Ct i=1 p(x s |C t i )P (C t i ) = Ct i=1 p s (x s )P (C t i ) = p s (x s ),<label>(15)</label></formula><p>where the first equality is due to Eq.(13) and the second one is because</p><formula xml:id="formula_16">Ct i=1 P (C t i ) = 1.</formula><p>Similarly, the result for any given image x t from target domain can be obtained as</p><formula xml:id="formula_17">p s (x t ) = Cs i=1 p(x t |C s i )P (C s i ) = Cs i=1 p t (x t )P (C s i ) = p t (x t ),<label>(16)</label></formula><p>where the first equality is due to Eq. <ref type="bibr" target="#b13">(14)</ref> and the second one is because Cs i=1 P (C s i ) = 1. Collectively, the above two results indicate that for any image x from either source or target domain, the following result can be obtained.</p><formula xml:id="formula_18">p s (x) = p t (x), ∀x ∈ S ∪ T .<label>(17)</label></formula><p>This means that the two distributions, p s (x) and p t (x), are identical on the set S ∪ T . With respect to the definitions of the two distributions, this indicates that upon the learned feature representation, the data distributions of source and target domains become identical on the set S ∪ T and that the distribution discrepancy is therefore removed.</p><p>In addition, it is worth mentioning that the ideal minimization of the CCE loss does not theoretically guarantee that in the shared subspace, an image from either source or target domain will not feel the distribution discrepancy among the camera classes in its own domain. In other words, the results that p(x s |C s 1 ) = · · · = p(x s |C s Cs ) or p(x t |C t 1 ) = · · · = p(x t |C t Ct ) cannot directly be derived from the ideal minimization of the CCE loss.</p><p>Nevertheless, note that Eq. <ref type="bibr" target="#b16">(17)</ref> implies that at any place x in S∪T , the probability density of images from source domain is the same as the probability density of images form target domain. This means that the images from two domains have been adequately mixed up. In this case, considering that the result p(x s |C t 1 ) = · · · = p(x s |C t Ct ) is true (as proved in Eq.(13)), we can reasonably expect that this result shall be generalized from x s to x t , that is, p(x t |C t 1 ) = · · · = p(x t |C t Ct ) becomes true. Applying the same argument can obtain the result p(x s |C s 1 ) = · · · = p(x s |C s Cs ). Therefore, it can be reasonably expected that in practice in the shared subspace, an image from either source or target domain will not feel the distribution discrepancy among the camera classes in its own domain. Experimental study has been conducted to show that this tendency can indeed be observed in practice, as shown by <ref type="table" target="#tab_4">Table 6</ref> in our paper and the following <ref type="figure">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2. Visualization of data distributions</head><p>The data distributions are visualized at the domain-level and camera-level via t-SNE <ref type="bibr" target="#b17">[18]</ref> in <ref type="figure" target="#fig_0">Fig. A1</ref>. We extract the features of each image by the baseline model (BL), DAL, CAL-GRL and CAL-CCE, respectively, in the task of "DukeMTMC-reID→Market1501". The top row shows the distributions of source and target domains (i.e., interdomain), where blue and red colors indicate source and target domains, respectively. The bottom row illustrates the distribution of each camera class in target domain (i.e., inter-camera on Market1501), where different colors denote different camera classes.</p><p>First, from the inter-domain results shown in the top row of <ref type="figure" target="#fig_0">Fig. A1</ref>, it can be seen that DAL, CAL-GRL and CAL-CCE can effectively "mix" the two domains when compared with BL. This validates that they are all able to reduce the data distribution discrepancy between source and target domains.</p><p>Second, from the inter-camera result shown in the bottom row in <ref type="figure" target="#fig_0">Fig. A1</ref>, it can be seen that both CAL-GRL and CAL-CCE seem to better "mix" these camera classes than BL and DAL which do not consider any camera-level discrepancy. Furthermore, consistent with its lowest intercamera distance reported in <ref type="table" target="#tab_4">Table 6</ref> of our paper, CAL-CCE displays an excellent "mixture" of different camera classes as expected, further illustrating its best capability in reducing the camera-level discrepancy in target domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A3. On the parameter sensitivity of UOT.</head><p>In this section, we conduct experiments to observe the parameter sensitivity of UOT. The results are reported in <ref type="figure" target="#fig_1">Fig. A2</ref>. Firstly, note that k p is not directly preset but decided by k and the specific data (Recall that k p is the number of positive samples of an anchor within the top-k positions and share its fragment). As in <ref type="figure" target="#fig_1">Fig. A2</ref>, when k is as small as 1, no positive samples (i.e., k p is often zero) could be found and thus performance is poor. When k goes up to 5,   the performance tends to plateau. Secondly, for k n , if we only select one negative sample (i.e., k n is set as 1), this sample often undesirably shares the same identity as the anchor. Meanwhile, setting k n too large could include many easy negative samples, instead of hard negative ones preferred by UOT. In all experiments, we uniformly set k and k n as 5 and 2.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Illustration of training the backbone network B with the proposed cross-domain camera equiprobability (CCE) loss in our CAL method at each iteration. FC and GAP stand for fully connected layer and global average pooling. The top of the figure shows that each image in target domain (in blue) is required to be equiprobably misclassified into all camera classes in source domain (in red). The bottom similarly shows the case for each image in source domain. The discriminator is set as a 128-d FC layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>[t] + equals t if t &gt; 0 and zero otherwise. m is the margin.d p (I a ) = 1 kp kp i=1 d(I a , I i p ) andd n (I a ) = 1 kn kn i=1 d(I a , I i n ) are the average distances of the positive and negative samples from the anchor, respectively. Using average distances here helps to mitigate the adverse effect Illustration of the proposed unsupervised camera-aware domain adaptation framework, where FC and GAP denote fully connected layer and global average pooling. #PID and UOT denote the total number of person classes in source domain and unsupervised online triplet generation, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure A1 .</head><label>A1</label><figDesc>Visualization of data distributions at the domain-level and camera-level via t-SNE [18]. The features of each image are extracted by the baseline (BL), DAL, CAL-GRL and CAL-CCE in the task of "DukeMTMC-reID→Market1501", respectively. The top shows the distributions of source and target domains (i.e., inter-domain), where blue and red colors indicate source and target domains, respectively. The bottom illustrates the distribution of each camera class in target domain (i.e., inter-camera on Market1501), where different colors denote different camera classes. Note that all figures correspond to the experimental results in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure A2 .</head><label>A2</label><figDesc>Parameter sensitivity (by BL+UOT in "Duke→Market1501").</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison with the state-of-the-art methods of unsupervised Re-ID on Market1501 and DukeMTMC-reID (Duke).</figDesc><table><row><cell></cell><cell cols="4">Duke→Market1501 Market1501→Duke</cell></row><row><cell>Method</cell><cell>mAP</cell><cell>Rank-1</cell><cell>mAP</cell><cell>Rank-1</cell></row><row><cell>UMDL [19]</cell><cell>12.4</cell><cell>34.5</cell><cell>7.3</cell><cell>18.5</cell></row><row><cell>UJSDL [20]</cell><cell>-</cell><cell>50.9</cell><cell>-</cell><cell>32.2</cell></row><row><cell>TFusion [16]</cell><cell>-</cell><cell>60.8</cell><cell>-</cell><cell>-</cell></row><row><cell>TJ-AIDL [23]</cell><cell>26.5</cell><cell>58.2</cell><cell>23</cell><cell>44.3</cell></row><row><cell>PTGAN [25]</cell><cell>-</cell><cell>38.6</cell><cell>-</cell><cell>27.2</cell></row><row><cell>SPGAN+LMP [6]</cell><cell>26.7</cell><cell>57.7</cell><cell>26.2</cell><cell>46.4</cell></row><row><cell>HHL [31]</cell><cell>31.4</cell><cell>62.2</cell><cell>27.2</cell><cell>46.9</cell></row><row><cell>UCDA-GRL</cell><cell>30.9</cell><cell>60.4</cell><cell>31.0</cell><cell>47.7</cell></row><row><cell cols="2">UCDA-CCE (Ours) 34.5</cell><cell>64.3</cell><cell>36.7</cell><cell>55.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison with the state-of-the-art approach of semisupervised person Re-ID on Market-1501 and Duke.</figDesc><table><row><cell></cell><cell cols="4">Duke→Market1501 Market1501→Duke</cell></row><row><cell>Method</cell><cell>mAP</cell><cell>Rank-1</cell><cell>mAP</cell><cell>Rank-1</cell></row><row><cell>TAUDL [12]</cell><cell>41.2</cell><cell>63.7</cell><cell>43.5</cell><cell>61.7</cell></row><row><cell>BL+SOT+CAL-GRL</cell><cell>46.6</cell><cell>72.2</cell><cell>44.3</cell><cell>62.0</cell></row><row><cell cols="2">BL+SOT+CAL-CCE (Ours) 49.6</cell><cell>73.7</cell><cell>45.6</cell><cell>64.0</cell></row><row><cell cols="5">Table 3. Comparison of camera-aware adversarial learning (CAL)</cell></row><row><cell cols="5">and domain-aware adversarial learning (DAL) in unsupervised and</cell></row><row><cell cols="4">semi-supervised settings on Market-1501 and Duke.</cell><cell></cell></row><row><cell></cell><cell cols="4">Duke→Market1501 Market1501→Duke</cell></row><row><cell>Method</cell><cell>mAP</cell><cell>Rank-1</cell><cell>mAP</cell><cell>Rank-1</cell></row><row><cell>BL+UOT+DAL</cell><cell>25.5</cell><cell>54.1</cell><cell>26.2</cell><cell>42.4</cell></row><row><cell>BL+UOT+CAL-GRL</cell><cell>30.9</cell><cell>60.4</cell><cell>31.0</cell><cell>47.7</cell></row><row><cell cols="2">BL+UOT+CAL-CCE (Ours) 34.5</cell><cell>64.3</cell><cell>36.7</cell><cell>55.4</cell></row><row><cell>BL+SOT+DAL</cell><cell>40.2</cell><cell>67.3</cell><cell>34.8</cell><cell>52.2</cell></row><row><cell>BL+SOT+CAL-GRL</cell><cell>46.6</cell><cell>72.2</cell><cell>44.3</cell><cell>62.0</cell></row><row><cell cols="2">BL+SOT+CAL-CCE (Ours) 49.6</cell><cell>73.7</cell><cell>45.6</cell><cell>64.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Performance of the proposed framework when using different components on Market1501 and DukeMTMC-reID (Duke).</figDesc><table><row><cell>Method</cell><cell cols="4">Duke→Market1501 Market1501→Duke mAP Rank-1 mAP Rank-1</cell></row><row><cell>BL</cell><cell>19.4</cell><cell>47.1</cell><cell>21.3</cell><cell>38.4</cell></row><row><cell>BL+UOT(eud)</cell><cell>23.6</cell><cell>51.0</cell><cell>24.1</cell><cell>40.2</cell></row><row><cell>BL+UOT</cell><cell>27.4</cell><cell>55.5</cell><cell>27.5</cell><cell>44.3</cell></row><row><cell>BL+UOT+CAL-GRL</cell><cell>30.9</cell><cell>60.4</cell><cell>31.0</cell><cell>47.7</cell></row><row><cell cols="2">BL+UOT+CAL-CCE (Ours) 34.5</cell><cell>64.3</cell><cell>36.7</cell><cell>55.4</cell></row><row><cell>BL+CAL-GRL</cell><cell>20.5</cell><cell>47.6</cell><cell>22.7</cell><cell>41.4</cell></row><row><cell>BL+CAL-CCE</cell><cell>8.4</cell><cell>27.6</cell><cell>23.8</cell><cell>45.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Effectiveness of CAL in the semi-supervised setting on Market-1501 and DukeMTMC-reID (Duke).</figDesc><table><row><cell></cell><cell cols="4">Duke→Market1501 Market1501→Duke</cell></row><row><cell>Method</cell><cell>mAP</cell><cell>Rank-1</cell><cell>mAP</cell><cell>Rank-1</cell></row><row><cell>BL+SOT</cell><cell>39.2</cell><cell>65.9</cell><cell>39.2</cell><cell>56.6</cell></row><row><cell>BL+SOT+CAL-GRL</cell><cell>46.6</cell><cell>72.2</cell><cell>44.3</cell><cell>62.0</cell></row><row><cell cols="2">BL+SOT+CAL-CCE (Ours) 49.6</cell><cell>73.7</cell><cell>45.6</cell><cell>64.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>The data distribution discrepancy of inter-domain (between source and target domains) and inter-camera (across all cameras in target domain) on the task of Duke→Market1501. Note that a smaller value indicates better performance in this table.</figDesc><table><row><cell cols="3">BL DAL CAL-GRL CAL-CCE</cell></row><row><cell>Inter-domain (×10 3 ) 1.48 1.22</cell><cell>1.28</cell><cell>1.25</cell></row><row><cell>Inter-camera (×10 2 ) 6.76 5.97</cell><cell>4.36</cell><cell>2.90</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .</head><label>7</label><figDesc>Comparison of the CCE-based scheme and the AOEbased scheme on Market1501 and DukeMTMC-reID (Duke).</figDesc><table><row><cell>Method</cell><cell cols="4">Duke→Market1501 Market1501→Duke mAP Rank-1 mAP Rank-1</cell></row><row><cell>AOE</cell><cell>29.6</cell><cell>59.9</cell><cell>31.5</cell><cell>51.3</cell></row><row><cell cols="2">CCE (Ours) 34.5</cell><cell>64.3</cell><cell>36.7</cell><cell>55.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 .</head><label>8</label><figDesc>Comparison of offline and online triplet generation on Market-1501 and DukeMTMC-reID (Duke).</figDesc><table><row><cell>Method</cell><cell cols="4">Duke→Market1501 Market1501→Duke mAP Rank-1 mAP Rank-1</cell></row><row><cell>Offline Triplets</cell><cell>13.5</cell><cell>37.1</cell><cell>10.2</cell><cell>21.1</cell></row><row><cell cols="2">Online Triplets (Ours) 27.4</cell><cell>55.5</cell><cell>27.5</cell><cell>44.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>[ 28 ]</head><label>28</label><figDesc>Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jingdong Wang, and Qi Tian. Scalable person re-identification: A benchmark. In IEEE International Conference on Computer Vision (ICCV), 2015. 6 [29] Zhedong Zheng, Liang Zheng, and Yi Yang. Unlabeled samples generated by GAN improve the person re-identification baseline in vitro.</figDesc><table><row><cell>In International Conference on Computer</cell></row><row><cell>Vision (ICCV), 2017. 1, 6, 7</cell></row><row><cell>[30] Zhun Zhong, Liang Zheng, Donglin Cao, and Shaozi Li. Re-</cell></row><row><cell>ranking person re-identification with k-reciprocal encoding.</cell></row><row><cell>In IEEE Conference on Computer Vision and Pattern Recog-</cell></row><row><cell>nition (CVPR), 2017. 5, 6</cell></row><row><cell>[31] Zhun Zhong, Liang Zheng, Shaozi Li, and Yi Yang. Gener-</cell></row><row><cell>alizing a person retrieval model hetero-and homogeneously.</cell></row><row><cell>In European Conference on Computer Vision (ECCV), 2018.</cell></row><row><cell>1, 2, 6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6</head><label>6</label><figDesc>of our paper.</figDesc><table><row><cell></cell><cell></cell><cell>mAP</cell><cell></cell><cell>Rank-1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>mAP</cell><cell>Rank-1</cell><cell></cell><cell></cell></row><row><cell>47.7</cell><cell cols="3">51.3 53.8 53.7</cell><cell cols="3">55.5 55.5 54.9</cell><cell>51.7</cell><cell>55.5</cell><cell>54.5</cell><cell>53.2</cell><cell>50.8</cell><cell>49.7</cell></row><row><cell cols="2">21.4 23.4</cell><cell cols="5">27.0 25.8 27.4 26.0 27.0</cell><cell>24.1</cell><cell>27.4</cell><cell>26.1</cell><cell>25.2</cell><cell>23.4</cell><cell>22.9</cell></row><row><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell></row><row><cell></cell><cell cols="5">(a) k (It decides kp)</cell><cell></cell><cell></cell><cell></cell><cell cols="2">(b) kn</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For example, camera ID is provided for each frame in benchmark datasets of person Re-ID, as will be shown in the experiment.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">This setting is called semi-supervised because person identity labels are only available within each individual camera but not across all cameras.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Note that such an ideal case may not be really achieved in practice. Nevertheless, it helps to clearly reveal the effect of minimizing the CCE loss in the theoretical sense.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 11 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 11 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 11 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An improved deep learning architecture for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ejaz</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Domain adaptation through synthesis for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slawomir</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Francois</forename><surname>Lalonde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Domain separation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adaptive learning for target tracking and true linking discovering across multiple non-overlapping cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Chuan</forename><surname>Kuan-Wen Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Jyun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu-Song</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ping</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions Multimedia (TMM)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image-image domain adaptation with preserved self-similarity and domain-dissimilarity for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep reconstructionclassification networks for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bastiaan Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<title level="m">defense of the triplet loss for person re-identification. arXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Human semantic parsing for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emrah</forename><surname>Kalayeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhittin</forename><surname>Basaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gökmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Kamasak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification by deep learning tracklet association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minxian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with residual transfer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised cross-dataset person re-identification by transfer learning of spatial-temporal patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cross-domain person reidentification using domain adaptation ranking svms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy Jinhua</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2008" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised cross-dataset transfer learning for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peixi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaowei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised joint subspace and dictionary learning for enhanced cross-domain person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocong</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>JSTSP)</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Deep domain confusion: Maximizing for domain invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Transferable joint attribute-identity deep learning for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingya</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cross-scenario transfer person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Person transfer gan to bridge domain gap for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Importance weighted adversarial nets for partial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zewei</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Ogunbona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Deep transfer network: Unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
