<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fundus2Angio: A Conditional GAN Architecture for Generating Fluorescein Angiography Images from Retinal Fundus Photography</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharif</forename><forename type="middle">Amit</forename><surname>Kamran</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Nevada</orgName>
								<address>
									<postCode>89557</postCode>
									<settlement>Reno</settlement>
									<region>NV</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khondker</forename><surname>Fariha Hossain</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Deakin University</orgName>
								<address>
									<addrLine>AUS 3 Houston Eye Associates</addrLine>
									<postCode>77801</postCode>
									<settlement>Melbourne, Houston</settlement>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Tavakkoli</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Nevada</orgName>
								<address>
									<postCode>89557</postCode>
									<settlement>Reno</settlement>
									<region>NV</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stewart</forename><surname>Zuckerbrod</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salah</forename><forename type="middle">A</forename><surname>Baker</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Nevada School of Medicine</orgName>
								<address>
									<postCode>89557</postCode>
									<settlement>Reno</settlement>
									<region>NV</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><forename type="middle">M</forename><surname>Sanders</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Nevada School of Medicine</orgName>
								<address>
									<postCode>89557</postCode>
									<settlement>Reno</settlement>
									<region>NV</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fundus2Angio: A Conditional GAN Architecture for Generating Fluorescein Angiography Images from Retinal Fundus Photography</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Generative Adversarial Networks · Image-to-image Translation · Fluorescein Angiography · Retinal Fundoscopy</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Carrying out clinical diagnosis of retinal vascular degeneration using Fluorescein Angiography (FA) is a time consuming process and can pose significant adverse effects on the patient. Angiography requires insertion of a dye that may cause severe adverse effects and can even be fatal. Currently, there are no non-invasive systems capable of generating Fluorescein Angiography images. However, retinal fundus photography is a non-invasive imaging technique that can be completed in a few seconds. In order to eliminate the need for FA, we propose a conditional generative adversarial network (GAN) to translate fundus images to FA images. The proposed GAN consists of a novel residual block capable of generating high quality FA images. These images are important tools in the differential diagnosis of retinal diseases without the need for invasive procedure with possible side effects. Our experiments show that the proposed architecture achieves a low FID score of 30.3 and outperforms other state-of-the-art generative networks. Furthermore, our proposed model achieves better qualitative results indistinguishable from real angiograms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>For a long time Fluorescein Angiography (FA) combined with Retinal Funduscopy have been used for diagnosing retinal vascular and pigment epithelialchoroidal diseases <ref type="bibr" target="#b17">[18]</ref>. The process requires the injection of a fluorescent dye which appears in the optic vein within 8-12 seconds depending on the age and cardiovascular structure of the eye and stays up to 10 minutes <ref type="bibr" target="#b15">[16]</ref>. Although generally considered safe, there have been reports of mild to severe complications due to allergic reactions to the dye <ref type="bibr" target="#b0">[1]</ref>. Frequent side effects can range from nausea, vomiting, anaphylaxis, heart attack, to anaphylactic shock and death <ref type="bibr" target="#b14">[15]</ref>. In addition, leakage of fluorescein in intravaneous area is common. However, the concentration of fluorescein solutions don't have any direct impact on adverse effects mentioned above. <ref type="bibr" target="#b23">[24]</ref>.</p><p>Given the complications and the risks associated with this procedure, a noninvasive, affordable, and computationally effective procedure is quite imperative. The only current alternatives to flourecein angigraphy (FA) is carried out by Optical Coherence Tomography and basic image processing technique. These systems are generally quite expensive. Without a computationally effective and financially viable mechanism to generate reliable and reproducible flourecein angiograms, the only alternative is to utilize retina funduscopy for differential diagnosis. Although automated systems consisting of image processing and machine learning algorithms have been proposed for diagnosing underlying conditions and diseases from fundus images <ref type="bibr" target="#b19">[20]</ref>, there has not been an effective effort to generate FA images from retina photographs. In this paper, we propose a novel conditional Generative Adversarial Network (GAN) called Fundus2Angio, capable of synthesizing fluorescein angiograms from retinal fundus images. The procedure is completely automated and does not require any human intervention. We use both qualitative and quantitative metrics for testing the proposed architecture. We compare the proposed architecture with other state-of-the-art conditional GANs <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26]</ref>. Our model outperforms these networks in terms of quantitative measurement. For qualitative results, expert ophthalmologists were asked to distinguish fake angiograms from a random set of balanced real and fake angiograms over two trials. Results show that the angiograms generated by the proposed network are quite indistinguishable from real FA images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Literature Review</head><p>Generative adversarial networks have revolutionized many image manipulation tasks such as image editing <ref type="bibr" target="#b24">[25]</ref>, image styling <ref type="bibr" target="#b3">[4]</ref>, and image style transfer <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b25">26]</ref>. Multi-resolution architectures are common practice in computer vision, while coupled architectures have the capability to combine fine and coarse information from images <ref type="bibr" target="#b1">[2]</ref>. Recently, techniques on Conditional <ref type="bibr" target="#b8">[9]</ref> and Unconditional GANs <ref type="bibr" target="#b2">[3]</ref> have explored the idea of combined-resolutions within the architecture for domain specific tasks. Inspired by this, we propose an architecture that extract features at different scales.</p><p>Some approaches also used multi-scale discriminators for style-transfer <ref type="bibr" target="#b22">[23]</ref>. However, they only attached discriminators with generator that deals with fine features while ignoring discriminators for coarse generator completely. In order to learn useful features at coarsest scale, separate multi-scale discriminators are necessary. Our proposed architecture employs this for both coarse and fine generators.</p><p>For high quality image synthesis, a pyramid network with multiple pairs of discriminators and generators has also been proposed, termed SinGAN <ref type="bibr" target="#b20">[21]</ref>. Though it produces high quality synthesized images, the model works only on unpaired images. To add to this problem, each generator's input is the synthesized output produced by the previous generator. As a result, it can't be employed for pair-wise image training that satisfies a condition. To alleviate from this problem, a connection needs to be established that can propagate feature from coarse to fine generator. In this paper, we propose such an architecture that has a feature appending mechanism between the coarse and fine generators, making it a two level pyramid network with multi-scale discriminators as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Proposed Methodology</head><p>This paper introduces a new conditional generative adversarial network (GAN) comprising of a novel residual block for producing realistic FA from retinal fundus images. First, we introduce the residual block in section 3.1. We then delve into the proposed conditional GAN encompassing of fine and coarse generators and four multi-scale discriminators in sections 3.2 and 3.3. Lastly, in section 3.4, we discuss the objective function and loss weight distributions for each of the architectures that form the proposed architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Novel Residual Block</head><p>Recently, residual blocks have become the norm for implementing many image classification, detection and segmentation architectures <ref type="bibr" target="#b6">[7]</ref>. Generative architectures have employed these blocks in interesting applications ranging from image-to-image translation to super-resolution <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b22">23]</ref>. In its atomic form, a residual unit consists of two consecutive convolution layers. The output of the second layers is added to the input, allowing for deeper networks. Computationally, regular convolution layers are expensive compared to a newer convolution variant, called separable convolution <ref type="bibr" target="#b4">[5]</ref>. Separable convolution performs a depth-wise convolution followed by a point-wise convolution. This, in turn helps to extract and retain depth and spatial information through the network. It has been shown that interspersing convolutional layers allows for more efficient and accurate networks <ref type="bibr" target="#b11">[12]</ref>. We incorporate this idea to design a novel residual block to retain both depth and spatial information, decrease computational complexity and ensure efficient memory usage, as shown in <ref type="table">Table.</ref> 1. Ri FConv FSepConv + Ri Leaky-ReLU (Post) 10,784 1 FConv and FSepConv has kernel size K = 3, stride S = 1, padding P = 0 and No. of channel C = 32. As illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>, we replace the last convolution operation with a separable convolution. We also use Batch-normalization and Leaky-ReLU as post activation mechanism after both convolution and separable Convolution layers. For better results, we incorporate reflection padding as opposed to zero-padding before each convolution operation. The entire operation can be formulated as shown in Eq. 1:</p><formula xml:id="formula_0">R i+1 = R i F Conv F SepConv + R i = F (R i ) + R i<label>(1)</label></formula><p>Here, refers to convolution operation while F conv and F SepConv signify the back-to-back convolution and separable convolution operations. By exploiting convolution and separable convolution layer with Leaky-ReLU, we ensure that two distinct feature maps (spatial &amp; depth information) can be combined to generate fine fluorescein angiograms. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Coarse and Fine Generators</head><p>Using a coarse-to-fine generator for both conditional and unconditional GANs results in very high quality image generation, as observed in recent architectures, such as pix2pixHD <ref type="bibr" target="#b22">[23]</ref> and SinGan <ref type="bibr" target="#b20">[21]</ref>. Inspired by this idea, we use two generators (G f ine and G coarse ) in the proposed network, as illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>.</p><p>The generator G f ine synthesizes fine angiograms from fundus images by learning local information, including retinal venules, arterioles, hemorrhages, exudates and microaneurysms. On the other hand, the generator G coarse tries to extract and preserve global information, such as the structures of the macula, optic disc, color, contrast and brightness, while producing coarse angiograms.</p><p>The generator G f ine takes input images of size 512 × 512 and produces output images with the same resolution. Similarly, the generator G coarse network takes an image with half the size (256 × 256) and outputs an image of the same size as the input. In addition, the G coarse outputs a feature vector of the size 256 × 256 × 64 that is eventually added with one of the intermediate layers of G f ine . These hybrid generators are quite powerful for sharing local and global information between multiple architectures as seen in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23]</ref>. Both generators use convolution layers for downsampling and transposed convolution layers for upsampling. It should be noted that G coarse is downsampled twice (×2) before being upsampled twice again with transposed convolution. In both the generators, the proposed residual blocks are used after the last downsampling operation and before the first upsampling operations as illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>. On the other hand, in G f ine , downsampling takes place once with necessary convolution layer, followed by adding the feature vector, repetition of residual blocks and then upsampling to get fine angiography image. All convolution and transposed convolution operation are followed by Batch-Normalization and Leaky-ReLU activations. To train these generators, we start with G coarse by batch-training it on random samples once and then we train the G f ine once with a new set of random samples. During this time, the discriminator's weights are frozen, so that they are not trainable. Lastly, we jointly fine-tune all the discriminator and generators together to train the GAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multi-scale PatchGAN as Discriminator</head><p>For synthesizing fluorescein angiography images, GAN discriminators need to adapt to coarse and fine generated images for distinguishing between real and fake images. To alleviate this problem, we either need a deeper architecture or, a kernel with wider receptive field. Both these solutions result in over fitting and increase the number of parameters. Additionally, a large amount of processing power will be required for computing all the parameters. To address this issue, we exploit the idea of using two Markovian discriminators, first introduced in a technique called PatchGAN <ref type="bibr" target="#b13">[14]</ref>. This technique takes input from different scales as previously seen in <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>We use four discriminators that have a similar network structure but operate at different image scales. Particularly, we downsample the real and generated angiograms by a factor of 2 using the Lanczos sampling to create an image pyramid of three scales (original and 2×downsampled and 4×downsampled). We group the four discriminators into two, D f ine = [D1 f ine , D2 f ine ] and D coarse = [D1 coarse , D2 coarse ] as seen in <ref type="figure" target="#fig_0">Fig. 1</ref>. The discriminators are then trained to distinguish between real and generated angiography images at the three distinct resolutions respectively.</p><p>The outputs of the PatchGAN for D f ine are 64 × 64 and 32 × 32 and for D coarse are 32 × 32 and 16 × 16. With the given discriminators, the loss function can be formulated as given in Eq. 2. It's a multi-task problem of maximizing the loss of the discriminators while minimizing the loss of the generators.</p><formula xml:id="formula_1">min G f ine ,Gcoarse max D f ine ,Dcoarse L cGAN (G f ine , G coarse , D f ine , D coarse )<label>(2)</label></formula><p>Despite discriminators having similar network structure, the one that learns feature at a lower resolution has the wider receptive field. It tries to extract and retain more global features such as macula, optic disc, color and brightness etc to generate better coarse images. In contrast, the discriminator that learns feature at original resolution dictates the generator to produce fine features such as retinal veins and arteries, exudates etc. By doing this we combine feature information of global and local scale while training the generators independently with their paired multi-scale discriminators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Weighted Object Function and Adversarial Loss</head><p>We use LSGAN <ref type="bibr" target="#b16">[17]</ref> for calculating the loss and training our conditional GAN. The objective function for our conditional GAN is given in Eq. 3.</p><formula xml:id="formula_2">L cGAN (G, D) = E x,y (D(x, y) − 1) 2 + E x (D(x, G(x) + 1)) 2<label>(3)</label></formula><p>where the discriminators are first trained on the real fundus, x and real angiography image, y and then trained on the the real fundus, x and fake angiography image, G(x). We start with training the discriminators D f ine and D coarse for couple of iterations on random batches of images. Next, we train the G coarse while keeping the weights of the discriminators frozen. Following that, we train the the G f ine on a batch of random samples in a similar fashion. We use Mean-Squared-Error (MSE) for calculating the individual loss of the generators as shown in Eq. 4.</p><formula xml:id="formula_3">L L2 (G) = E x,y G(x) − y 2<label>(4)</label></formula><p>where, L L2 is the reconstruction loss for a real angiogram, y, given a generated angiogram, G(x). We use this loss for both G f ine and G coarse so that the model can generate high quality angiograms of different scales. Previous techniques have also exploited this idea of combining basic GAN objective with a MSE loss <ref type="bibr" target="#b18">[19]</ref>. From Eq. 3 and 4 we can formulate our final objective function as given in Eq. 5.</p><formula xml:id="formula_4">min G f ine ,Gcoarse max D f ine ,Dcoarse L cGAN (G f ine , G coarse , D f ine , D coarse ) +λ L L2 (G f ine ) + L L2 (G coarse )<label>(5)</label></formula><p>Here, λ dictates either to prioritize the discriminators or the generators. For our architecture, more weight is given to the reconstruction loss of the generators and thus we pick a large λ value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In the following section, different experimentation and evaluation is provided for our proposed architecture. First we elaborate about the data preparation and pre-prossessing scheme in Sec. 4.1. We then define our hyper-parameter settings in Sec. 4.2. Following that, different architectures are compared based on some quantitative and qualitative evaluation metrics in Sec. 4.3. Lastly, and Sec. 4.4,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>For training, we use the funuds and angiography data-set provided by Hajeb et al. <ref type="bibr" target="#b5">[6]</ref>. The data-set consists of 30 pairs of diabetic retinopathy and 29 pairs normal of angiography and fundus images from 59 patients. Because, not all of the pairs are perfectly aligned, we select 17 pairs for our experiment based on alignment. The images are either perfectly aligned or nearly aligned. The resolution for fundus and angiograms are as follows 576×720. Fundus photographs are in RGB format, whereas angiograms are in Gray-scale format. Due to shortage of data, we take 50 random crops of size 512 × 512 from each images for training our model. So, the total number of training sample is 850 (17 × 50).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Hyper-parameter tuning</head><p>LSGAN <ref type="bibr" target="#b16">[17]</ref> was found to be effective for generating desired synthetic images for our tasks. We picked λ = 10 (Eq. 5). For optimizer, we used Adam with learning rate α = 0.0002, β 1 = 0.5 and β 2 = 0.999. We train with mini-batches with batch size, b = 4 for 100 epochs. It took approximately 10 hours to train our model on an NVIDIA RTX2070 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Qualitative Evaluation</head><p>For evaluating the performance of the network, we took 14 images and cropped 4 sections from each quadrant of the image with a size of 512 × 512. We conducted two sets of experiments to evaluate both the network's robustness to global changes to the imaging modes and its ability to adapt to structural changes to the vascular patterns and structure of the eye. We used GNU Image Manipulation Program (GIMP) <ref type="bibr" target="#b21">[22]</ref> for transforming and distorting images. In the first set of experiments, three transformations were applied to the images: 1) blurring to represent out of focus funduscopy or fundus photography in the presence of severe cataracts, 2) sharpening to represent pupil dilation, and 3) noise to represent interference during photography. Good robustness is represented by the generated angiograms similarity to the real FA image since these transformation do not affect the vascular structure of the retina. A side by side comparison of different architecture's prediction is shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. As it can be observed from the image, the proposed architecture produces images very similar to the ground-truth (GT) under these global changes applied to the fundus image.</p><p>In the case of blurred fundus images, our model is less affected compared to other architectures, as seen in the second row of <ref type="figure" target="#fig_3">Fig. 4</ref>-structure of smaller veins are preserved better compared to Pix2Pix and Pix2PixHD.</p><p>In the case of sharpened images, the angiogram produced by Pix2Pix and Pix2PixHD show vein-like structures introduced in the back, which are not present in our prediction. These are seen in the third row of <ref type="figure" target="#fig_3">Fig. 4</ref>.</p><p>In the case of noisy images, as seen in the last row of <ref type="figure" target="#fig_3">Fig. 4</ref> our prediction is still unaffected with this pixel level alteration. However, both Pix2Pix and Pix2PixHD fails to generate thin and small vessel structures by failing to extract low level features.</p><p>In the second set of experiments we modified the vascular pattern of the retina and the fundus images. These structural changes are represented by two different types of distortions: 1) pinch, representing the flattening of the retina resulting in the pulled/pushed retinal structure, and 2) whirl, representing retina distortions caused by increased intra-ocular pressure (IOP). Good adaptation to structural changes in the retina is achieved if the generated angiograms are similar to the angiograms with changed vascular structure. The effects of Pinch and Whirl on predicted angiogram is illustrated in <ref type="figure" target="#fig_4">Fig. 5</ref>.</p><p>Pinch represents the globe flattening condition, manifesting vascular changes on the retina as a result of distortions of retinal subspace. This experiment shows the adaptability and reproduciblity of the proposed network to uncover the changes in vascular structure. From the first row in <ref type="figure" target="#fig_4">Fig. 5</ref> it is evident that our model can effectively locates the retinal vessels compared to other proposed techniques.</p><p>Whirl represented changes in the IOP or vitreous changes in the eye that may result in twists in the vascular structure. Similar to pinch, the network's ability to adapt to this structural change can be measured if the generated FA image is similar to the real angiogram showing the changed vascular structure. As seen in the last row of <ref type="figure" target="#fig_4">Fig. 5</ref> our network encodes the feature information vessel structures, and is much less affected this kind of distortion. The other architectures failed to generate micro vessel structure as it can be seen in <ref type="figure" target="#fig_4">Fig. 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Quantitative Evaluations</head><p>For quantitative evaluation, we also performed two experiments. In the first experiment we use the Fréchet inception distance (FID) <ref type="bibr" target="#b7">[8]</ref> that has been used to evaluate similar style-transfer GANs <ref type="bibr" target="#b12">[13]</ref>. We computed the FID scores for different architectures on the generated FA image and original angiogram, and those generated from the changed fundus images by the five global and structural changes -i.e., blurring, sharpening, noise, pinch, and whirl. The results are reported in <ref type="table">Table.</ref> 2. It should be noted that, lower FID score means better results.  <ref type="figure">6)</ref>. For the case of noisy images, the FID for Pix2Pix dropped slightly but increased for both Pix2PixHD and our technique. Notice that the FID for our technique is still better than both Pix2Pix and Pix2PixHD. For all other changes, the FID score of our technique increased slightly but still outperformed Pix2Pix and Pix2PixHD in both robustness and adaptation to the structural changes. In the next experiment we evaluate the quality of the generated angiograms by asking experts (e.g. ophthalmologists) to identify fake angiograms among a collection of 40 balanced (50%, 50%) and randomly mixed angiograms. For this experiment, the experts were not told how many of the images are real and how many are fake. The non-disclosed ratio of fake and real images was a significant design choice for this experiment, as it will allow us to evaluate three metrics: 1) incorrectly labeled fake images representing how real the generated images look, 2) correctly labeled real images representing how accurate the experts recognized angiogram salient features, and 3) the confusion metric representing how effective the overall performance of our proposed method was in confusing the expert in the overall experiment. The results are shown in <ref type="table" target="#tab_2">Table 3</ref>.</p><p>As it can be seen from <ref type="table" target="#tab_2">Table 3</ref>, experts assigned 85% of the fake angiogams as real. This result shows that experts had difficulty in identifying fake images, while they easily identified real angiograms with 80% accuracy. Overall, the experts misclassified 53% of all images. This resulted in a confusion factor of 52.5%. This is significant, as the confusion factor of 50% is the best achievable result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we introduced Fundus2Angio, a novel conditional generative architecture that capable of generating angiograms from retinal fundus images. We further demonstrated its robustness, adaptability, and reproducibility by synthesizing high quality angiograms from transformed and distorted fundus images. Additionally, we illustrated how changes in biological markers do not affect the adaptability and reproducibility of synthesizing angiograms by using our technique. This ensures that the proposed architecture effectively preserves known biological markers (e.g. vascular patterns and structures). As a result, the proposed network can be effectively utilized to produce accurate FA images for the same patient from his or her fundus images over time. This allows for a better control on patient's disease progression monitoring or to help uncover newly developed diseases or conditions. One future direction to this work is to improve upon this work to incorporate retinal vessel segmentation and exudate localization.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Proposed Generative Adversarial Network consisting of two Generators Gcoarse, G f ine , and four discriminators D1coarse, D1 f ine , D2 f ine , D2coarse. The generators take Fundus image as input and outputs FA image. Whereas, the discriminators take both Fundus and FA images as input and outputs if the pairs are real or fake.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Proposed Residual Block consisting of two residual units Fconv and FSepConv. First one cosists of Reflection padding, Convolution, Batch-Normalization and Leaky-ReLU layers. The second one has some layers except has Separable Convolution instead of Vanilla Convolution. Ri and Ri+1 signifies input and output of the residual block</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>The backbone for G f ine , Gcoarse Generators and D1, D2 Discriminator Architectures. The G f ine consists of two encoding blocks (light green), followed by three residual blocks (purple) and one decoding block (orange).The Gcoarse consists of four encoding blocks (light green), followed by nine residual blocks (purple) and three decoding blocks(orange). The discriminator consists of three encoding block (light green). Both the generators have intermediate Conv2D, Reflection padding layer and TanH as output activation function. Whereas, discriminators have intermediate Conv2D, Leaky-ReLU layers and Sigmoid as output activation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Angiogram generated from transformed Fundus images. The first row shows the original image, ground truth and generated angiograms from different architectures. The second row shows the closed-up version of a selected region. The red bounding box signifies that specific region. Each row pairs illustrated None, Blur, Sharp and Noise transformations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Angiogram generated from distorted Fundus images with biological markers. The first row shows the original image, ground truth and generated angiograms from different architectures. The second row shows the closed-up version of a selected region. The red bounding box signifies that specific region. Each row pairs illustrated Pinch and Whirl transformations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison between Original and Proposed Residual Block</figDesc><table><row><cell>Residual Block</cell><cell>Equation</cell><cell>Activation</cell><cell>No. of Parameters 1</cell></row><row><cell>Original</cell><cell>Ri FConv FConv + Ri</cell><cell>ReLU (Pre) [7]</cell><cell>18,688</cell></row><row><cell>Proposed</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Fréchet inception distance (FID) for different architectures From Table. 2, using the original fundus image, the FID of our network angiogrm is 30.3, while other techniques are at least 10 points worse, Pix2PixHD (42.8) and Pix2Pix (48.</figDesc><table><row><cell>Architecture</cell><cell>Orig.</cell><cell>Noise</cell><cell>Blur</cell><cell>Sharp</cell><cell>Whirl</cell><cell>Pinch</cell></row><row><cell>Ours</cell><cell cols="6">30.3 41.5 (11.2↑) 32.3 (2.0↑) 34.3 (4.0↑) 38.2 (7.9↑) 33.1 (2.8↑)</cell></row><row><cell cols="7">Pix2PixHD [23] 42.8 53.0 (10.2↑) 43.7 (1.1↑) 47.5 (4.7↑) 45.9 (3.1↑) 39.2 (3.6↓)</cell></row><row><cell>Pix2Pix [10]</cell><cell cols="6">48.6 46.8 (1.8 ↓) 50.8 (2.2↑) 47.1 (1.5↓) 43.0 (5.6↓) 43.7 (4.9↓)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Results of Qualitative with Undisclosed Portion of Fake/Real Experiment</figDesc><table><row><cell cols="2">Results</cell><cell>Average</cell><cell></cell></row><row><cell cols="4">Correct Incorrect Missed Found Confusion</cell></row><row><cell>Fake 15% Real 80%</cell><cell>85% 20%</cell><cell>53% 48%</cell><cell>52.5%</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hypersensitivity to contrast media and dyes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Brockow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sánchez-Borges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Immunology and Allergy Clinics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="547" to="564" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
		<title level="m">Recognising panoramas. In: ICCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">1218</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Photographic image synthesis with cascaded refinement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1511" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sketchygan: Towards diverse and realistic sketch to image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9416" to="9425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hajeb Mohammad Alipour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rabbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Akhlaghi</surname></persName>
		</author>
		<title level="m">Diabetic retinopathy grading by digital curvelet transform. Computational and mathematical methods in medicine 2012</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Poursaeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hopcroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5077" to="5086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Optic-net: A novel convolutional neural network for diagnosis of retinal diseases from optical tomography images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Kamran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Sabbir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tavakkoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th IEEE International Conference On Machine Learning And Applications (ICMLA)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="964" to="971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Precomputed real-time texture synthesis with markovian generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="702" to="716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adverse reactions of fluorescein angiography: a prospective study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P C</forename><surname>Lira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L D A</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V R B</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D C</forename><surname>Pessoa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arquivos brasileiros de oftalmologia</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="615" to="618" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fluorescein and icg angiography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mandava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Reichel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">St Louis: Mosby</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="800" to="808" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paul Smolley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2794" to="2802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Retinal fundus image analysis for diagnosis of glaucoma: a comprehensive survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Rajsingh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Naik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Prediction of cardiovascular risk factors from retinal fundus photographs via deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Poplin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Blumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V</forename><surname>Mcconnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Webster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">158</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Singan: Learning a generative model from a single natural image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Shaham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Michaeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4570" to="4580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Team</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">GIMP: GNU Image Manipulation Program. GIMP Team</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8798" to="8807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fluorescein angiography complication survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Yannuzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">T</forename><surname>Rohrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Tindel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sobel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Costanza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shields</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ophthalmology</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="611" to="617" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Generative visual manipulation on the natural image manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="597" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
