<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T09:14+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Natural Language Inference with Hierarchical BiLSTM Max Pooling Architecture</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aarne</forename><surname>Talman</surname></persName>
							<email>aarne.talman@helsinki.fi</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Helsinki</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anssi</forename><surname>Yli-Jyrä</surname></persName>
							<email>anssi.yli-jyra@helsinki.fi</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Helsinki</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
							<email>jorg.tiedemann@helsinki.fi</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Helsinki</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Natural Language Inference with Hierarchical BiLSTM Max Pooling Architecture</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recurrent neural networks have proven to be very effective for natural language inference tasks. We build on top of one such model, namely BiLSTM with max pooling, and show that adding a hierarchy of BiLSTM and max pooling layers yields state of the art results for the SNLI sentence encoding-based models and the SciTail dataset, as well as provides strong results for the MultiNLI dataset. We also show that our sentence embeddings can be utilized in a wide variety of transfer learning tasks, outperforming InferSent on 7 out of 10 and SkipThought on 8 out of 9 SentEval sentence embedding evaluation tasks. Furthermore, our model beats the InferSent model in 8 out of 10 recently published SentEval probing tasks designed to evaluate sentence embeddings&apos; ability to capture some of the important linguistic properties of sentences.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural networks have been shown to provide a powerful tool for building representations of natural language on multiple levels of abstraction. Perhaps the most widely used representations in natural language processing are word embeddings (e.g. <ref type="bibr">Bengio et al., 2003;</ref><ref type="bibr">Mikolov et al., 2013;</ref><ref type="bibr">Pen- nington et al., 2014)</ref>. Recently there has been a growing interest in models for sentence-level representations using neural networks. Sentence embeddings are distributed representations of natural language sentences with the intention to encode the meaning of the sentences in a neural network representation. Sentence embeddings have been generated using unsupervised learning approaches (e.g. <ref type="bibr">Kiros et al., 2015;</ref><ref type="bibr">Hill et al., 2016)</ref>, and supervised learning (e.g. <ref type="bibr">Bowman et al., 2016;</ref><ref type="bibr">Con- neau et al., 2017)</ref>.</p><p>Sentence-level representations have shown promise in multiple different NLP tasks. One prominent example is natural language inference. Natural language inference (NLI) is the task of determining the inferential relationship between two or more sentences. That is, given two sentences, the premise p and the hypothesis h, the task is to determine whether h is entailed by p, whether the sentences are in contradiction with each other or whether they are neutral. There are two main approaches to NLI utilizing neural networks. Some approaches focus on building sentence embeddings for the premises and the hypothesis separately and then combine those using a classifier (e.g. <ref type="bibr">Bowman et al., 2015</ref><ref type="bibr">Bowman et al., , 2016</ref><ref type="bibr">Con- neau et al., 2017</ref>). Other approaches do not treat the two sentences separately but utilize e.g. crosssentence attention ( <ref type="bibr">Tay et al., 2018;</ref><ref type="bibr">Chen et al., 2017a)</ref>.</p><p>In this paper we focus on the sentence embedding approach. Motivated by the success of the architecture of InferSent ( <ref type="bibr">Conneau et al., 2017)</ref>, we build a hierarchical architecture utilizing bidirectional LSTM (BiLSTM) layers and max pooling. All in all, our model in on par with the state of the art for Stanford Natural Language Inference corpus (SNLI) <ref type="bibr">(Bowman et al., 2015</ref>) sentence encoding-based models and improves the previous state of the art for SciTail ( <ref type="bibr">Khot et al., 2018</ref>). We also achieve strong results for the Multi-Genre Natural Language Inference corpus (MultiNLI) ( <ref type="bibr">Williams et al., 2018)</ref>.</p><p>We also test our model on a number of transfer learning tasks using the SentEval testing library ( <ref type="bibr">Conneau et al., 2017)</ref>, and show that our model outperforms the InferSent model on 7 out of 10 and SkipThought on 8 out of 9 tasks, comparing to the scores reported by <ref type="bibr">Conneau et al. (2017)</ref>. Moreover, our model outperforms the InferSent model in 8 out of 10 recently published SentEval probing tasks designed to evaluate sentence embeddings' ability to capture some of the important linguistic properties of sentences ( <ref type="bibr">Conneau et al., 2018)</ref>. This highlights the generalization capability of our proposed model, confirming that the proposed architecture is able to produce sentence embeddings with strong performance across a wide variety of different NLP tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Sentence embeddings have been utilized in a wide variety of approaches to natural language inference. <ref type="bibr">Bowman et al. (2015</ref><ref type="bibr">Bowman et al. ( , 2016</ref>) explore RNN and LSTM architectures, <ref type="bibr">Mou et al. (2016)</ref> convolutional neural networks and <ref type="bibr">Vendrov et al. (2015)</ref> GRUs, to name a few. The basic idea behind these approaches is to encode the premise and hypothesis sentences separately and then combine those using a neural network classifier. <ref type="bibr">Conneau et al. (2017)</ref> explore multiple different sentence embedding architectures ranging from LSTM, BiLSTM and intra-attention to convolution neural networks and the performance of these architectures on NLI tasks. They show that out of these models BiLSTM with max pooling achieves the strongest results in NLI. They also show that their model trained on NLI data achieves strong performance on various transfer learning tasks.</p><p>Although sentence embedding approaches have shown their effectiveness in NLI, there are multiple studies showing that treating the hypothesis and premise sentences together and focusing on the relationship between those sentences yields better results (e.g. <ref type="bibr">Tay et al., 2018;</ref><ref type="bibr">Chen et al., 2017a</ref>). However, as these methods are focused on the inference relations rather than the internal semantics of the sentences, they cannot as straightforwardly be used outside of the NLI context and do not offer similar insights about the sentence level semantics, as sentence embeddings do. By choosing a sentence embedding-based architecture we can more easily use the models in a wide variety of NLP tasks requiring sentence-level semantic information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model Architecture</head><p>Our proposed architecture follows a sentence embedding-based approach for NLI introduced by <ref type="bibr">Bowman et al. (2015)</ref>. The model illustrated in <ref type="figure" target="#fig_0">Figure 1</ref> contains sentence embeddings for the two input sentences, where the output of the sentence embeddings are combined using a heuristic introduced by <ref type="bibr">(Mou et al., 2015)</ref>, putting together the concatenation (u, v), absolute element-wise difference |u − v|, and element-wise product u * v. The combined vector is then passed on to a 3-layered multi-layer perceptron (MLP) with a 3-way softmax classifier. The first two layers of the MLP both utilize dropout and a ReLU activation function. We use a variant of ReLU called Leaky ReLU ( <ref type="bibr">Maas et al., 2013)</ref>, defined by:</p><formula xml:id="formula_0">LeakyReLU (x) = max(0, x) + y * min(0, x)</formula><p>where we set y = 0.01 as the negative slope for x &lt; 0. This prevents the gradient from dying when x &lt; 0. For the sentence representations we first embed the individual words with pre-trained word embedding. The sequence of the embedded words is then passed on to the sentence encoder which utilizes BiLSTM with max pooling.</p><p>Given a sequence T of words (w 1 . . . , w T ), the output of the bi-directional LSTM is a set of vectors (h 1 , . . . , h T ), where each h t ∈ (h 1 , . . . , h T ) is the concatenation</p><formula xml:id="formula_1">h t = [ − → h t , ← − h t ]</formula><p>of a forward and backward LSTMs</p><formula xml:id="formula_2">− → h t = −−−−→ LST M t (w 1 , . . . , w T ) ← − h t = ←−−−− LST M t (w 1 , . . . , w T ).</formula><p>The max pooling layer produces a vector of the same dimensionality as h t , returning, for each dimension, its maximum value over the hidden units</p><formula xml:id="formula_3">(h 1 , . . . , h T ).</formula><p>Motivated by the strong results of the BiLSTM max pooling network by <ref type="bibr">Conneau et al. (2017)</ref>, we experimented with combining BiLSTM max pooling networks as a hierarchical structure. 1 To improve the BiLSTM layers' ability to remember the input words, we let each layer of the stack re-read the input sentence. In our baseline model we stack three BiLSTM max pooling networks as a hierarchical structure, where each BiLSTM reads the input sentence as the input. At each BiLSTM layer except the first one, we initialize the initial hidden state and the cell state with the final state of the previous layer. We take the max value over each dimension of the hidden units for each BiLSTM layer. The final output of the sentence embedding is the concatenation of each of these max pooling layers. Our sentence embedding architecture is visualized in <ref type="figure" target="#fig_1">Figure 2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data</head><p>We evaluated the sentence embedding architecture with three different natural language inference datasets, including the Stanford Natural Language Inference (SNLI) corpus, the Multi-Genre Natural Language Inference (MultiNLI) corpus and the SciTail dataset. In all our experiments with the three datasets we used only the training data provided in the respective corpus. For the transfer learning tasks, described in Section 7, we used training data from both the SNLI and the MultiNLI datasets in order to compare to the results by <ref type="bibr">Conneau et al. (2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SNLI</head><p>The Stanford Natural Language Inference (SNLI) corpus <ref type="bibr">(Bowman et al., 2015</ref>) is a dataset of 570k human-written sentence pairs manually labeled with entailment, contradiction, and neutral. The dataset is divided into training (550,152 pairs), development (10,000 pairs) and test sets (10,000 pairs). The source for the premise sentences in SNLI were image captions taken from the Flickr30k corpus ( <ref type="bibr">Young et al., 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MultiNLI</head><p>The Multi-Genre Natural Language Inference (MultiNLI) corpus <ref type="bibr">(Williams et al., 2018</ref>) is a broad-coverage corpus for natural language inference, consisting of 433k human-written sentence pairs labeled with entailment, contradiction and neutral. Unlike the SNLI corpus, which draws the premise sentence from image captions, MultiNLI consists of sentence pairs from ten distinct genres of both written and spoken English. The dataset is divided into training (392,702 pairs), development (20,000 pairs) and test sets (20,000 pairs).</p><p>All of the genres are included in the test and development sets, but only five are included in the training set. The development and test datasets have been divided into matched and mismatched, where the former includes only sentences from the same genres as the training data, and the latter includes sentences from the remaining genres not present in the training data.</p><p>In addition to the training, development and test sets, MultiNLI provides a smaller annotation dataset, which contains approximately 1000 sentence pairs annotated with linguistic properties of the sentences and is split between the matched and mismatched datasets. <ref type="bibr">2</ref> This annotation dataset provides a simple way to assess what kind of sentence pairs an NLI system is able to predict correctly and where it makes errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SciTail</head><p>The SciTail dataset ( <ref type="bibr">Khot et al., 2018</ref>) is an NLI dataset created from multiple-choice science exams consisting of 27k sentence pairs. Each question and the correct answer choice have been converted into an assertive statement to form the hypothesis. The dataset is divided into training (23,596 pairs), development (1,304 pairs) and test sets (2,126 pairs). Unlike the SNLI and MultiNLI datasets, SciTail uses only two labels: entailment and neutral.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training Details</head><p>The architecture was implemented using PyTorch. We have published our code in GitHub: https: //github.com/Helsinki-NLP/HBMP.</p><p>For all of our models we used a gradient descent optimization algorithm based on the Adam update rule <ref type="bibr">(Kingma and Ba, 2014)</ref>, which is preimplemented in PyTorch. We used a learning rate of 5e-4 for all our models. The learning rate was decreased by the factor of 0.2 after each epoch if the model did not improve. We used a batch size of 64. The models were evaluated with the development data after each epoch and training was stopped if the development loss increased for more than 3 epochs. The model with the highest development accuracy was selected for testing.</p><p>We use pre-trained GloVe word embeddings of size 300 dimensions (GloVe 840B 300D) <ref type="bibr">(Pen- nington et al., 2014</ref>), which were fine-tuned during training. The sentence embeddings have hidden dimensionality of 600 for both direction (except for SentEval test, where we test models with 600D and 1200D per direction) and the 3-layer multi-layer perceptron (MLP) have the size of 600 dimensions. We use a dropout of 0.1 in the BiL-STM layers and between the MLP layers (except just before the final layer).</p><p>All our models were trained using one NVIDIA Tesla P100 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>The proposed architecture provides strong results on all of the three datasets. We achieve the new state of the art for SciTail 86.0%. On SNLI our results are on par with the current state of the art 86.6%. On MultiNLI our model provides strong results on both the matched and mismatched test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SNLI</head><p>For for the SNLI corpus our model provides the test accuracy of 86.6% after 4 epochs of training, which is on par with the previously published state of the art for sentence embedding approaches using BiLSTM and generalized pooling <ref type="bibr">(Chen et al.)</ref>. However, our model requires less trainable parameters than Chen et al.'s model (32m vs 65m). Comparison of our results with the previous state of the art and selected other sentence embedding based results are reported in <ref type="table" target="#tab_1">Table 1</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MultiNLI</head><p>For the MultiNLI matched test set (MultiNLI-m) our model achieves a test accuracy of 73.7% after 3 epochs of training, which is 0.8% points lower than the state of the art 74.5% by <ref type="bibr">Nie and Bansal (2017a)</ref>. For the mismatched test set (MultiNLImm) our model achieves a test accuracy of 73.0% after 3 epochs of training, which is 0.6% points lower than the state of the art 73.6% by <ref type="bibr">Chen et al. (2017b)</ref>. Comparison of our results with the previous state of the art and selected sentence embedding based results are reported in <ref type="table" target="#tab_3">Table 2</ref>.</p><p>Although we did not achieve state of the art results for the MultiNLI dataset, we believe that a systematic study of different hierarchical BiLSTM max pooling structures could reveal an architecture providing the needed improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SciTail</head><p>On the SciTail dataset we compared our model against non-sentence embedding-based models, as no results have been previously published which are based only on sentence embeddings. For this reason we also implemented the basic LSTM (using the last hidden state as the output) and BiL-STM with max pooling models and tested them on the SciTail dataset.</p><p>For SciTail we obtain a test score of 86.0% after 4 epochs of training, which is +2.7% points abso-   lute improvement on the previous state of the art by <ref type="bibr">Tay et al. (2018)</ref>. The results achieved by our proposed model are significantly higher than the previously published results. It has been argued that the lexical similarity of the sentences in SciTail sentence pairs make it a particularly difficult dataset ( <ref type="bibr">Khot et al., 2018)</ref>. We hypothesize that our model is indeed better at identifying entailment relations beyond focusing on the lexical similarity or difference of the sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Error Analysis</head><p>To understand better what kind of inferential relationships our model is able to identify, we conducted an error analysis for the three datasets. We report the results below. <ref type="bibr">3</ref> We also conducted a linguistic error analysis and compared our results to the results obtained with the InferSent BiLSTM max pooling model of <ref type="bibr">Conneau et al. (2017)</ref> (our implementation). 4 <ref type="bibr">3</ref> For more detailed error statistics, see the appendix. <ref type="bibr">4</ref> The scores for our implementation of InferSent are on par or slightly higher than the scores reported by <ref type="bibr">Conneau et al. (2017)</ref> using their training setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SNLI</head><p>On the SNLI dataset our model makes the least errors on sentence pairs labeled with entailment, having an accuracy of: 90.5%. The model is also almost effective in predicting contradictions, with an accuracy of 87.7%. Our model is less effective in recognizing correctly sentence pairs labeled with neutral, having an accuracy of 81.5%.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MultiNLI</head><p>As the labeled test data is not openly available for MultiNLI, we analyzed the error statistics for this dataset based on the development data. For the matched dataset (MultiNLI-m) our model had a development accuracy of 73.2%. On this dataset our model makes fewer errors on sentence pairs labeled as entailment, having an accuracy of 75.4% compared to pairs with label contradiction, where the accuracy was 73.1% and neu- tral, where the accuracy was 70.8%. <ref type="table" target="#tab_7">Table 5</ref> lists the prediction accuracies per each gold label and compares the results to InferSent results. For the mismatched dataset (MultiNLI-mm) our model had a development accuracy of 74.2%. On this dataset our model makes significantly fewer errors on sentence pairs labeled as entailment, having an accuracy of 80.3% compared to pairs with label contradiction, where the accuracy was 72.7% and neutral, where the accuracy was just 69.0%. <ref type="table" target="#tab_9">Table 6</ref> summarizes the prediction accuracies per each gold label and compares the results to InferSent results.</p><note type="other">Label HBMP InferSent contradiction 73.1 69.8 entailment 75.4 75.0 neutral 70.8 68.5</note><p>We also conducted additional linguistic error analysis using the annotation test set provided for MultiNLI. The results are reported in the appendix and they are mostly inconclusive.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Label</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SciTail</head><p>For SciTail, unlike for the SNLI and both of the MultiNLI datasets, our model makes more errors on sentence pairs labeled as entailment, having an accuracy of 75.1% compared to pairs marked with neutral, where the accuracy was 93.1%.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Transfer Learning</head><p>To better understand how well our model generalizes to different tasks, we conducted additional transfer learning tests using the SentEval sentence embedding evaluation library 5 ( <ref type="bibr">Conneau et al., 2017</ref>) and compared our results to the results published for InferSent and SkipThought ( <ref type="bibr">Kiros et al., 2015)</ref>. For the transfer learning tasks, we trained our model on NLI data consisting of the concatenation of the SNLI and MultiNLI training sets consisting of 942,854 sentence pairs in total. This allows us to compare our results to the InferSent results which were obtained using a model trained on the same data ( <ref type="bibr">Conneau et al., 2017)</ref>. <ref type="bibr">Conneau et al. (2017)</ref> have shown that including all the training data from SNLI and MultiNLI improves significantly the model performance on transfer learning tasks, compared to training the model only on SNLI data. For training the model we used the same setup as described above in Section 4.</p><p>We used the SentEval sentence embedding evaluation library using the default settings 6 recommended on the SentEval website, with a logistic regression classifier, Adam optimizer with learning rate of 0.001, batch size of 64 and epoch size of 4. <ref type="table" target="#tab_13">Table 8</ref> lists the transfer learning results for our models with 1200D and 2400D hidden dimensionality and compares our model to the InferSent and SkipThought scores reported by <ref type="bibr">Conneau et al. (2017)</ref>. The downstream datasets included in the tests were MR movie reviews, CR product reviews, SUBJ subjectivity status, MPQA opinionpolarity, SST binary sentiment analysis, TREC question-type classification, MRPC paraphrase detection, SICK-Relatedness (SICK-R) semantic textual similarity, SICK-Entailment (SICK-E) natural language inference and STS14 semantic textual similarity.</p><p>Our 2400D model outperforms the InferSent model on 7 out of 10 tasks. The model achieves higher score on 8 out of 9 tasks reported for SkipThought, having equal score on the SUBJ dataset. No MRPC results have been reported for SkipThought.</p><p>To study in more detail the linguistic properties <ref type="bibr">5</ref> The SentEval test suite for evaluating sentence embeddings is available online at https://github.com/ facebookresearch/SentEval. <ref type="bibr">6</ref> Refer to the SentEval website for details about the different settings:</p><p>https://github.com/ facebookresearch/SentEval.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MR CR SUBJ MPQA SST TREC MRPC SICK-R SICK-E STS14</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this paper we have introduced an architecture based on BiLSTM max pooling, that achieves on par results with the current state of the art for SNLI sentence encoding-based models and a new state of the art score for SciTail.</p><p>We furthermore tested our model using the SentEval sentence embedding evaluation library, showing that our model achieves great generalization capability, outperforming InferSent on 7 out of 10 downstream and 8 out of 10 probing tasks, and SkipThought on 8 out of 9 downstream tasks. Our model also achieves a strong result on the Breaking NLI dataset outperforming Infersent on 11 out of 14 lexical categories.</p><p>The success of the proposed hierarchical architecture raises a number of additional interesting questions. First, it would be important to understand what kind of semantic information the different layers are able to capture. Second, a detailed and systematic comparison of different hierarchical architecture configurations, combining BiLSTM and max pooling in different ways, could lead to even stronger results, as indicated by the results we obtained on the SciTail dataset with the modified 4-layered model. Also, as the sentence embedding approaches for NLI focus mostly on the sentence encoder, we think that more should be done to study the classifier part of the overall NLI architecture. There is not enough research on classifiers for NLI and we hypothesize that further improvements can be achieved by a systematic study of different classifier architectures, starting from the way the two sentence embeddings are combined before passing on to the classifier. This is also something we intend to undertake as a next step.</p><p>Finally, there are number of other NLP tasks, including neural machine translation (NMT), where HBMP models should be evaluated. We plan to evaluate the performance of the HBMP encoder in other NLP tasks like on encoder-decoder NMT models in the future. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Detailed Error Statistics for the HBMP model</head><p>To see in more detail how our HBMP model is able to classify sentence pairs with different gold labels and what kind of errors it makes, we summarize error statistics as confusion matrices for the different datasets.     The confusion matrices highlight the HBMP model's strong performance across all the labels. However, the accuracy for neutral in the SNLI and MultiNLI datasets is clearly lower than for the other two labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Linguistic Error Analysis</head><p>We also conducted a linguistic error analysis for the HBMP model using the MultiNLI annotation set. We compare our model and the type of linguistic reasoning it is capable of to InferSent BiL-STM max pooling model ( <ref type="bibr">Conneau et al., 2017)</ref>, in order to see what benefits are achieved by adding a hierarchical BiLSTM max pooling structure on top of the basic BiLSTM max pooling architecture. We provide a detailed comparison of the prediction accuracies of our HBMP model with InferSent with respect to the type of linguistic properties present in the sentence pairs. <ref type="table" target="#tab_1">Table 10</ref> contains the comparison for MultiNLI-m dataset and <ref type="table" target="#tab_1">Table 11</ref> for MultiNLI-mm dataset.</p><p>The analysis show that our HBMP model outperforms InferSent in some of the categories, but the results are mostly at this point inconclusive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional tests with the Breaking NLI dataset</head><p>We conducted additional testing of the proposed sentence embedding architecture using the Breaking NLI test set recently published by <ref type="bibr">Glockner et al. (2018)</ref>. The test set is designed to highlight   the lack of lexical reasoning capability of NLI systems.</p><p>We trained our HBMP model and the InferSent model using the SNLI training data. We compare our results with the results published by <ref type="bibr">Glockner et al. (2018)</ref> and to results obtained with InferSent sentence encoder (our implementation).</p><p>Our HBMP model outperforms the InferSent model in 7 out of 14 categories, receiving an overall score of 65.1% (InferSent: 65.6%). Our model also compares well against the other models, outperforming Decomposable Attention model (51.90%) ( <ref type="bibr">Parikh et al., 2016)</ref> and Residual Encoders (62.20%) <ref type="bibr">(Nie and Bansal, 2017b</ref>) in the overall score. As these models are not based purely on sentence embeddings, the obtained result highlights that sentence embedding approaches can be competitive when handling inferences requiring lexical information. Our model is still outperformed by and ESIM <ref type="bibr">(Chen et al., 2017a</ref>) and KIM, an ESIM model incorporating external knowledge <ref type="bibr">(Chen et al., 2018)</ref>. The results of the comparison are summarized in   <ref type="bibr">Glockner et al. (2018)</ref>. InferSent results obtained with our implementation using the architecture and training set-up described in ( <ref type="bibr">Conneau et al., 2017</ref>). Scores highlighted with bold are top scores when comparing the InferSent and our HBMP model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overall NLI Architecture</figDesc><graphic url="image-1.png" coords="2,307.28,257.68,218.28,185.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Architecture of the sentence embeddings: Hierarchical BiLSTM max pooling (HBMP) architecture (where T = 4).</figDesc><graphic url="image-2.png" coords="3,72.00,318.73,249.46,212.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 contains</head><label>3</label><figDesc>the confusion matrix for the SNLI dataset. Figure 4 contains the confusion matrix for the MultiNLI Matched dataset. Figure 5 contains the confusion matrix for the MultiNLI Mismatched dataset. Finally, Fig- ure 6 contains the confusion matrix for the SciTail dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: SNLI Confusion Matrix.</figDesc><graphic url="image-3.png" coords="9,72.00,493.18,218.27,92.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: MultiNLI Matched Confusion Matrix.</figDesc><graphic url="image-4.png" coords="9,72.00,634.65,218.27,91.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: MultiNLI Mismatched Confusion Matrix.</figDesc><graphic url="image-6.png" coords="9,331.37,194.55,170.08,78.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: SciTail Confusion Matrix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>SNLI test accuracies (%). Results marked 
with a by Conneau et al. (2017), b by Im and Cho 
(2017), c by Shen et al. (2018). and d by (Chen et al.) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>MultiNLI test accuracies (%). Results marked with a are baseline results by Williams et al. (2018), b by 
Vu (2017), c by Balazs et al. (2017), d by Chen et al. (2017b) and e by Nie and Bansal (2017a). Our results for the 
MultiNLI test sets were obtained by submitting the predictions to the respective Kaggle competitions. 

Model 
Accuracy 
DecompAtt a 
72.3 
ESIM a 
70.6 
Ngram a 
70.6 
DGEM w/o edges a 
70.8 
DGEM a 
77.3 
CAFE b 
83.3 
Our LSTM 
67.3 
Our BiLSTM max pooling 
84.9 
Our HBMP 
86.0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>SciTail test accuracies (%). As the previously 
published results are not based on purely sentence em-
beddings we include our results for simple LSTM and 
BiLSTM with max pooling. Results marked with a are 
baseline results reported by Khot et al. (2018) and b by 
Tay et al. (2018). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 summarizes</head><label>4</label><figDesc>the prediction accuracies per each gold label and compares our results to InferSent results. We also tested our model against the recently published Breaking NLI test set for the SNLI (Glockner et al., 2018), designed to test NLI mod- els' ability to recognize inferences requiring lexi- cal knowledge. Our model outperforms InferSent on 7 out of 14 lexical categories and achieves the overall accuracy of 65.1% compared to InferSent's score of 65.6%. Detailed description of our results for the Breaking NLI dataset are included in the appendix.</figDesc><table>Label 
HBMP InferSent 
contradiction 
87.7 
85.1 
entailment 
90.5 
88.1 
neutral 
81.5 
79.9 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>SNLI error analysis by gold label (accuracy 
%), comparing HBMP to InferSent BiLSTM max pool-
ing (Conneau et al., 2017) (our implementation). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 : MultiNLI-m error analysis by gold label (ac- curacy %), comparing HBMP to InferSent</head><label>5</label><figDesc></figDesc><table>BiLSTM 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>MultiNLI-mm error analysis by gold label 
(accuracy %), comparing HBMP to InferSent BiLSTM 
max pooling (Conneau et al., 2017) (our implementa-
tion). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 7 summarizes</head><label>7</label><figDesc>the prediction accuracies per each of the two gold labels and compares the results to In- ferSent results.</figDesc><table>Label 
HBMP InferSent 
entailment 
75.1 
79.9 
neutral 
93.1 
89.1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>SciTail error analysis by gold label (accu-
racy %), comparing HBMP to InferSent BiLSTM max 
pooling (Conneau et al., 2017) (our implementation). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" validated="false"><head>Table 8 : Transfer learning test results for the HBMP model on a number of SentEval downstream</head><label>8</label><figDesc></figDesc><table>sentence 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" validated="false"><head>Table 9 :</head><label>9</label><figDesc></figDesc><table>SentEval probing task results (accuracy %). InferSent results are BiLSTM Max (NLI) results as reported 
by Conneau et al. (2018). 

of our proposed model, we ran the recently pub-
lished SentEval probing tasks described in (Con-
neau et al., 2018). Our 24000D HBMP model 
outperforms the InferSent BiLSTM max pooling 
model in 8 out of 10 probing tasks. These results 
further highlight our model's applicability for var-
ious NLP tasks. The results are listed in Table 9. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17" validated="false"><head>Table 10 :</head><label>10</label><figDesc></figDesc><table>MultiNLI-m linguistic error analysis (accuracy %), comparing our HBMP results to the InferSent 
BiLSTM max pooling model (Conneau et al., 2017) results (our implementation). 

Entailment 
Contradiction 
Neutral 
HBMP 
InferSent 
HBMP 
InferSent 
HBMP InferSent 
ACTIVE/PASSIVE 
88.9 
100.0 
100.0 
100.0 
-
-
ANTO 
-
-
76.9 
69.2 
85.7 
71.4 
BELIEF 
70.6 
82.4 
55.6 
61.1 
91.3 
78.3 
CONDITIONAL 
81.8 
81.8 
37.5 
62.5 
71.4 
71.4 
COREF 
75.0 
75.0 
78.6 
64.3 
72.7 
81.8 
LONG SENTENCE 
67.7 
77.4 
61.1 
61.1 
76.2 
71.4 
MODAL 
76.6 
78.7 
60.0 
68.6 
84.1 
70.5 
NEGATION 
60.0 
64.0 
76.4 
76.4 
66.7 
45.8 
PARAPHRASE 
83.8 
86.5 
-
-
-
-
QUANTIFIER 
67.5 
72.5 
71.2 
73.1 
81.3 
77.1 
QUANTITY/TIME REASONING 
58.3 
50.0 
25.0 
41.7 
0.0 
33.3 
TENSE DIFFERENCE 
75.0 
75.0 
0.0 
0.0 
91.7 
75.0 
WORD OVERLAP 
95.2 
95.2 
33.3 
41.7 
75.0 
0.0 
Total 
81.2 
83.0 
66.7 
68.5 
76.4 
72.7 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18" validated="false"><head>Table 11 :</head><label>11</label><figDesc></figDesc><table>MultiNLI-mm linguistic error analysis (accuracy %), comparing our HBMP results to the InferSent 
BiLSTM max pooling model (Conneau et al., 2017) results (our implementation). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19" validated="false"><head>Table 12 .</head><label>12</label><figDesc></figDesc><table>Category 

Decomposable 
Attention * 
ESIM * 
Residual 
Encoders * 

WordNet 
Baseline * KIM * InferSent Our HBMP 
antonyms 
41.6 
70.4 
58.2 
95.5 
86.5 
51.6 
54.7 
antonyms (wordnet) 
55.1 
74.6 
67.9 
94.5 
78.8 
63.7 
69.1 
cardinals 
53.5 
75.5 
53.1 
98.6 
93.4 
49.4 
58.8 
colors 
85 
96.1 
87 
98.7 
98.3 
90.6 
90.4 
countries 
15.2 
25.4 
66.2 
100 
70.8 
77.2 
81.2 
drinks 
52.9 
63.7 
52 
94.8 
96.6 
85.1 
81.3 
instruments 
96.9 
90.8 
96.9 
67.7 
96.9 
98.5 
96.9 
materials 
65.2 
89.7 
79.9 
75.3 
98.7 
81.6 
82.6 
nationalities 
37.5 
35.9 
70.9 
78.5 
73.5 
47.3 
49.8 
ordinals 
2.1 
21 
5.4 
40.7 
56.6 
7.4 
4.5 
planets 
31.7 
3.3 
21.7 
100 
5 
75 
45.0 
rooms 
59.2 
69.4 
63.4 
89.9 
77.6 
76.3 
72.1 
synonyms 
97.5 
99.7 
86.1 
70.5 
92.1 
99.6 
84.5 
vegetables 
43.1 
31.2 
37.6 
86.2 
79.8 
39.5 
40.4 
Total 
51.9 
65.6 
62.2 
85.8 
83.5 
65.6 
65.1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20" validated="false"><head>Table 12 :</head><label>12</label><figDesc></figDesc><table>Breaking NLI scores (accuracy %). Results marked with * as reported by </table></figure>

			<note place="foot" n="1"> Conneau et al. (2017) explore a similar hierarchical architecture of convolutional neural networks.</note>

			<note place="foot" n="2"> The annotated dataset and description of the annotations are available at http://www.nyu.edu/ projects/bowman/multinli/multinli_1.0_ annotations.zip</note>
		</body>
		<back>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
