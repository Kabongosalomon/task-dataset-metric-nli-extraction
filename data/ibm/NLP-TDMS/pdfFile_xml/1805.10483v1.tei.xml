<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Look at Boundary: A Boundary-Aware Face Alignment Algorithm</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Tsinghua National Laboratory for Information Science and Technology (TNList)</orgName>
								<orgName type="department" key="dep2">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">SenseTime Research 3 Amazon Rekognition</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">SenseTime Research 3 Amazon Rekognition</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
							<email>wangquan@sensetime.com3shuoy@amazon.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">SenseTime Research 3 Amazon Rekognition</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yici</forename><surname>Cai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Tsinghua National Laboratory for Information Science and Technology (TNList)</orgName>
								<orgName type="department" key="dep2">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Tsinghua National Laboratory for Information Science and Technology (TNList)</orgName>
								<orgName type="department" key="dep2">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Look at Boundary: A Boundary-Aware Face Alignment Algorithm</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a novel boundary-aware face alignment algorithm by utilising boundary lines as the geometric structure of a human face to help facial landmark localisation. Unlike the conventional heatmap based method and regression based method, our approach derives face landmarks from boundary lines which remove the ambiguities in the landmark definition. Three questions are explored and answered by this work: 1. Why using boundary? 2. How to use boundary? 3. What is the relationship between boundary estimation and landmarks localisation? Our boundaryaware face alignment algorithm achieves 3.49% mean error on 300-W Fullset, which outperforms state-of-the-art methods by a large margin. Our method can also easily integrate information from other datasets. By utilising boundary information of 300-W dataset, our method achieves 3.92% mean error with 0.39% failure rate on COFW dataset, and 1.25% mean error on AFLW-Full dataset. Moreover, we propose a new dataset WFLW to unify training and testing across different factors, including poses, expressions, illuminations, makeups, occlusions, and blurriness. Dataset and model will be publicly available at https://wywu. github.io/projects/LAB/LAB.html</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Face alignment, which refers to facial landmark detection in this work, serves as a key step for many face applications, e.g., face recognition <ref type="bibr" target="#b74">[75]</ref>, face verification <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49]</ref> and face frontalisation <ref type="bibr" target="#b20">[21]</ref>. The objective of this paper is to devise an effective face alignment algorithm to handle faces with unconstrained pose variation and occlusion across multiple datasets and annotation protocols. * This work was done during an internship at SenseTime Research. datasets with different number of landmarks. The second column illustrates the universally defined facial boundaries estimated by our methods. With the help of boundary information, our approach achieves high accuracy localisation results across multiple datasets and annotation protocols, as shown in the third column.</p><p>Different to face detection <ref type="bibr" target="#b44">[45]</ref> and recognition <ref type="bibr" target="#b74">[75]</ref>, face alignment identifies geometry structure of human face which can be viewed as modeling highly structured output. Each facial landmark is strongly associated with a well-defined facial boundary, e.g., eyelid and nose bridge. However, compared to boundaries, facial landmarks are not so well-defined. Facial landmarks other than corners can hardly remain the same semantical locations with large pose variation and occlusion. Besides, different annotation schemes of existing datasets lead to a different number of landmarks <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b29">30]</ref> (19/29/68/194 points) and annotation scheme of future face alignment datasets can hardly be determined. We believe the reasoning of a unique facial structure is the key to localise facial landmarks since human face does not include ambiguities.</p><p>To this end, we use well-defined facial boundaries to represent the geometric structure of the human face. It is easier to identify facial boundaries comparing to facial landmarks under large pose and occlusion. In this work, we represent facial structure using 13 boundary lines. Each facial boundary line can be interpolated from a sufficient number of facial landmarks across multiple datasets, which will not suffer from inconsistency of the annotation schemes.</p><p>Our boundary-aware face alignment algorithm contains two stages. We first estimate facial boundary heatmaps and then regress landmarks with the help of boundary heatmaps. As noticed in <ref type="figure" target="#fig_0">Fig. 1</ref>, facial landmarks of different annotation schemes can be derived from boundary heatmaps with the same definition. To explore the relationship between facial boundaries and landmarks, we introduce adversarial learning ideas by using a landmark-based boundary effectiveness discriminator. Experiments have shown that the better quality estimated boundaries have, the more accurate landmarks will be. The boundary heatmap estimator, landmark regressor, and boundary effectiveness discriminator can be jointly learned in an end-to-end manner.</p><p>We used stacked hourglass structure <ref type="bibr" target="#b34">[35]</ref> to estimate facial boundary heatmap and model the structure between facial boundaries through message passing <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b62">63]</ref> to increase its robustness to occlusion. After generating facial boundary heatmaps, the next step is deriving facial landmarks using boundaries. The boundary heatmaps serve as structure cue to guide feature learning for the landmark regressor. We observe that a model guided by ground truth boundary heatmaps can achieve 76.26% AUC on 300W <ref type="bibr" target="#b38">[39]</ref> test while the state-of-the-art method <ref type="bibr" target="#b14">[15]</ref> can only achieve 54.85%. This suggests the richness of information contained in boundary heatmaps. To fully utilise the structure information, we apply boundary heatmaps at multiple stages in the landmark regression network. Our experiment shows that the more stages boundary heatmaps are used in feature learning, the better landmark prediction results we will get.</p><p>We evaluate the proposed method on three popular face alignment benchmarks including 300W <ref type="bibr" target="#b38">[39]</ref>, COFW <ref type="bibr" target="#b4">[5]</ref>, and AFLW <ref type="bibr" target="#b27">[28]</ref>. Our approach significantly outperforms previous state-of-the-art methods by a large margin. 3.49% mean error on 300-W Fullset, 3.92% mean error with 0.39% failure rate on COFW and 1.25% mean error on AFLW-Full dataset respectively. To unify the evaluation, we propose a new large dataset named Wider Facial Landmarks in-the-wild (WFLW) which contain 10, 000 images. Our new dataset introduces large pose, expression, and occlusion variance. Each image is annotated with 98 landmarks and 6 attributes. Comprehensive ablation study demonstrates the effectiveness of each component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In the literature of face alignment, besides classic methods (ASMs <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b22">23]</ref>, AAMs <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b24">25]</ref>, CLMs <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b41">42]</ref> and Cascaded Regression Models <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b17">18]</ref>), recently, state-of-the-art performance has been achieved with Deep Convolutional Neural Networks (DC-NNs). These methods mainly fall into two categories, i.e., coordinate regression model and heatmap regression model.</p><p>Coordinate regression models directly learn the mapping from the input image to the landmark coordinates vector. Zhang et al. <ref type="bibr" target="#b69">[70]</ref> frames the problem as a multi-task learning problem, learns landmark coordinates and predicts facial attributes at the same time. MDM <ref type="bibr" target="#b50">[51]</ref> is the first end-to-end recurrent convolutional system for face alignment from coarse to fine. TSR <ref type="bibr" target="#b30">[31]</ref> splits face into several parts to ease the parts variations and regresses the coordinates of different parts respectively. Even though coordinate regression models have the advantage of explicit inference of landmark coordinates without any post-processing. Nevertheless, they are not performing as well as heatmap regression models.</p><p>Heatmap regression models, which generate likelihood heatmaps for each landmark respectively, have recently achieved state-of-the-art performance in face alignment. CALE <ref type="bibr" target="#b3">[4]</ref> is a two-stage convolutional aggregation model to aggregate score maps predicted by detection stage along with early CNN features for final heatmap regression. Yang et al. <ref type="bibr" target="#b59">[60]</ref> uses a two parts network, i.e., a supervised transformation to normalise faces and a stacked hourglass network <ref type="bibr" target="#b34">[35]</ref> to get prediction heatmaps. Most recently, JMFA <ref type="bibr" target="#b14">[15]</ref> achieves state-of-the-art accuracy by leveraging stacked hourglass network <ref type="bibr" target="#b34">[35]</ref> for multi-view face alignment and demonstrates better than the best three entries of the last Menpo Challenge <ref type="bibr" target="#b65">[66]</ref>.</p><p>Since boundary detection was set as one of the most fundamental problems in computer vision and there have emerged a large number of materials <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b42">43]</ref>. It has been proved efficient in vision tasks as segmentation <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b21">22]</ref> and object detection <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b36">37]</ref>. In face alignment, boundary information demonstrates especial importance because almost all of the landmarks are defined lying on the facial boundaries. However, as far as we know, in face alignment task, no work before has investigated the use of boundary information from an explicit perspective.</p><p>The recent advance in human pose estimation partially inspires our method of boundary heatmaps estimation. Stacked hourglass network <ref type="bibr" target="#b34">[35]</ref> achieves compelling accuracy with a bottom-up, top-down design which endows the network with capabilities of obtaining multi-scale information. Message passing <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b62">63]</ref> has shown great power in structure modeling of human joints. Recently, adversarial learning <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> is adopted to further improve the accuracy of estimated human pose under heavy occlusion.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Boundary-Aware Face Alignment</head><p>As mentioned in the introduction, landmarks have difficulty in presenting accurate and universal geometric structure of face images. We propose facial boundary as geometric structure representation and help landmarks regression problem in the end. Boundaries are detailed and welldefined structure descriptions, which are consistent across head poses and datasets. They are also closely related to landmarks since most of the landmarks are located along boundary lines.</p><p>Other choices are also available for geometric structure representations. Recent works <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b18">19</ref>] has adopted facial parts to aid face alignment tasks. However, facial parts are too coarse thus not as powerful as boundary lines. Another choice would be face parsing results. Face parsing leads to disjoint facial components which needs the boundaries of each component form a closed loop. However, some facial organs such as nose are naturally blended into the whole face thus are inaccurate to be defined as separate parts. On the contrary, boundary lines are not necessary to form a closed loop, which is more flexible in representing geometric structure. Experiments in Sec 4.2 have shown that boundary lines are the best choice to aid landmark coordinates regression.</p><p>The detailed configuration of our proposed Boundary-Aware Face Alignment framework is illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. It is composed of three closely related components: Boundary-Aware Landmark Regressor, Boundary Heatmap Estimator and Landmark-Based Boundary Effectiveness Discriminator. Boundary-Aware Landmark Regressor incorporates boundary information in a multi-stage manner to predict landmark coordinates. Boundary Heatmap Estimator produces boundary heatmaps as face geometric structure. Since boundary information is used heavily, the quality of boundary heatmaps is crucial for final landmark regression. We introduce adversarial learning idea <ref type="bibr" target="#b19">[20]</ref> by proposing Landmark-Based Boundary Effectiveness Dis-criminator, which is paired with the Boundary Heatmap Estimator. This discriminator can further improve the quality of boundary heatmaps and lead to better landmark coordinates prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Boundary-aware landmarks regressor</head><p>In order to fuse boundary line into feature learning, we transform landmarks to boundary heatmaps to aid the learning of feature. The responses of each pixel in boundary heatmap are decided by its distance to the corresponding boundary line. As shown in <ref type="figure">Fig. 3</ref>, the details of boundary heatmap are defined as follows.</p><p>Given a face image I, denote its ground truth annotation by L landmarks as S = {s l } L l=1 . K subsets S i ⊂ S are defined to represent landmarks belongs to K boundaries respectively, such as upper left eyelid and nose bridge. For each boundary, S i is interpolated to get a dense boundary line. Then a binary boundary map B i , the same size as I, is formed by setting only points on the boundary line to be 1, others 0. Finally, a distance transform is performed based on each B i to get distance map D i . We use a gaussian expression with standard deviation σ to transform the distance map to ground-truth boundary heatmap M i . 3σ is used to threshold D i to make boundary heatmaps focus more on boundary areas. In practice, the length of the ground-truth boundary heatmap side is set to a quarter of the size of I for computation efficiency.</p><formula xml:id="formula_0">M i (x, y) = exp(− Di(x,y) 2 2σ 2 ), if D i (x, y) &lt; 3σ 0, otherwise<label>(1)</label></formula><p>In order to fully utilise the rich information contained in boundary heatmaps, we propose a multi-stage boundary heatmap fusion scheme. As illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>, A fourstage res-18 network is adopted as our baseline network. Boundary heatmap fusion is conducted at the input and every stage of the network. Comprehensive results in Sec. <ref type="bibr" target="#b3">4</ref>  have shown that the more fusion we conducted to the baseline network, the better performance we can get. Input image fusion. To fuse boundary heatmap M with input image I, the fused input H is defined as:</p><formula xml:id="formula_1">H = I ⊕ (M 1 ⊗ I) ⊕ ... ⊕ (M T ⊗ I)<label>(2)</label></formula><p>where ⊗ represents the element-wise dot product operation and ⊕ represents channel-wise concatenation. The above design makes fused input focus only on detailed texture around boundaries. Thus most background and texture-less face regions are ignored which greatly enhance the effectiveness of input. The original input is also concatenated to the fused ones to keep other valuable information in the original image. Feature map fusion. Similar to above, to fuse boundary heatmap M with feature map F , the fused feature map H is defined as:</p><formula xml:id="formula_2">H = F ⊕ (F ⊗ T (M ⊕ F ))<label>(3)</label></formula><p>Since the number of channels of M equals to the number of pre-defined boundaries, which is constant. A transform function T is necessary to convert M to have the same channels with F . We choose hourglass structure subnet as T to keep feature map size. Down-sampling and upsampling are performed symmetrically. Skip connections</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Face Image Baseline Hourglass Baseline + Message Passing</head><p>Baseline + Message Passing + Adversarial Learning <ref type="figure">Figure 5</ref>: An illustration of the effectiveness of message passing and adversarial learning. With the message passing and adversarial learning addition, the quality of the estimated boundary is well improved to be more and more plausible and focused.</p><p>are used to combine multi-scale information. Then a sigmoid layer normalises the output range to [0, 1]. Another simple choice would be consecutive convolutional layers with stride equals to one, which covers relatively local areas. Experiments in Sec. 4.2 have demonstrated the superiority of hourglass structure. Details of feature map fusion subnet are illustrated in <ref type="figure">Fig. 4</ref>.</p><p>Since boundary heatmaps are used heavily in landmarks coordinates regression. The quality of boundary heatmaps is essential to the prediction accuracy. By fusing ground truth boundary heatmaps, our method can achieve 76.26% AUC on 300-W test, comparing to the state-of-art result 54.85%. Based on this experiment, in the following sections, several methods will be introduced to improve the quality of generated boundary heatmaps. Experiment in ablation study also shows the consistent performance gain with better heatmap quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Boundary heatmap estimator</head><p>Following previous work in face alignment <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b59">60]</ref> and human pose <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b61">62]</ref>, we use stacked hourglass as the baseline of boundary heatmap estimator. Mean square error (MSE) between generated and groundtruth boundary heatmaps is optimized. However, as demonstrated in <ref type="figure">Fig. 5</ref>, when heavy occlusions happen, the generated heatmaps always suffer from the noisy and multi-mode response, which has also been mentioned in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>In order to relieve the problem caused by occlusion, we introduce message passing layers to pass information between boundaries. This process is visualised in <ref type="figure">Fig. 6</ref>. During occlusion, visible boundaries can provide help to occluded ones according to face structure. Intra-level message passing is used at the end of each stack to pass information between different boundary heatmaps. Thus, information can be passed from visible boundaries to occluded ones. Moreover, since different stacks of hourglass focus on different aspects of face information. Inter-level message passing is adopted to pass message from lower stacks to the higher stacks to keep the quality of boundary heatmaps when stacking more hourglass subnets.</p><p>We implemented message passing following <ref type="bibr" target="#b10">[11]</ref>. In this implementation, the feature map at the end of each stack needs be divided into K branches, where K is the number Stack n Stack n+1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intra-level Message Passing</head><p>Inter-level Message Passing <ref type="figure">Figure 6</ref>: An illustration of message pass scheme. A bi-direction tree structure is used for intra-level message passing. Inter-level message is passed between adjacent stacks from lower to higher.</p><p>of boundaries, each represents a type of boundary feature map. This requirement demonstrates the advantage of our boundary heatmaps compared with landmark heatmaps <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b59">60]</ref> for the small and constant number K of them. Thus, the computational and parameter cost of message passing layers within boundaries is small while it is not practical for message passing within 68 or even 194 landmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Boundary effectiveness discriminator</head><p>In structured boundary heatmap estimator, mean squared error (MSE) is used as the loss function. However, minimizing MSE sometimes makes the prediction look blurry and implausible. This regression-to-the-mean problem is a well-known fact in the literature of super-resolution <ref type="bibr" target="#b39">[40]</ref>. It damages the learning of regression network when bad boundary heatmaps are generated.</p><p>However, in our framework, the hard-to-define term "quality" of heatmaps has a very clear evaluation metric. If helping to produce accurate landmark coordinates, the boundary heatmap has a good quality. According to this, we propose a landmark based boundary effectiveness discriminator to decide the effectiveness of the generated boundary heatmaps. For a generated boundary heatmapM (all index i such asM i is omitted for the simplicity of notation), denote its corresponding generated landmark coordinates set asŜ, the ground-truth distance matric map as Dist. The ground truth d fake of discriminator D that determines whether the generated boundary heatmap is fake can be defined as</p><formula xml:id="formula_3">d fake (M ,Ŝ) = 0, Pr s∈Ŝ (Dist(s) &lt; θ) &lt; δ 1, otherwise<label>(4)</label></formula><p>Where θ is the distance threshold to ground truth boundary and δ is the probability threshold. This discriminator predicts whether most generated corresponding landmarks would be close to the ground truth boundary. Following <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>, we introduce the idea of adversarial learning by pairing the boundary effectiveness discriminator D and the boundary heatmaps estimator G. The loss of D can be expressed as:</p><formula xml:id="formula_4">L D = −(E[log D(M )] + E[log(1 − |D(G(I)) − d fake |)])</formula><p>(5) Where M is the ground truth boundary heatmap. The discriminator learns to predict ground truth boundary heatmap as one while predict generated boundary heatmap according to d fake .</p><p>With effectiveness discriminator, the adversarial loss can be expressed as:</p><formula xml:id="formula_5">L A = E[log(1 − D(G(I)))]<label>(6)</label></formula><p>Thus, the estimator is optimised to fool D by giving more plausible and high-confidence maps that will benefit the learning of regression network.</p><p>The following pseudo-code shows the training process of the whole methods. Forward D byd f ake = D(M ) and optimize D by minimizing the second term of L D defined in Eq.5;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Forward R byŜ = R(I,M ) and optimize R by minimizing Ŝ − S 2 2 ; 6: end while</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Cross-Dataset Face Alignment</head><p>Recently, together with impressive progress of algorithms for face alignment, various benchmarks have also been released, e.g., LFPW <ref type="bibr" target="#b2">[3]</ref>, AFLW <ref type="bibr" target="#b27">[28]</ref> and 300-W <ref type="bibr" target="#b38">[39]</ref>. However, because of the gap between annotation schemes, these datasets can hardly be jointly used. Models trained on one specific dataset perform poorly on recent in-the-wild test sets.</p><p>However, introduction of an annotation transfer component <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b52">53]</ref> will bring new problems. From a new perspective, we take facial boundaries as an all-purpose middle-level face geometry representation. Facial boundaries naturally unify different landmark definitions with enough landmarks. And it can also be applied to help training landmarks regressor with any specific landmarks definition. The cross-dataset capacity is an important by-product of our methods. Its effectiveness is evaluated in Sec. 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Datesets. We conduct evaluation on four challenging datasets including 300W <ref type="bibr" target="#b38">[39]</ref>, COFW <ref type="bibr" target="#b4">[5]</ref>, AFLW <ref type="bibr" target="#b27">[28]</ref> and WFLW which is annotated by ourself.</p><p>300W <ref type="bibr" target="#b38">[39]</ref> dataset: 300W is currently the most widelyused benchmark dataset. We regard all the training samples (3148 images) as the training set and perform testing on (i) full set and (ii) test set. (i) Full set contains 689 images and is split into common subset (554 images) and challenging subsets (135 images). (ii) Test set is the private test-set used for the 300W competition which contains 600 images.</p><p>COFW <ref type="bibr" target="#b4">[5]</ref> dataset consists of 1345 images for training and 507 faces for testing which are all occluded to different degrees. Each COFW face originally has 29 manually annotated landmarks. We also use the test set which has been re-annotated by <ref type="bibr" target="#b18">[19]</ref> with 68 landmarks annotation scheme to allow easy comparison to previous methods.</p><p>AFLW <ref type="bibr" target="#b27">[28]</ref> dataset: AFLW contains 24386 in-the-wild faces with large head pose up to 120 • for yaw and 90 • for pitch and roll. We follow <ref type="bibr" target="#b71">[72]</ref> to adopt three settings on our experiments: (i) AFLW-Full: 20000 and 4386 images are used for training and testing respectively. (ii) AFLW-Frontal: 1314 images are selected from 4386 testing images for evaluation on frontal faces.</p><p>WFLW dataset: In order to facilitate future research of face alignment, we introduce a new facial dataset base on WIDER Face <ref type="bibr" target="#b60">[61]</ref> named Wider Facial Landmarks inthe-wild (WFLW), which contains 10000 faces (7500 for training and 2500 for testing) with 98 fully manual annotated landmarks. Apart from landmark annotation, out new dataset includes rich attribute annotations, i.e., occlusion, pose, make-up, illumination, blur and expression for comprehensive analysis of existing algorithms. Compare to previous dataset, faces in the proposed dataset introduce large variations in expression, pose and occlusion. We can simply evaluate the robustness of pose, occlusion, and expression on proposed dataset instead of switching between multiple evaluation protocols in different datasets. The comparison of WFLW with popular benchmarks is illustrated in the supplementary material. Evaluation metric. We evaluate our algorithm using standard normalised landmarks mean error and Cumulative Errors Distribution (CED) curve. In addition, two further statistics i.e. the area-under-the-curve (AUC) and the failure rate for a maximum error of 0.1 are reported. Because of various profile face on AFLW <ref type="bibr" target="#b27">[28]</ref> dataset, we follow <ref type="bibr" target="#b71">[72]</ref> to use face size as the normalising factor. For other dataset, we follow MDM <ref type="bibr" target="#b50">[51]</ref> and <ref type="bibr" target="#b38">[39]</ref> to use outer-eye-corner distance as the "inter-ocular" normalising factor. Specially, to compare with the results that reported to be normalised by "inter-pupil" (eye-centre-distance) distance, we report our results with both two normalising factors on  and resized to 256 × 256 according to provided bounding boxes. The estimator is stacked four times if not specially indicated in our experiment. For ablation study, the estimator is stacked two times due to the consideration of time and computation cost. All our models are trained with Caffe [24] on 4 Titan X GPUs. Note that all testing images are cropped and resized according to provided bounding boxes without any spatial transformation for fair comparison with other methods. For the limited space of paper, we report all of the training details and experiment settings in our supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparison with existing approaches 4.1.1 Evaluation on 300W</head><p>We compare our approach against the state-of-the-art methods on 300W Fullset. The results are shown in <ref type="table" target="#tab_2">Table 1</ref>.</p><p>Our method significantly outperforms previous methods by a large margin. Note that, our method achieves 6.98% mean   error on the Challenging subset which reflects the effectiveness of handling large head rotation and exaggerated expressions. Apart from 300W Fullset, we also show our results on 300W Testset in <ref type="table" target="#tab_3">Table 2</ref>. Our method performs best among all of the state-of-the-art methods.</p><p>To verify the effectiveness and potential of boundary maps, we use ground truth boundary in the proposed method and report results named "LAB+oracle" which significantly outperform all the methods. The results demonstrate the effectiveness of boundary information and show great potential performance gain if the boundary information can be well captured.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Evaluation on WFLW</head><p>For comprehensively evaluating the robustness of our method, we report mean error, failure rate and AUC on the Testset and six typical subsets of WFLW on <ref type="table">Table.</ref> 3. These six subsets were split from Testset by the provided attribute annotations. Though reasonable performance is obtained, there is illustrated to be still a lot of room for improvement for the extreme diversity of samples on WFLW, e.g., large pose, exaggerated expressions and heavy occlusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Cross-dataset evaluation on COFW and AFLW</head><p>COFW-68 is produced by re-annotating COFW dataset with 68 landmarks annotation scheme to perform cross-dataset experiments by <ref type="bibr" target="#b18">[19]</ref>. <ref type="figure" target="#fig_4">Fig. 7</ref> shows the CED curves of our method against state-of-the-art methods on the COFW-68 <ref type="bibr" target="#b18">[19]</ref> dataset. Our model outperforms previous results with a large margin. We achieve 4.62% mean error with 2.17% failure rate. The failure rate is significantly reduced by 3.75%, which indicates the robustness of our method to handle occlusions. In order to verify the capacity of handling cross-dataset face alignment of our method, we use boundary heatmaps estimator trained on 300W Fullset which has no overlap with COFW and AFLW dataset and compare the performance with and without using boundary information fusion ("LAB w/o boundary"). The results are reported in <ref type="table" target="#tab_6">Table 4</ref>. The performance of previous methods without using 300-W datasets is also attached as a reference. There is a clear boost between our method without and with using boundary information. Thanks to the generalization of facial boundaries, the estimator learned on 300W can be conveniently used to supply boundary information for coordinate regression on COFW-29 <ref type="bibr" target="#b4">[5]</ref> and AFLW <ref type="bibr" target="#b27">[28]</ref> dataset, even though these datasets have different annotation protocols.</p><p>Moreover, our method uses boundary information achieves 29%, 32% and 29% relative performance improve-ment over the baseline method ("LAB without boundary") on COFW-29, AFLW-Full and AFLW-Frontal respectively. Since COFW covers different level of occlusion and AFLW has significant view changes and challenging shape variations, the results emphasise the robustness brought by boundary information to occlusion, pose and shape variations. More qualitative results are demonstrated in our supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation study</head><p>Our framework consists of several pivotal components, i.e., boundary information fusion, message passing and adversarial learning. In this section, we validate their effectiveness within our framework on the 300W Challenging Set and WFLW Dataset. Based on the baseline res-18 network (BL), we analyse each proposed component, i.e., with the baseline hourglass boundary estimator ("HBL"), message passing ("MP"), and adversarial learning ("AL"), by comparing their mean error and failure rate. The overall results are shown in <ref type="figure" target="#fig_5">Fig. 8</ref>.</p><p>Boundary information is chosen as geometric structure representation in our work. We verify the potential of other structure information as well, i.e., facial parts gaussian ("FPG") and face parsing results ("FP"). We report the landmarks accuracy with oracle results in <ref type="table" target="#tab_7">Table 5</ref> using different structure information. It can be observed easily that boundary map ("BM") is the most effective one.</p><p>Boundary information fusion is one of the key steps in our algorithm. We can fuse boundary information at different levels for the regression network. As indicated in <ref type="table" target="#tab_8">Table 6</ref>, our final model that fuses boundary information in all four levels improves mean error from 7.12% to 6.13%. To evaluate the relationship between the quantity of boundary information fusion and the final prediction accuracy, we vary the number of fusion levels from 1 to 4 and report the mean error results in <ref type="table" target="#tab_8">Table 6</ref>. It can be observed that performance is improved consistently by fusing boundary heatmaps at more levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>BL BL+FPG BL+FP BL+BM Mean Error 7.12 5.25 4.16 3.28     To verify the effectiveness of the fusion scheme shown in <ref type="figure">Fig. 4</ref>, we report the results of mean error on several settings in <ref type="table" target="#tab_9">Table 7</ref>, i.e., the baseline res-18 network ("BL"), hourglass module without boundary feature ("HG/B"), hourglass module with boundary feature ("HG") and consecutive convolutional layers with boundary feature ("CL"). The comparison between "BL+HG" and "BL+HG/B" indicates the effectiveness of boundary information fusion rather than network structure changes. The comparison between "BL+HG" and "BL+CL" indicates the effectiveness of the using hourglass structure design.</p><p>Message passing plays a vital role for heatmap quality improvement when severe occlusions happen. As illustrated in <ref type="figure" target="#fig_5">Fig. 8 (b)</ref> on Occlusion Subset of WFLW, message passing, which combines information from visible boundaries and occluded ones, reduce the mean error over 11% relatively.</p><p>Adversarial learning further improves the quality and effectiveness of boundary heatmaps. As illustrated in <ref type="figure">Fig. 5</ref>, heatmaps can be observed to be more focused and salience when adversarial loss is added. To verify the effectiveness of our landmark based boundary effectiveness discriminator, a baseline method using traditionally defined discriminator is tested on 300W Challenging Set. The failure rate is reduced from 5.19% to 3.70%.</p><p>Relationship between boundary estimator and landmarks regressor is evaluated by analyzing the quality of estimated heatmap and final prediction accuracy. We report the MSE of estimated heatmaps and corresponding landmarks accuracy in <ref type="table" target="#tab_10">Table 8</ref>. We observe that with message passing ("HBL+MP") and adversarial learning ("HBL+AL"), the errors of estimated heatmaps are reduced together with landmarks accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conculsion</head><p>Unconstrained face alignment is an emerging topic. In this paper, we present a novel use of facial boundary to derive facial landmarks. We believe the reasoning of a unique facial structure is the key to localise facial landmarks, since human face does not include ambiguities. By estimating facial boundary, our method is capable of handling arbitrary head poses as well as large shape, appearance, and occlusion variations. Our experiment shows the great potential of modeling facial boundary. The runtime of our algorithm is 60ms on TITAN X GPU.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The first column shows the face images from different</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Overview of our Boundary-Aware Face Alignment framework. (a) Boundary heatmap estimator, which based on hourglass network is used to estimate boundary heatmaps. Message passing layers are introduced to handle occlusion. (b) Boundary-aware landmarks regressor is used to generate the final prediction of landmarks. Boundary heatmap fusion scheme is introduced to incorporate boundary information into the feature learning of regressor. (c) Boundary effectiveness discriminator, which distinguishes "real" boundary heatmaps from "fake", is used to further improve the quality of the estimated boundary heatmaps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>An illustration of the process of ground truth heatmap generation. Each row represents the process of one specific facial boundary, i.e., facial outer contour, left eyebrow, right eyebrow, nose bridge, nose boundary, left/right upper/lower eyelid and upper/lower side of upper/lower lip. An illustration of the feature map fusion scheme.Boundary cues and input feature maps are fused together to get a refined feature with the usage of a hourglass module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1 3 :</head><label>13</label><figDesc>The training pipeline of the our method. Require: Training image I, the corresponding groundtruth boundary heatmaps M and landmark coordinates S, the generation network G , the regression network R and the discrimination network D. 1: while the accuracy of landmarks predicted by R in validation set stops do 2: Forward G byM = G(I) and optimize G by minimizing M − M 2 2 + L A where L A is defined in Eq.6; Forward D byd real = D(M ) and optimize D by minimizing the first term of L D defined in Eq.5; 4:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>,LFPW), Error: 6.72%, Failure: 6.71% SAPM(HELEN), Error: 6.64%, Failure: 5.72% RCPR(HELEN,LFPW), Error: 8.76%, Failure: 20.12% TCDCN(HELEN,LFPW,AFW,MAFL), Error: 7.66%, Failure: 16.17% CFSS(HELEN,LFPW,AFW), Error: 6.28%, Failure: 9.07% LAB(HELEN,LFPW,AFW), Error: 4.62%, Failure: 2.17% CED for COFW-68 testset (68 landmarks). Train set (in parentheses), mean error and failure rate are also reported.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>(a) Mean error (%) and failure rate (%) on 300W Challenging Subset. (b) Mean error (%) on 5 typical testing subset of WFLW Dataset, i.e. Expression, Illumination, Makeup, Occlusion and Blur Subset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Boundary Heatmap Estimator Boundary-Aware Landmarks Regressor</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Boundary Effectiveness</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Estimated</cell><cell></cell><cell cols="2">Discriminator</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Boundary</cell><cell></cell><cell></cell></row><row><cell></cell><cell>MPL</cell><cell>MPL</cell><cell>Estimated Boundary Heatmaps</cell><cell>Heatmaps Ground Truth</cell><cell></cell><cell></cell><cell>Real/Fake Vector</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Boundary</cell><cell></cell><cell></cell></row><row><cell>Stack 1</cell><cell></cell><cell>S t a c k n</cell><cell></cell><cell>Heatmaps</cell><cell></cell><cell></cell></row><row><cell>FMF</cell><cell>FMF</cell><cell>FMF</cell><cell></cell><cell>Convolution</cell><cell>Residual Unit</cell><cell>Down Sampling</cell><cell>Hourglass Concatenation</cell></row><row><cell></cell><cell></cell><cell cols="2">Predicted Landmark Coordinates</cell><cell>Message Passing Layers</cell><cell>Feature Map Fusion</cell><cell>Fully Connection</cell><cell>Element-wise Dot Product</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>.2 Distance Transform Map Ground Truth Heatmap Boundary Line Original Points Set</head><label></label><figDesc></figDesc><table><row><cell>...</cell><cell>...</cell><cell>...</cell><cell>...</cell></row></table><note>K Facial Boundary Heatmaps</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Implementation details. All training images are cropped</figDesc><table><row><cell>Method</cell><cell cols="3">Common Challenging Fullset Subset Subset</cell></row><row><cell cols="3">Inter-pupil Normalisation</cell><cell></cell></row><row><cell>RCPR [6] CFAN [69] ESR [7] SDM [57] LBF [38] CFSS [72] 3DDFA [74] TCDCN [70] MDM [51] RAR [55] DVLN [53] TSR [31] LAB LAB+Oracle</cell><cell>6.18 5.50 5.28 5.57 4.95 4.73 6.15 4.80 4.83 4.12 3.94 4.36 3.42 2.57</cell><cell>17.26 16.78 17.00 15.40 11.98 9.98 10.59 8.60 10.14 8.35 7.62 7.56 6.98 4.72</cell><cell>8.35 7.69 7.58 7.50 6.32 5.76 7.01 5.54 5.88 4.94 4.66 4.99 4.12 2.99</cell></row><row><cell cols="3">Inter-ocular Normalisation</cell><cell></cell></row><row><cell>PCD-CNN [2] SAN [59] LAB LAB+Oracle</cell><cell>3.67 3.34 2.98 1.85</cell><cell>7.62 6.60 5.19 3.28</cell><cell>4.44 3.98 3.49 2.13</cell></row><row><cell cols="4">Table 1: Mean error (%) on 300-W Common Subset, Challeng-ing Subset and Fullset (68 landmarks).</cell></row><row><cell>Method</cell><cell>AUC</cell><cell cols="2">Failure Rate (%)</cell></row><row><cell cols="2">Deng et al. [14] Fan et al. [16] DenseReg + MDM [1] 0.5219 0.4752 0.4802 JMFA [15] 0.5485 LAB 0.5885 LAB+Oracle 0.7626</cell><cell></cell><cell>5.5 14.83 3.67 1.00 0.83 0.00</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Mean error (%) on 300-W testset (68 landmarks). Accuracy is reported as the AUC and the Failure Rate.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Evaluation of LAB and several state-of-the-arts on Testset and 6 typical subsets of WFLW (98 landmarks).</figDesc><table><row><cell>Method Human RCPR [6] HPM [19] CCR [17] DRDA [68] RAR [55] SFPD [54] DAC-CSR [18] LAB w/o boundary LAB</cell><cell>Mean Error (%) Failure Rate (%) 5.6 -8.50 20.00 7.50 13.00 7.03 10.9 6.46 6.00 6.03 4.14 6.40 -6.03 4.73 5.58 2.76 3.92 0.39</cell><cell>Method CDM [64] RCPR [6] ERT [26] LBF [38] CFSS [72] CCL [73] TSR [31] DAC-OSR [18] LAB w/o boundary LAB</cell><cell>AFLW-Full (%) AFLW-Frontal (%) 5.43 3.77 3.73 2.87 4.35 4.35 4.25 2.74 3.92 2.68 2.72 2.17 2.17 -2.27 1.81 1.85 1.62 1.25 1.14</cell></row><row><cell cols="2">(a) Mean error (%) on COFW-29 testset.</cell><cell cols="2">(b) Mean error (%) on AFLW testset.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Cross-dataset evaluation on COFW and AFLW.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Mean error (%) on 300W Challenging Set for evaluation the potential of boundary map as the facial structure information.</figDesc><table><row><cell>Method Mean Error 7.12 BL BL+L1 BL+L1&amp;2 BL+L1&amp;2&amp;3 BL+L1&amp;2&amp;3&amp;4 6.56 6.32 6.19 6.13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table><row><cell>Mean error (%) on 300W Challenging Subset for vari-</cell></row><row><cell>ous fusion levels.</cell></row><row><cell>Method Mean Error 7.12 BL BL+HG/B BL+CL BL+HG 6.95 6.24 6.13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Mean error (%) on 300W Challenging Set for different settings of boundary fusion scheme.</figDesc><table><row><cell>Method Error of heatmap Error of landmark 6.13 HBL HBL+MP HBL+MP+AL 0.85 0.76 0.63 5.82 5.59</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Normalised pixel-to-pixel error (%) of heatmap estimation, mean error (%) and faliure rate (%) of landmark prediction on 300W Challenging Set for evaluation the relationship between the quality of estimated boundary and final prediction.</figDesc><table><row><cell></cell><cell cols="2">BL BL+HBL BL+HBL+MP BL+HBL+MP+GAN</cell><cell></cell><cell>BL</cell><cell>BL+HBL</cell><cell>BL+HBL+MP</cell><cell></cell><cell>BL+HBL+MP+GAN</cell><cell></cell><cell></cell><cell></cell></row><row><cell>5 6 7 8 9 10 11 12 4 3</cell><cell>7.12 Mean Error 6.13 5.82 5.59</cell><cell>10.37 Failure rate 7.41 5.93 3.7</cell><cell>22.00 21.00 20.00 19.00 18.00 17.00 16.00 15.00 14.00 13.00 12.00 11.00 10.00</cell><cell>19.31 Expression 18.39 17.63 17.19</cell><cell>13.97 Illumination 12.8 12.5 12.1</cell><cell>16.41 Makeup 14.59 13.51</cell><cell>12.81</cell><cell>18.86 Occlusion 17.66 15.64 14.91</cell><cell>17.19</cell><cell>16.51 Blur 15.8</cell><cell>15.34</cell></row><row><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Densereg: Fully convolutional dense shape regression in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Snape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Disentangling 3d pose in a dendritic cnn for unconstrained 2d face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06713</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Localizing parts of faces using a consensus of exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convolutional aggregation of local evidence for large pose face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Robust face landmark estimation under occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">P</forename><surname>Burgos-Artizzu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust face landmark estimation under occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">P</forename><surname>Burgos-Artizzu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Face alignment by explicit shape regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="177" to="190" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Joint cascade face detection and alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adversarial posenet: A structure-aware convolutional network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Self adversarial training for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.02439</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Structured feature learning for pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-context attention for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="681" to="685" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">M 3 CSR: multi-view, multi-scale and multi-component cascade shape regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vision Comput</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="19" to="26" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Joint multi-view face alignment in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.06023</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Approaching human level facial landmark localization by deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vision Comput</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="27" to="35" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cascaded collaborative regression for robust facial landmark detection trained using a mixture of synthetic and real images with dynamic weighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Christmas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3425" to="3440" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dynamic attention-controlled cascaded shape regression exploiting training data augmentation and fuzzy-set sample weighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Christmas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Occlusion coherence: Localizing occluded faces with a hierarchical deformable part model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2661</idno>
		<title level="m">Generative adversarial networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Effective face frontalization in unconstrained images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Enbar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Boundary-aware instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hayder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Hogg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Boyle</surname></persName>
		</author>
		<title level="m">Proceedings of the British Machine Vision Conference, BMVC 1992</title>
		<meeting>the British Machine Vision Conference, BMVC 1992<address><addrLine>Leeds, UK</addrLine></address></meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="1992-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An active illumination and appearance (AIA) model for face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kahraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gökmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Darkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Larsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">One millisecond face alignment with an ensemble of regression trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Instancecut: From edges to instances with multicut</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Levinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Savchynskyy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Annotated facial landmarks in the wild: A large-scale, realworld database for facial landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Köstinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Facetracer: A search engine for large collections of images with faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Interactive facial feature localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A deep regression architecture with two-stage re-initialization for high performance facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Classification with an edge: Improving semantic image segmentation with boundary detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marmanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Galliani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Stilla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.01337</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Active appearance models revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="135" to="164" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Locating facial features with an extended active shape model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Milborrow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nicolls</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="504" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A boundary-fragmentmodel for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Opelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Comic: Good features for detection and matching at object boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1957</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Face alignment at 3000 FPS via regressing local binary features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">300 faces in-the-wild challenge: The first facial landmark localization challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Enhancenet: Single image super-resolution through automated texture synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S M</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A nonlinear discriminative approach to AAM fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Göcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deformable model fitting by regularized landmark mean-shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="200" to="215" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multistage multi-recursive-input fully convolutional networks for neuronal boundary detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deepcontour: A deep convolutional feature learned by positivesharing loss for contour detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">From facial parts responses to face detection: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Collaborative facial landmark localization for transferring annotations across datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep convolutional network cascade for facial point detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep learning face representation from predicting 10, 000 classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Hybrid deep learning for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1997" to="2009" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Shape-based object detection via boundary structure segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="146" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Mnemonic descent method: A recurrent process applied for end-to-end face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Snape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Situational object boundary detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Leveraging intra and inter-dataset variations for robust face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Simultaneous facial landmark detection, pose and deformation estimation under facial occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Kassim. Robust facial landmark detection via recurrent attentiverefinement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">D</forename><surname>La Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Global supervised descent method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">D</forename><surname>La Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Style aggregated network for facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xuanyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wanli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.04108</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Stacked hourglass network for robust facial landmark localisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Wider face: A face detection benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Learning feature pyramids for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">End-to-end learning of deformable mixture of parts and deep convolutional neural networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Posefree facial landmark fitting via optimized part mixtures and cascaded deformable shape model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Casenet: Deep category-aware semantic edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramalingam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">The menpo facial landmark localisation challenge: A step towards the solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chrysos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Leveraging datasets with varying annotations for face alignment via deep regression network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Occlusion-free face alignment: Deep regression networks coupled with decorrupt autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Coarse-to-fine auto-encoder networks (cfan) for real-time face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Learning deep representation for face alignment with auxiliary attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="918" to="930" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Transferring landmark annotations for cross-dataset face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno>abs/1409.0602</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Face alignment by coarse-to-fine shape searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Unconstrained face alignment via cascaded compositional learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Face alignment across large poses: A 3d solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Deep learning identity-preserving face space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
