<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Robust Multi-Modality Multi-Object Tracking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zhou</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
							<email>shuyang.sun@eng.ox.ac.uk</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
							<email>shijianping@sensetime.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
							<email>ccloy@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Robust Multi-Modality Multi-Object Tracking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-sensor perception is crucial to ensure the reliability and accuracy in autonomous driving system, while multiobject tracking (MOT) improves that by tracing sequential movement of dynamic objects. Most current approaches for multi-sensor multi-object tracking are either lack of reliability by tightly relying on a single input source (e.g., center camera), or not accurate enough by fusing the results from multiple sensors in post processing without fully exploiting the inherent information. In this study, we design a generic sensor-agnostic multi-modality MOT framework (mmMOT), where each modality (i.e., sensors) is capable of performing its role independently to preserve reliability, and further improving its accuracy through a novel multimodality fusion module. Our mmMOT can be trained in an end-to-end manner, enables joint optimization for the base feature extractor of each modality and an adjacency estimator for cross modality. Our mmMOT also makes the first attempt to encode deep representation of point cloud in data association process in MOT. We conduct extensive experiments to evaluate the effectiveness of the proposed framework on the challenging KITTI benchmark and report stateof-the-art performance. Code and models are available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Reliability and accuracy are the two fundamental requirements for autonomous driving system. Dynamic object perception is vital for autonomous driving. To improve its reliability, multi-modality sensors can be employed to provide loosely coupled independent clues to prevent failure showed in <ref type="figure" target="#fig_0">Figure 1</ref> (a). To improve accuracy, sequential information from multiple object tracking can be incorporated, and better multi-sensor information can reinforce the final score as in <ref type="figure" target="#fig_0">Figure 1</ref>  <ref type="bibr">(b)</ref>. In this paper, we propose the multi-modality Multi-Object Tracking (mmMOT) framework, which preserves reliability by a novel fusion module for multiple sensors and improves accuracy with at- tention guided multi-modality fusion mechanism.</p><p>It is non-trivial for traditional methods to design a multimodality (i.e., multi-sensor) MOT framework and preserve both reliability and accuracy. A majority of traditional methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b24">25]</ref> use camera, LiDAR or radar with hand-crafted features fused by Kalman Filter or Bayesian framework. Their accuracy is bounded by the expression ability of hand-crafted features. Another stream of methods uses deep feature extractors <ref type="bibr" target="#b10">[11]</ref>, which significantly improve the accuracy. Nevertheless, they focus on image level deep representation to associate object trajectories and use LiDAR only in detection stage. Such a binding method cannot preserve reliability if the camera is down.</p><p>In this work, we design a multi-modality MOT (mm-MOT) framework that is extendable to camera, LiDAR and radar. Firstly, it obeys a loose coupling scheme to allow high reliability during the extraction and fusion of multisensor information. Specifically, multi-modality features are extracted from each sensor independently, then a fusion module is applied to fuse these features, and pass them to an adjacency estimator, which is capable of performing inference based on each modality. Second, to enable the network to learn to infer from different modalities simultaneously, our mmMOT is trained in an end-to-end manner, so that the multi-modality feature extractor and cross-modality ad-jacency estimator are jointly optimized. Last but not least, we make the first attempt of using deep representation of point cloud in the data association process for MOT and achieve competitive results.</p><p>We conduct extensive experiments on the fusion module and evaluate our framework on the KITTI tracking dataset <ref type="bibr" target="#b12">[13]</ref>. Without bells and whistles, we achieve state-of-the-art results on KITTI tracking benchmark <ref type="bibr" target="#b12">[13]</ref> under the online setting, purely relying on image and point cloud, and our results with single modality (under sensor failure condition) by the same model are also competitive (only 0.28% worse).</p><p>To summarize, our contributions are as follows:</p><p>1. We propose a multi-modality MOT framework with a robust fusion module that exploits multi-modality information to improve both reliability and accuracy. 2. We propose a novel end-to-end training method that enables joint optimization of cross-modality inference. 3. We make the first attempt to apply deep features of point cloud for tracking and obtain competitive results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Multi-Object Tracking Framework. Recent research of MOT primarily follows the tracking-by-detection paradigm <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b49">50]</ref>, where object of interests is first obtained by an object detector and then linked into trajectories via data association. The data association problem could be tackled from various perspectives, e.g., min-cost flow <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b36">37]</ref>, Markov decision processes (MDP) <ref type="bibr" target="#b47">[48]</ref>, partial filtering <ref type="bibr" target="#b5">[6]</ref>, Hungarian assignment <ref type="bibr" target="#b37">[38]</ref> and graph cut <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b48">49]</ref>. However, most of these methods are not trained in an end-to-end manner thus many parameters are heuristic (e.g., weights of costs) and susceptible to local optima. To achieve end-to-end learning within the min-cost flow framework, Schulter et al. <ref type="bibr" target="#b36">[37]</ref> applies bi-level optimization by smoothing the linear programming and Deep Structured Model (DSM) <ref type="bibr" target="#b10">[11]</ref> exploits the hinge loss. Their frameworks, however, are not designed for cross-modality. We solve this problem by adjacency matrix learning.</p><p>Apart from different data association paradigms, correlation features have also been explored widely to determine the relation of detections. Current image-centric methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b49">50]</ref> mainly use deep features of image patches. Hand-crafted features are occasionally used as auxiliary inputs, including but not limited to bounding box <ref type="bibr" target="#b14">[15]</ref>, geometric information <ref type="bibr" target="#b26">[27]</ref>, shape information <ref type="bibr" target="#b37">[38]</ref> and temporal information <ref type="bibr" target="#b44">[45]</ref>. 3D information is also beneficial and thus exploited by prediction from 3D detection <ref type="bibr" target="#b10">[11]</ref> or estimation from RGB image with either neural networks <ref type="bibr" target="#b35">[36]</ref> or geometric prior <ref type="bibr" target="#b37">[38]</ref>. Osep et al. <ref type="bibr" target="#b24">[25]</ref> fuses the information from RGB images, stereo, visual odometry, and optionally scene flow, but it cannot be trained in an end-to-end manner. All the aforementioned methods must work with camera thus lack of reliability. By contrast, our mmMOT extracts feature from each sensor independently (both deep image features and deep representation of point cloud), and each sensor plays an equally important role and they can be decoupled. The proposed attention guided fusion mechanism further improves accuracy. Deep Representation of Point Cloud. A traditional usage of point cloud for tracking is to measure distances <ref type="bibr" target="#b30">[31]</ref>, provide 2.5D grid representation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10]</ref> or to derive some hand-crafted features <ref type="bibr" target="#b41">[42]</ref>. None of them fully exploit the inherent information of the point cloud for the data association problem. Recent studies <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b23">24]</ref> have demonstrated the value of using 3D point cloud as perception features in autonomous driving. To learn a good deep representation for point cloud, PointNet <ref type="bibr" target="#b28">[29]</ref> and PointNet++ <ref type="bibr" target="#b29">[30]</ref> process raw unstructured point clouds using symmetric functions. We adopt this effective method in our framework. Other studies such as PointSIFT <ref type="bibr" target="#b16">[17]</ref> proposes an orientationencoding unit to learn SIFT-like features of point cloud, and 3DSmoothNet <ref type="bibr" target="#b13">[14]</ref> learns a voxelized smoothed density value representation. There are also methods <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47]</ref> which project the point cloud to a sphere thus 2D CNN can be applied for the segmentation task. Object Detection. An object detector is also a vital component in the tracking by detection paradigm. Deep learning approaches for 2D object detection have improved drastically <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b42">43]</ref> since Faster R-CNN <ref type="bibr" target="#b32">[33]</ref>. 3D object detection receives increasing attention recently. To exploit both image and point cloud, some methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18]</ref> aggregate point cloud and image features from different views, while F-PointNet <ref type="bibr" target="#b27">[28]</ref> obtains frustum proposal from an image, and then applies PointNet <ref type="bibr" target="#b28">[29]</ref> for 3D object localization with the point cloud. There exist state-of-the-art methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b50">51]</ref> that use point cloud only. One-stage detectors <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b50">51]</ref> usually apply CNN on the voxelized representation, and two-stage detectors such as Point RCNN <ref type="bibr" target="#b38">[39]</ref> generates proposals first by segmentation, which are refined in the second stage. Our mmMOT is readily adaptable for both 2D and 3D object detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Multi-Modality Multi-Object Tracking</head><p>We propose a multi-modality MOT (mmMOT) framework, which preserves reliability via independent multisensor feature extraction and improves accuracy via modality fusion. It generally follows the widely adopted trackingby-detection paradigm from the min-cost flow perspective. Specifically, our framework contains four modules including an object detector, feature extractor, adjacency estimator and min-cost flow optimizer, as shown in <ref type="figure" target="#fig_2">Fig. 2</ref> (a), (b), (c), (d), respectively. First, an arbitrary object detector is used to localize objects of interests. We use PointPillar <ref type="bibr" target="#b18">[19]</ref> for convenience. Second, the feature extractor extracts features from each sensor independently for each detection   (Section 3.2), after which a fusion module is applied to fuse and pass the single modality feature to the adjacency estimator (Section 3.3). The adjacency estimator is modality agnostic. It infers the scores necessary for the min-cost flow graph computation. The structure of the adjacency estimator and the associated end-to-end learning method will be demonstrated in Section 3.4. The min-cost flow optimizer is a linear programming solver that finds the optimal solution based on the predicted scores (Section 3.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation</head><p>Our mmMOT follows the tracking-by-detection paradigm to define the data association cost, which is solved as a min-cost flow problem <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b36">37]</ref>. Take the online MOT setting for example, assume there are N and M detections in two consecutive frames i and i + 1, denoted by X i = x i j | j = 1, · · · , N and X i+1 = x i+1 k | k = 1, · · · , M , respectively. Each detection is associated to four types of binary variables in this paradigm. We introduce them following the notation of Deep Structured Model (DSM) <ref type="bibr" target="#b10">[11]</ref>. First, for any x j , a binary variable y true j indicates whether the detection is a true positive. Second, binary variables y link jk indicates if the j-th detection in the first frame and k-th detection in the second frame belong to the same trajectory, and all these y link jk form an adjacency matrix</p><formula xml:id="formula_0">A i ∈ R N ×M , where A i jk = y link jk .</formula><p>The other two variables y new j , y end j represents whether the detection is the start or the end of a trajectory, respectively. For convenience, we flatten the adjacency matrix into a vector Y link , and gather all the binary variables having the same type as Y true , Y new , Y end , then all these variables are collapsed into a vector Y = Y true , Y link , Y new , Y end , which comprises all states of edges in the network flow. For each binary variable in Y true , Y link , Y new , Y end , the corresponding scores are predicted by the confidence estimator, affinity estimator, start and end estimator, respectively. These estimators form the adjacency estimator, and we solve them in a multi-task learning network as shown in <ref type="figure" target="#fig_2">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Single Modality Feature Extractor</head><p>In an online setting, only detections in two consecutive frames are involved. To estimate their adjacency, their deep representations are first extracted from the respective image or point cloud. The features of each single modality form a tensor with a size of 1 × D × (N + M ), where D = 512 is the vector length, and N + M is the total number of detections in the two frames. Image Feature Extractor. Upon obtaining 2D bounding boxes from either a 2D or 3D detector, the image patches associated to each detection are cropped and resized to a square with a side length of 224 pixels to form a batch. All these patches form a 4D tensor with a size of (N + M ) ×  <ref type="figure">Figure 3</ref>. The robust fusion module and three multi-modality fusion modules. The robust fusion module can apply any one of the fusion modules A, B and C to produce the fused modality. Unlike the conventional fusion modules, the robust fusion module produces both the single modalities and the fused modality as an output. Fusion module A concatenates the multi-modality features, module B fuses them with a linear combination, module C introduces attention mechanism to weights the importance of sensor's feature adaptively.</p><p>3 × 224 × 224. We use VGG-Net <ref type="bibr" target="#b39">[40]</ref> as the image feature extractor's backbone. To exploit features at different level, we modify the skip-pooling <ref type="bibr" target="#b3">[4]</ref> so as to pass different level's feature to the top, as shown in the VGG-Net depicted in <ref type="figure" target="#fig_2">Figure 2</ref>. The details of skip-pooling are provided in the supplementary material. Point Cloud Feature Extractor. One of our contributions is to apply deep representation of point cloud in data association process for MOT. While the LiDAR point cloud associated to a single detection could be easily obtained by a 3D bounding box, it remains a challenge if only a 2D bounding box is provided. It is possible to obtain a 3D bounding box using F-PointNet <ref type="bibr" target="#b27">[28]</ref>, or directly estimated the 3D bounding box with other geometric information and priori <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b37">38]</ref>. In this study, we choose not to locate the detection in 3D space because we observed more errors. Rather, inspired by F-PointNet <ref type="bibr" target="#b27">[28]</ref>, we exploit all the point clouds in the frustum projected by the 2D bounding box. This leads to high flexibility and reliability, and save computation from obtaining 3D bounding box. The point cloud forms a tensor with a size 1 × D × L, where L is the total number of all the points in all bounding boxes, and D = 3 is the dimension of the point cloud information. We empirically found the reflectivity of point cloud provides only with marginal improvement, thus we only used the coordinates in 3D space. We modify the vanilla PointNet <ref type="bibr" target="#b29">[30]</ref> to extract features from point cloud for each detection as shown in the PointNet depicted in <ref type="figure" target="#fig_2">Figure 2</ref>. To enhance the global information of points in each bounding box, we employ the global feature branch originally designed for the segmentation task in PointNet <ref type="bibr" target="#b29">[30]</ref>, and we found that average pooling works better than max pooling in PointNet for tracking. During pooling, only the feature of points belonging to the same detection are pooled together. The feature vector of point cloud has a length of 512 for each detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Robust Multi-Modality Fusion Module</head><p>In order to better exploit multi-sensor features while maintaining the ability to track with each single sensor, our robust fusion module is designed to have the capability of fusing features of multiple modalities as well as handing original features from just a single modality. Robust Fusion Module. The operations in the adjacency estimator is batch-agnostic, thus we concatenate single modalities and the fused modality in the batch dimension to ensure that the adjacency estimator could still work as long as there is an input modality. This design enables the proposed robust fusion module to skip the fusion process or fuse the remaining modalities (if there are still multiple sensors) during sensor malfunctioning, and pass them to the adjacency estimator thus the whole system could work with any sensor combination. Formally, we denote the feature vectors of different modalities as {F s i } S s=0 , where the number of sensors is S, and the fused feature is denoted as The robust fusion module could employ arbitrary fusion module, and we investigate three fusion modules as shown in <ref type="figure">Figure 3</ref>. Take two sensors' setting as an example, the fusion module A naively concatenates features of multiple modalities; the module B add these features together; the module C introduces attention mechanism. Fusion Module A. A common approach is to concatenate these features, and use point-wise convolution with weight W to adapt the length of the output vector to be the same as a single sensor's feature as follows:</p><formula xml:id="formula_1">F f use i = W ⊗ CONCAT F 0 i , · · · , F S i ,<label>(1)</label></formula><p>where ⊗ denotes a convolution operation, and CONCAT (·) denotes a concatenation operation. Fusion Module B. Another intuitive approach is to fuse these two features with addition, we reproject the features of each modality and add them together as follows:</p><formula xml:id="formula_2">F f use i = S s=0 W s ⊗ F s i ,<label>(2)</label></formula><p>where W s denotes the corresponding convolution kernels to the s-th sensor's feature. By addition the module gathers information from each sensor, and correlation feature of fused modality is also more like that of single sensor. It is favorable for the adjacency estimator to handle different modality since the correlation operation is multiplication or subtraction.</p><p>Fusion Module C. The module C introduces an attention mechanism for guiding the information fusion from different sensors, since the significance of a sensor's information might vary in different situations, e.g., the point cloud feature might be more important when the illumination condition is bad, and the image feature might be more important when the point cloud is affected in rainy days. The attention map G s i for each sensor is first calculated as follows:</p><formula xml:id="formula_3">G s i = σ (W s att ⊗ F s i ) ,<label>(3)</label></formula><p>where W s att is the convolution parameter and σ is a sigmoid function. We expect the W s att to learn predict the importance conditioned on the feature itself, and the sigmoid function normalizes the attention map to be in the range from 0 to 1. Then the information is fused as follows:</p><formula xml:id="formula_4">F f use i = 1 S s=0 G s i S s=0 G s i (W s ⊗ F s i ) ,<label>(4)</label></formula><p>where denotes element-wise multiplication, and the summation of G s i is taken as a denominator for normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Deep Adjacency Matrix Learning</head><p>Given the extracted multi-modality features, the adjacency estimator infers the confidence, affinity, start and end scores in the min-cost flow graph <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b36">37]</ref> based on each modality. These features are shared for each branch of the adjacency estimator, namely the confidence estimator, affinity estimator, start and end estimator. It is straightforward to learn a model for confidence estimator by taking it as a binary classification task. We focus on the design of the two other branches. Correlation Operation. To infer the adjacency, the correlation of each detection pair is needed. The correlation operation is batch-agnostic thus it can handle cross-modality, and the operation applied channel by channel to take advantage of the neural network. The commutative property is theoretically favorable for learning paired data, since it is agnostic of the order of F i j and F i+1 k . In this work, we compare three simple yet effective operators as follows:</p><p>• Element-wise multiplication,:</p><formula xml:id="formula_5">F jk = F i j F i+1 k , • Subtraction: F jk = F i j − F i+1 k , • Absolute subtraction: F jk = F i j − F i+1 k .</formula><p>The element-wise multiplication is equivalent to a depthwise correlation filter <ref type="bibr" target="#b20">[21]</ref>, where the filter size is 1 × 1.</p><p>The subtraction measures the distance of two vectors. By taking the absolute value of subtraction, the operation becomes commutative and agnostic to the chronology of detection, which makes the network more robust. Affinity Estimator. The obtained F jk is then used by the affinity estimator to predict the adjacency matrix A i . Since the correlation operation handles multi-modality in  <ref type="figure">Figure 4</ref>. The structure of the affinity estimator and start and end estimator. The affinity estimator estimates the adjacency using point-wise convolution. The start and end estimator gathers the correlation feature of each detection to check whether a detection is linked to make prediction more robust.</p><p>the batch dimension and is performed on each detection pair between two frames, the correlation feature map has a size of 3 × D × N × M . We use 2D point-wise convolution as shown in <ref type="figure">Figure 4</ref>. This makes the network handle each correlation feature separately since it only needs to determine whether F jk indicates a link. Since the convolution is batch-agnostic, it could work on any combination of modality, and the output adjacency matrix will has a size of 3 × 1 × N × M . Because these three predictions have the same target, we apply supervision signal to each of them, which enables joint optimization of feature extractor for each modality and affinity estimator for cross modality. During inference, the affinity estimator needs no modification if the sensor combination is changed, which allows both flexibility and reliability.</p><p>Start and End Estimator. The start and end estimator estimate whether a detection is linked, thus their parameters are shared for efficiency. Given the correlation feature F jk , after gathering all the correlation information of one detection in each row or column by average pooling, the estimator also uses point-wise convolution to infer whether one detection is linked as shown in <ref type="figure">Figure 4</ref>. Since the pooling layer is batch-agnostic, the start and end estimator is also flexible for different sensor settings. During inference, we simply pad zeros for new score of detection in the first frame and end score of detection in the last frame, since they cannot be estimated from the correlation feature map. Ranking Mechanism. We denote the raw output of the neural network's last layer as o i jk , and we found that a jk should also be the greatest value among a js , s = 1, ...M and a tk , t = 1, ...N , but directly take A i jk = o i jk does not exploit this global information, thus we design a ranking mechanism to handle this problem. Specifically, we apply a softmax function for each row and each column in the output matrix, and gather these two matrices to get the final adjacency matrix. In this work, we investigate three operations to combine the two softmax feature maps: max, multiplication and average. Taking the multiplication for example, the ranking mechanism is introduced as follows:</p><formula xml:id="formula_6">a i jk = e o i jk N s=0 e o i js × e o i jk M t=0 e o i tk .<label>(5)</label></formula><p>Loss Function. The whole framework can be learnt in an end-to-end manner in a multi-task learning framework. We adopt the cross entropy loss for the classification branch and the L2 loss for the other two, thus the overall loss function can be written as follows:</p><formula xml:id="formula_7">L = L link + αL start + γL end + βL true ,<label>(6)</label></formula><p>where α, γ and β indicates the weight of loss for each task. We empirically set α = γ = 0.4 and β = 1.5 in all the experiments in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Linear Programming</head><p>After obtaining the prediction score from the neural networks, the framework needs to find an optimal solution from the min-cost flow graph. There are several facts that could be exploited as linear constraints among these binary variables in Y. Firstly, if a detection is a true positive, it has to be either linked to another detection in the previous frame or the start of a new trajectory. Therefore, for one detection in current frame and all detections in its previous frame, a linear constraint can be defined in the form as follows: </p><p>Symmetrically, for one detection in previous frame and all detections in current frame, a linear constraint can be defined as follows:</p><p>∀j, y true j = M k=0 y link jk + y end j .</p><p>These two constraints can be collapsed in a matrix form to yield CY = 0, which has already encodes all valid trajectories. Then the data association problem is formulated as an integer linear program as follows:</p><formula xml:id="formula_10">arg max y = Θ (X) Y s.t. CY = 0, Y ∈ {0, 1} |Y| ,<label>(9)</label></formula><p>where Θ (X) is a flattened vector comprising all the predicted scores by the adjacency estimator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Dataset. Our method is evaluated on the challenging KITTI Tracking Benchmark <ref type="bibr" target="#b12">[13]</ref>. This dataset contains 21 training sequences and 29 test sequences. We select 10 sequences from the training partition as the training set and the remaining 11 sequences as the validation set. The train/validation set split is entirely based on frame number of these sequences to make the total frame number of training set (3975) close to that of validation set (3945). We submit our test-set result with the model trained only on split training set for fair comparison <ref type="bibr" target="#b35">[36]</ref>. Each vehicle in the dataset is annotated with 3D and 2D bounding boxes with a unique ID across different frames, and this allows us to obtain the ground truth adjacency matrix for each detection predicted by the detector. We calculate the Intersection over Union (IoU) between each detection and ground truth (GT) bounding boxes, and assign the ID of one GT box to a detection if one has an IoU greater than 0.5 and has the greatest IoU among other detections. This setting is consistent with the test setting of KITTI Benchmark. The KITTI Benchmark <ref type="bibr" target="#b12">[13]</ref> assesses the performance of tracking algorithms relying on standard MOT metrics, CLEAR MOT <ref type="bibr" target="#b4">[5]</ref> and MT/PT/ML <ref type="bibr" target="#b21">[22]</ref>. This set of metrics measures recall and precision of detection, and counts the number of identity switches and fragmentation of trajectories. It also counts the mostly tracked or mostly lost objects, and provides an overall tracking accuracy (MOTA). Implementation Details. We first produce detections using the official code of PointPillar 1 <ref type="bibr" target="#b18">[19]</ref>. The whole tracking framework is implemented with PyTorch <ref type="bibr" target="#b25">[26]</ref>. The image appearance model's backbone is VGG-16 <ref type="bibr" target="#b39">[40]</ref> with Batch Normalization <ref type="bibr" target="#b15">[16]</ref> pretrained on ImageNet-1k <ref type="bibr" target="#b33">[34]</ref>. For linear programming, we use the mixed integer programming (MIP) solver provided by Google OR-Tools 2 . We train the model for 40 epochs using ADAM optimizer with a learning rate of 6e −4 and the super convergence strategy <ref type="bibr" target="#b40">[41]</ref>. We manually set the score to be −1 if the confidence score falls below 0.2, this forces any detection having low confidence to be ignored during linear programming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Ablation Study</head><p>To evaluate the proposed approach and demonstrate the effectiveness of the key components, we conduct an ablation study on the KITTI benchmark <ref type="bibr" target="#b12">[13]</ref> under the online setting, with the state-of-the-art detector PointPillar <ref type="bibr" target="#b18">[19]</ref>. We found that PointPillar detector produces large amount of false positive detections with low prediction score, so we discard detections with a score below 0.3. This does not hurt the mAP of detection, but saves a lot of memory in training. Competency of Point Cloud for Tracking. We set a 2D tracker as our baseline, which only employs 2D image patches as cues and use multiplication as correlation operator during data association, without the ranking mechanism. We first compare the effectiveness of image and Li-DAR point cloud, and evaluate two approaches to employ the point cloud: using point cloud in the frustum or in the bounding box. From the row of baseline in <ref type="table" target="#tab_2">Table 1</ref>, it is observed that using the point cloud in the frustum yields competitive results as using that in the bounding box. The results suggest the applicability of point cloud even with 2D detections (as discussed in Section 3.2), thus the proposed framework is adaptable for 2D or 3D detector with arbitrary modality. More surprisingly, all point cloud methods perform better than the image baseline, which suggests the efficacy of point cloud's deep representation, and indicates that the system could still work when camera is failed. Robust Multi-Modality Fusion Module. We compare the effectiveness of the robust fusion modules A, B, and C. Baselines comprise of trackers using a single sensor, i.e, camera or LiDAR; we train and evaluate each modality separately. To form a stronger baseline, we ensemble the image model (MOTA 74.88) and point cloud model in bounding box (MOTA 75.70), and yields much better result (MOTA 77.54). As shown in <ref type="table" target="#tab_2">Table 1</ref>, only robust fusion module C with attention mechanism surpasses the ensemble result remarkably, although all fusion methods surpass singlesensor baselines. The results suggest the non-triviality of finding a robust fusion module for multi-sensor inputs.</p><p>Since each methods with robust fusion module also provides prediction of single sensor, we compare the single sensor results of each robust fusion module in <ref type="table" target="#tab_2">Table 1</ref>. As can be observed, while the proposed Robust Module is capable of fusing multi-modality effectively, it can maintain competitive performance on the single modality in comparison to baselines (wherein dedicated training on single modality is conducted). Such kind of reliability in fusion is new in the literature. Fusion Module. We further compare the results of normal fusion modules, which only outputs fused modality to the  <ref type="table">Table 3</ref>. Further improvement on fusion results. 'Correlation' indicates using absolute subtraction as correlation operation, 'Ranking' indicates using softmax with addition in ranking mechanism.</p><p>Correlation Ranking MOTA↑ ID-s↓ FP↓ FN↓ 78.18 129 895 1401 79. <ref type="bibr" target="#b17">18</ref> 23 873 1418 80.08 13 790 1411 adjacency estimator, thus the tracker cannot perform tracking with single modality under multi-modality setting. The results in the last row of <ref type="table" target="#tab_2">Table 1</ref> shows that the proposed robust module outperforms the baseline modules A, B, and C consistently, with the additional capability of handling single modality. The results suggests that by preserving reliability, mmMOT gets more supervision signal which is favorable, and thus further improves the accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Further Analysis</head><p>Correlation Operator. We further conduct experiments on correlation function discussed in Section 3.4, and compare the effectiveness of three different correlation functions on the 2D baseline. As shown in <ref type="table" target="#tab_3">Table 2</ref>, the subtraction variant always performs better than the multiplication variant, and with commutative property the absolute subtraction performs the best. Ranking Mechanism. We also examine the effectiveness of the ranking mechanism, and investigate three different variants: the Softmax w mul, Softmax w max, Softmax w add, which indicate combining the softmax output by multiplication, argmax, addition, respectively. From <ref type="table" target="#tab_3">Table 2</ref>, we can see that the ranking mechanism could improve MOTA by 0.2 at least, and adding the softmax output could yield improvement of about 2.5 in MOTA.</p><p>Best Results with 3D Detection. We further improve the results of fusion model. Following the conclusion in <ref type="table" target="#tab_3">Table  2</ref>, we use the absolute subtraction for correlation operation, and softmax activation by addition for ranking mechanism. We compare the efficacy of each modification in <ref type="table">Table 3</ref>.</p><p>The absolute subtraction correlation improves the fusion model's MOTA by 1, and the softmax activation with addition further improves 1 in MOTA and decreases the count of ID switches to 13, which is a remarkable improvement.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">KITTI Results</head><p>We achieve state-of-the-art and competitive results using 2D detection from RRC-Net <ref type="bibr" target="#b31">[32]</ref> provided by MOT-BeyondPixels <ref type="bibr" target="#b37">[38]</ref>. We use PointNet <ref type="bibr" target="#b29">[30]</ref> to process point clouds in frustum, and VGG-16 <ref type="bibr" target="#b39">[40]</ref> for image patches. More details are provided in the supplementary material. <ref type="table" target="#tab_4">Table 4</ref> compares our method with other published stateof-the-art online methods. We first test mmMOT using all the modalities, namely mmMOT-normal. Then we simulate the sensor failure case by only passing single modality to the same model, named mmMOT-lose image/point cloud. Under both conditions our mmMOT surpass all the other published state-of-the-art online methods on MOTA.</p><p>The proposed method by modality fusion surpasses the previous best method MOTBeyondPixels <ref type="bibr" target="#b37">[38]</ref> by far fewer ID switches (184 fewer) with the same detection method. It is noteworthy that our single modality results still perform better, and we did not use bounding box and shape information of detections while MOTBeyondPixels does. PMBM <ref type="bibr" target="#b26">[27]</ref>, JCSTD <ref type="bibr" target="#b44">[45]</ref>, and IMMDP <ref type="bibr" target="#b47">[48]</ref> exhibit fewer ID switches but miss approximately one to two thousand detections. Those missed detections are hard examples not only for detection but also for tracking, so it is likely that they would exhibit higher number of ID switches than our method if they use the same detections. Our method with each of the modalities surpasses the DSM <ref type="bibr" target="#b10">[11]</ref> and extraCK <ref type="bibr" target="#b14">[15]</ref> with fewer False Negatives and ID switches, i.e, our method makes fewer mistakes even when more hard examples are given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Failure Case Analysis</head><p>We observe several conditions that could cause failure in our mmMOT. The statistical results are provided in the supplementary material, and the examples are shown in <ref type="figure" target="#fig_6">Figure   5</ref>, where each row includes four consecutive frames in a video. First, for objects far away, early errors caused by 2D detector will lead to false negative detection as shown by the car with ID 9 in the first row. The error could also cause ID switches if the car is missed but recovered, as shown by the car with ID 6 in the first row and the car with ID 7 in the second row. Second, the illumination also affects the performance, e.g., the black car in the shade with ID 9 in the second row. Third, the occlusion also causes difficulties, e.g., the detector missed the car with ID 7 in the first row. And partial observation makes the cars hard to be distinguished, e.g., the cars with ID 5 and 7 in the first row both only have black rears observed, thus are inferred to be the same. To further address the challenge caused by the occlusion, illumination and long distance, one may further exploit multi-modality in detection to prevent early errors, or exploit more information (e.g., temporal information) in data association to reinforce the prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have presented the mmMOT: a multi-modality Multi-object Tracking framework. We make the first attempt to avoid single-sensor instability while keeping multimodality effective via a deep end-to-end network. Such a function is crucial for safe autonomous driving and has been overlooked by the community. Our framework is learned in an end-to-end manner with adjacency matrix learning, thus could learn to infer from arbitrary modality well in the same time. In addition, this framework is the first to introduce deep representation of LiDAR point cloud into data association problem, and enhances the multi stream framework's robustness against sensor malfunctioning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1. Model Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1.1. Skip Pooling</head><p>The skip pooling layer passes the output feature of each max pooling layer (except the first) in the VGG-Net to the top. Specifically, the numbers of channels for the output of the max pooling layer are 64, 128, 256, 512 in VGG-Net. The global average pooling is first applied to these outputs to gather the spatial information in each level's feature. Next, we use two point-wise convolutions with normalization and ReLU activation to re-scale their number of channels to 128. Then we concatenate these four vectors into a vector with length 512, which is taken as the image feature of each detected bounding-box in the following pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1.2. Best Model</head><p>In the best model on KITTI test set, we still use VGG-16 <ref type="bibr" target="#b39">[40]</ref> with Batch Normalization <ref type="bibr" target="#b15">[16]</ref> as our image feature extractor's backbone, pretrained on ImageNet-1k <ref type="bibr" target="#b33">[34]</ref> by Pytorch <ref type="bibr" target="#b25">[26]</ref>. The hyper-parameters of PointNet <ref type="bibr" target="#b29">[30]</ref> are kept to be the same as before. For detection we use a 2D detector RRC-Net <ref type="bibr" target="#b31">[32]</ref>, which has higher recall and precision than the 3D detector PointPillar <ref type="bibr" target="#b18">[19]</ref>. Thus, we use the point cloud in the frustum for each detection. We use fusion module C to exploit image and point cloud stream, and use absolute subtraction as correlation operation, for ranking mechanism we use softmax activation with addition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2. Failure Analysis</head><p>We further analyse the failure cases of our best mmMOT model with different modality. We focus on the amount of ID switches in the data association process, since the false negative and false positive are mainly caused by the detector. We analyse the occlusion condition, distance from ego car and the bounding box size of each object whose ID is switched. The statistical results are shown in the <ref type="figure" target="#fig_0">Figure A1</ref>.</p><p>From <ref type="figure" target="#fig_0">Figure A1</ref> we observe that the fusion modality indeed makes the tracker more robust to more difficult occlusion and distance conditions. More interestingly, from Figure (a) we can observe that most of id switches come with occlusion, because partial observation could make the object hard to recognize or distinguish. And the occlusion causes more errors when only using point cloud than using image, because we use point cloud in the frustum for 2D detector, and more occlusion could also cause more points of occlusion in the frustum, which provide more information noise. From <ref type="figure">Figures (b)</ref> and (c) we observe that more errors come with small bounding box size and long distance, under which condition the objects' image patches are small and the point cloud is sparse. We also observe that point cloud modality faces more errors, because the number of points in small bounding box or at long distance is insufficient to represent the object, while the image patches could still be interpolated to have the size of 224 × 224.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure (a) For reliability: camera is disabled when overexposure or crashed in transmission. Figure (b) For accuracy: multi-sensor information could reinforce the perception ability. The image is cropped and best viewed in color and zoomed in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>The pipeline of mmMOT. The feature extractors first extract features from image and LiDAR, and the robust fusion module fuses the multi-sensor features. Next, the correlation operator produces the correlation features for each detection pair, by which the adjacency estimator predicts the adjacency matrix. All the predicted scores are optimized to predict the binary variable Y .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>F f use i . In our formulation, the features of fused modality has the same size as a single modality. The robust fusion module concatenates {F s i } S s=0 and F f use i along the batch dimension and feeds them to the adjacency estimator. They form a tensor of size (S + 1) × D × (N + M ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Failure case analysis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure A1 .</head><label>A1</label><figDesc>Failure case analysis. Occlusion level 0, 1, 2, 3 indicates the object is not, moderately, highly, extremely occluded and truncated in image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Comparison of different modalities. 'Frustum' indicates using point cloud in the frustum. Robust Modules X indicates using fusion module X in the robust fusion module.</figDesc><table><row><cell>Method</cell><cell>Modality</cell><cell cols="2">MOTA↑ ID-s↓ FP↓ FN↓</cell></row><row><cell></cell><cell>Image</cell><cell>74.88</cell><cell>454 951 1387</cell></row><row><cell>Baseline</cell><cell cols="2">Frustum Point Cloud 75.70 75.50</cell><cell>387 918 1418 362 946 1393</cell></row><row><cell></cell><cell>Ensemble</cell><cell>77.54</cell><cell>158 949 1388</cell></row><row><cell></cell><cell>Image</cell><cell>75.40</cell><cell>396 951 1387</cell></row><row><cell>Robust Module A</cell><cell cols="2">Point Cloud 76.13</cell><cell>317 948 1392</cell></row><row><cell></cell><cell>Fusion</cell><cell>77.57</cell><cell>177 910 1406</cell></row><row><cell></cell><cell>Image</cell><cell>75.17</cell><cell>421 951 1387</cell></row><row><cell>Robust Module B</cell><cell cols="2">Point Cloud 74.55</cell><cell>490 951 1387</cell></row><row><cell></cell><cell>Fusion</cell><cell>77.62</cell><cell>193 850 1444</cell></row><row><cell></cell><cell>Image</cell><cell>74.86</cell><cell>456 951 1387</cell></row><row><cell>Robust Module C</cell><cell cols="2">Point Cloud 74.94</cell><cell>452 946 1398</cell></row><row><cell></cell><cell>Fusion</cell><cell>78.18</cell><cell>129 895 1401</cell></row><row><cell>Module A</cell><cell>Fusion</cell><cell>77.31</cell><cell>176 934 1412</cell></row><row><cell>Module B</cell><cell>Fusion</cell><cell>77.31</cell><cell>212 913 1396</cell></row><row><cell>Module C</cell><cell>Fusion</cell><cell>77.62</cell><cell>142 945 1400</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Comparison of 2D trackers with further modification.</figDesc><table><row><cell>Modification</cell><cell cols="2">MOTA↑ ID-s↓ FP↓ FN↓</cell></row><row><cell>Multiplication</cell><cell>74.88</cell><cell>454 951 1387</cell></row><row><cell>Subtraction</cell><cell>75.27</cell><cell>410 951 1387</cell></row><row><cell cols="2">Absolute subtraction 77.76</cell><cell>143 941 1387</cell></row><row><cell>Softmax w mul</cell><cell>75.08</cell><cell>431 951 1387</cell></row><row><cell>Softmax w max</cell><cell>76.24</cell><cell>313 940 1387</cell></row><row><cell>Softmax w add</cell><cell>77.40</cell><cell>234 891 1387</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Comparison on the testing set of KITTI tracking benchmark. Only published online methods are reported.</figDesc><table><row><cell>Method</cell><cell cols="4">MOTA↑ MOTP↑ Prec.↑ Recall↑</cell><cell>FP↓</cell><cell cols="4">FN↓ ID-s↓ Frag↓ MT↑</cell><cell>ML↓</cell></row><row><cell>DSM [11]</cell><cell>76.15</cell><cell>83.42</cell><cell>98.09</cell><cell>80.23</cell><cell>578</cell><cell>7328</cell><cell>296</cell><cell>868</cell><cell>60.00</cell><cell>8.31</cell></row><row><cell>extraCK [15]</cell><cell>79.99</cell><cell>82.46</cell><cell>98.04</cell><cell>84.51</cell><cell>642</cell><cell>5896</cell><cell>343</cell><cell>938</cell><cell>62.15</cell><cell>5.54</cell></row><row><cell>PMBM [36]</cell><cell>80.39</cell><cell>81.26</cell><cell>96.93</cell><cell>85.01</cell><cell cols="2">1007 5616</cell><cell>121</cell><cell>613</cell><cell>62.77</cell><cell>6.15</cell></row><row><cell>JCSTD [45]</cell><cell>80.57</cell><cell>81.81</cell><cell>98.72</cell><cell>83.37</cell><cell>405</cell><cell>6217</cell><cell>61</cell><cell>643</cell><cell>56.77</cell><cell>7.38</cell></row><row><cell>IMMDP [48]</cell><cell>83.04</cell><cell>82.74</cell><cell>98.82</cell><cell>86.11</cell><cell>391</cell><cell>5269</cell><cell>172</cell><cell>365</cell><cell cols="2">60.62 11.38</cell></row><row><cell>MOTBeyondPixels [38]</cell><cell>84.24</cell><cell>85.73</cell><cell>97.95</cell><cell>88.80</cell><cell>705</cell><cell>4247</cell><cell>468</cell><cell>944</cell><cell>73.23</cell><cell>2.77</cell></row><row><cell>mmMOT-normal</cell><cell>84.77</cell><cell>85.21</cell><cell>97.93</cell><cell>88.81</cell><cell>711</cell><cell>4243</cell><cell>284</cell><cell>753</cell><cell>73.23</cell><cell>2.77</cell></row><row><cell>mmMOT-lose image</cell><cell>84.53</cell><cell>85.21</cell><cell>97.93</cell><cell>88.81</cell><cell>711</cell><cell>4243</cell><cell>368</cell><cell>832</cell><cell>73.23</cell><cell>2.77</cell></row><row><cell>mmMOT-lose point cloud</cell><cell>84.59</cell><cell>85.21</cell><cell>97.93</cell><cell>88.81</cell><cell>711</cell><cell>4243</cell><cell>347</cell><cell>809</cell><cell>73.23</cell><cell>2.77</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/nutonomy/second.pytorch 2 https://developers.google.com/optimization</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment This work is supported by Sense-Time Group Limited, Singapore MOE AcRF Tier 1 (M4012082.020), NTU SUG, and NTU NAP.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">3D object tracking using RGB and LIDAR data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Asvadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Girão</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paulo</forename><surname>Peixoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urbano</forename><surname>Nunes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ITSC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Detection and tracking of moving objects using 2.5D motion grids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Asvadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paulo</forename><surname>Peixoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urbano</forename><surname>Nunes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ITSC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Shrinidhi Kowshika Lakshmikanth, and Raquel Urtasun. Deep multi-sensor lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gellért</forename><surname>Máttyus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namdar</forename><surname>Homayounfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IROS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kavita</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Evaluating multiple object tracking performance: The CLEAR MOT metrics. EURASIP J. Image and Video Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keni</forename><surname>Bernardin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Online multiperson tracking-by-detection from a single, uncalibrated camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">D</forename><surname>Breitenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Reichlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esther</forename><surname>Koller-Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><forename type="middle">J</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">IntentNet: Learning to predict intention from raw sensor data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoRL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-view 3D object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A multi-sensor fusion system for moving object detection and tracking in urban driving environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunggi</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Woo</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V K</forename><surname>Vijaya Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ragunathan</forename><surname>Rajkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-target tracking using a 3D-lidar sensor for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaebum</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Ulbrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Lichte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Maurer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ITSC</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">End-to-end learning of multi-sensor 3D tracking by detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davi</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multiple sensor fusion and classification for moving object detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><forename type="middle">Omar</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chávez</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Aycard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TITS</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The perfect match: 3D point cloud matching with smoothed densities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zan</forename><surname>Gojcic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caifa</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Wieser</surname></persName>
		</author>
		<idno>abs/1811.06879</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A lightweight online multiple object vehicle tracking method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gultekin</forename><surname>Gunduz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tankut</forename><surname>Acarman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>abs/1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">PointSIFT: A SIFT-like network module for 3D point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<idno>abs/1807.00652</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Joint 3D proposal generation and object detection from view aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melissa</forename><surname>Mozifian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungwook</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IROS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">PointPillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
		<idno>abs/1812.05784</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Followme: Efficient online min-cost flow tracking with bounded memory and computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">SiamRPN++: Evolution of siamese visual tracking with very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to associate: Hybridboosted multi-target tracker for crowded scene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fast and furious: Real time end-to-end 3D detection, tracking and motion forecasting with a single convolutional net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Combined image-and world-space tracking in traffic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljoša</forename><surname>Ošep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Mehner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Occlusion geodesics for online multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Possegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mauthner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Frustum PointNets for 3D object detection from RGB-D data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
		<idno>abs/1711.08488</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">PointNet: Deep learning on point sets for 3D classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">PointNet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">No blind spots: Full-surround multi-object tracking for autonomous vehicles using cameras &amp; lidars. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Rangesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
		<idno>abs/1802.08755</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Accurate single stage detector using recurrent rolling convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<imprint>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<title level="m">ImageNet Large Scale Visual Recognition Challenge. IJCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Tracking the untrackable: Learning to track multiple cues with long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mono-camera 3D multi-object tracking using deep learning detections and PMBM filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Scheidegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Benjaminsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emil</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amrit</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Granström</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Deep network flow for multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Vernaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Beyond pixels: Leveraging geometry and shape cues for online multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarthak</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junaid</forename><surname>Ahmed Ansari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Krishna</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Madhava Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICRA</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">PointR-CNN: 3D object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1812.04244</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Super-convergence: Very fast training of neural networks using large learning rates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leslie</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholay</forename><surname>Topin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Object tracking with 3D lidar via multi-task sparse learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMA</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">FishNet: A versatile backbone for image, region, and pixel level prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Subgraph decomposition for multi-target tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Online multi-object tracking using joint domain information in traffic scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Lauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TITS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">SqueezeSeg: Convolutional neural nets with recurrent CRF for real-time road-object segmentation from 3D lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICRA</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">SqueezeSegV2: Improved model structure and unsupervised domain adaptation for road-object segmenta</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sicheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<idno>abs/1809.08495</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning to track: Online multi-object tracking by decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Global multi-object tracking using generalized minimum clique graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afshin</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gmcp-Tracker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep continuous conditional random fields with asymmetric inter-object constraints for online multiobject tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TCSVT</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">VoxelNet: End-to-end learning for point cloud based 3D object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
