<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GhostVLAD for set-based face recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujie</forename><surname>Zhong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering Science</orgName>
								<orgName type="laboratory">VGG</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>ArandjeloviÄ‡</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">DeepMind</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering Science</orgName>
								<orgName type="laboratory">VGG</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">DeepMind</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GhostVLAD for set-based face recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The objective of this paper is to learn a compact representation of image sets for template-based face recognition. We make the following contributions: first, we propose a network architecture which aggregates and embeds the face descriptors produced by deep convolutional neural networks into a compact fixed-length representation. This compact representation requires minimal memory storage and enables efficient similarity computation. Second, we propose a novel GhostVLAD layer that includes ghost clusters, that do not contribute to the aggregation. We show that a quality weighting on the input faces emerges automatically such that informative images contribute more than those with low quality, and that the ghost clusters enhance the network's ability to deal with poor quality images. Third, we explore how input feature dimension, number of clusters and different training techniques affect the recognition performance. Given this analysis, we train a network that far exceeds the state-of-the-art on the IJB-B face recognition dataset. This is currently one of the most challenging public benchmarks, and we surpass the state-of-the-art on both the identification and verification protocols.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>While most research on face recognition has focused on recognition from a singleimage, template based face recognition, where a set of faces of the same subject is available, is now gaining attention. In the unconstrained scenario considered here, this can be a challenging task as face images may have various poses, expression, illumination, and may also be of quite varying quality.</p><p>A straightforward method to tackle multiple images per subject is to store per-image descriptors extracted from each face image (or frame in a video), and compare every pair of images between sets at query time <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b30">31]</ref>. However, this type of approach can be memory-consuming and prohibitively slow, especially for searching tasks in large-scale datasets. Therefore, an aggregation method that can produce a compact template representation is desired. Furthermore, this representation should support efficient computation of similarity and require minimal memory storage.</p><p>More importantly, the representation obtained from image sets should be discriminative i.e. template descriptors of the same subject should be close to each other in the descriptor space, whereas those of different subjects should be far apart. Although common aggregation strategies, such as average pooling and max-pooling, are able to aggregate face descriptors to produce a compact template representation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b24">25]</ref> and currently achieves the state-of-the-art results <ref type="bibr" target="#b4">[5]</ref>, we seek a better solution in this paper.</p><p>As revealed by <ref type="bibr" target="#b16">[17]</ref>, image retrieval encoding methods like Fisher Vector encoding and T-embedding increase the separation between descriptors extracted from related and unrelated image patches. We therefore expect a similar encoding to be beneficial for face recognition, including both verification and identification tasks. This insight inspires us to include a similar encoding, NetVLAD <ref type="bibr" target="#b1">[2]</ref>, in the design of our network.</p><p>In this paper, we propose a convolutional neural network ( <ref type="figure">Fig. 1</ref>) that satisfies all the desired properties mentioned above: it can take any number of input faces and produce a compact fixed-length descriptor to represent the image set. Moreover, this network embeds face descriptors such that the resultant template-descriptors are more discriminative than the original descriptors. The representation is efficient in both memory and query speed aspects, i.e. it only stores one compact descriptor per template, regardless of the number of face images in a template, and the similarity between two templates is simply measured as the scalar product (i.e. cosine similarity) of two template descriptors.</p><p>However, one of the key problems in unconstrained real-world situations is that some faces in a template may be of low quality -for example, low resolution, or blurred, or partially occluded. These low-quality images are distractors and are likely to hurt the performance of the face recognition system if given equal weight as the other (good quality) faces. Therefore, a sophisticated network should be able to reduce the impact of such distracting images and focus on the informative ones.</p><p>To this end, we extend the NetVLAD architecture to include ghost clusters. These are clusters that face descriptors can be soft assigned to, but are excluded from the aggregation. They provide a mechanism for the network to handle low quality faces, by mainly assigning them to the ghost clusters. Interestingly, although we do not explicitly learn any importance weightings between faces in each template, such property emerges automatically from our network. Specifically, low quality faces generally contribute less to the final template representation than the high-quality ones.</p><p>The networks are trained in an end-to-end fashion with only identity-level labels. They outperform state-of-the-art methods by a large margin on the public IJB-A <ref type="bibr" target="#b19">[20]</ref> and IJB-B <ref type="bibr" target="#b36">[37]</ref> face recognition benchmarks. These datasets are currently the most challenging in the community, and we evaluate on these in this paper. This paper is organized as following: Sec. 2 reviews some related work on face recognition based on image sets or videos; the proposed network and implementation details are introduced in Sec. 3, followed by experimental results reported in Sec. 4. Finally a conclusion is drawn in Sec. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Early face recognition approaches which make use of sets of face examples (extracted from different images or video frames) aim to represent image sets as manifolds <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b40">41]</ref>, convex hulls <ref type="bibr" target="#b5">[6]</ref>, Gaussian Mixture Models <ref type="bibr" target="#b34">[35]</ref>, or set covariance matrices <ref type="bibr" target="#b35">[36]</ref>, and measure the dissimilarity between image sets as distance between these spaces.</p><p>Later methods represent face sets more efficiently using a single fixed-length descriptor. For example, <ref type="bibr" target="#b23">[24]</ref> aggregates local descriptors (RootSIFT <ref type="bibr" target="#b2">[3]</ref>) extracted from face crops using Fisher Vector <ref type="bibr" target="#b25">[26]</ref> (FV) encoding to obtain a single descriptor per face track. Since the success of deep learning in image-based face recognition <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b43">44]</ref>, simple strategies for face descriptor aggregation prevailed, such as average-and max-pooling <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b24">25]</ref>. However, none of these strategies are trained end-to-end for face recognition as typically only the face descriptors are learnt, while aggregation is performed post hoc.</p><p>A few methods go beyond simple pooling by computing a weighted average of face descriptors based on some measure of per-face example importance. For example, <ref type="bibr" target="#b8">[9]</ref> train a module to predict human judgement on how memorable a face is, and use this memorability score as the weight. In <ref type="bibr" target="#b39">[40]</ref>, an attention mechanism is used to compute face example weights, so that the contribution of low quality images to the final set representation is down-weighted. However, these methods rely on pretrained face descriptors and do not learn them jointly with the weighting functions, unlike our method where the entire system is trained end-to-end for face recognition.</p><p>Two other recent papers are quite related in that they explicitly take account of image quality: <ref type="bibr" target="#b10">[11]</ref> first bins face images of similar quality and pose before aggregation; whilst <ref type="bibr" target="#b21">[22]</ref> introduces a fully end-to-end trainable method which automatically learns to down-weight low quality images. As will be seen in the sequel, we achieve similar functionality implicitly due to the network architecture, and also exceed the performance of both these methods (see Sec. 4.4). As an interesting yet different method which can also filter low-quality images, <ref type="bibr" target="#b26">[27]</ref> learns to aggregate the raw face images and then computes a descriptor.</p><p>Our aggregation approach is inspired by the image retrieval literature on aggregating local descriptors <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">17]</ref>. Namely, JÃ©gou and Zisserman <ref type="bibr" target="#b16">[17]</ref> find that, compared to simple average-pooling, Fisher Vector encoding and T-embedding increase the contrast between the similarity scores of matching and mismatching local descriptors. Motivated by this fact, we make use of a trainable aggregation layer, NetVLAD <ref type="bibr" target="#b1">[2]</ref>, and improve it for the face recognition task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Set-based face recognition</head><p>We aim to learn a compact representation of a face. Namely, we train a network which digests a set of example face images of a person, and produces a fixedlength template representation useful for face recognition. The network should satisfy the following properties: <ref type="figure">Fig. 1</ref>: Network architecture. Input images in each template are first passed through a convolutional neural network (e.g. ResNet-50 or SENet-50 with an additional FC layer and L2-normalization) to produce a face descriptor per image. The descriptors are aggregated into a single fixed-length vector using the GhostVLAD layer. The final D-dimensional template descriptor is obtained by reducing dimensionality using a fully-connected layer, followed by batch normalization (BN) and L2-normalization.</p><formula xml:id="formula_0">. . . . . . Aggregation template ... Feature Extraction 2 1 N (D -dim) (DF -dim) 1 2 N CNN L2 GhostVLAD K + G clusters L2 BN FC (DF Ã— K) Ã— D</formula><p>(1) Take any number of images as input, and output a fixed-length template descriptor to represent the input image set. <ref type="bibr" target="#b1">(2)</ref> The output template descriptor should be compact (i.e. low-dimensional) in order to require little memory and facilitate fast template comparisons. (3) The output template descriptor should be discriminative, such that the similarity of templates of the same subject is much larger than that of different subjects.</p><p>We propose a convolutional neural network that fulfils all three objectives. (1) is achieved by aggregating face descriptors using a modified NetVLAD <ref type="bibr" target="#b1">[2]</ref> layer, GhostVLAD. Compact template descriptors (2) are produced by a trained layer which performs dimensionality reduction. Discriminative representations (3) emerge because the entire network is trained end-to-end for face recognition, and since our GhostVLAD layer is able to down-weight the contribution of lowquality images, which is important for good performance <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b39">40]</ref>.</p><p>The network architecture and the new GhostVLAD layer are described in Sec. 3.1 and Sec. 3.2, respectively, followed by the network training procedure (Sec. 3.3) and implementation details (Sec. 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Network architecture</head><p>As shown in <ref type="figure">Fig. 1</ref>, the network consists of two parts: feature extraction, which computes a face descriptor for each input face image, and aggregation, which aggregates all face descriptors into a single compact template representation of the input image set.</p><p>Feature extraction. A neural network is used to extract a face descriptor for each input face image. Any network can be used in our learning framework, but in this paper we opt for ResNet-50 <ref type="bibr" target="#b11">[12]</ref> or SENet-50 <ref type="bibr" target="#b12">[13]</ref>. Both networks are cropped after the global average pooling layer, and an extra FC layer is added to reduce the output dimension to D F . We typically pick D F to be low-dimensional (e.g. 128 or 256), and do not see a significant drop in face recognition performance compared to using the original 2048-D descriptors. Finally, the individual face descriptors are L2 normalized.</p><p>Aggregation. The second part uses GhostVLAD (Sec. 3.2) to aggregate multiple face descriptors into a single D F Ã— K vector (where K is a parameter of the method). To keep computational and memory requirements low, dimensionality reduction is performed via an FC layer, where we pick the output dimensionality D to be 128. The compact D-dimensional descriptor is then passed to a batch-normalization layer <ref type="bibr" target="#b14">[15]</ref> and L2-normalized to form the final template representation x template .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">GhostVLAD: NetVLAD with ghost clusters</head><p>The key component of the aggregation block is our GhostVLAD trainable aggregation layer, which given N D F -dimensional face descriptors computes a single D F Ã— K dimensional output. It is based on the NetVLAD <ref type="bibr" target="#b1">[2]</ref> layer which implements an encoding similar to VLAD encoding <ref type="bibr" target="#b15">[16]</ref>, while being differentiable and thus fully-trainable. NetVLAD has been shown to outperform average and max pooling for the same vector dimensionality, which makes it perfectly suited for our task. Here we provide a brief overview of NetVLAD (for full details please refer to <ref type="bibr" target="#b1">[2]</ref>), followed by our improvement, GhostVLAD.</p><p>NetVLAD. For N D F -dimensional input descriptors {x i } and a chosen number of clusters K, NetVLAD pooling produces a single D F Ã— K vector V (for convenience written as a D F Ã— K matrix) according to the following equation:</p><formula xml:id="formula_1">V (j, k) = N i=1 e a T k xi+b k K k =1 e a T k xi+b k (x i (j) âˆ’ c k (j))<label>(1)</label></formula><p>where {a k }, {b k } and {c k } are trainable parameters, with k âˆˆ [1, 2, . . . , K]. The first term corresponds to the soft-assignment weight of the input vector x i for cluster k, while the second term computes the residual between the vector and the cluster centre. The final output is obtained by performing L2 normalization.</p><p>GhostVLAD. We extend NetVLAD with "ghost" clusters to form GhostVLAD, as shown in <ref type="figure">Fig. 2</ref>. Namely, we add further G "ghost" clusters which contribute to the soft assignments in the same manner as the original K clusters, but residuals between input vectors and the ghost cluster centres are ignored and do not contribute to the final output. In other words, the summation in the denominator of eq. 1 instead of to K goes to K + G, while the output is still D F Ã— K dimensional; this means {a k } and {b k } have K +G elements each, while {c k } still has K. Another view is that we are computing NetVLAD with K + G clusters, followed by removing the elements that correspond to the G ghost clusters. Note that GhostVLAD is a generalization of NetVLAD as with G = 0 the two are equivalent. As with NetVLAD, GhostVLAD can be implemented efficiently using soft-assign. soft-assign.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>soft-assignment</head><formula xml:id="formula_2">Wáµ€x + b DF Ã— (K + G) soft-max residual computation with K clusters aggregation x 1 , â€¦, x N V X removal of ghost assign. Ã—N Ã—N L2 Fig. 2: GhostVLAD.</formula><p>For each input descriptor, NetVLAD performs softassignment into K cluster centres, computed as a linear transformation followed by a soft-max. It then, for each cluster centre, aggregates all residuals between input descriptors and the cluster centre, weighted with the soft-assignment values. The final vector is produced as a concatenation of the per-cluster aggregated residuals; for more details see eq. 1 and <ref type="bibr" target="#b1">[2]</ref>. We introduce G "ghost" clusters in the soft-assignment stage, where the "ghost" assignment weight is illustrated with a dotted red bar (here we show only G = 1 ghost cluster). The ghost assignments are then eliminated and residual aggregation proceeds as with NetVLAD. This mechanism enables the network to assign uninformative descriptors to ghost clusters thus decreasing their soft-assignment weights for non-ghost clusters, and therefore reducing their contribution to the final template representation. standard convolutional neural network building blocks, e.g. the soft-assignment can be done by stacking input descriptors and applying a convolution operation, followed by a convolutional soft-max; for details see <ref type="bibr" target="#b1">[2]</ref>.</p><p>The intuition behind the incorporation of ghost clusters is to make it easier for the network to adjust the contribution of each face example to the template representation by assigning examples to be ignored to the ghost clusters. For example, in an ideal case, a highly blurry face image would be strongly assigned to a ghost cluster, making the assignment weights to non-ghost clusters close to zero, thus causing its contribution to the template representation to be negligible; in Sec. 4.5 we qualitatively validate this intuition. However, note that we do not explicitly force low-quality images to get assigned to ghost clusters, but instead let the network discover the optimal behaviour through end-to-end training for face recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Network training</head><p>In this section we describe how to train the network for face recognition, but note that GhostVLAD is a general layer which can also be used for other tasks.</p><p>Training loss. Just for training purposes, we append the network with a fullyconnected "classification" layer of size D Ã—T , where D is the size of the template representation and T is the number of identities available in the training set. We use the one-versus-all logistic regression loss as empirically we found that it converges faster and outperforms cross-entropy loss. The classification layer is discarded after training and the trained network is used to extract a single fixed-length template representation for the input face images.</p><p>Training with degraded images. For unconstrained face recognition, it is important to be able to handle images of varying quality that typically occur in the wild. The motivation behind our network architecture, namely the GhostVLAD layer, is to enable it to down-weight the influence of these images on the template representation. However, since our training dataset only contains good quality images, it is necessary to perform data augmentation in the form of image degradation, such as blurring or compression (see Sec. 3.4 for details), in order to more closely match the varying image quality encountered at test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Implementation details</head><p>This section discusses full details of the training process, including training data, data augmentation, network initialization, etc.</p><p>Training data. We use face images from the training set of the VGGFace2 dataset <ref type="bibr" target="#b4">[5]</ref> to train the network. It consists of around 3 million images, covering 8631 identities. For each identity, there are on average 360 face images across different ages and poses. To perform set-based training, we form image sets on-the-fly by repeatedly sampling a fixed number of images belonging to the same identity.</p><p>Data augmentation. Training images are resized such that the smallest dimension is 256 and random crops of size 224 Ã— 224 are used as inputs to the network. Further augmentations include random horizontal flipping and a random rotation of no greater than 10 degrees.</p><p>We adopt four methods to degrade images for training: isotropic blur, motion blur, decreased resolution and JPEG compression. Each degradation method has a probability of 0.1 to be applied to a training image, where, to prevent overdegradation, a maximum of two transformations per image is allowed. Isotropic blur is implemented using a Gaussian filter where the standard deviation is uniformly sampled between 6 and 16. For motion blur, the angle of motion is uniformly sampled between 0 and 359 degrees, and the motion length is fixed to 11. Resolution decrease is simulated by downscaling the image by a factor of 10 and scaling it back up to the original size. Finally, we add JPEG compression artefacts by randomly compressing the images to one of three compression ratios: 0.01, 0.05 and 0.09.</p><p>Training procedure. The network can be trained end-to-end in one go, but, to make the training faster and more stable, we divide it into three stages; all stages only use the VGGFace2 <ref type="bibr" target="#b4">[5]</ref> dataset. In the first two stages, parts of the network are trained for single-image face classification (i.e. the input image set consists of a single image), and image degradation is not performed. Firstly, the feature extractor network is pre-trained for single-image face classification by temporarily (just for this stage) appending a classification FC layer on top of it, and training the network with the cross-entropy loss. Secondly, we train the whole network end-to-end for single-image classification with one-versus-all logistic regression loss, but exclude the ghost clusters from GhostVLAD because training images are not degraded in this stage. Finally, we add ghost clusters and enable image degradation, and train the whole network using image sets with one-versus-all logistic regression loss.</p><p>Parameter initialization. The non-ghost clusters of GhostVLAD are initialized as in NetVLAD <ref type="bibr" target="#b1">[2]</ref> by clustering its input features with k-means into K clusters, where only non-degraded images are used. The G ghost clusters are initialized similarly, but using degraded images for the clustering; note that for G = 1 (a setting we often use) k-means simplifies to computing the mean over the features. The FC following GhostVLAD which performs dimensionality reduction is then initialized using the PCA transformation matrix computed on the GhostVLAD output features.</p><p>Training details. The network is trained using stochastic gradient descend with momentum, implemented in MatConvNet <ref type="bibr" target="#b32">[33]</ref>. The mini-batch consists of 84 face images, i.e. if we train with image sets of size two, a batch contains 42 image sets, one per identity. When one-versus-all logistic regression loss is used, for each image set, we update the network weights based on the positive class and only 20 negative classes (instead of 8631) that obtain the highest classification scores. The initial learning rate of 0.0001 is used for all parameters apart from GhostVLAD's assignment parameters and the classification FC weights, for which we use 0.1 and 1, respectively. The learning rates are divided by 10 when validation error stagnates, while weight decay and momentum are fixed to 0.0005 and 0.9, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we describe the experimental setup, investigate the impact of our design choices, and compare results with the state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Benchmark datasets and evaluation protocol</head><p>Standard and most challenging public face recognition datasets IJB-A <ref type="bibr" target="#b19">[20]</ref> and IJB-B <ref type="bibr" target="#b36">[37]</ref> are used for evaluation. In contrast to single-image based face datasets such as <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">25]</ref>, IJB-A and IJB-B are intended for template-based face recognition, which is exactly the task we consider in this work. The IJB-A dataset contains 5,712 images and 2,085 videos, covering 500 subjects; thus the average number of images and videos per subject are 11.4 and 4.2 videos, respectively. The IJB-B dataset is an extension of IJB-A with a total of 11,754 images and 7,011 videos from 1,845 subjects, as well as 10,044 non-face images. There is no overlap between subjects in VGGFace2, which we use for training, and the test datasets. Faces are detected from images and all video frames using MTCNN <ref type="bibr" target="#b42">[43]</ref>, the face crops are then resized such that the smallest dimension is 224 and the central 224 Ã— 224 crop is used as the face example.</p><p>Evaluation protocol. We follow the standard benchmark procedure for IJB-A and IJB-B, and evaluate on "1:1 face verification" and "1:N face identification".</p><p>The goal of 1:1 verification is to make a decision whether two templates belong to the same person, done by thresholding the similarity between the templates. Verification performance is assessed via the receiver operating characteristic (ROC) curve, i.e. by measuring the trade-off between the true accept rates (TAR) vs false accept rates (FAR).</p><p>For 1:N identification, templates from the probe set are used to rank all templates in a given gallery. The performance is measured using the true positive identification rate (TPIR) vs false positive identification rate (FPIR) (i.e. the decision error trade-off (DET) curve) and vs Rank-N (i.e. the cumulative match characteristic (CMC) curve).</p><p>Evaluation protocols are the same for both benchmark datasets, apart from the fact that IJB-A defines 10 test splits, while IJB-B only has one split for verification and two galleries for identification. For IJB-A and for IJB-B identification, we report, as per standard, the mean and standard deviation of the performance measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Networks, deployment and baselines</head><p>Our networks. As explained earlier in Sec. 3.1, we use two different architectures as backbone feature extractors: ResNet-50 <ref type="bibr" target="#b11">[12]</ref> and SENet-50 <ref type="bibr" target="#b12">[13]</ref>. They are cropped after global average-pooling which produces a D F = 2048 dimensional face descriptor, while we also experiment with reducing the dimensionality via an additional FC, down to D F = 256 or D F = 128.</p><p>To disambiguate various network configurations, we name the networks as Ext-GV-S(-gG), where Ext is the feature extractor network (Res for ResNet-50 or SE for SENet-50), S is the size of image sets used during training, and G is the number of ghost clusters (if zero, the suffix is dropped). For example, SE-GV-3-g2 denotes a network which uses the SENet-50 as the feature extractor, training image sets of size 3, and 2 ghost clusters.</p><p>Network deployment. In the IJB-A and IJB-B datasets, there are images and videos available for each subject. Here we follow the established approach of <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8]</ref> to balance the contributions of face examples from different sources, as otherwise a single very long video could completely dominate the representation. In more detail, face examples are extracted from all video frames, and their additive contributions to the GhostVLAD representation are down-weighed by the number of frames in the video.</p><p>The similarity between two templates is measured as the scalar product between the template representations; recall that they have unit norm <ref type="figure">(Fig. 1)</ref>.</p><p>Baselines. Our network is compared with several average-pooling baselines. The baseline architecture consists of a feature extractor network which produces a face descriptor for each input example, and the template representation is performed by average-pooling the face descriptors (with source balancing), followed by L2 normalization. The same feature extractor networks are used as for our method, ResNet-50 or SENet-50, abbreviated as Res and SE, respectively, with an optionally added FC layer to perform dimensionality reduction down to 128-D or 256-D. These networks are trained for single-image face classification, which is equivalent to stage 1 of our training procedure from Sec. 3.4, and also corresponds to the current state-of-the-art approach <ref type="bibr" target="#b4">[5]</ref> (albeit with more training data -see Sec. 4.4 for details and comparisons). No image degradation is performed as it decreases performance when combined with single-image classification training.</p><p>In addition, we train the baseline architecture SENet-50 with average-pooling using our training procedure (Sec. 3.3), i.e. with image sets of size 2 and degraded images, and refer to it as SE-2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation studies on IJB-B</head><p>Here we evaluate various design choices of our architecture and compare it to baselines on the IJB-B dataset, as it is larger and more challenging than IJB-A; results on verification and identification are shown in <ref type="table" target="#tab_0">Tables 1 and Table 2</ref>, respectively.</p><p>Feature extractor and dimensionality reduction. Comparing rows 1 vs 2 of the two tables shows that reducing the dimensionality of the face features from 2048-D to 128-D does not affect the performance much, and in fact sometimes improves it due to added parameters in the form of the dimensionality reduction FC. As the feature extractor backbone, SENet-50 consistently beats ResNet-50, as summarized in rows 2 vs 3.</p><p>Training for set-based face recognition. The currently adopted set-based face recognition approach of training with single-image examples and performing aggregation post hoc (SE, row 4) is clearly inferior to our training procedure which is aware of image sets (SE-2, row 5).</p><p>Learnt GhostVLAD aggregation. Using the GhostVLAD aggregation layer (with G = 0 i.e. equivalent to NetVLAD) together with our set-based training framework strongly outperforms the standard average-pooling approach, regardless of whether training is done with non-degraded images (SE-GV-2, row 8 vs SE, rows 3 and 4), degraded images (SE-GV-2, row 9 vs SE-2, row 5), or if a different feature extractor architecture (ResNet-50) is used (Res-GV-2, row 6 vs Res, row 2). Using 256-D vs 128-D face descriptors as inputs to GhostVLAD, while keeping the same dimensionality of the final template representation (128-D), achieves better results (rows 9 vs 7), so we use 256-D in all latter experiments.</p><p>Training with degraded images. When using our set-based training procedure, training with degraded images brings a consistent boost, as shown in rows 9 vs  Number of clusters K. GhostVLAD (and NetVLAD) have a hyperparameter K -the number of non-ghost clusters -which we vary between 4 and 16 (rows 9 to 11) to study its effect on face recognition performance. It is expected that K shouldn't be too small so that underfitting is avoided (e.g. K = 1 is similar to average-pooling) nor too large in order to prevent over-quantization and overfitting. As in traditional image retrieval <ref type="bibr" target="#b15">[16]</ref>, we find that a wide range of K achieves good performance, with K = 8 being the best.</p><p>Ghost clusters. Introducing a single ghost cluster (G = 1) brings significant improvement over the vanilla NetVLAD, as shown by comparing SE-GV-3-g1 vs SE-GV-3 (rows 14 vs 12) and SE-GV-4-g1 vs SE-GV-4 (rows 15 vs 13).</p><p>Using one ghost cluster is sufficient as increasing the number of ghost clusters to two does not result in significant differences (row 16 vs row 14). Ghost clusters enable the system to automatically down-weight the contribution of low quality images, as will be shown in Sec. 4.5, which improves the template representations and benefits face recognition.</p><p>Set size used during training. To perform set-based training, as described in Sec. 3.4, image sets are created by sampling a fixed number of faces for a subject; the number of sampled faces is another parameter of the method. Increasing the set size from 2 to 3 consistently improves results (rows 9 vs 12), while there is no clear winner between using 3 or 4 face examples (worse for G = 0, rows 12 vs 13, better for G = 1, rows 15 vs 14).</p><p>Output dimensionality. Comparisons are also made between networks with 128-D output features and those with 256-D (i.e. row 13 vs 17 and row 15 vs 18), and we can see that networks with 128-D output achieve better performance while being more memory-efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with state-of-the-art</head><p>In this section, our best networks, SE-GV-3 and SE-GV-4-g1, are compared against the state-of-the-art on the IJB-A and IJB-B datasets. The currently best performing method <ref type="bibr" target="#b4">[5]</ref> is the same as our SE baseline (i.e. average-pooling of SENet-50 features trained for single-image classification) but trained on a much larger training set, MS-Celeb-1M dataset <ref type="bibr" target="#b9">[10]</ref>, and then fine-tuned on VGGFace2.</p><p>From <ref type="table" target="#tab_3">Tables 3 and 4</ref>, and <ref type="figure" target="#fig_0">Figure 3</ref>, it is clear our GhostVLAD network (SE-GV-4-g1 ) convincingly outperforms previous methods and sets the new state-ofthe-art for both identification and verification on both IJB-A and IJB-B datasets. In particular, it surpasses <ref type="bibr" target="#b43">[44]</ref> marginally on the IJB-A verification task, despite the fact that <ref type="bibr" target="#b43">[44]</ref> uses a deeper ResNet and performs an exhaustive scoring using each face image in the templates. The only points for which the GhostVLAD network doesn't beat the state-of-the-art, though it is on par with it, is in TPIR at Rank-1 to Rank-10 for identification on IJB-A; but this is because IJB-A is not challenging enough and the TPIR values have saturated to a 99% mark. For the same measures on the more challenging IJB-B benchmark, our network achieves the best TAR at FAR=1E âˆ’ 5 and FAR=1E âˆ’ 4, and is only lower than a concurrent work <ref type="bibr" target="#b37">[38]</ref> at FAR=1E âˆ’ 3 and FAR=1E âˆ’ 2. Furthermore, our networks produce much smaller template descriptors than the previous stateof-the-art networks (128-D vs 2048-D), making them more useful in real-world <ref type="bibr" target="#b10">[11]</ref> ImNet+CAS 4096 --0.631 0.819 -NAN <ref type="bibr" target="#b39">[40]</ref> priv1 128 --0.881 Â± 0.011 0.941 Â± 0.008 0.978 Â± 0.003 QAN <ref type="bibr" target="#b21">[22]</ref> VF+priv2 ---0.893 Â± 0.039 0.942 Â± 0.015 0.980 Â± 0.006 DREAM <ref type="bibr" target="#b3">[4]</ref> MS ---0.868 Â± 0.015 0.944 Â± 0.009 -SF+R <ref type="bibr" target="#b43">[44]</ref> MS-clean 512-pi --0.932 --MN-vc <ref type="bibr" target="#b38">[39]</ref> VF2 2048 --0.920 Â± 0.013 0.962 Â± 0.005 0.989 Â± 0.002 SE <ref type="bibr" target="#b4">[5]</ref> VF2 2048 --0.904 Â± 0.020 0.958 Â± 0.004 0.985 Â± 0.002 SE <ref type="bibr" target="#b4">[5]</ref> MS+VF2   <ref type="bibr" target="#b43">[44]</ref> (MS-clean), ImageNet <ref type="bibr" target="#b27">[28]</ref> and CASIA WebFace <ref type="bibr" target="#b41">[42]</ref> (ImNet+CAS), and private datasets used by <ref type="bibr" target="#b39">[40]</ref> (priv1) and <ref type="bibr" target="#b21">[22]</ref> (priv2). '512-pi' means that a 512-D descriptor is used per image. '*' denotes the value given by the author. Our best network, SE-GV-4-g1, sets the state-of-the-art by a significant margin on both datasets (except for concurrent work <ref type="bibr" target="#b37">[38]</ref>).</p><formula xml:id="formula_3">Network Training D 1:1 Verification TAR dataset FAR=1E âˆ’ 5 FAR=1E âˆ’ 4 FAR=1E âˆ’ 3 FAR=1E âˆ’ 2 FAR=1E âˆ’ 1 IJB-A Bin</formula><p>applications due to smaller memory requirements and faster template comparisons.</p><p>The results are especially impressive as we only train using VGGFace2 <ref type="bibr" target="#b4">[5]</ref> and beat methods which train with much more data, such as <ref type="bibr" target="#b4">[5]</ref> which combine VGGFace2 and MS-Celeb-1M <ref type="bibr" target="#b9">[10]</ref>, e.g. TAR at FAR=1E âˆ’ 5 of 0.762 vs 0.705 for verification on IJB-B, and TPIR at FPIR=0.01 of 0.776 vs 0.743 for identification on IJB-B. When considering only methods trained on the same data (VGGFace2), our improvement over the state-of-the-art is even larger: TAR at FAR=1E âˆ’5 of 0.762 vs 0.671 for verification on IJB-B, and TPIR at FPIR=0.01 of 0.776 vs 0.706 for verification on IJB-B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Analysis of ghost clusters</head><p>Addition of ghost clusters was motivated by the intuition that it enables our network to learn to ignore uninformative low-quality images by assigning them to the discarded ghost clusters. Here we evaluate this hypothesis qualitatively.</p><p>Recall that GhostVLAD computes a template representation by aggregating residual vectors of input descriptors, where a residual vector is a concatenation of per non-ghost cluster residuals weighted by their non-ghost assignment weights (Sec. 3.2). Therefore, the contribution of a specific example image towards the template representation can be measured as the norm of the residual.  <ref type="bibr" target="#b39">[40]</ref> priv1 128 0.817 Â± 0.041 0.917 Â± 0.009 0.958 Â± 0.005 0.980 Â± 0.005 0.986 Â± 0.003 DREAM <ref type="bibr" target="#b3">[4]</ref> MS ---0.946 Â± 0.011 0.968 Â± 0.010 -SE <ref type="bibr" target="#b4">[5]</ref> VF2 2048 0.847 Â± 0.051 0.930 Â± 0.007 0.981 Â± 0.003 0.994 Â± 0.002 0.996 Â± 0.001 SE <ref type="bibr" target="#b4">[5]</ref> MS+VF2 2048 0.883 Â± 0.038 0.946 Â± 0.004 0.982 Â± 0.004 0.993 Â± 0.002 0.994 Â± 0.001 SE-GV-3 VF2 128 0.872 Â± 0.066 0.951 Â± 0.007 0.979 Â± 0.005 0.990 Â± 0.003 0.992 Â± 0.003 SE-GV-4-g1 VF2 128 0.884 Â± 0.059 0.951 Â± 0.005 0.977 Â± 0.004 0.991 Â± 0.003 0.994 Â± 0.002 IJB-B SE <ref type="bibr" target="#b4">[5]</ref> VF2 2048 0.706 Â± 0.047 0.839 Â± 0.035 0.901 Â± 0.030 0.945 Â± 0.016 0.958 Â± 0.010 SE <ref type="bibr" target="#b4">[5]</ref> MS+VF2 2048 0.743 Â± 0.037 0.863 Â± 0.032 0.902 Â± 0.036 0.946 Â± 0.022 0.959 Â± 0.015 SE-GV-3 VF2 128 0.764 Â± 0.041 0.885 Â± 0.032 0.921 Â± 0.023 0.955 Â± 0.013 0.962 Â± 0.010 SE-GV-4-g1 VF2 128 0.776 Â± 0.030 0.888 Â± 0.029 0.921 Â± 0.020 0.956 Â± 0.013 0.964 Â± 0.010 <ref type="table">Table 4</ref>: Comparison with state-of-the-art for identification on the IJB-A and IJB-B datasets. A higher value of TPIR is better. See caption of Tab. 3 for explanations of abbreviations. Our best network, SE-GV-4-g1, sets the state-of-the-art by a significant margin on both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Verification ROC Identification DET</head><p>(higher is better) (lower is better)  <ref type="figure" target="#fig_1">Figure 4</ref> show that our intuition is correct -the network automatically learns to dramatically down-weight blurry and low-resolution images, thus improving the signal-to-noise ratio. Note that this behaviour emerges completely automatically without ever explicitly teaching the network to down-weight low-quality images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We introduced a neural network architecture and training procedure for learning compact representations of image sets for template-based face recognition. Due to the novel GhostVLAD layer, the network is able to automatically learn to weight face descriptors depending on their information content. Our template representations outperform the state-of-the-art on the challenging IJB-A and IJB-B benchmarks by a large margin.  The network architecture proposed here could also be applied to other imageset tasks such as person re-identification, and set-based retrieval. More generally, the idea of having a 'null' vector available for assignments could have applicability in many situations where it is advantageous to have a mechanism to remove noisy or corrupted data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 :</head><label>3</label><figDesc>Results on the IJB-B dataset. Our SE-GV-4-g1 network which produces 128-D templates, beats the best baseline (SE with 256-D templates) and the state-of-the-art trained on a much larger dataset (SE with 2048-D templates) trained on VGGFace2 and MS-Celeb-1M).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 :</head><label>4</label><figDesc>Effect of ghost clusters. Each row shows shows 6 images from a template in the IJB-B dataset. The contribution (relative to the max) of each image to the final template representation is shown (see Sec. 4.5 for details), for the cases of no ghost clusters (G = 0, network SE-GV-3 ) and one ghost cluster (G = 1, network SE-GV-4-g1 ) in GhostVLAD. Introduction of a single ghost cluster dramatically reduces the contribution of low-quality images to the template, improving the signal-to-noise ratio.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Verification performance on the IJB-B dataset. A higher value of TAR is better. DF is the face descriptor dimension before aggregation. D is the dimensionality of the final template representation. K and G are the number of non-ghost and ghost clusters in GhostVLAD, respectively. 'No. faces' is the number of faces per set used during training. 'Deg.' indicates whether the training images are degraded. All training is done using the VGGFace2 dataset.</figDesc><table><row><cell>Row</cell><cell>Network</cell><cell>DF</cell><cell>D</cell><cell></cell><cell cols="2">K G</cell><cell>No.</cell><cell>Deg.</cell><cell cols="3">1:1 Verification TAR (FAR=)</cell></row><row><cell>id</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>faces</cell><cell></cell><cell cols="3">1E âˆ’ 5 1E âˆ’ 4 1E âˆ’ 3 1E âˆ’ 2</cell></row><row><cell>1</cell><cell>Res [5]</cell><cell cols="3">2014 2048</cell><cell>-</cell><cell>-</cell><cell>1</cell><cell></cell><cell>0.647</cell><cell>0.784</cell><cell>0.878</cell><cell>0.938</cell></row><row><cell>2</cell><cell>Res</cell><cell>128</cell><cell cols="2">128</cell><cell>-</cell><cell>-</cell><cell>1</cell><cell></cell><cell>0.646</cell><cell>0.785</cell><cell>0.890</cell><cell>0.954</cell></row><row><cell>3</cell><cell>SE</cell><cell>128</cell><cell cols="2">128</cell><cell>-</cell><cell>-</cell><cell>1</cell><cell></cell><cell>0.670</cell><cell>0.803</cell><cell>0.896</cell><cell>0.954</cell></row><row><cell>4</cell><cell>SE</cell><cell>256</cell><cell cols="2">256</cell><cell>-</cell><cell>-</cell><cell>1</cell><cell></cell><cell>0.677</cell><cell>0.807</cell><cell>0.892</cell><cell>0.955</cell></row><row><cell>5</cell><cell>SE-2</cell><cell>256</cell><cell cols="2">256</cell><cell>-</cell><cell>-</cell><cell>2</cell><cell></cell><cell>0.679</cell><cell>0.810</cell><cell>0.902</cell><cell>0.958</cell></row><row><cell>6</cell><cell>Res-GV-2</cell><cell>128</cell><cell cols="2">128</cell><cell>8</cell><cell>0</cell><cell>2</cell><cell></cell><cell>0.715</cell><cell>0.835</cell><cell>0.916</cell><cell>0.963</cell></row><row><cell>7</cell><cell>SE-GV-2</cell><cell>128</cell><cell cols="2">128</cell><cell>8</cell><cell>0</cell><cell>2</cell><cell></cell><cell>0.721</cell><cell>0.835</cell><cell>0.916</cell><cell>0.963</cell></row><row><cell>8</cell><cell>SE-GV-2</cell><cell>256</cell><cell cols="2">128</cell><cell>8</cell><cell>0</cell><cell>2</cell><cell></cell><cell>0.685</cell><cell>0.823</cell><cell>0.925</cell><cell>0.963</cell></row><row><cell>9</cell><cell>SE-GV-2</cell><cell>256</cell><cell cols="2">128</cell><cell>8</cell><cell>0</cell><cell>2</cell><cell></cell><cell>0.738</cell><cell>0.850</cell><cell>0.923</cell><cell>0.964</cell></row><row><cell>10</cell><cell>SE-GV-2</cell><cell>256</cell><cell cols="2">128</cell><cell>4</cell><cell>0</cell><cell>2</cell><cell></cell><cell>0.729</cell><cell>0.841</cell><cell>0.914</cell><cell>0.957</cell></row><row><cell>11</cell><cell>SE-GV-2</cell><cell>256</cell><cell cols="2">128</cell><cell>16</cell><cell>0</cell><cell>2</cell><cell></cell><cell>0.722</cell><cell>0.848</cell><cell>0.921</cell><cell>0.964</cell></row><row><cell>12</cell><cell>SE-GV-3</cell><cell>256</cell><cell cols="2">128</cell><cell>8</cell><cell>0</cell><cell>3</cell><cell></cell><cell>0.741</cell><cell>0.853</cell><cell>0.925</cell><cell>0.963</cell></row><row><cell>13</cell><cell>SE-GV-4</cell><cell>256</cell><cell cols="2">128</cell><cell>8</cell><cell>0</cell><cell>4</cell><cell></cell><cell>0.747</cell><cell>0.852</cell><cell>0.922</cell><cell>0.961</cell></row><row><cell>14</cell><cell>SE-GV-3-g1</cell><cell>256</cell><cell cols="2">128</cell><cell>8</cell><cell>1</cell><cell>3</cell><cell></cell><cell>0.753</cell><cell>0.861</cell><cell>0.926</cell><cell>0.963</cell></row><row><cell>15</cell><cell>SE-GV-4-g1</cell><cell>256</cell><cell cols="2">128</cell><cell>8</cell><cell>1</cell><cell>4</cell><cell></cell><cell>0.762</cell><cell>0.863</cell><cell>0.926</cell><cell>0.963</cell></row><row><cell>16</cell><cell>SE-GV-3-g2</cell><cell>256</cell><cell cols="2">128</cell><cell>8</cell><cell>2</cell><cell>3</cell><cell></cell><cell>0.754</cell><cell>0.861</cell><cell>0.926</cell><cell>0.964</cell></row><row><cell>17</cell><cell>SE-GV-4</cell><cell>256</cell><cell cols="2">256</cell><cell>8</cell><cell>0</cell><cell>4</cell><cell></cell><cell>0.713</cell><cell>0.838</cell><cell>0.919</cell><cell>0.963</cell></row><row><cell>18</cell><cell>SE-GV-4-g1</cell><cell>256</cell><cell cols="2">256</cell><cell>8</cell><cell>1</cell><cell>4</cell><cell></cell><cell>0.739</cell><cell>0.853</cell><cell>0.924</cell><cell>0.963</cell></row><row><cell>Row</cell><cell>Network</cell><cell>DF</cell><cell>D</cell><cell cols="4">K G No. Deg.</cell><cell></cell><cell cols="3">1:N Identification TPIR</cell></row><row><cell>id</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>faces</cell><cell></cell><cell cols="2">FPIR= FPIR=</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.01</cell><cell>0.1</cell><cell cols="2">Rank-1 Rank-5 Rank-10</cell></row><row><cell>1</cell><cell>Res [5]</cell><cell cols="3">2048 2048 -</cell><cell>-</cell><cell>1</cell><cell></cell><cell>0.701</cell><cell>0.824</cell><cell>0.886</cell><cell>0.936</cell><cell>0.953</cell></row><row><cell>2</cell><cell>Res</cell><cell cols="2">128 128</cell><cell>-</cell><cell>-</cell><cell>1</cell><cell></cell><cell>0.688</cell><cell>0.833</cell><cell>0.901</cell><cell>0.950</cell><cell>0.963</cell></row><row><cell>3</cell><cell>SE</cell><cell cols="2">128 128</cell><cell>-</cell><cell>-</cell><cell>1</cell><cell></cell><cell>0.712</cell><cell>0.849</cell><cell>0.908</cell><cell>0.949</cell><cell>0.963</cell></row><row><cell>4</cell><cell>SE</cell><cell cols="2">256 256</cell><cell>-</cell><cell>-</cell><cell>1</cell><cell></cell><cell>0.718</cell><cell>0.854</cell><cell>0.908</cell><cell>0.948</cell><cell>0.962</cell></row><row><cell>5</cell><cell>SE-2</cell><cell cols="2">256 256</cell><cell>-</cell><cell>-</cell><cell>2</cell><cell></cell><cell>0.717</cell><cell>0.857</cell><cell>0.909</cell><cell>0.949</cell><cell>0.962</cell></row><row><cell>6</cell><cell>Res-GV-2</cell><cell cols="4">128 128 8 0</cell><cell>2</cell><cell></cell><cell>0.762</cell><cell>0.872</cell><cell>0.917</cell><cell>0.953</cell><cell>0.964</cell></row><row><cell>7</cell><cell>SE-GV-2</cell><cell cols="4">128 128 8 0</cell><cell>2</cell><cell></cell><cell>0.753</cell><cell>0.880</cell><cell>0.917</cell><cell>0.953</cell><cell>0.964</cell></row><row><cell>8</cell><cell>SE-GV-2</cell><cell cols="4">256 128 8 0</cell><cell>2</cell><cell></cell><cell>0.751</cell><cell>0.884</cell><cell>0.912</cell><cell>0.952</cell><cell>0.962</cell></row><row><cell>9</cell><cell>SE-GV-2</cell><cell cols="4">256 128 8 0</cell><cell>2</cell><cell></cell><cell>0.760</cell><cell>0.879</cell><cell>0.918</cell><cell>0.955</cell><cell>0.964</cell></row><row><cell>10</cell><cell>SE-GV-2</cell><cell cols="4">256 128 4 0</cell><cell>2</cell><cell></cell><cell>0.749</cell><cell>0.868</cell><cell>0.914</cell><cell>0.953</cell><cell>0.963</cell></row><row><cell>11</cell><cell>SE-GV-2</cell><cell cols="4">256 128 16 0</cell><cell>2</cell><cell></cell><cell>0.759</cell><cell>0.879</cell><cell>0.918</cell><cell>0.954</cell><cell>0.965</cell></row><row><cell>12</cell><cell>SE-GV-3</cell><cell cols="4">256 128 8 0</cell><cell>3</cell><cell></cell><cell>0.764</cell><cell>0.885</cell><cell>0.921</cell><cell>0.955</cell><cell>0.962</cell></row><row><cell>13</cell><cell>SE-GV-4</cell><cell cols="4">256 128 8 0</cell><cell>4</cell><cell></cell><cell>0.752</cell><cell>0.878</cell><cell>0.914</cell><cell>0.952</cell><cell>0.960</cell></row><row><cell cols="6">14 SE-GV-3-g1 256 128 8 1</cell><cell>3</cell><cell></cell><cell>0.770</cell><cell>0.888</cell><cell>0.923</cell><cell>0.956</cell><cell>0.965</cell></row><row><cell cols="6">15 SE-GV-4-g1 256 128 8 1</cell><cell>4</cell><cell></cell><cell>0.776</cell><cell>0.888</cell><cell>0.921</cell><cell>0.957</cell><cell>0.964</cell></row><row><cell cols="6">16 SE-GV-3-g2 256 128 8 2</cell><cell>3</cell><cell></cell><cell>0.772</cell><cell>0.886</cell><cell>0.922</cell><cell>0.957</cell><cell>0.964</cell></row><row><cell>17</cell><cell>SE-GV-4</cell><cell cols="4">256 256 8 0</cell><cell>4</cell><cell></cell><cell>0.732</cell><cell>0.870</cell><cell>0.912</cell><cell>0.952</cell><cell>0.963</cell></row><row><cell cols="6">18 SE-GV-4-g1 256 256 8 1</cell><cell>4</cell><cell></cell><cell>0.776</cell><cell>0.883</cell><cell>0.921</cell><cell>0.957</cell><cell>0.965</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Identification performance on the IJB-B dataset. A higher value of TPIR is better. See caption of Tab. 1 for the explanations of column titles. Note, for readability standard deviations are not included here, but are included in Tab. 4. 8, since it better matches the test-time scenario which contains images of varying quality.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison with state-of-the-art for verification on the IJB-A and IJB-B datasets. A higher value of TAR is better. D is the dimension of the tem-</figDesc><table><row><cell>plate representation. The training datasets abbreviations are VGGFace [25] (VF), VG-</cell></row><row><cell>GFace2 [5] (VF2), MS-Celeb-1M [10] (MS), a cleaned subset of MS-Celeb-1M refined</cell></row><row><cell>by</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements We thank Weidi Xie for his useful advice, and we thank Li Shen for providing pre-trained networks. This work was funded by an EPSRC studentship and EPSRC Programme Grant Seebibyte EP/M013774/1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bibliography</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An information-theoretic approach to face recognition from face motion manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>ArandjeloviÄ‡</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="639" to="647" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">NetVLAD: CNN architecture for weakly supervised place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>ArandjeloviÄ‡</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gronat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Three things everyone should know to improve object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>ArandjeloviÄ‡</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pose-robust face recognition via deep residual equivariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5187" to="5196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">VGGFace2: A dataset for recognising faces across pose and age</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Autom. Face and Gesture Recog</title>
		<meeting>Int. Conf. Autom. Face and Gesture Recog</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Face recognition based on image sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cevikalp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An end-to-end system for unconstrained face verification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="118" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Template adaptation for face verification and identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Crosswhite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Byrne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stauffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Autom. Face and Gesture Recog</title>
		<meeting>Int. Conf. Autom. Face and Gesture Recog</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">MDLFace: Memorability augmented deep learning for video face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bhardwaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vatsa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Joint Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">MS-Celeb-1M: A dataset and benchmark for large-scale face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pooling faces: template based face recognition with pooled face images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="59" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Projection metric learning on grassmann manifold with application to video based face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="140" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Aggregating local descriptors into a compact image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>JÃ©gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>PÃ©rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Triangulation embedding and democratic aggregation for image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>JÃ©gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The Megaface Benchmark: 1 million faces for recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Boosted manifold principal angles for image set-based recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>ArandjeloviÄ‡</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2475" to="2484" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pushing the frontiers of unconstrained face detection and recognition: IARPA Janus benchmark-A</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Taborsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Grother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1931" to="1939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Video-based face recognition using probabilistic appearance manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Quality aware network for set to set recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5790" to="5799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sphereface: Deep hypersphere embedding for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A compact and discriminative face track descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR. IEEE</title>
		<meeting>CVPR. IEEE</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Large-scale image retrieval with compressed fisher vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>SÃ¡nchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Poirier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning discriminative aggregation network for video-based face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3781" to="3790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deeply learned face representations are sparse, selective, and robust</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2892" to="2900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Deep-Face: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>IEEE CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Statistical computations on grassmann and stiefel manifolds for image and video-based recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Turaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veeraraghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2273" to="2286" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Matconvnet: Convolutional neural networks for matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACMM</title>
		<meeting>ACMM</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Object image retrieval by exploiting online knowledge resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR pp</title>
		<meeting>CVPR pp</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Discriminant analysis on riemannian manifold of gaussian distributions for face recognition with image sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Discriminative covariance oriented representation learning for face recognition with image sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5599" to="5608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Whitelam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Taborsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Maze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IARPA Janus benchmark-B face dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Comparator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multicolumn networks for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Neural aggregation network for video face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4362" to="4371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Face recognition based on regularized nearest points betweenimage sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Autom. Face and Gesture Recog</title>
		<meeting>Int. Conf. Autom. Face and Gesture Recog</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Learning face representation from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1411.7923</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Ring loss: Convex feature normalization for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5089" to="5097" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
