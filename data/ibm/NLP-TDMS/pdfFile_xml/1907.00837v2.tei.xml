<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">XNect: Real-time Multi-Person 3D Motion Capture with a Single RGB Camera</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Saarland Informatics Campus</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Saarland Informatics Campus</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Mueller</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Saarland Informatics Campus</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Saarland Informatics Campus</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elgharib</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Saarland Informatics Campus</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">EPFL</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Saarland Informatics Campus</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">EPFL</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of British Columbia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Saarland Informatics Campus</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Saarland Informatics Campus</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gereon</forename><surname>Fox</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhimitra</forename><surname>Meka</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikhsanul</forename><surname>Habibie</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikram</forename><surname>Singh</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Habermann</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neng</forename><surname>Qian</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayi</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladislav</forename><surname>Golyanik</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Tretschk</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tarun</forename><surname>Yenamandra</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingjie</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Mueller</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elgharib</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="laboratory">MPI For Informatics, Saarland Informatics Campus; Pascal Fua, École Poly-technique Fédérale de Lausanne; Helge Rhodin, University of British Columbia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">XNect: Real-time Multi-Person 3D Motion Capture with a Single RGB Camera</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3386569.3392410</idno>
					<note>This work was funded by the ERC Consolidator Grant 4DRepLy (770784). We thank for helping with data capture and the preparation of video materials. Authors&apos; addresses: This is a pre-print version of the work. The definitive Version of Record was published in ACM Transactions on Graphics, https://</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS Concepts: • Computing methodologies → Motion capture</term>
					<term>Computer vision</term>
					<term>Neural networks Additional Key Words and Phrases: human body pose, motion capture, real-time, monocular, RGB</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Fig. 1</ref><p>. Our real-time monocular RGB based 3D motion capture provides temporally coherent estimates of the full 3D pose of multiple people in the scene, handling occlusions and interactions in general scene settings, and localizing subjects relative to the camera. Our design allows the system to handle large groups of people in the scene with the run-time only minimally affected by the number of people in the scene. Our method yields full skeletal pose in terms of joint angles, which can readily be employed for live character animation. Some images courtesy KNG Music (https://youtu.be/_xCKmEhKQl4), Music Express Magazine (https://youtu.be/kX6xMYlEwLA). 3D characters from Mixamo [Adobe 2020].</p><p>We present a real-time approach for multi-person 3D motion capture at over 30 fps using a single RGB camera. It operates successfully in generic scenes which may contain occlusions by objects and by other people. Our method operates in subsequent stages. The first stage is a convolutional neural network (CNN) that estimates 2D and 3D pose features along with identity assignments for all visible joints of all individuals. We contribute a new architecture for this CNN, called SelecSLS Net, that uses novel selective long and short range skip connections to improve the information flow allowing for a drastically faster network without compromising accuracy. In the second stage, a fullyconnected neural network turns the possibly partial (on account of occlusion) 2D pose and 3D pose features for each subject into a complete 3D pose estimate per individual. The third stage applies space-time skeletal model fitting to the predicted 2D and 3D pose per subject to further reconcile the 2D and 3D pose, and enforce temporal coherence. Our method returns the full skeletal pose in joint angles for each subject. This is a further key distinction from previous work that do not produce joint angle results of a coherent skeleton in real time for multi-person scenes. The proposed system runs on consumer hardware at a previously unseen speed of more than 30 fps given 512x320 images as input while achieving state-of-the-art accuracy, which we will demonstrate on a range of challenging real-world scenes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Optical human motion capture is a key enabling technology in visual computing and related fields <ref type="bibr" target="#b12">[Chai and Hodgins 2005</ref>; Menache 2010; <ref type="bibr" target="#b83">Starck and Hilton 2007]</ref>. For instance, it is widely used to animate virtual avatars and humans in VFX. It is a key component of many man-machine interfaces and is central to biomedical motion analysis. In recent years, computer graphics and computer vision researchers have developed new motion capture algorithms that operate on ever simpler hardware and under far less restrictive constraints than before. These algorithms do not require special body suits, dense camera arrays, in-studio recording, or markers. Instead, they only need a few calibrated cameras to capture people wearing everyday clothes outdoors, e.g. <ref type="bibr" target="#b17">Elhayek et al. [2016]</ref>; <ref type="bibr" target="#b18">Fang et al. [2018]</ref>; <ref type="bibr" target="#b30">Huang et al. [2017a]</ref>; <ref type="bibr" target="#b40">Kanazawa et al. [2018]</ref>; <ref type="bibr" target="#b54">Mehta et al. [2017b]</ref>; <ref type="bibr" target="#b59">Omran et al. [2018]</ref>; <ref type="bibr" target="#b61">Pavlakos et al. [2019]</ref>; <ref type="bibr" target="#b74">Rhodin et al. [2016b]</ref>; <ref type="bibr" target="#b84">Stoll et al. [2011]</ref>; <ref type="bibr" target="#b104">Xiang et al. [2019]</ref>. The latest approaches leverage the power of deep neural networks to capture 3D human pose from a single color image, opening the door to many exciting applications in virtual and augmented reality. Unfortunately, the problem remains extremely challenging due to depth ambiguities, occlusions, and the large variety of appearances and scenes.</p><p>More importantly, most methods fail under occlusions and focus on a single person. Some recent methods instead focus on the egocentric setting <ref type="bibr" target="#b73">[Rhodin et al. 2016a;</ref><ref type="bibr" target="#b96">Tome et al. 2019;</ref>]. Single person tracking in the outside-in setting (non-egocentric) is already hard and starkly under-constrained; multi-person tracking is incomparably harder due to mutliple occlusions, challenging body part to person assignment, and is computationally more demanding. This presents a practical barrier for many applications such as gaming and social VR/AR, which require tracking multiple people from low cost sensors, and in real time.</p><p>Prior work on multi-person pose estimation runs at best at interactive frame rates (10-15 fps) <ref type="bibr" target="#b15">[Dabral et al. 2019;</ref><ref type="bibr" target="#b76">Rogez et al. 2019]</ref> or offline <ref type="bibr">[Moon et al. 2019]</ref>, and produces per-frame joint position estimates which cannot be directly employed in many end applications requiring joint angle based avatar animations.</p><p>We introduce a real-time algorithm for motion capture of multiple people in common interaction scenarios using a single color camera. Our full system produces the skeletal joint angles of multiple people in the scene, along with estimates of 3D localization of the subjects in the scene relative to the camera. Our method operates at more than 30 frames-per-second and delivers state-of-the-art accuracy and temporal stability. Our results are of a similar quality as commercial depth sensing based mocap systems.</p><p>To this end, we propose a new pose formulation and a novel neural network architecture, which jointly enable real-time performance, while handling inter-person and person-object occlusions. A subsequent model-based pose fitting stage produces temporally stable 3D skeletal motions. Our pose formulation uses two deep neural network stages that perform local (per body joint) and global (all body joints) reasoning, respectively. Stage I is fully convolutional and jointly reasons about the 2D and 3D pose for all the subjects in the scene at once, which ensures that the computational cost does not increase with the number of individuals. Since Stage I handles the already complex task of parsing the image for body parts, as well as associating the body parts to identities, our key insight with regards to the pose formulation is to have Stage I only consider body joints for which direct image evidence is available, i.e., joints that are themselves visible or their kinematic parents are visible. This way Stage I does not have to spend representational capacity in hallucinating poses for joints that have no supporting image evidence. For each visible body joint, we predict the 2D part confidence maps, information for associating parts to an individual, and an intermediate 3D pose encoding for the bones that connect at the joint. Thus, the 3D pose encoding is only cognizant of the joint's immediate neighbours (local) in the kinematic chain. A compact fully-connected network forms Stage II, which relies on the intermediate pose encoding and other evidence extracted in the preceding stage, to decode the complete 3D pose. The Stage II network is able to reason about occluded joints using the full body context (global) for each detected subject, and leverages learned pose priors, and the subject 2D and 3D pose evidence. This stage is compact, highly efficient, and acts in parallel for all detected subjects.</p><p>Stage I is the most computationally expensive part of our pipeline, and the main bottleneck in achieving real-time performance.</p><p>We achieve real-time performance by contributing a new convolutional neural network (CNN) architecture in Stage I to speed up the most computationally expensive part of our pipeline. We refer to the new architecture as SelecSLS Net. Our proposed architecture depends on far fewer features than competing ones, such as ResNet-50 <ref type="bibr" target="#b27">[He et al. 2016]</ref>, without any accuracy loss thanks to our insights on selective use of short and long range concatenation-skip connections. This enables fast inference on the complete input frame, without the added pre-or post-processing complexity of a separate bounding box tracker for each subject. Further, the compactness of our Stage II network, which reconciles the partially incomplete 2D pose and 3D pose encoding to a full body pose estimate, enables it to simultaneously handle many people with minimal overhead on top of Stage I. Additionally, we fit a model-based skeleton to the 3D and 2D predictions in order to satisfy kinematic constraints and reconcile the 2D and 3D predictions across time. This produces temporally stable predictions, with skeletal angle estimates, which can readily drive virtual characters.</p><p>In summary, our technical innovations and new design insights at the individual stages, as well as our insights guiding the proposed multi-stage design enable our final contribution: a complete algorithm for multi-people 3D motion capture from a single camera that achieves real-time performance without sacrificing reliability or accuracy. The run time of our system only mildly depends on the number of subjects in the scene, and even crowded scenes can be tracked at high frame rates. We demonstrate our system's performance on a variety of challenging multi-person scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>We focus our discussion on relevant 2D and 3D human pose estimation from monocular RGB methods, in both single-and multi-person scenarios-for overview articles refer to <ref type="bibr" target="#b81">Sarafianos et al. [2016]</ref>; <ref type="bibr" target="#b103">Xia et al. [2017]</ref>. We also discuss prior datasets, and neural network architectures that inspired ours.</p><p>Multi-Person 2D Pose Estimation: Multi-person 2D pose estimation methods can be divided into bottom-up and top-down approaches. Top-down approaches first detect individuals in a scene and fall back to single-person 2D pose approaches or variants for pose estimation <ref type="bibr" target="#b20">[Gkioxari et al. 2014;</ref><ref type="bibr" target="#b34">Iqbal and Gall 2016;</ref><ref type="bibr" target="#b60">Papandreou et al. 2017;</ref><ref type="bibr" target="#b66">Pishchulin et al. 2012;</ref><ref type="bibr" target="#b86">Sun and Savarese 2011]</ref>. Reliable detection of individuals under significant occlusion, and tracking of people through occlusions remains challenging.</p><p>Bottom-up approaches instead first localize the body parts of all subjects and associate them to individuals in a second step. Associations can be obtained by predicting joint locations and their identity embeddings together <ref type="bibr" target="#b57">[Newell and Deng 2017]</ref>, or by solving a graph cut problem <ref type="bibr" target="#b32">[Insafutdinov et al. 2017;</ref>]. This involves solving an NP-hard integer linear program which easily takes hours per image. The work of <ref type="bibr" target="#b32">Insafutdinov et al. [2017]</ref> improves over  by including image-based pairwise terms and stronger detectors based on ResNet <ref type="bibr" target="#b27">[He et al. 2016]</ref>. This way reconstruction time reduces to several minutes per frame. <ref type="bibr" target="#b10">Cao et al. [2017]</ref> predict joint locations and part affinities (PAFs), which are 2D vectors linking each joint to its parent. PAFs allow quick and greedy part association, enabling real time mutli-person 2D pose estimation. Our Stage I uses similar ideas to localize and assign joints in 2D, but we also predict an intermediate 3D pose encoding per joint which enables our subsequent stage to produce accurate 3D body pose estimates. <ref type="bibr" target="#b23">Güler et al. [2018]</ref> compute dense correspondences from pixels to the surface of SMPL <ref type="bibr">[2015]</ref>, but they do not estimate 3D pose.</p><p>Single-Person 3D Pose Estimation: Monocular single person 3D pose estimation was previously approached with generative methods using physics priors <ref type="bibr" target="#b102">[Wei and Chai 2010]</ref>, or semi-automatic analysis-by-synthesis fitting of parametric body models <ref type="bibr" target="#b21">[Guan et al. 2009;</ref><ref type="bibr" target="#b35">Jain et al. 2010]</ref>. Recently, methods employing CNN based learning approaches led to important progress <ref type="bibr" target="#b33">[Ionescu et al. 2014;</ref><ref type="bibr" target="#b47">Li and Chan 2014;</ref><ref type="bibr" target="#b63">Pavlakos et al. 2017;</ref><ref type="bibr" target="#b82">Sigal et al. 2010;</ref><ref type="bibr" target="#b94">Tekin et al. 2016</ref>]. These methods can broadly be classified into direct regression and 'lifting' based approaches. Regressing straight from the image requires large amounts of 3D-pose labelled images, which are difficult to obtain. Therefore, existing datasets are captured in studio scenarios with limited pose and appearance diversity <ref type="bibr" target="#b33">[Ionescu et al. 2014]</ref>, or combine real and synthetic imagery <ref type="bibr" target="#b13">[Chen et al. 2016]</ref>. Consequently, to address the 3D data scarcity, transfer learning using features learned on 2D pose datasets has been applied to improve 3D pose estimation <ref type="bibr">[Mehta et al. 2017a,b;</ref><ref type="bibr" target="#b68">Popa et al. 2017;</ref><ref type="bibr" target="#b95">Tekin et al. 2017;</ref>.</p><p>'Lifting' based approaches predict the 3D pose from a separately detected 2D pose <ref type="bibr" target="#b52">[Martinez et al. 2017</ref>]. This has the advantages that 2D pose datasets are easier to obtain in natural environments, and the lifting can be learned from MoCap data without overfitting on the studio conditions. While this establishes a surprisingly strong baseline, lifting is ill-posed and body-part depth disambiguation is often not possible without additional cues from the image. Other work has proposed to augment the 2D pose with relative depth ordering of body joints as additional context to disambiguate 2D to 3D lifting <ref type="bibr" target="#b62">[Pavlakos et al. 2018a;</ref><ref type="bibr" target="#b67">Pons-Moll et al. 2014]</ref>. Our approach can be seen as a hybrid of regression and lifting methods: An encoding of the 3D pose of the visible joints is regressed directly from the image (Stage I), with each joint only reasoning about its immediate kinematic neighbours (local context). This encoding, along with 2D joint detection confidences augments the 2D pose and is 'decoded' into a complete 3D body pose by Stage II reasoning about all body joints (global context).</p><p>Some recent methods integrate a 3D body model <ref type="bibr" target="#b50">[Loper et al. 2015</ref>] within a network, and train using a mixture of 2D poses and 3D poses to predict 3D pose and shape from single images <ref type="bibr" target="#b40">[Kanazawa et al. 2018;</ref><ref type="bibr" target="#b59">Omran et al. 2018;</ref><ref type="bibr" target="#b64">Pavlakos et al. 2018b;</ref><ref type="bibr" target="#b98">Tung et al. 2017]</ref>. Other approaches optimize a body model or a template <ref type="bibr" target="#b25">[Habermann et al. 2019;</ref>] to fit 2D poses or/and silhouettes <ref type="bibr" target="#b1">[Alldieck et al. 2019</ref><ref type="bibr" target="#b2">[Alldieck et al. , 2018a</ref><ref type="bibr" target="#b7">Bogo et al. 2016;</ref><ref type="bibr" target="#b22">Guler and Kokkinos 2019;</ref><ref type="bibr">Kolotouros et al. 2019b,a;</ref><ref type="bibr" target="#b46">Lassner et al. 2017]</ref>. Very few are able to work in real time, and none of them handles multiple people.</p><p>Prior real-time 3D pose estimation approaches <ref type="bibr" target="#b54">[Mehta et al. 2017b</ref>] designed for single-person scenarios fail in multi-person scenarios.</p><p>Recent offline single-person approaches <ref type="bibr" target="#b41">[Kanazawa et al. 2019]</ref> produce temporally coherent sequences of SMPL <ref type="bibr">[2015]</ref> parameters, but work only for unoccluded single-person scenarios. In contrast, our proposed approach runs in real time for multi-person scenarios, and produces temporally coherent joint angle estimates at par with offline approaches, while successfully handling object and inter-person occlusions.</p><p>Multi-Person 3D Pose: Earlier work on monocular multi-person 3D pose capture often followed a generative formulation, e.g. estimating 3D body and camera pose from 2D landmarks using a learned pose space <ref type="bibr" target="#b69">[Ramakrishna et al. 2012]</ref>. We draw inspiration from and improve over limitations of recent deep learning-based methods. <ref type="bibr" target="#b75">Rogez et al. [2017]</ref> use a Faster- <ref type="bibr">RCNN [2015]</ref> based approach and first find representative poses of discrete pose clusters that are subsequently refined. The LCRNet++ implementation of this algorithm uses a ResNet-50 base network and achieves non-real-time interactive 10−12fps on consumer hardware even with the faster but less accurate 'demo' version that uses fewer anchor poses. <ref type="bibr" target="#b15">Dabral et al. [2019]</ref> use a similar Faster-RCNN based approach, and predict 2D keypoints for each subject. Subsequently, the predicted 2D keypoints are lifted to 3D pose. We show that incorporating additional information, such as the keypoint confidence, and 3D pose encodings in the 'lifting' step results in a much higher prediction accuracy. <ref type="bibr">Moon et al. [2019]</ref> employ a prior person detection step, and pass resized image crops of each detected subject to the pose estimation network. As prior work <ref type="bibr" target="#b10">[Cao et al. 2017]</ref> has shown, such an approach results high pose estimation accuracy, but comes at the cost of a significant increase in inference time. Not only does such an approach work at offline rates, the per-frame inference time scales with the number of subjects in the scene, making it unsuitable for real-time applications.</p><p>The aforementioned detection based approaches predict multiple proposals per individual and fuse them afterwards. This is time consuming, and in many cases it can either incorrectly merge nearby individuals with similar poses, or fail to merge multiple proposals for the same individual. Beyond the cost and potential errors from fusing pose estimates, multiple detections of the same subject further increase the inference time for the approach of Moon et al. <ref type="bibr">[2019]</ref>.</p><p>Our approach, being bottom-up, does not produce multiple detections per subject. The bottom-up approach of <ref type="bibr" target="#b54">Mehta et al. [2018b]</ref> predicts the 2D and 3D pose of all individuals in the scene using a fixed number of feature maps, which jointly encode for any number of individuals in the scene. This introduces potential conflicts when subjects overlap, for which a complex encoding and read-out scheme is introduced. The 3D encoding treats each limb and the torso as distinct objects, and encodes the 3D pose of each 'object' in the feature maps at the pixel locations corresponding to the 2D joints of the 'object'. The encoding can thus handle partial inter-personal occlusion by dissimilar body parts. Unfortunately, the approach still fails when similar body parts of different subjects overlap. Similarly, <ref type="bibr" target="#b111">Zanfir et al. [2018b]</ref> jointly encode the 2D and 3D pose of all subjects in the scene using a fixed number of feature maps. Different from <ref type="bibr" target="#b54">Mehta et al. [2018b]</ref>, they encode the full 3D pose vector at all the projected pixels of the skeleton, and not just at the body joint locations, which makes the 3D feature space rife with potential encoding conflicts. For association, they learn a function to evaluate limb grouping proposals. A 3D pose decoding stage extracts 3D pose features per limb and uses <ref type="bibr">Fig. 2</ref>. Overview: Computation is separated into three stages, the first two respectively performing per-frame local (per body joint) and global (all body joints) reasoning, and the third performing temporal reasoning across frames: Stage I infers 2D pose and intermediate 3D pose encoding for visible body joints, using a new SelecSLS Net architecture. The 3D pose encoding for each joint only considers local context in the kinematic chain. Stage II is a compact fully-connected network that runs in parallel for each detected person, and reconstructs the complete 3D pose, including occluded joints, by leveraging global (full body) context. Stage III provides temporal stability, localization relative to the camera, and a joint angle parameterization through kinematic skeleton fitting.</p><p>an attention mechanism to combine these into a 3D pose prediction for the limb.</p><p>One of our key contributions is to only reason about body joints for which there is direct image evidence available, i.e., the joint itself, or its parent/child is visible. A subsequent compact fully-connected network can decode this potentially incomplete information to the full 3D pose. Such a hybrid of image-to-pose regression and 2D-to-3D lifting helps overcome the limitations of the individual approaches. The 3D pose encodings are a strong cue for the 3D pose in the absence of conflicts, whereas the global context in Stage II and the 2D pose help resolve the very-limited conflicts when they occur, and use learned pose priors to fill in the missing body joints. In contrast, the encodings of <ref type="bibr" target="#b111">Zanfir et al. [2018b]</ref> and <ref type="bibr" target="#b54">Mehta et al. [2018b]</ref> encode the pose for all joints in the full body or limb-wise, respectively, regardless of available image evidence for each joint, making the already difficult task more difficult, and having several avenues of potential encoding conflicts. Furthermore, we impose kinematic constraints with a model based fitting stage, which also allows for temporal smoothness. The approach of <ref type="bibr" target="#b110">Zanfir et al. [2018a]</ref> also combines learning and optimization, but their space-time optimization over all frames is not real-time.</p><p>Different from prior approaches, our approach works in real-time at 25 − 30 fps using a single consumer GPU, yielding skeletal joint angles and camera relative positioning of the subject, which can be readily be used to control animated characters in a virtual environment. Our approach predicts the complete body pose even under significant person-object occlusions, and is more robust to interpersonal occlusions.</p><p>3D Pose Datasets: There exist many datasets with 3D pose annotations in single-person scenarios <ref type="bibr" target="#b33">[Ionescu et al. 2014;</ref><ref type="bibr" target="#b53">Mehta et al. 2017a;</ref><ref type="bibr" target="#b82">Sigal et al. 2010;</ref><ref type="bibr" target="#b97">Trumble et al. 2017;</ref><ref type="bibr" target="#b100">von Marcard et al. 2016]</ref> or multi-person with only 2D pose annotations <ref type="bibr" target="#b4">[Andriluka et al. 2014;</ref><ref type="bibr" target="#b49">Lin et al. 2014]</ref>. As multi-person 3D pose estimation started to receive more attention, datasets such as MarCOnI <ref type="bibr" target="#b17">[Elhayek et al. 2016</ref>] with a lower number of scenes and subjects, and the more diverse Panoptic <ref type="bibr" target="#b39">[Hanbyul Joo and Sheikh 2015]</ref> and MuCo-3DHP <ref type="bibr" target="#b54">[Mehta et al. 2018b</ref>] datasets have come about. LCRNet <ref type="bibr" target="#b75">[Rogez et al. 2017</ref>] uses 2D to 3D lifting to create pseudo annotations on the MPII 2D pose dataset <ref type="bibr" target="#b4">[Andriluka et al. 2014]</ref>, and LCRNet++ <ref type="bibr" target="#b76">[Rogez et al. 2019</ref>] uses synthetic renderings of humans from a multitude of single person datasets.</p><p>Recently, the 3D Poses in the Wild (3DPW) dataset <ref type="bibr" target="#b99">[von Marcard et al. 2018]</ref> features multiple people outdoors recorded with a moving camera and includes ground truth 3D pose. The number of subjects with ground truth pose is however limited. To obtain more variation in training, we use the recently published MuCo-3DHP <ref type="bibr" target="#b54">[Mehta et al. 2018b]</ref>, which is a multi-person training set of composited real images with 3D pose annotations from the single person MPI-INF-3DHP [2017a] dataset.</p><p>Convolutional Network Designs: ResNet <ref type="bibr" target="#b27">[He et al. 2016</ref>] and derivatives <ref type="bibr" target="#b105">[Xie et al. 2017</ref>] incorporate explicit information flowing from earlier to later feature layers in the network through summationskip connections. This permits training of deeper and more powerful networks. Many architectures based on this concept have been proposed, such as Inception <ref type="bibr" target="#b90">[Szegedy et al. 2017]</ref> and ResNext <ref type="bibr" target="#b105">[Xie et al. 2017]</ref>.</p><p>Because increased depth and performance comes at the price of higher computation times during inference, as well as a higher number of parameters, specialized architectures for faster test time computation were proposed, such as AmoebaNet <ref type="bibr" target="#b71">[Real et al. 2019]</ref>, Mo-bileNet <ref type="bibr" target="#b28">[Howard et al. 2019;</ref><ref type="bibr" target="#b80">Sandler et al. 2018]</ref>, ESPNet <ref type="bibr" target="#b55">[Mehta et al. 2018a]</ref>, ERFNet <ref type="bibr" target="#b77">[Romera et al. 2018]</ref>, EfficientNet <ref type="bibr" target="#b93">[Tan and Le 2019]</ref>, as well as architectures such as SqueezeNet <ref type="bibr" target="#b31">[Iandola et al. 2016</ref>] that target parameter efficiency. These are however not suited for our use case for various reasons: Many architectures with depthwise convolutions are optimized for inference on specific edge devices <ref type="bibr" target="#b80">[Sandler et al. 2018]</ref>, and lose accuracy in lieu of speed. Increasing the width or depth of these networks <ref type="bibr" target="#b28">[Howard et al. 2019;</ref><ref type="bibr" target="#b93">Tan and Le 2019]</ref> to bring the accuracy closer to that of vanilla ResNets results in GPU runtimes comparable to typical ResNet architectures. ESPNet uses hierarchical feature fusion but produces non-smooth output maps with grid artifacts due to the use of dilated convolutions. These artifacts impair part association performance in our pose estimation setting. ShuffleNet <ref type="bibr" target="#b51">[Ma et al. 2018;</ref>] use group convolutions and depthwise convolutions, and shuffle the channels between layers to promote information flow between channel groups. DenseNet <ref type="bibr" target="#b29">[Huang et al. 2017b</ref>] uses full dense concatenation-skip connectivity, which results in a parameter efficient network but is slow due to the associated memory cost of the enormous number of concatenation operations. Recent work has also proposed highly computation-efficient networks <ref type="bibr" target="#b85">[Sun et al. 2019a;</ref> which maintain high to low resolution feature representations throughout the network, and do not lose accuracy in lieu of computational efficiency. However, the theoretical computational efficiency does not translate to computational speed in practice, and the models are up to twice as slow as ResNet networks for a given accuracy level. Approaches targeting parameter efficiency often do not result in computational speedups because either the computational cost is still high <ref type="bibr" target="#b31">[Iandola et al. 2016]</ref>, or the non-structured sparse operations resulting from weight pruning <ref type="bibr" target="#b19">[Frankle and Carbin 2018]</ref> cannot be executed efficiently on current hardware.</p><p>The key novel insight behind our proposed CNN architecture is the use of selective long-range and short-range concatenation-skip connections rather than the dense connectivity pattern of DenseNet. This results in a network significantly faster than ResNet-50 while retaining the same level of accuracy, avoids the artifacts and accuracy deficit of ESPNet, and eliminates the memory (and hence speed) bottlenecks associated with DenseNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD OVERVIEW</head><p>The input to our method is a live video feed, i.e., a stream of monocular color frames showing a multi-person scene. Our method has three subsequent stages, as shown in <ref type="figure">Fig. 2</ref>. In Section 4, we discuss the first two stages, which together produce 2D and 3D pose estimates per frame.</p><p>Stage I uses a convolutional neural network to process the complete input frame, jointly handling all subjects in the scene. The Stage I CNN predicts 2D body joint heatmaps, Part Affinity Fields to associate joints to individuals in the scene, and an intermediate 3D pose encoding per detected joint. After grouping the 2D joint detections from the first stage into individuals following the approach of <ref type="bibr" target="#b10">[Cao et al. 2017</ref>], 3D pose encodings per individual are extracted at the pixel locations of the visible joints and are input to the second stage together with the 2D locations and detection confidences of the individual's joints. Stage I only reasons about visible body joints, and the 3D pose encoding per joint only captures the joint's pose relative to its immediate kinematic neighbours. The 3D pose encoding is discussed in Section 4.1.2.</p><p>Stage II, which we discuss in Section 4.2, uses a lightweight fullyconnected neural network that 'decodes' the input from the previous stage into a full 3D pose, i.e. root-relative 3D joint positions for visible and occluded joints, per individual. This network incorporates 2D pose and 3D pose encoding evidence over all visible joints and an implicitly learned prior on 3D pose structure, which allows it to reason about occluded joints and correct any 3D pose encoding conflicts. A further advantage of a separate stage for full 3D pose reasoning is that it allows the use of a body joint set different from that used for training Stage I. In our system, 3D pose inference of Stage I and Stage II can be parallelized on a GPU, with negligible dependence of inference time on the number of subjects.</p><p>Stage III, discussed in Section 5, performs sequential model fitting on the live stream of 2D and 3D predictions from the previous stages. A kinematic skeleton is fit to the history of per-frame 2D and rootrelative 3D pose predictions to obtain temporally coherent motion capture results. We also track person identity, full skeletal joint angles, and the camera relative localization of each subject in real time.</p><p>Our algorithm and pose representation applies to any CNN architecture suitable for keypoint prediction. However, to enable fast inference on typical consumer-grade GPUs, we propose the novel SelecSLS Net architecture for the backbone of Stage I CNN. It employs selective long and short range concatenation-skip connections to promote information flow across network layers which allows to use fewer features and have a much smaller memory footprint leading to a much faster inference time but comparable accuracy in comparison to ResNet-50. We discuss our contributions in that regard separately in Section 6.</p><p>In Section 7, we present ablation and comparison studies, both quantitative and qualitative, and show applications to animated character control.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PER-FRAME POSE ESTIMATION: STAGE I &amp; STAGE II</head><p>Given an image I of dimensions w × h pixels, we seek to estimate the 3D pose {P 3D k } K k =1 of the unknown number of K individuals in the scene. P 3D k ∈ R 3×J represents the root (pelvis)-relative 3D coordinates of the J body joints. The task is implemented in the first two stages of our algorithm, which we detail in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Stage I Prediction</head><p>Our first stage uses a CNN that features an initial core (or backbone) network that splits into two separate branches for 2D pose prediction and 3D pose encoding, as shown in <ref type="figure">Figure 2</ref>. The core network outputs features at w 16 × h 16 pixel spatial resolution, and uses our new proposed network design that offers a high accuracy at high runtime efficiency, which we detail in Section 6. The outputs of each of the 2D and 3D branches are at w 8 × h 8 pixels spatial resolution. The 3D pose branch also makes use of features from the 2D pose branch. We explain both branches and the Stage I network training in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">2D Branch: 2D Pose Prediction and Part</head><formula xml:id="formula_0">Association. 2D pose is predicted as 2D heatmaps H = {H j ∈ R w 8 × h 8 } J j=1 ,</formula><p>where each map represents the per-pixel confidence of the presence of body joint type j jointly for all subjects in the scene. Similar to <ref type="bibr" target="#b10">[Cao et al. 2017]</ref>, we use Part Affinity fields</p><formula xml:id="formula_1">F = {F j ∈ R w 8 × h 8 ×2 } J j=1</formula><p>to encode body joint ownership using a unit vector field that points from a joint to its kinematic parent, and spans the width of the respective limb. For an input image, these Part Affinity Fields can be used to detect the individuals present in the scene and the visible body joints, and to associate visible joints to individuals. If the neck joint (which we hypothesize is visible in most situations) of an individual is not detected, we discard that individual entirely from the subsequent stages. For K detected individuals, this stage outputs the 2D body joint locations in absolute image coordinates P 2D k ∈ Z 2×J + . Further, we get an estimate of the detection confidence c j,k of each body part j and person k from the heatmap maximum. </p><formula xml:id="formula_2">L = {L j ∈ R w 8 × h 8 ×3 } J j=1 .</formula><p>The encoding at the spatial location of each visible joint only encapsulates its 3D pose relative to the joints to which it directly connects in the kinematic chain.</p><p>The general idea of such an encoding map is inspired by the approaches of <ref type="bibr" target="#b54">Mehta et al. [2017b]</ref>, <ref type="bibr" target="#b63">Pavlakos et al. [2017]</ref> which represent the 3D pose information of joints in output maps at the spatial locations of the 2D detections of the respective joints.</p><p>Our specific encoding in L works as follows: Consider the 1×1×(3· J ) vector l j,k extracted at the pixel location (u,v) j,k from the 3D output maps L. Here (u,v) j,k is the location of body joint j of individual k. This 1×1×(3· J ) feature vector is of the dimensions of the full 3D body pose, where the kinematic parent-relative 3D locations of each joint reside in separate channels. Importantly however, and in contrast to previous work <ref type="bibr" target="#b54">[Mehta et al. 2018b;</ref><ref type="bibr" target="#b111">Zanfir et al. 2018b</ref>], instead of encoding the full 3D body pose, or per-limb pose, at each 2D detection location (u,v) j,k , we only encode the pose of the corresponding joint (relative to its parent) and the pose of its children (relative to itself). In other words, at each joint location (u,v) j,k , we restrict the supervision of the encoding vector l j,k to the subset of channels corresponding to the bones that meet at joint j, parent-to-joint and joint-to-child in the kinematic chain. We will refer to this as channelsparse supervision of {l j,k } J j=1 , and emphasize the distinction from channel-dense supervision. <ref type="figure" target="#fig_3">Figure 4</ref> shows examples for head, neck and right shoulder. Consequently, 3D pose information for all the visible joints of all subjects is still encoded in L, albeit in a spatially distributed manner, and each 2D joint location (u,v) j,k is used to extract its corresponding 3D bones of subject k. Our motivation for such a pose encoding is that the task of parsing in-the-wild images to detect 2D body part heatmaps under occlusion and clutter, as well as grouping the body parts with their respective person identities under inter-personal interaction and overlap is already challenging. Reasoning about the full 3D pose, including occluded body parts, adds further complexity, which not only requires increased representation capacity (thus increasing the inference cost), but also more labelled training data, which is scarce for multi-person 3D pose. The design of our formulation responds to both of these challenges. Supervising only the 3D bones corresponding to each visible joint ensures that mostly local image evidence is used for prediction, where the full body context is already captured by the detected 2D pose. For instance, it should be possible to infer the kinematic-parent relative pose of the upper arm and the fore arm by looking at the region centered at the elbow. This means better generalization and less risk to overfit to dataset specific long-range correlations.</p><p>Further, our use of channel-sparse (joint-type-dependent) supervision of l j,k is motivated by the fact that convolutional feature maps cannot contain sharp transitions <ref type="bibr" target="#b54">[Mehta et al. 2018b</ref>]. In consequence, full poses of two immediately nearby people are hard to encode. E.g., the wrist of one person being in close proximity in the image plane to the shoulder of another person would require the full pose of two different individuals to be encoded in possibly adjacent pixel locations in the output map. Such encoding conflicts often lead to failures of previous methods, as shown in <ref type="figure" target="#fig_4">Figure 5</ref> in the supplemental document. In contrast, our encoding in L does not lead to encoding conflicts when different joints of separate individuals are in spatial proximity or even overlap in the image plane, because supervision is restricted to the channels corresponding to the body joint type. Consequently, our target output maps are smoother without sharp transitions, and more suitable for representation by CNN outpus. In Section 7.5 we show the efficacy of channel-sparse supervision for {l j,k } J j=1 over channel-dense supervision across various 2D and 3D pose benchmarks. Importantly, unlike <ref type="bibr" target="#b111">Zanfir et al. [2018b]</ref> and <ref type="bibr" target="#b54">Mehta et al. [2018b]</ref>, the 2D pose information is not discarded, and is utilized as additional relevant information for 3D pose inference in Stage II, allowing for a compact and fast network. This makes it more suited for a real-time system than, for instance, the attention-mechanism-based inference scheme of <ref type="bibr" target="#b111">Zanfir et al. [2018b]</ref>.</p><p>For each individual k, the 2D pose P 2D k , joint confidences {c j,k } J j=1 , and 3D pose encodings {l j,k } J j=1 at the visible joints are extracted and input to Stage II (Sec. 4.2). Stage II uses a fully-connected decoding network that leverages the full body context that is available to it, to give the complete 3D pose with the occluding joints filled in. We provide details of Stage II in Section 4.2.  The supervision for the 1 × 1 × (3 · J ) predicted 3D pose encoding vector l j (shown on the right) at each joint j (shown on the skeleton on the left) is dependent on the type of the joint. l j only encodes the 3D pose information of joint j relative to the joints it is directly connected to in the kinematic chain. This results in a channel-sparse supervision pattern as shown here, as opposed to each l j encoding the full body pose. The regions marked purple are not supervised and the network is free to predict any values there. For example, 'right shoulder' (j = 3) connects to joints j = 2 and j = 4, so in l 3 the pose of j = 3 relative to j = 2 (x 3 ,y 3 ,z 3 ), and the pose of j = 4 relative to j = 3 (x 4 ,y 4 ,z 4 ) are supervised. See Section 4.1.2.</p><p>for single person 2D pose estimation on the MPII <ref type="bibr" target="#b4">[Andriluka et al. 2014]</ref> and LSP <ref type="bibr">Everingham 2010, 2011</ref>] single person 2D datasets. Then, using these weights as initialization, it is trained for multi-person 2D pose estimation on MS-COCO <ref type="bibr" target="#b49">[Lin et al. 2014</ref>]. Subsequently the 3D pose branch is added and the two branches are individually trained on crops from MS-COCO and MuCo-3DHP <ref type="bibr" target="#b54">[Mehta et al. 2018b]</ref>, with the core network seeing gradients from both datasets via the two branches. Additionally, the 2D pose branch sees supervision from MuCo-3DHP dataset via heatmaps of the common minimum joint set between MS-COCO and MuCo-3DHP. We found that the pretraining on multi-person 2D pose data before introducing the 3D branch is important.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Stage II Prediction</head><p>Stage II uses a lightweight fully-connected network to predict the root-relative 3D joint positions {P 3D k } K k =1 for each individual considered visible after Stage I. Before feeding the output from Stage I as input, we convert the 2D joint position predictions P 2D k to a representation relative to the neck joint. For each individual k, at each detected joint location, we extract the 1×1×(3· J ) 3D pose encoding vector l j,k , as explained in the preceding section. The input to Stage</p><formula xml:id="formula_3">II, S k ∈ R J ×(3+3·J )</formula><p>, is the concatenation of the neck relative (u,v) j,k coordinates of the joint, the joint detection confidence c j,k and the feature vector l j,k , for each joint j. If the joint is not visible, we instead concatenate zero vectors of appropriate dimensions (see <ref type="figure" target="#fig_0">Figure 3</ref>). Stage II comprises a 5-layer fully-connected network, which converts S k to a root-relative 3D pose estimate P 3D k (see <ref type="figure" target="#fig_4">Figure 5</ref>). We emphasize that unlike <ref type="bibr" target="#b54">Mehta et al. [2018b]</ref>, we have a much sparser pose encoding l j,k , and further only use it as a feature vector and not directly as the body part's pose estimate because jointly encoding body parts of all individuals in the same feature volume may result in corrupted predictions in the case of conflicts-same parts of different individuals in close proximity. Providing the 2D joint positions and part confidences along with the feature vectors as input to the Stage II network allows it to correct any conflicts that may arise. See <ref type="figure" target="#fig_4">Figure 5</ref> in the supplemental document for a visual comparison of results against <ref type="bibr" target="#b54">[Mehta et al. 2018b</ref>].</p><p>The inference time for Stage II with a batch size of 10 is 1.6ms on an Nvidia K80, and 1.1ms on a TitanX (Pascal).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Stage II Training.</head><p>The Stage II network is trained on uncropped frames from MuCo-3DHP <ref type="bibr" target="#b54">[Mehta et al. 2018b]</ref>. We run Stage I on these frames and extract the 2D pose and 3D pose encodings. Then for each detected individual, we use the ground-truth root-relative 3D pose as the supervision target for {(X j ,Y j ,Z j )} J j=1 . Since the pose prediction can be drastically different from the ground truth when there are severe occlusions, we use the smooth-L1 <ref type="bibr" target="#b72">[Ren et al. 2015]</ref> loss to mitigate the effect of such outliers. In addition to providing an opportunity to reconcile the 3D pose predictions with the 2D pose, another advantage of a second stage trained separately from the first stage is that the output joint set can be made different from the joint set used for Stage I, depending on which dataset was used for training Stage II (joint sets typically differ across datasets). In our case, though there are no 2D predictions for foot tip, the 3D pose encoding for ankle encodes information about the foot tip, which is used in Stage II to produce 3D predictions for foot tips.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">SEQUENTIAL MOTION CAPTURE: STAGE III</head><p>After Stage I and Stage II we have per-frame root-relative pose estimates for each individual. However, we have no estimates of person size or metric distance from the camera, person identities are not tracked across frames, and reconstructions are not in terms of joint angles. To remedy this, we infer and track person appearance over time, optionally infer absolute height from ground plane geometry, and fuse 2D and 3D predictions with temporal smoothness and joint limit constraints in a space-time kinematic pose fitting method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Identity Tracking and Re-identification</head><p>To distinguish poses estimated at distinct frames, we extend the previous pose notation with temporal indices in square brackets. So far, per-frame 2D and 3D poses have been estimated for the current and past frames. We need a fast method that maintains identity of a detected person across frames and re-identifies it after a period of full occlusion. To this end, we assign correspondences between person detections at the current timestep t,</p><formula xml:id="formula_4">{P i [t]} K [t ] i=1 , to the pre- ceding ones {P k [t − 1]} K [t −1]</formula><p>k =1 . We model and keep track of person appearance with an HSV color histogram of the upper body region. We discretize the hue and saturation channels into 30 bins each and determine the appearance A i[t ] as the class probabilities across the bounding box enclosing the torso joints in {P 2D i [t]} i . This descriptor is efficient to compute and can model loose and tight clothing alike, but might suffer from color ambiguities across similarly dressed subjects. Other, more complex identity tracking methods can be used instead when real-time performance is not needed, such as when processing pre-recorded sequences.</p><p>To be able to match subjects robustly, we assign current detections to previously known identities not only based on appearance similarity,</p><formula xml:id="formula_5">S A i,k = (A i [t]−A k [t −1]) 2 , but also on the 2D pose similarity S P 2D i,k (i,k) = (P 2D i[t ] − P 2D k [t −1] ) 2 and 3D pose similarity S P 3D i,k (i,k) = (P 3D i[t ] −P 3D k [t −1] ) 2 .</formula><p>A threshold on the dissimilarity is set to detect occlusions, persons leaving the field of view, and new persons entering. That means the number of persons K[t] can change. Person identities are maintained for a certain number of frames after disappearance, to allow for re-identification after momentary occlusions such as those caused by the tracked subjects passing behind an occluder. We update the appearance histogram of known subjects at arrival time and every 30 seconds to account for appearance changes such as varying illumination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Relative Bone Length and Absolute Height Calculation</head><p>Relative bone length between body parts is a scale-invariant property that is readily estimated by P 3D k in Stage II. To increase robustness, we take the normalized skeleton bone lengths b k as the distance between linked joints in P 3D k averaged across the first 10 frames. Translating relative pose estimates from pixel coordinates to absolute 3D coordinates in cm is a difficult task as it requires either a reference object of known position and scale or knowledge of the person's height, which in turn can only be guessed with uncertainty from monocular footage <ref type="bibr" target="#b24">[Gunel et al. 2019]</ref>.</p><p>In Section 5.3 we explain how the camera relative position up to a scale is recovered through a re-projection constraint. We can optionally utilize the ground plane as reference geometry since camera calibration is less cumbersome than measuring the height of every person appearing in the scene. See the supplementary document for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Kinematic Skeleton Fitting</head><p>After 2D and 3D joint position prediction, we optimize for the skeletal</p><formula xml:id="formula_6">pose {θ k [t]} K [t ] k =1 of all K[t] people in the scene, withθ k [t] ∈ R D where D = 29</formula><p>is the number of degrees of freedom (DOF) for one skeleton. Both, per-frame 2D and 3D pose estimates from previous stages are temporally filtered <ref type="bibr" target="#b11">[Casiez et al. 2012</ref>] before skeleton fitting. Note that θ k ∈ R D describes the pose of a person in terms of joint angles of a fixed skeleton plus the global root position, meaning our final output is directly compatible with CG character animation pipelines. We jointly fit to both 2D and root-relative 3D predictions as this leads to better reprojection error while maintaining plausible and robust 3D articulation. We estimate θ k [t] by minimizing the fitting energy</p><formula xml:id="formula_7">E(θ 1 [t],···,θ K [t]) =w 3D E 3D +w 2D E 2D +w lim E lim +w temp E temp +w depth E depth .<label>(1)</label></formula><p>We formulate ∂ E ∂θ k <ref type="bibr">[t ]</ref> in closed form to perform efficient minimization by gradient descent using a custom implementation. The influence of the individual terms is balanced with w 3D = 9e−1, w 2D = 1e−5, w lim = 5e−1, w temp = 1e−7, and w depth = 8e−6. In the following, we explain each term in more detail.</p><p>3D Inverse Kinematics Term: The 3D fitting term measures the 3D distance between predicted root-relative 3D joint positions P 3D k [t] and the root-relative joint positions in the skeletonP(θ k [t],b k ) posed by forward kinematics for every person k, joint j and previously estimated relative bone lengths b k ,</p><formula xml:id="formula_8">E 3D = K k =1 J 3D j=1 ||P(θ k [t],b k ) j −P 3D k, j [t]|| 2 2 .<label>(2)</label></formula><p>2D Re-projection Term: The 2D fitting term is calculated as the 2D distance between predicted 2D joint positions P 2D k [t] and the projection of the skeleton joint positions P(θ k [t],b k ) j for every person k and joint j,</p><formula xml:id="formula_9">E 2D = K k =1 J 2D j=1 w 2D j c j,k ||Π(h k P(θ k [t],b k )) j −P 2D k, j [t]|| 2 2 ,<label>(3)</label></formula><p>where c is the 2D prediction confidence, w 2D j is per-joint relative weighting, and Π is the camera projection matrix. The lower limb joints have a relative weighting of 1.7, elbows 1.5 and wrist joints 2.0 as compared to torso joints (hips, neck, shoulders). Note that P outputs unit height, the scaling with h k maps it to metric coordinates, and the projection constraint thereby reconstructs absolute position in world coordinates.</p><p>Joint Angle Limit Term: The joint limits regularizer enforces a soft limit on the amount of joint angle rotation based on the anatomical joint rotation limits θ min and θ max . We write it as</p><formula xml:id="formula_10">E lim = K k =1 D j=7          (θ min j −θ k, j [t]) 2 ,if θ k, j [t] &lt;θ min j (θ k, j [t]−θ max j ) 2 ,if θ k, j [t] &gt;θ max j 0 ,otherwise ,<label>(4)</label></formula><p>where we start from j = 7 since we do not have limits on the global position and rotation parameters. Note that our neural network is trained to estimate joint positions and hence has no explicit knowledge about joint angle limits. Therefore, E lim ensures biomechanical plausibility of our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Temporal Smoothness Term:</head><p>Since our neural network estimates poses on a per-frame basis, the results might exhibit temporal jitter. The temporal stability of our estimated poses is improved by</p><formula xml:id="formula_11">E temp (Θ) = K k =1 ||∇θ k [t −1]−∇θ k [t]|| 2 2 ,<label>(5)</label></formula><p>where the rate of change in parameter values, ∇θ k , is approximated using backward differences. In addition, we penalize variations in the less constrained depth direction stronger, using the smoothness term E depth = ||θ k,2 [t] z −θ k,2 [t − 1] z ||, where θ k,2 is the degree of freedom that drives the z-component of the root position.</p><p>Inverse Kinematics Tracking Initialization: For the first frame of a new person track, the local joint angles of the skeleton are fit to the 3D prediction only considering E 3D and E l im . The local angles are then held fixed while minimizing E 2D to find the best fit for global translation and rotation of the skeleton. Subsequently, the complete energy formulation E(θ 1 [t],···,θ K [t]) is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">SELECSLS NET: A FAST AND ACCURATE POSE INFERENCE CNN</head><p>Our Stage I core network is the most expensive component of our algorithm in terms of computation time. We evaluate various popular network architectures (see <ref type="table" target="#tab_0">Table 1</ref> in the supplemental document) on the task of single person 2D pose estimation, and determine that despite various parameter-efficient depthwise-convolution-based designs, for GPU-based deployment, ResNet architectures provide a better or comparable speed-accuracy tradeoff, and thus we use be them as baselines.</p><p>ResNet-50 <ref type="bibr">[2016]</ref> has been employed for other multi-person pose estimation methods such as Mehtaet al.</p><p>[2018b], <ref type="bibr" target="#b76">Rogez et al. [2019]</ref>, and <ref type="bibr" target="#b15">Dabral et al. [2019]</ref>. Only on top-end hardware (Nvidia 1080TI, 11.4TFLOPs) does the ResNet-50 variant of our system run in real time. For more widespread lower performance hardware like ours (Nvidia 1080-MaxQ, 7.5TFLOPs), ResNet-50-based Stage I forward pass takes &gt; 30ms, which, together with the additional overhead of ≈ 15ms from the other components of our system, does not reach real-time performance of &gt; 25 fps.</p><p>We therefore propose a new network architecture module, called SelecSLS module, that uses short range and long range concatenationskip connections in a selective way instead of additive-skip connections. SelectSLS is the main building block of the novel SelecSLS Net architecture for the Stage I core CNN. Additive-skip, as is used in ResNet architectures, performs element-wise addition to the features at the skip connection point, whereas concatenative-skip connections, as used in DenseNet, performs concatenation along the channel-dimension. Our new selective use of concatenation-skip connectivity promotes information flow through the network, without the exorbitant memory and compute cost of the full connectivity of DenseNet. Our new Stage I network shows comparable accuracy to a ResNet-50 core with a substantially faster inference time (≈ 1.4×), across single person and multi-person 2D and 3D pose benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">SelecSLS Module</head><p>Our Stage I core network is comprised of building blocks, Selec-SLS modules, with intra-module short-range skip connectivity and cross-module longer-range skip connectivity.The module design is as shown in <ref type="figure" target="#fig_5">Figure 6</ref> (a) and (b). It comprises of a series of 3×3 convolutions interleaved with 1×1 convolutions. This is to enable mixing of channels when grouped 3×3 convolutions are used. All convolutions are followed by batch normalization and ReLU non-linearity. The module hyperparameter k dictates the number of features output by the convolution layers within the module. The outputs of all 3 × 3 convolutions (2k) are concatenated and fed to a 1 × 1 convolution which produces n o features. The first 3×3 filters in the module are convolved with a stride of 1 or 2, which dictate the feature resolution  of the entire module. The cross-module skip connection is the second input to the module. Different from the traditional inter-module skip connectivity pattern, which connects each module to the previous one, our design uses long range skip connectivity to the first module of each level <ref type="figure" target="#fig_5">(Figure 6 (b)</ref>). This is intended to promote information flow through the network on the forward and the backward pass, and performs better than the traditional inter-module skip connectivity in practice. We define a level as all modules in succession which output feature maps of a particular spatial resolution. See the supplemental document for a detailed design ablation study leading to the proposed module design and the overall architecture. <ref type="table" target="#tab_6">Table 2</ref>. Evaluation of 2D keypoint detections of the complete Stage I of our system (both 2D and 3D branches trained), with different core networks on a subset of validation frames of MS COCO dataset. Also reported are the forward pass timings of the first stage on different GPUs (K80, TitanX (Pascal)) for an input image of size 512 × 320 pixels. We also show the 2D pose accuracy when using channel-dense supervision of {l j,k } J j=1 in the 3D branch in place of our proposed channel-sparse supervision (Section 4.1.2).  <ref type="table" target="#tab_0">Table 1</ref> shows the overall architecture of the proposed SelecSLS Net, parameterized by the stride of the module (s), the intermediate features in the module (k), and number of outputs of the module (n o ). All 3×3 convolutions with more than 96 outputs use a group size of 2, and those with more than 192 outputs use a group size of 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Core</head><p>Design Evaluation: We compare the proposed architecture against ResNet-50 and ResNet-34 architectures as core networks to establish appropriate baselines. We compare our proposed architecture's performance, both on 2D pose estimation and 3D pose estimation in Section 7.4. As we show in Tables 2, 5, and 8, the proposed architecture is 1.3−1.8× faster than ResNet-50, while retaining the same accuracy.</p><p>Memory Footprint: The speed advantage of SelecSLS Net over ResNet-50 stems primarily from its significantly smaller memory footprint. With a mini-batch comprised of a single 512×320px image, SelecSLS Net occupies just 80% of the memory (activations and bookkeeping for backward pass) occupied by ResNet-50. For larger mini-batch sizes, such as a mini-batch size of 32, the memory occupancy of SelecSLS Net is 50% of that of ResNet-50. Beyond the associated speed advantage, the significantly smaller memory footprint of SelecSLS Net allows mini-batches of twice the size compared to ResNet-50, both for training and batched inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RESULTS</head><p>In this section we evaluate the results of our real-time multi-person motion capture solution qualitatively and quantitatively on various benchmarks, provide extensive comparison with prior work, and conduct a detailed ablative analysis of the different components of our system. To ensure that the reported results on 3D pose benchmarks are actually indicative of the deployed system's performance, there is no test-time augmentation applied for our quantitative and qualitative results. We do not use procrustes alignment to the ground truth, nor do we rely on ground truth camera relative localization of the subjects to generate or modify our predictions.</p><p>For additional qualitative results, please refer to the accompanying video. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">System Characteristics and Applications</head><p>First, we show that our system provides efficient and accurate 3D motion capture that is ready for live character animation and other interactive CG applications, rivaling depth-based solutions despite using only a single RGB video feed.</p><p>Real-time Performance: Our live system uses a standard webcam as input, and processes 512×320 pixel resolution input frames. For a scene with 10 subjects, the system running on a Desktop with an Intel Xeon E5 with 3.5 GHz and an Nvidia GTX 1080Ti is capable of processing input at &gt; 30 fps, while on a laptop with an Intel i7-8780H and a 1080-MaxQ it runs at ≈ 27 fps. On the laptop, Stage I takes 21.5 ms, part association and feature extraction take 1 ms, Stage II takes 1 ms, and Stage III takes ≈ 9 ms (2.4 ms for identity matching to the previous frame, 6.8 ms for skeleton fitting).</p><p>We compare our timing against the faster but less accurate 'demo' version of LCRNet++ <ref type="bibr">[2019]</ref>, but compare our accuracy against the slower but more accurate version of LCRNet++. LCRNet++ demo system uses ResNet-50 with less post-processing overhead than the accuracy-benchmarked system, and we measured its forward pass time on a TitanX-Pascal GPU (11 TFLOPs) to be 16 ms, while on a K80 GPU (4.1 TFLOPs) it takes &gt;100 ms. Our SelecSLS-based system takes only 14 ms on TitanX-Pascal and 35 ms on a K80 GPU while producing more accurate per-frame joint position predictions (Stage II ) than the slower version of LCRNet++, as shown in <ref type="table">Table 7</ref>. An additional CPU-bound overhead of ≈ 9 ms for Stage III results in temporally smooth joint-angle estimates which can readily be used to drive virtual characters. The accompanying video contains examples of our live setup running on the laptop. Note that throughout the manuscript we report the timings of the various stages of our system on a set of GPUs with FP32 performance representative of current low-end and high-end consumer GPUs (≈ 4−12TFLOPs). , which utilize multi-person 2D parsing as a pre-processing step along with temporal information, fail under occlusions. Our approach is also more robust to occlusions than the multi-person approach of <ref type="bibr" target="#b54">Mehta et al. [2018b]</ref>, and unlike top-down approaches <ref type="bibr" target="#b15">[Dabral et al. 2019;</ref><ref type="bibr">Moon et al. 2019;</ref><ref type="bibr" target="#b76">Rogez et al. 2019</ref>] it does not produce spurious predictions, as seen in <ref type="figure" target="#fig_4">Figure 5</ref> in the supplemental document. For further qualitative results on a variety of scene settings, including community videos and live scene setups, please refer to <ref type="figure" target="#fig_10">Figure 9</ref>, the accompanying video, and Figures 4 and 6 in the supplemental document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Person Scenes and Occlusion</head><p>Comparison With KinectV2: The quality of our pose estimates with a single RGB camera is comparable to off-the-shelf depth-sensingbased systems such as KinectV2 <ref type="figure" target="#fig_9">(Figure 8)</ref>, with our approach succeeding in certain cluttered scenarios where person identification from depth input would be ambiguous. The accompanying video contains further visual comparisons.</p><p>Character Animation: Since we reconstruct temporally coherent joint angles and our camera-relative subject localization estimates are stable, the output of our system can readily be employed to animate virtual avatars as shown in <ref type="figure" target="#fig_7">Figure 7</ref>. The video demonstrates the stability of the localization estimates of our system and contains further examples of real-time interactive character control with a single RGB camera.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Performance on Single Person 3D Pose Datasets</head><p>Our method is capable of real-time motion capture of multi-person scenes with notable occlusions. Previous single-person approaches, irrespective of runtime, would fail on this task. For completeness, we show that our method shows competitive accuracy on single-person 3D pose estimation. In <ref type="table" target="#tab_8">Table 4</ref>, we compare the 3D pose output after Stage II and Stage III against other single-person methods on the MPI-INF-3DHP benchmark dataset <ref type="bibr" target="#b53">[Mehta et al. 2017a</ref>] using the commonly used 3D Percentage of Correct Keypoints (3DPCK, higher is better), Area under the Curve (AUC, higher is better) and mean 3D joint position error (MJPE, lower is better). Similarly, with Stage II trained on Human3.6m <ref type="table" target="#tab_7">(Table 3)</ref>, we again see that our system compares favourably to recent approaches designed to handle single-person and multi-person scenarios. Further, this is an example of the ability of our system to adapt to different datasets by simply retraining the inexpensive Stage II network. Some approaches such as , obtain state-of-the-art accuracy on Human3.6m, but critically use the camera intrinsics and the ground-truth distance of the subject from the camera to convert their (u,v) predictions to (x,y), and also use flip augmentation at test time. Therefore, their reported results are not representative of the deployed system's performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Performance on Multi-Person 3D Pose Datasets</head><p>We quantitatively evaluate our method's accuracy on the MuPoTS-3D monocular multi-person benchmark data set from <ref type="bibr" target="#b54">Mehta et al. [2018b]</ref>, which has ground-truth 3D pose annotations from a multi-view marker-less motion capture system. We perform two types of comparison. In <ref type="table">Table 7</ref>(All), we compare on all annotated poses in sequences T1-T20 of the annotated test set, including poses of humans that were not detected by our algorithm. In <ref type="table">Table 7</ref> (Matched), we compare only on annotated poses of humans detected by the respective algorithms.</p><p>Both tables show that our per-frame predictions (Stage II ), as well as our temporally smooth model-fitting predictions (Stage III ) achieve better accuracy in terms of the 3D percentage of correct keypoints metric (3DPCK, higher is better) to <ref type="bibr">LCRNet [2017]</ref>, LCRNet++ <ref type="bibr">[2019]</ref> and <ref type="bibr" target="#b54">[Mehta et al. 2018b]</ref>, while being comparable to other recent methods, namely <ref type="bibr" target="#b15">[Dabral et al. 2019</ref>]. The approach of Moon et al. <ref type="bibr">[2019]</ref> exceeds the performance of our system, but it uses a prior person detection step, and passes resized person crops to the pose estimation network. As has been shown in prior work <ref type="bibr" target="#b10">[Cao et al. 2017</ref>], such an approach results in a higher accuracy, however, it cannot run at real-time frame rates, and the run time scales almost linearly with the number of subjects in the scene. In contrast, our approach runs in real-time for multi-person scenes, and only has a very mild dependence on the number of subjects in the scene.</p><p>Note that we apply no test-time augmentation or ensembling to our system, making the reported performance on various benchmarks accurately reflect the real per-frame prediction performance of our system.</p><p>In <ref type="table">Table 7</ref> (Matched), we compare only on annotated poses of humans detected by the respective algorithms.</p><p>Additionally, we quantitatively evaluate our method's accuracy on the 3DPW <ref type="bibr" target="#b99">[von Marcard et al. 2018</ref>] monocular multi-person dataset, which comprises of videos recorded in in-the-wild settings with a phone camera. The 3D pose annotations for the dataset are generated from a combination of inertial-sensing measurements and video based approaches. We use the 24 sequences in the 'test' split for evaluating. Our method does not use any data from 3DPW for training. Unlike prior and concurrent work <ref type="bibr" target="#b40">[Kanazawa et al. 2018</ref><ref type="bibr" target="#b41">[Kanazawa et al. , 2019</ref><ref type="bibr" target="#b43">Kocabas et al. 2020</ref>] which use bounding-box tracks of each subject, our approach handles all the subjects in the complete input frame jointly, and also matches person identities from frame to frame, all while running in real time. We report the mean 3D joint position error (MPJPE) in mm in <ref type="table">Table 6</ref>. Our per-frame predictions (Stage II ) are comparable or better than most recent approaches, while being realtime and using no bounding box information. The person tracking heuristics for Stage III are designed for stationary or slowly moving cameras, and the assumptions break under fast moving cameras in dense crowds where multiple subjects can have similar clothing and 2D poses. This leads to a noticeable decrease in accuracy with Stage III, which can be improved by incorporating a better identity tracking approach, that is less prone to failure in dense crowds. See the accompanying video for further results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Core Network Architecture Evaluation</head><p>We evaluate the performance of the proposed network architecture against ResNet baselines on 2D pose estimation and 3D pose estimation tasks. For ResNet, we keep the network until the first residual module in level-5 and remove striding from level-5. For 2D pose estimation, we evaluate on a held-out 1000 frame subset of the MS-COCO validation set, and report the Average Precision (AP) and Recall (AR), as well as inference time on different hardware in <ref type="table" target="#tab_6">Table 2</ref>. The performance of SelecSLS Net (47.0 AP, 51.8 AR) is better than ResNet-50 (46.6 AP, 51.4 AR), while being 1.3−1.4× faster on GPUs. The inference speed improvement increases to 1.8× when running on a CPU, as shown in the supplemental document.</p><p>We evaluate our network trained for multi-person pose estimation (after Stage II) on MPI-INF-3DHP <ref type="bibr" target="#b53">[Mehta et al. 2017a</ref>] single person 3D pose benchmark, comparing the performance of different core <ref type="table" target="#tab_7">Table 3</ref>. Results of Stage II predictions on Human3.6m, evaluated on all camera views of Subject 9 and 11 without alignment to GT. The Stage II network is trained with only Human3.6m. The top part has single person 3D pose methods, while the bottom part shows methods designed for multiperson pose estimation. Mean Per Joint Position Error (MPJPE) in millimeters is the metric used (lower is better). Note that our reported results do not use any test time augmentation. Nor do we exploit ground truth 3D pose information in other ill-informed ways, such as rigid alignment to the ground truth or the use of ground truth depth from the camera to glean the image coordinate to 3D space transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>MPJPE (mm) <ref type="bibr" target="#b42">[Katircioglu et al. 2018]</ref> 67.3 <ref type="bibr" target="#b63">[Pavlakos et al. 2017]</ref> 67.1  64.9 <ref type="bibr" target="#b52">[Martinez et al. 2017]</ref> 62.9 Hossain &amp; Little <ref type="bibr">[2018]</ref> 58.3 <ref type="bibr" target="#b54">[Mehta et al. 2018b]</ref> 69.  <ref type="table" target="#tab_2">Table 5</ref>, we see that using our SelecSLS core architecture we overall perform significantly better than ResNet-34 and slightly better than ResNet-50, with a higher 3DPCK and AUC and a lower MPJPE error. SelecSLS particularly results in significantly better performance for lower body joints (Knee, Ankle) than the ResNet baselines. Similarly, our SelectSLS network architecture outperforms ResNet-50 and ResNet-34 on the multi person 3D pose benchmark MuPoTS-3D, as shown in <ref type="table">Table 8</ref>.  <ref type="bibr" target="#b40">[Kanazawa et al. 2018</ref><ref type="bibr" target="#b41">[Kanazawa et al. , 2019</ref><ref type="bibr" target="#b43">Kocabas et al. 2020</ref>] which uses bounding-box crops around the subject and the supplied annotations to establish temporal association, our approach handles all the subjects in the complete input frame jointly, and also matches person identities from frame to frame, all while running in real time. The person identity tracking heuristics are designed for a stationary or slow moving camera, but the faster moving camera in 3DPW as well as the dense crowds cause identity tracking failures, which results in the markedly worse performance of Stage III compared to Stage II. With Stage II trained on the single-person pose estimation task on Human3.6m, we again see that our proposed faster core network architecture outperforms ResNet baselines. The use of SelecSLS results in a mean per joint position error of 63.6mm, compared to 64.8mm using ResNet-50 and 67.6mm using ResNet-34.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Channel-Sparse 3D Pose Encoding Evaluation</head><p>As discussed at length in Section 4.1.2, different choices for supervision of {l j,k } J j=1 have different implications. Here we show that our channel-sparse supervision of the encoding, such that only the local kinematic context is accounted for, performs better than the naive channel-dense supervision.</p><p>The 2D pose accuracy of the Stage I network with channel-dense supervision of 3D branch is comparable to that with channel-sparse supervision, as shown in <ref type="table" target="#tab_6">Table 2</ref>. However, our proposed encoding <ref type="table">Table 7</ref>. Comparison of our per-frame estimates (Stage II) on the MuPoTS-3D benchmark data set <ref type="bibr" target="#b54">[Mehta et al. 2018b</ref>]. The metric used is 3D percentage of correct keypoints (3DPCK), so higher is better. The data set contains 20 test scenes TS1-TS20. We evaluate once on all annotated poses (top row -All), and once only on the annotated poses detected by the respective algorithm (bottom row -Matched). Our approach achieves comparable accuracy to the previous monocular multi-person 3D methods (SingleShot <ref type="bibr" target="#b54">[Mehta et al. 2018b]</ref>, LCRNet <ref type="bibr" target="#b75">[Rogez et al. 2017]</ref>, MP3D <ref type="bibr" target="#b15">[Dabral et al. 2019]</ref>, LCRNet++ <ref type="bibr" target="#b76">[Rogez et al. 2019</ref>]) while having a drastically faster runtime. The accuracy of our system lags behind that of <ref type="bibr">Root+PoseNet [Moon et al. 2019]</ref> which uses a prior person detection step, runs offline, has its per-frame inference time scale linearly with the number of subjects in the scene. * Indicates no test time augmentation is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>All</head><p>TS1 <ref type="table" target="#tab_0">TS2 TS3 TS4 TS5 TS6 TS7 TS8 TS9 TS10 TS11 TS12 TS13 TS14 TS15 TS16 TS17 TS18 TS19</ref>  performs much better across single person and multi-person 3D pose benchmarks. <ref type="table" target="#tab_2">Table 5</ref> shows that channel-sparse encoding significantly outperforms channel-dense encoding, yielding an overall 3DPCK of 82.8 compared to 80.1 3DPCK for the latter. As shown in <ref type="table" target="#tab_8">Table 4</ref> in the supplemental document, the difference particularly emerges for difficult pose classes such as sitting on a chair or on the floor, where our channel-sparse supervision shows substantial gains. Also refer to the supplemental document for additional ablative analysis of input to Stage II.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.6">Evaluation of Skeleton Fitting (Stage III)</head><p>Skeleton fitting to reconcile 2D and 3D poses across time results in smooth joint angle estimates which can be used to drive virtual characters. The accompanying video shows that the resulting multiperson pose estimates from our real-time system are similar or better in quality than the recently published offline single-person approach of <ref type="bibr" target="#b41">[Kanazawa et al. 2019]</ref>, while also succeeding under significant occlusions.</p><p>On the multi-person benchmark MuPoTS-3D in <ref type="table">Table 7</ref>, the overall performance of Stage III appears to lag behind that of Stage II. However, the efficacy of Stage III is revealed through joint-wise breakdown of performance on MuPoTS-3D in <ref type="table">Table 9</ref>. Stage III predictions show a significant improvement for limb joints such as elbows, wrists, knees, and ankles over Stage II. The decrease in accuracy for some other joints, such as the head, is partly due to lack of 2D constraints imposed for those joints in our system. The overall breakdown of accuracy by joint visibility shows that Stage III improves the accuracy for visible joints, i.e. where 2D constraints were available, while slightly decreasing accuracy for occluded joints.</p><p>On the single-person benchmark MPI-INF-3DHP <ref type="table" target="#tab_8">(Table 4)</ref>, the performance of Stage III is comparable to other methods specifically designed for single-person scenarios, however it loses accuracy compared to Stage II predictions for similar reasons as in the multi-person case. For complex poses, not all 2D keypoints may be detected. This does not pose a problem for Stage II predictions due to redundancies in the pose encoding, however, analogous to the case of occluded joints discussed before, missing 2D keypoints can cause the Stage III accuracy to worsen.</p><p>However, despite a minor decrease in quantitative accuracy due to Stage III, it produces a marked improvement in the quality and temporal stability of the pose estimates, and increases the accuracy of the end effectors. The resulting, temporally smooth joint angle estimates can be used in interactive graphics applications. Refer to the accompanying video for qualitative results, and virtual character control examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">DISCUSSION AND FUTURE WORK</head><p>Our approach is the first to perform real-time 3D motion capture of challenging multi-person scenes with one color camera. Nonetheless, it has certain limitations that will be addressed in future work.</p><p>As with other monocular approaches, the accuracy of our method is not comparable yet to the accuracy of multi-view capture algorithms. Failure cases in our system can arise from each of the constituent stages. The 3D pose estimates can be incorrect if the underlying 2D pose estimates or part associations are incorrect. Also, since we require the neck to be visible for a successful detection of a person, scenarios where the neck is occluded result in the person not being detected despite being mostly visible. See <ref type="figure" target="#fig_0">Figure 3</ref> in the supplemental document for a visual representation of the limitations.</p><p>Our algorithm successfully captures the pose of occluded subjects even under difficult inter-person occlusions that are generally hard for monocular methods. However, the approach still falls short of reliably capturing extremely close interactions, like hugging. Incorporation of physics-based motion constraints could further improve pose stability in such cases, may add further temporal stability, and may allow capturing of fine-grained interactions of persons and objects.</p><p>In some cases individual poses have higher errors for a few frames, e.g. after strong occlusions (see accompanying video for example). However, our method manages to recover from this. The kinematic fitting stage may suffer from inaccuracies under cases of significant inter-personal or self occlusion, making the camera relative localization less stable in those scenarios. Still, reconstruction accuracy and stability is appropriate for many real-time applications.</p><p>Our algorithm is fast, but the relatively simple identity tracker may swap identities of people when tracking through extended person occlusions, drastic appearance changes, and similar clothing appearance. More sophisticated space-time tracking would be needed to remedy this. As with all learning-based pose estimation approaches, pose estimation accuracy worsens on poses very dissimilar from the training poses. To approach this, we plan to extend our algorithm in future such that it can be refined in an unsupervised or semi-supervised way on unlabeled multi-person videos.</p><p>Our SelecSLS Net leads to a drastic performance boost. There are other strategies that could be explored to further boost the speed of our network and convolutional architectures in general, or target it to specific hardware, such as using depthwise 3×3 convolutions or factorized 3 × 3 convolutions <ref type="bibr" target="#b77">[Romera et al. 2018;</ref><ref type="bibr" target="#b90">Szegedy et al. 2017]</ref> or binarized operations <ref type="bibr" target="#b8">[Bulat and Tzimiropoulos 2017]</ref>, all of which our proposed design can support. Its faster inference speed and significantly smaller memory footprint than ResNet-50 without compromising accuracy makes it an attractive candidate to replace ResNet core networks for a broad range of tasks beyond body pose estimation. It can support much larger training and inference batch sizes, or can be trained on lower-end GPUs with similar batch sizes as ResNet-50 on higher-end GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">CONCLUSION</head><p>We present the first real-time approach for multi-person 3D motion capture using a single RGB camera. It operates in generic scenes and is robust to occlusions both by other people and objects. It provides joint angle estimates and localizes subjects relative to the camera. To this end, we jointly designed pose representations, network architectures, and a model-based pose fitting solution, to enable real-time performance. One of the key components of our system is a new CNN architecture that uses selective long and short range skip connections to improve the information flow and have a significantly smaller memory footprint, allowing for a drastically faster network without compromising accuracy. The proposed system runs on consumer hardware at more than 30 fps while achieving state-of-the-art accuracy. We demonstrate these advances on a range of challenging real-world scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplemental Document: XNect: Real-time Multi-Person 3D Motion Capture with a Single RGB Camera</head><p>Here we present additional qualitative results of our approach, ablation studies of design variations of SelecSLS architecture, further ablation studies of our proposed pose representation, and additional details about Stage III of our system. Refer to the main manuscript, the accompanying video, and the project website (http://gvv.mpiinf.mpg.de/projects/XNect/) for further details and results. <ref type="figure" target="#fig_11">Figure 1</ref> shows variants of the overall architecture of the proposed SelecSLS Net that were considered. The architecture is parameterized by the type of module (SelecSLS concatenation-skip CS vs additionskip AS), the stride of the module (s), the intermediate features in the module (k), cross-module skip connectivity (previous module or first module in the level), and number of outputs of the module (n o (B)ase case). With the aim to promote information flow in the network, we also consider (W)ider n o at transitions in spatial resolution. All 3×3 convolutions with more than 96 outputs use a group size of 2, and those with more than 192 outputs use a group size of 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">SELECSLS NET DESIGN EVALUATION</head><p>We experimentally determine the best network design by testing the Stage I network with a SelecSLS Net core on 2D multi-person pose estimation, i.e., only using the 2D branch, which plays an integral role in the overall pipeline. Our conclusions transfer to the full Stage I network, as further evidenced in Section 7.4 in the main manuscript.</p><p>As mentioned in Section 6.2 in the main manuscript, and shown in <ref type="table" target="#tab_0">Table 1</ref>, for GPU-based deployment ResNet architectures provide a better or comparable speed-accuracy tradeoff to various parameterefficient depthwise-convolution based designs. Thus, we compare against ResNet-50 and ResNet-34 architectures as core networks to establish the appropriate baselines. For ResNet, we keep the network until the first residual module in level-5 and remove striding from level-5. We evaluate on a held-out 1000 frame subset of the MS-COCO validation set, and report the Average Precision (AP) and Recall (AR), as well as inference time on different hardware in <ref type="table" target="#tab_6">Table 2</ref>. Using the AS module with Prev connectivity and n o (B) outputs for modules, the performance as well as the inference time on an Nvidia K80 GPU is close to that of ResNet-34. Using CS instead of addition-skip significantly improves the average precision from 47.0 to 47.6, and the average recall from 51.7 to 52.6. Switching the number of module outputs to the wider n o (W) scheme leads to further improvement in AP and AR, at a slight increase in inference time. Using First connectivity further improves performance, namely to 48.6 AP and 53.3 AR, reaching close to ResNet-50 in AP (48.8) and performing slightly better with regard to AR (53.2). Still our new design has a 1.4-1.8× faster inference time across all devices. We also evaluate the publicly available model of <ref type="bibr" target="#b10">[Cao et al. 2017]</ref> on the same validation subset. Their multi-stage network is 11 percentage points better on AP and AR than our network, at the price of being 10−20× slower. The follow-up versions <ref type="bibr" target="#b9">[Cao et al. 2019</ref>] are ≈ 2× faster on the GPU and ≈ 5× slower on the CPU, and 4−5 percentage points better on AP than the original ( <ref type="bibr" target="#b10">[Cao et al. 2017]</ref>), though it still remains 5× slower than our network on a GPU, and ≈ 80× slower than our network on a CPU.  Thus, of the different possible designs of the SelecSLS module, and the inter-module skip connectivity choices, the best design for Selec-SLS Net is the one with concatenation-skip modules, cross-module skip connectivity to the first module in the level, and n o (W) scheme for module outputs. Refer to Section 7.4 in the main manuscript for further comparisons of our architecture against ResNet-50 and ResNet-34 baselines on single-person and multi-person 3D pose benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">SELECSLSNET ON IMAGE CLASSIFICATION</head><p>To demonstrate the efficacy of our proposed architecture on tasks beyond 2D and 3D body pose estimation, we train a variant of Selec-SLSNet on ImageNet <ref type="bibr">[Russakovsky et al. 2015</ref>], a large scale image classification dataset. The network architecture is shown in <ref type="figure">Figure 2</ref>, and the results are shown in <ref type="table" target="#tab_7">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ABLATION OF INPUT TO STAGE II</head><p>We evaluate variants of Stage II network taking different subsets of outputs from Stage I as input. We compare the Stage II output, without Stage III on MPI-INF-3DHP single person benchmark.On the single person benchmark <ref type="table" target="#tab_8">(Table 4</ref>), using only the 2D pose from the 2D branch as input to Stage II, without having trained the 3D branch for Stage I, results in a 3DPCK of 76.0. When using 2D pose from a network with a 3D branch, trained additionally on MuCo-3DHP dataset,  , we use the same core architecture design as SelecSLSNet for the multi-person task. Group convolutions are not used in the core network for this task. Inplace of the '2D Branch' and '3D Branch' described in <ref type="figure">Fig.  2</ref> in the main manuscript, we use a 5 layer network as the classification head. As with the various 2D and 3D pose estimation tasks described previously, the network matches the accuracy of ResNet-50 on ImageNet as well, while being 1.3−1.4× faster.</p><p>we see a minor performance decrease to 75.5 3DPCK. Though it comes with a performance improvement on challenging pose classes such as 'Sitting' and 'On The Floor' which are under-represrented in MSCOCO. Adding other components on top of 2D pose, such as the joint detection confidences C k , and output features from the 3D branch {l j,k } J j=1 (as described in Section 4.1.2 in the main manuscript) leads to consistent improvement as more components are subsequently used as input to Stage II. Using joint detection confidences C k with 2D pose increases the accuracy to 77.2 3DPCK, and incorporating 3D pose features {l j,k } J j=1 increases the accuracy to 82.8 3DPCK, and both lead to improvements in AUC and MPJPE as well as improvements for both simpler poses such as upright 'Standing/walking' as well as more difficult poses such as 'Sitting' and 'On the Floor'</p><p>This shows the limitations of 2D-3D 'lifing' approaches, and demonstrates that incorporating additional information, such as the joint detection confidences, and our proposed 3D pose encoding that uses local kinematic context (channel-sparse supervision) improve the pose performance, leads to significant improvements in 3D pose accuracy. MOTION CAPTURE (STAGE III): ADDITIONAL DETAILS Absolute Height Calibration. As mentioned in the main document, to allow more accurate camera relative localization, we can optionally utilize the ground plane as reference geometry. First, we determine the camera relative position of a person by shooting a ray from the camera origin through the person's foot detection in 2D and computing its intersection with the ground plane. The subject height, h k , is then the distance from the ground plane to the intersection point of a virtual billboard placed at the determined foot position and the view ray through the detected head position. Because we want to capture dynamic motions such as jumping, running, and partial (self-)occlusions, we cannot assume that the ankle is visible and touches the ground at every frame. Instead, we use this strategy only once when the person appears. As shown in the accompanying video, such a height calibration strategy allows reliable camera-relative localization of subjects in the scene even when they are not in contact with the ground plane.</p><p>In practice, we compute intrinsic and extrinsic camera parameters once prior to recording using checkerboard calibration. Other objectfree calibration approaches would be feasible alternatives <ref type="bibr" target="#b108">[Yang and Zhou 2018;</ref><ref type="bibr" target="#b110">Zanfir et al. 2018a]</ref>.</p><p>Inverse Kinematics Tracking Error Recovery: Since we use gradient descent for optimizing the fitting energy, we can monitor the gradients of E 3D and E l im terms in E(θ 1 [t],···,θ K [t]) to identify when tracking has failed, either due to a failure to correctly match subjects to tracks because of similar appearance and pose, or when the fitting gets stuck in a local minimum. When the gradients associated with these terms exceed a certain threshold for a subject for 30 frames, the identity and pose track of the subject is re-initialized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">MORE QUALITATIVE RESULTS</head><p>Limitations: <ref type="figure" target="#fig_0">Figure 3</ref> shows visual examples of the limitations of our approach, as discussed in Section 8 in the main manuscript. Our system mispredicts when the underlying 2D pose prediction mispredicts limb associations across subject. When the neck of the subject is occluded, we treat the subject as not present, even when the rest of the body is visible. This could be handled by using multiple reference joints on the body, instead of just the neck. Also, as our approach is a learning based approach, it mispredicts when the presented pose is outside the training distribution.</p><p>Comparisons With Prior Work: <ref type="figure" target="#fig_4">Figure 5</ref> shows visual comparisons of our approach to prior single-person and multi-person approaches. Also refer to the accompanying video for further comparisons. Results of our real-time system are comparable or better in quality than both, single-person and multi-person approaches, many of which run at offline <ref type="bibr" target="#b40">[Kanazawa et al. 2018</ref><ref type="bibr" target="#b41">[Kanazawa et al. , 2019</ref><ref type="bibr" target="#b54">Mehta et al. 2018b;</ref><ref type="bibr">Moon et al. 2019</ref>] and interactive <ref type="bibr" target="#b15">[Dabral et al. 2019;</ref><ref type="bibr" target="#b76">Rogez et al. 2019</ref>] frame-rates. As shown in the video, the temporal stability of our approach is comparable to real-time <ref type="bibr" target="#b54">[Mehta et al. 2017b</ref>] and offline <ref type="bibr" target="#b41">[Kanazawa et al. 2019</ref>] temporally consistent single-person approaches. Our approach differs from much of recent multi-person approaches in that ours is a bottom-up approach, while others employ a top-down formulation <ref type="bibr" target="#b15">[Dabral et al. 2019;</ref><ref type="bibr">Moon et al. 2019;</ref><ref type="bibr" target="#b75">Rogez et al. 2017</ref><ref type="bibr" target="#b76">Rogez et al. , 2019</ref> inspired by work on object detection. Top-down approaches produce multiple predictions (proposals) per subject in the scene, and require a post-processing step to filter. Even when carefully tuned, this filtering step can either suppress valid predictions (two subjects with similar poses in close proximity) or fail to suppress invalid predictions (ghost predictions where there is no subject in the scene).</p><p>Live Interaction and Character Control: <ref type="figure" target="#fig_3">Figure 4</ref> shows additional examples of live character control with our real-time monocular motion capture approach. Also refer to the accompanying video for more character control examples. Our system can act as a dropin replacement for typical depth sensing based game controllers, allowing subjects to interact with their live avatars.</p><p>Diverse Pose and Scene Settings: <ref type="figure" target="#fig_5">Figure 6</ref> shows the 3D capture results from our system (Stage III ) overlaid on input images from diverse and challenging scenarios. See the accompanying video for additional results. Our approach can handle a wide range of poses, in a wide variety of scenes with different lighting conditions, background, and person density. <ref type="figure" target="#fig_3">Fig. 4</ref>. Live Interaction and Virtual Character Control: The temporally smooth joint angle predictions from Stage III can be readily employed for driving virtual characters in real-time. The top shows our system driving virtual skeletons and characters in real-time with the captured motion. On the bottom, our system is set up as a Kinect-like game controller, allowing subjects to interact with their virtual avatars live. Some images courtesy Boxing School Alexei Frolov (https://youtu.be/dbuz9Q05bsM), and Music Express Magazine (https://youtu.be/kX6xMYlEwLA, https://youtu.be/lv-h4WNnw0g). Please refer to the supplemental video for more results.  <ref type="bibr">VNect [2017b]</ref> only work in single person scenarios, and not designed to be occlusion robust or to handle other subjects in close proximity to the tracked subject. LCRNet++ [2019] is able to handle multi-person scenarios, but works at interactive frame rates, and requires post processing to be able to fuse the multiple pose proposals generated per subject. The post-processing is not always successful at fusing all proposals, leading to ghost predictions. The offline single-person approach HMMR [2019] uses 2D multi-person pose estimation as a pre-processing step and is thus able to handle unoccluded subjects in multi-person scenes in a top-down way. However, the approach fails under occlusion, and the run-time scales linearly with the number of subjects in the scene. The multi-person approach of <ref type="bibr" target="#b54">[Mehta et al. 2018b</ref>] jointly handles multiple subjects in the scene, however shows failures in cases of inter-personal proximity. The multi-person approach of <ref type="bibr">[Moon et al. 2019]</ref> works offline, and similar to LCRNet++ it often produces spurious predictions due to the difficulty of filtering multiple proposals from top-down approaches. Here for our bottom-up approach (bottom), we show the 3D skeleton from Stage III reprojected on the image. Some images courtesy Music Express Magazine (https://youtu.be/kX6xMYlEwLA). <ref type="figure" target="#fig_5">Fig. 6</ref>. Monocular 3D motion capture results from our full system (Stage III) on a wide variety of multi-person scenes, including Panoptic <ref type="bibr">[2015]</ref> and MuPoTS-3D [2018b] datasets. Our approach handles challenging motions and poses, including interactions and cases of self-occlusion. Some images courtesy KNG Music (https://youtu.be/_xCKmEhKQl4), 1MILLION TV (https://youtu.be/9HkVnFpmXAw), Indian dance world (https://youtu.be/PN6tRmj6xGU), 7 Minute Mornings (https://youtu.be/oVgG5ENXyVs), Crush Fitness (https://youtu.be/8qFwPKfllGI), Boxing School Alexei Frolov (https://youtu.be/dbuz9Q05bsM), and Brave Entertainment (https://youtu.be/ZhuDSdmby8k). Please refer to the accompanying video for more results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 .</head><label>3</label><figDesc>Input to Stage II: S k for each detected individual k, is comprised of the individual's 2D joint locations P 2D k , the associated joint detection confidence values C extracted from the 2D branch output, and the respective 3D pose encodings {l j,k } J j=1 extracted from the output of the 3D branch. Refer to Section 4 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>4.1.2 3D Branch: Predicting Intermediate 3D Pose Encoding. The 3D branch of the Stage I network uses the features from the core network and the 2D branch to predict 3D pose encoding maps</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>4.1.3 Stage I Training. The Stage I network is trained in multiple stages. First the core network and the 2D pose branch are trained</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>3D Pose Encoding With Local Kinematic Context:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Lightweight fully connected network that forms Stage II of our pipeline. The network decodes the inferred 2D body pose, joint detection confidences and 3D pose encodings coming from Stage I to root-relative full body 3D pose (X j ,Y j ,Z j ) estimates, leveraging full body context to fill in occluded joints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>The proposed SelecSLS module design (a) is comprised of interleaved 1×1 and 3×3 convolutions, and handles cross-module skip connections internally as concatenative-skip connections. The cross module skip connections themselves come from the first module that outputs features at a particular spatial resolution (b). See the supplemental document for ablation studies on alternative skip connectivity choices, through which this design emerged. Our design is parameterized by module stride (s), the number of intermediate features (k ), and the number of module ouputs n o .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>FP Time Network K80 TitanX AP AP 0.5 AP 0.75 AR AR 0.5 AR 0.75 ResNet-34 29.0ms 6.5ms 45.0 72.0 46.1 49.9 74.4 51.6 ResNet-50 39.3ms 10.5ms 46.6 73.0 48.9 51.4 75.4 54.0 SelecSLS 28.6ms 7.4ms 47.0 73.5 49.5 51.8 75.6 54.1 3D Branch With Channel-Dense {l j,k } J j=1 Supervision SelecSLS 28.6ms 7.4ms 46.8 73.5 49.0 51.5 75.9 53.8 6.2 SelecSLS Net Architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Live Interaction and Virtual Character Control: The temporally smooth joint angle predictions from Stage III can be readily employed for driving virtual characters in real-time. The top two rows show our system driving virtual skeletons and characters with the motion captured in real time. On the bottom, our system is set up as a Kinect-like game controller, allowing subjects to interact with their virtual avatars live. Some images courtesy Music Express Magazine (https://youtu.be/kX6xMYlEwLA, https:// youtu.be/lv-h4WNnw0g). See the accompanying video and the supplemental document for further character control examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Robustness: In Figure 9, we show qualitative results of our full system on several scenes containing multiple interacting and overlapping subjects, including frames from MuPoTS-3D [2018b] and Panoptic [2015] datasets. Single-person real-time approaches such as VNect [2017b] are unable to handle occlusions or multiple people in close proximity as shown in Figure 5 in the supplemental document, while our approach can. Even offline single-person approaches such as HMMR [2019]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>The quality of our pose estimates is comparable to depth sensing based approaches such as KinectV2, and our system handles certain cases of significant inter-personal overlap and cluttered scenes better than KinectV2. In the top row, due to scene clutter, KinectV2 predicts multiple skeletons for one subject. In the bottom row, under occlusion, KinectV2 mispredicts the pose, while our approach succeeds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9 .</head><label>9</label><figDesc>Monocular 3D motion capture results from our full system (Stage III) on a wide variety of multi-person scenes, including Panoptic[2015]  and MuPoTS-3D [2018b] datasets. Our approach handles challenging motions and poses, including interactions and cases of self-occlusion. Some images courtesy KNG Music (https://youtu.be/_xCKmEhKQl4), 1MILLION TV (https://youtu.be/9HkVnFpmXAw), Boxing School Alexei Frolov (https://youtu.be/dbuz9Q05bsM), TPLA:Terra Prime Light Armory (https://youtu.be/xmFVfUKr1MQ), and Brave Entertainment (https://youtu.be/ZhuDSdmby8k). Please refer to the supplemental document and video for more results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 1 .</head><label>1</label><figDesc>Variants of SelecSLS module design (a) and(b). Both share a common design comprised of interleaved 1×1 and 3×3 convolutions, with different ways of handling cross-module skip connections internally: (a) as additive-skip connections, or (b) as concatenative-skip connections. The cross module skip connections can themselves come either from the previous module (c) or from the first module which outputs features at a particular spatial resolution (d). In addition to the different skip connectivity choices, our design is parameterized by module stride (s), the number of intermediate features (k), and the number of module ouputs n o . The table on the right shows the network levels, overall number of modules, number of intermediate features k, and the spatial resolution of features of the network designs we evaluate in Section 1. The design choices evaluated are the type of module (additive skip AS vs concatenation skip CS), the type of cross module skip connectivity (From previous module (Prev) or first module (First in the level), and the scheme for the number of outputs of modules n o ((B)ase or (W)ide).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 3 .</head><label>3</label><figDesc>Limitations: a)3D pose inaccuracy due to 2D pose limb confusion, b),c) Person not detected due to neck occlusion and extreme occlusion, d) Body orientation confusion due to occluded face e),f) Unreliable pose estimates for poses drastically different from the training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 5 .</head><label>5</label><figDesc>Real-time 3D Pose approaches such as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>SelecSLS Net Architecture: The table shows the network levels, overall number of modules, number of intermediate features k, the number of outputs of modules n o , and the spatial resolution of features of the network proposed in Section 6.</figDesc><table><row><cell cols="2">Level Output</cell><cell cols="2">Module Stride</cell><cell></cell><cell>Cross-Module</cell></row><row><cell></cell><cell cols="2">Resolution Type</cell><cell>s</cell><cell>k</cell><cell cols="2">Skip Conn. n o</cell></row><row><cell>L0</cell><cell cols="2">w/2 x h/2 Conv. 3x3</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>32</cell></row><row><cell>L1</cell><cell cols="2">w/4 x h/4 SelecSLS</cell><cell>2</cell><cell>64</cell><cell>No</cell><cell>64</cell></row><row><cell></cell><cell cols="2">w/4 x h/4 SelecSLS</cell><cell>1</cell><cell>64</cell><cell>First</cell><cell>128</cell></row><row><cell>L2</cell><cell cols="2">w/8 x h/8 SelecSLS</cell><cell>2</cell><cell>128</cell><cell>No</cell><cell>128</cell></row><row><cell></cell><cell cols="2">w/8 x h/8 SelecSLS</cell><cell>1</cell><cell>128</cell><cell>First</cell><cell>128</cell></row><row><cell></cell><cell cols="2">w/8 x h/8 SelecSLS</cell><cell>1</cell><cell>128</cell><cell>First</cell><cell>288</cell></row><row><cell cols="3">L3 w/16 x h/16 SelecSLS</cell><cell>2</cell><cell>288</cell><cell>No</cell><cell>288</cell></row><row><cell></cell><cell cols="2">w/16 x h/16 SelecSLS</cell><cell>1</cell><cell>288</cell><cell>First</cell><cell>288</cell></row><row><cell></cell><cell cols="2">w/16 x h/16 SelecSLS</cell><cell>1</cell><cell>288</cell><cell>First</cell><cell>288</cell></row><row><cell></cell><cell cols="2">w/16 x h/16 SelecSLS</cell><cell>1</cell><cell>288</cell><cell>First</cell><cell>416</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Table 4. Comparison on the single person MPI-INF-3DHP dataset. Top part are methods designed and trained for single-person capture. Bottom part are multi-person methods trained for multi-person capture but evaluated on single-person capture. Metrics used are: 3D percentage of correct keypoints (3DPCK, higher is better), area under the curve (AUC, higher is better) and mean 3D joint position error (MJPE, lower is better). * Indicates that no test time augmentation is employed. †Indicates that no ground-truth bounding box information is used and the complete image frame is processed.</figDesc><table><row><cell></cell><cell></cell><cell>9</cell></row><row><cell cols="2">LCRNet++[2019] (Res50)</cell><cell>63.5</cell></row><row><cell cols="2">LCRNet+[2019] (VGG16)</cell><cell>61.2</cell></row><row><cell>[Moon et al. 2019]</cell><cell></cell><cell>54.4</cell></row><row><cell>Ours (Stage II )</cell><cell></cell><cell>63.6</cell></row><row><cell>Method</cell><cell cols="3">3DPCK AUC MJPE</cell></row><row><cell>[Mehta et al. 2017b]</cell><cell cols="3">78.1 42.0 119.2</cell></row><row><cell>[Mehta et al. 2017b]*</cell><cell cols="3">75.0 39.2 132.8</cell></row><row><cell>[Nibali et al. 2019]</cell><cell cols="3">87.6 48.8 87.6</cell></row><row><cell>[Yang et al. 2018]</cell><cell cols="2">69.0 32.0</cell><cell>-</cell></row><row><cell>[Zhou et al. 2017]</cell><cell cols="2">69.2 32.5</cell><cell>-</cell></row><row><cell cols="3">[Pavlakos et al. 2018a] 71.9 35.3</cell><cell>-</cell></row><row><cell>[Dabral et al. 2018]</cell><cell cols="3">72.3 34.8 116.3</cell></row><row><cell cols="4">[Kanazawa et al. 2018] 72.9 36.5 124.2</cell></row><row><cell>[Mehta et al. 2018b]</cell><cell cols="3">76.2 38.3 120.5</cell></row><row><cell>[Mehta et al. 2018b]</cell><cell cols="3">74.1 36.7 125.1</cell></row><row><cell>[Mehta et al. 2018b]*</cell><cell cols="3">72.1 35.1 130.3</cell></row><row><cell>Ours (Stage II )* †</cell><cell cols="3">82.8 45.3 98.4</cell></row><row><cell>Ours (Stage III )* †</cell><cell cols="3">77.8 38.9 115.0</cell></row><row><cell>network architectures. In</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 .</head><label>5</label><figDesc>Comparison of Stage II prediction limb joint 3D pose accuracy on MPI-INF-3DHP (Single Person) for different core network choices with our channelsparse supervision of 3D pose branch of Stage I, as well as a comparison to channel-dense supervision. Metrics used are 3DPCK and AUC (higher is better).</figDesc><table><row><cell></cell><cell></cell><cell cols="2">3DPCK</cell><cell></cell><cell>Total</cell><cell></cell></row><row><cell></cell><cell cols="6">Elbow Wrist Knee Ankle 3DPCK AUC</cell></row><row><cell>ResNet-34</cell><cell>79.6</cell><cell>61.2</cell><cell>83.0</cell><cell>52.7</cell><cell>79.3</cell><cell>41.8</cell></row><row><cell>ResNet-50</cell><cell>82.4</cell><cell>61.8</cell><cell>87.1</cell><cell>58.9</cell><cell>82.0</cell><cell>44.1</cell></row><row><cell>SelecSLS</cell><cell>81.2</cell><cell>62.0</cell><cell>87.6</cell><cell>63.3</cell><cell>82.8</cell><cell>45.3</cell></row><row><cell cols="4">Channel-Dense {l j,k } J j=1 Supervision</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SelecSLS</cell><cell>79.0</cell><cell>60.2</cell><cell>82.5</cell><cell>59.0</cell><cell>80.1</cell><cell>43.3</cell></row><row><cell cols="7">Table 6. Evaluation of 3D joint position error on the 'test' split of 3DPW. The</cell></row><row><cell cols="7">error is reported as the Mean Per-Joint Position Error (MPJPE) in mm for the</cell></row><row><cell cols="7">following 14 joints: head, neck, shoulders, elbows, wrists, hips, knees, ankles.</cell></row><row><cell cols="7">PA-MPJPE indicates the error after procrustes alignment. Lower is better. Note</cell></row><row><cell cols="2">that unlike prior work</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Table 8. Evaluations on the multi-person 3D pose benchmark MuPoTS-3D [2018b] of Stage II predictions for different Stage I core network choices with channel-sparse supervision of 3D pose branch of Stage I, as well as a comparison to channel-dense supervision. We evaluate on all annotated subjects using the 3D percentage of correct keypoints (3DPCK) metric, and also show the 3DPCK only for predictions that were matched to an annotation.Table 9. Comparison of limb joint 3D pose accuracy on MuPoTS-3D (Multi Person) for predictions from Stage II and Stage III of our proposed design. The metric used is 3D Percentage of Correct Keypoints (3DPCK), evaluated with a threshold of 150mm.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>TS20 Total</cell></row><row><cell>LCRNet*</cell><cell cols="4">67.7 49.8 53.4 59.1 67.5 22.8 43.7 49.9 31.1 78.1 33.4 33.5 51.6 49.3 56.2 66.5 65.2 62.9 66.1 59.1 53.8</cell></row><row><cell>Single Shot</cell><cell cols="4">81.0 59.9 64.4 62.8 68.0 30.3 65.0 59.2 64.1 83.9 67.2 68.3 60.6 56.5 69.9 79.4 79.6 66.1 64.3 63.5 65.0</cell></row><row><cell>MP3D*</cell><cell cols="4">85.1 67.9 73.5 76.2 74.9 52.5 65.7 63.6 56.3 77.8 76.4 70.1 65.3 51.7 69.5 87.0 82.1 80.3 78.5 70.7 71.3</cell></row><row><cell>Root+PoseNet*</cell><cell cols="4">94.4 77.5 79.0 81.9 85.3 72.8 81.9 75.7 90.2 90.4 79.2 79.9 75.1 72.7 81.1 89.9 89.6 81.8 81.7 76.2 81.8</cell></row><row><cell cols="5">LCRNet++ (Res50)* 87.3 61.9 67.9 74.6 78.8 48.9 58.3 59.7 78.1 89.5 69.2 73.8 66.2 56.0 74.1 82.1 78.1 72.6 73.1 61.0 68.9</cell></row><row><cell>Ours (Stage II )*</cell><cell cols="4">89.7 65.4 67.8 73.3 77.4 47.8 67.4 63.1 78.1 85.1 75.6 73.1 65.0 59.2 74.1 84.6 87.8 73.0 78.1 71.2 72.1</cell></row><row><cell>Ours (Stage III )*</cell><cell cols="4">86.3 63.5 66.1 71.1 69.7 48.4 68.4 62.9 76.4 85.4 72.7 75.1 61.9 62.9 70.3 84.4 84.6 72.2 70.4 69.4 70.4</cell></row><row><cell>Matched</cell><cell cols="4">TS1 TS2 TS3 TS4 TS5 TS6 TS7 TS8 TS9 TS10 TS11 TS12 TS13 TS14 TS15 TS16 TS17 TS18 TS19 TS20 Total</cell></row><row><cell>LCRNet*</cell><cell cols="3">69.1 67.3 54.6 61.7 74.5 25.2 48.4 63.3 69 78.1 53.8 52.2 60.5 60.9 59.1 70.5 76</cell><cell>70 77.1 81.4 62.4</cell></row><row><cell>Single Shot</cell><cell cols="4">81.0 64.3 64.6 63.7 73.8 30.3 65.1 60.7 64.1 83.9 71.5 69.6 69 69.6 71.1 82.9 79.6 72.2 76.2 85.9 69.8</cell></row><row><cell>MP3D*</cell><cell cols="4">85.8 73.6 61.1 55.7 77.9 53.3 75.1 65.5 54.2 81.3 82.2 71.0 70.1 67.7 69.9 90.5 85.7 86.3 85.0 91.4 74.2</cell></row><row><cell>Root+PoseNet*</cell><cell cols="4">94.4 78.6 79.0 82.1 86.6 72.8 81.9 75.8 90.2 90.4 79.4 79.9 75.3 81.0 81.0 90.7 89.6 83.1 81.7 77.2 82.5</cell></row><row><cell cols="5">LCRNet++ (Res50)* 88.0 73.3 67.9 74.6 81.8 50.1 60.6 60.8 78.2 89.5 70.8 74.4 72.8 64.5 74.2 84.9 85.2 78.4 75.8 74.4</cell><cell>-</cell></row><row><cell>Ours (Stage II )*</cell><cell cols="4">89.7 78.6 68.4 74.3 83.7 47.9 67.4 75.4 78.1 85.1 78.7 73.8 73.9 77.9 74.8 87.1 88.3 79.5 88.3 97.5 78.0</cell></row><row><cell>Ours (Stage III )*</cell><cell cols="4">86.3 76.2 66.7 72.1 75.3 48.5 68.4 75.1 76.4 85.4 75.7 75.8 70.3 82.9 70.9 86.9 85.1 78.7 79.5 95.0 76.2</cell></row><row><cell></cell><cell cols="2">3DPCK</cell><cell>% Subjects</cell></row><row><cell></cell><cell>All</cell><cell cols="2">Matched Matched</cell></row><row><cell>ResNet-34</cell><cell>69.3</cell><cell>75.2</cell><cell>92.1</cell></row><row><cell>ResNet-50</cell><cell>71.8</cell><cell>77.2</cell><cell>93.0</cell></row><row><cell>SelecSLS</cell><cell>72.1</cell><cell>78.0</cell><cell>92.5</cell></row><row><cell cols="3">Channel-Dense {l j,k } J j=1 Supervision</cell><cell></cell></row><row><cell>SelecSLS</cell><cell>70.2</cell><cell>75.7</cell><cell>92.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell>All Joints</cell></row><row><cell cols="4">Shoul Elbow Wrist Knee Ankle Total Vis. Occ.</cell></row><row><cell cols="4">Stage II 81.4 65.8 53.2 71.0 47.3 72.1 76.0 58.8</cell></row><row><cell cols="4">Stage III 73.8 67.2 54.0 75.8 54.3 70.4 75.0 55.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 .</head><label>1</label><figDesc>Evaluation of possible baseline architecture choices for the core network. The networks are trained on MPI LSP [2010; single person 2D pose datasets, and evaluated on LSP testset. The inference speed ratios are with respect to ResNet-50 forward pass time for 320×320 pixel images on an NVIDIA K80 GPU, using [Jolibrain Caffe Fork 2018] with optimized depthwise convolution implementation.</figDesc><table><row><cell>Core Network</cell><cell cols="2">PCK FP Speed Ratio</cell></row><row><cell cols="2">MobileNetV2 1.0x [2018] 85</cell><cell>1.78</cell></row><row><cell cols="2">MobileNetV2 1.3x [2018] 86</cell><cell>1.51</cell></row><row><cell>Xception [2017]</cell><cell>81</cell><cell>0.67</cell></row><row><cell>InceptionV3 [2016]</cell><cell>88</cell><cell>0.96</cell></row><row><cell>ResNet-34 [2016]</cell><cell>89</cell><cell>1.27</cell></row><row><cell>ResNet-50 [2016]</cell><cell>89</cell><cell>1.00</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 .</head><label>2</label><figDesc>Evaluation of design decisions for first stage of our system. We evaluate different core networks with the 2D pose branch on a subset of validation frames of MS COCO dataset. Also reported are the forward pass timings of the core network and the 2D pose branch on different GPUs (K80, TitanX (Pascal)) as well as Xeon E5-1607 CPU on 512×320 pixel input. We also evaluate the publicly available model of<ref type="bibr" target="#b10">[Cao et al. 2017]</ref> on the same subset of validation frames.</figDesc><table><row><cell></cell><cell>FP Time</cell></row><row><cell>Core Network</cell><cell>K80 TitanX CPU AP AP 0.5 AP 0.75 AR AR 0.5 AR 0.75</cell></row><row><cell>ResNet-50</cell><cell>35.7ms 9.6ms 349ms 48.8 74.6 52.1 53.2 76.8 56.3</cell></row><row><cell>ResNet-34</cell><cell>25.7ms 5.7ms 269ms 46.4 72.7 47.3 51.3 75.2 52.8</cell></row><row><cell>Ours</cell><cell></cell></row><row><cell cols="2">Add-Skip Prev. (B) 24.5ms 6.5ms 167ms 47.0 73.4 49.7 51.7 75.6 54.5</cell></row><row><cell cols="2">Conc.-Skip Prev. (B) 24.3ms 6.3ms 172ms 47.6 73.3 50.7 52.6 76.1 55.6</cell></row><row><cell cols="2">Conc.-Skip Prev. (W) 25.0ms 6.7ms 184ms 48.3 74.4 51.1 52.9 76.5 55.7</cell></row><row><cell cols="2">Conc.-Skip First (W) 25.0ms 6.7ms 184ms 48.6 74.2 52.2 53.3 76.6 56.7</cell></row><row><cell>[Cao et al. 2017]</cell><cell>243ms 73.4ms 3660ms 58.0 79.5 62.9 62.1 81.2 66.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 .</head><label>3</label><figDesc>Results of SelecSLSNet on image classification on the ImageNet dataset. The top-1 and top-5 accuracy on the Imagenet validation set is shown, as well the maximum batch size of 224 × 224 pixel images that can be run in inference mode on an Nvidia V100 16GB card, with FP16 compute. Also shown is the peak throughput obtained with each network, and the batch size of peak throughput (in brackets). SelecSLSNet achieves comparable performance to ResNet-50<ref type="bibr" target="#b27">[He et al. 2016]</ref>, while being 1.3−1.4× faster, and with a much smaller memory footprint.</figDesc><table><row><cell></cell><cell>Speed</cell><cell>Maximum</cell><cell cols="2">Accuracy</cell></row><row><cell></cell><cell cols="4">(Images / sec) Batch Size top-1 top-5</cell></row><row><cell>ResNet-50</cell><cell>2700</cell><cell>1200 (1024)</cell><cell>78.5</cell><cell>94.3</cell></row><row><cell>SelecSLSNet</cell><cell>3900</cell><cell>2000 (1800)</cell><cell>78.4</cell><cell>98.1</cell></row><row><cell cols="5">Fig. 2. For experiments on the image classification task on ImageNet [Rus-</cell></row><row><cell>sakovsky et al. 2015]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 .</head><label>4</label><figDesc>Evaluation of the impact of the different components from Stage I that form the input to Stage II. The method is trained for multi-person pose estimation and evaluated on the MPI-INF-3DHP [2017a] single person 3D pose benchmark. The components evaluated are the 2D pose predictions P 2D k , the body joint confidences C k , and the set of extracted 3D pose encodings {l j,k } J j=1 . Metrics used are: 3D percentage of correct keypoints (3DPCK, higher is better), area under the curve (AUC, higher is better) and mean 3D joint position error (MJPE, lower is better). Also shown are the results with channel-dense supervision of 3D pose encodings {l j,k } J j=1 , as well as evaluation of Stage III output.</figDesc><table><row><cell></cell><cell cols="2">3DPCK</cell><cell></cell></row><row><cell>Stage II</cell><cell>Stand</cell><cell>On The</cell><cell>Total</cell></row><row><cell>Input</cell><cell cols="3">/Walk Sitt. Floor 3DPCK AUC</cell></row><row><cell cols="3">P 2D k (2D Branch Only) 86.4 76.3 44.9</cell><cell>76.0 42.1</cell></row><row><cell cols="3">P 2D k P 2D k +C k P 2D k +C k +{l j,k } J j=1 Channel-Dense {l j,k } J j=1 Supervision 79.8 78.4 58.5 85.9 79.4 58.7 88.4 85.8 70.7</cell><cell>75.5 41.3 77.2 42.2 82.8 45.3</cell></row><row><cell>P 2D k +C k +{l j,k } J j=1</cell><cell cols="2">87.0 83.6 61.5</cell><cell>80.1 43.3</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adobe</forename></persName>
		</author>
		<ptr target="https://www.mixamo.com/" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to Reconstruct People in Clothing from a Single RGB Camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thiemo</forename><surname>Alldieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Magnor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><forename type="middle">Lal</forename><surname>Bhatnagar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Detailed Human Avatars from Monocular Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thiemo</forename><surname>Alldieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Magnor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Video Based Reconstruction of 3D People Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thiemo</forename><surname>Alldieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Magnor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">2D Human Pose Estimation: New Benchmark and State of the Art Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Exploiting temporal context for 3D human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3395" to="3404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-Garment Net: Learning to Dress 3D People from Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garvita</forename><surname>Bharat Lal Bhatnagar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Keep it SMPL: Automatic Estimation of 3D Human Pose and Shape from a Single Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Binarized convolutional landmark localizers for human pose estimation and face alignment with limited resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gines</forename><surname>Hidalgo Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser A</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">1 &amp;#8364; Filter: A Simple Speed-based Low-pass Filter for Noisy Input in Interactive Systems (CHI &apos;12)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Géry</forename><surname>Casiez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Roussel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Vogel</surname></persName>
		</author>
		<idno type="DOI">10.1145/2207676.2208639</idno>
		<ptr target="https://doi.org/10.1145/2207676.2208639" />
		<imprint>
			<date type="published" when="2012" />
			<publisher>ACM</publisher>
			<biblScope unit="page" from="2527" to="2530" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Performance animation from low-dimensional control signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxiang</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><forename type="middle">K</forename><surname>Hodgins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="686" to="696" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Synthesizing Training Images for Boosting Human 3D Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhe</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-Person 3D Human Pose Estimation from Monocular Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Dabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Gundavarapu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="405" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning 3d human pose from structure and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Dabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Mundhada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uday</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Safeer</forename><surname>Afaque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV</title>
		<meeting>the European Conference on Computer Vision (ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="668" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">MARCOnI -ConvNet-based MARker-less Motion Capture in Outdoor and Indoor Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elhayek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Aguiar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning pose grammar to encode human body configuration for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Carbin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03635</idno>
		<title level="m">The lottery ticket hypothesis: Finding sparse, trainable neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Using k-poselets for detecting people and localizing their keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3582" to="3589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Estimating human shape and pose from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Peng Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Bãčlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2009.5459300</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2009.5459300" />
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1381" to="1388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Holopose: Holistic 3d human reconstruction in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alp</forename><surname>Riza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Guler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10884" to="10894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Dense-Pose: Dense Human Pose Estimation in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Riza Alp Güler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kokkinos</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00762</idno>
		<idno>CVPR. 7297-7306</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2018.00762" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">What Face and Body Shapes Can Tell Us About Height</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Semih</forename><surname>Gunel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">LiveCap: Real-time Human Performance Capture from Monocular Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Habermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollhoefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH)</title>
		<meeting>SIGGRAPH)</meeting>
		<imprint>
			<date type="published" when="2019-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Panoptic Studio: A Massively Multiview System for Social Motion Capture</title>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<editor>Lei Tan Lin Gui Bart Nabbe Iain Matthews Takeo Kanade Shohei Nobuhara Hanbyul Joo, Hao Liu and Yaser Sheikh</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Searching for mobilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1314" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Towards accurate marker-less human shape and pose estimation over time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ijaz</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="421" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrest</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalid</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<title level="m">SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt;0.5MB model size</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bjoern Andres, Bernt Schiele, and Saarland Informatics Campus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Levinkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ArtTrack: Articulated multi-person tracking in the wild</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hu-man3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multi-person pose estimation with local joint-to-person associations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="627" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">MovieReshape: Tracking and Reshaping of Humans in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Thormählen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<idno type="DOI">10.1145/1882261.1866174</idno>
		<ptr target="https://doi.org/10.1145/1882261.1866174" />
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">148</biblScope>
			<date type="published" when="2010-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Clustered Pose and Nonlinear Appearance Models for Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<idno type="DOI">10.5244/C.24.12</idno>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning Effective Human Pose Estimation from Inaccurate Annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title/>
		<ptr target="https://github.com/jolibrain/caffe" />
	</analytic>
	<monogr>
		<title level="j">Jolibrain Caffe Fork</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Panoptic Studio: A Massively Multiview System for Social Motion Capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Nabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohei</forename><surname>Nobuhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3334" to="3342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">End-to-end Recovery of Human Shape and Pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning 3D Human Dynamics from Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panna</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning latent representations of 3d human pose with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isinsu</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bugra</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="page" from="1326" to="1341" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">VIBE: Video Inference for Human Body Pose and Shape Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning to Reconstruct 3D Human Pose and Shape via Model-fitting in the Loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2252" to="2261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Convolutional mesh regression for single-image human shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4501" to="4510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unite the People: Closing the Loop Between 3D and 2D Human Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">3d human pose estimation from monocular images with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>In ACCV</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Maximum-margin structured learning with deep networks for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2848" to="2856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">SMPL: A Skinned Multi-Person Linear Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV</title>
		<meeting>the European Conference on Computer Vision (ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="116" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayat</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Monocular 3D Human Pose Estimation In The Wild Using Improved CNN Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<idno type="DOI">10.1109/3dv.2017.00064</idno>
		<ptr target="https://doi.org/10.1109/3dv.2017.00064" />
	</analytic>
	<monogr>
		<title level="m">3DV. IEEE</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">VNect: Real-time 3D Human Pose Estimation with a Single RGB Camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<idno type="DOI">10.1145/3072959.3073596</idno>
		<ptr target="https://doi.org/10.1145/3072959.3073596" />
	</analytic>
	<monogr>
		<title level="m">3DV. IEEE</title>
		<editor>SingleShotMultiPerson Dushyant Mehta, Srinath Sridhar, Oleksandr Sotnychenko, Helge Rhodin, Mohammad Shafiei, Hans-Peter Seidel, Weipeng Xu, Dan Casas, and Christian Theobalt</editor>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">36</biblScope>
		</imprint>
	</monogr>
	<note>Single-Shot Multi-Person 3D Pose Estimation From Monocular RGB</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">ESPNet: Efficient Spatial Pyramid of Dilated Convolutions for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anat</forename><surname>Caspi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Camera Distance-aware Top-down Approach for 3D Multi-person Pose Estimation from a Single RGB Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Menache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on International Conference on Computer Vision (ICCV)</title>
		<editor>Kyoung Mu Lee</editor>
		<meeting><address><addrLine>San Francisco, CA, USA. Gyeongsik Moon</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Understanding Motion Capture for Computer Animation</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Associative Embedding: End-to-End Learning for Joint Detection and Grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">3D Human Pose Estimation with 2D Marginal Heatmaps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aiden</forename><surname>Nibali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Prendergast</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Neural Body Fitting: Unifying Deep Learning and Model Based Human Pose and Shape Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christop</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Towards Accurate Multi-person Pose Estimation in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nori</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Expressive Body Capture: 3D Hands, Face, and Body from a Single Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">A A</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Ordinal Depth Supervision for 3D Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Coarse-to-Fine Volumetric Prediction for Single-Image 3D Human Pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Learning to Estimate 3D Human Pose and Shape from a Single Color Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">DeepCut: Joint Subset Partition and Labeling for Multi Person Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Articulated people detection and pose estimation: Reshaping the future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Thormählen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. IEEE</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3178" to="3185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Posebits for monocular human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2337" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Deep Multitask Architecture for Integrated 2D and 3D Human Sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alin-Ionut</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Reconstructing 3d human pose from 2d image landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="573" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Exploiting temporal information for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imtiaz</forename><surname>Mir Rayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="68" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4780" to="4789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">EgoCap: Egocentric Marker-less Motion Capture with Two Fisheye Cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TOG (Proc. SIGGRAPH Asia</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">General Automatic Human Shape and Motion Capture Using Volumetric Contour Cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadia</forename><surname>Robertini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46454-1_31</idno>
		<idno>ECCV. 509-526</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46454-1_31" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">LCR-Net: Localization-Classification-Regression for Human Pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">LCR-Net++: Multi-person 2D and 3D Pose Detection in Natural Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grégory</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Erfnet: Efficient residual factorized convnet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Luis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="263" to="272" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint>
			<pubPlace>Alexander C</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-015-0816-y</idno>
		<ptr target="https://doi.org/10.1007/s11263-015-0816-y" />
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. IEEE</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">3D Human pose estimation: A review of the literature and analysis of covariates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Sarafianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Boteanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kakadiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Humaneva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Alexandru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="4" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Surface capture for performance-based animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Starck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Hilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE computer graphics and applications</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Fast articulated motion tracking using a sums of Gaussians body model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Stoll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Hasler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="951" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Deep High-Resolution Representation Learning for Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Articulated part-based model for joint object detection and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV. IEEE</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="723" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Compositional human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxiang</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; •</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="529" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Human mesh recovery from monocular images via a skeleton-disentangled representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yili</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5349" to="5358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quoc V Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<title level="m">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Structured Prediction of 3D Human Pose with Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isinsu</forename><surname>Bugra Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Learning to Fuse 2D and 3D Image Cues for Monocular Body Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bugra Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Márquez-Neila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">xR-EgoPose: Egocentric 3D Human Pose from an HMD Camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Peluse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lourdes</forename><surname>Agapito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hernan</forename><surname>Badino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7728" to="7738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">Total Capture: 3D Human Pose Estimation Fusing Video and Inertial Sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Trumble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Malleson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Collomosse</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Self-supervised Learning of Motion Capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Yu</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Wei</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5242" to="5252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Recovering Accurate 3D Human Pose in The Wild Using IMUs and a Moving Camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Timo Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Bodo Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Human Pose Estimation from Video and IMUs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Timo Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1533" to="1547" />
			<date type="published" when="2016-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Deep High-Resolution Representation Learning for Visual Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaorui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<title level="m" type="main">VideoMocap: Modeling Physically Realistic Human Motion from Monocular Video Sequences. TOG 29, 4, Article 42</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxiang</forename><surname>Chai</surname></persName>
		</author>
		<idno type="DOI">10.1145/1778765.1778779</idno>
		<ptr target="https://doi.org/10.1145/1778765.1778779" />
		<imprint>
			<date type="published" when="2010-07" />
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">A Survey on Human Performance Capture and Animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihong</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Kun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxiang</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Sci. Technol</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="536" to="554" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Monocular total capture: Posing face, body, and hands in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10965" to="10974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. IEEE</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Mo 2 Cap 2: Real-time Mobile 3D Motion Capture with a Cap-mounted Fisheye Camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avishek</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollhoefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="2093" to="2101" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Monoperfcap: Human performance capture from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avishek</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Recovering 3D Planes from a Single Image via Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengting</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="85" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">3d human pose estimation in the wild by adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Monocular 3D Pose and Shape Estimation of Multiple People in Natural Scenes-The Importance of Multiple Scene Constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeta</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2148" to="2157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Deep Network for the Integrated 3D Sensing of Multiple People in Natural Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeta</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Zanfir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Alin-Ionut Popa, and Cristian Sminchisescu</note>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Towards 3D Human Pose Estimation in the Wild: A Weakly-Supervised Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="398" to="407" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
