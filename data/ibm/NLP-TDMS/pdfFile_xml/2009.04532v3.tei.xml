<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attention based Writer Independent Verification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Abuzar</forename><surname>Shaikh</surname></persName>
							<email>*mshaikh2@buffalo.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering State</orgName>
								<orgName type="institution">University of New York at Buffalo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiehang</forename><surname>Duan</surname></persName>
							<email>†tiehangd@buffalo.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering State</orgName>
								<orgName type="institution">University of New York at Buffalo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihir</forename><surname>Chauhan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering State</orgName>
								<orgName type="institution">University of New York at Buffalo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sargur</forename><forename type="middle">N</forename><surname>Srihari</surname></persName>
							<email>§srihari@buffalo.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering State</orgName>
								<orgName type="institution">University of New York at Buffalo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Attention based Writer Independent Verification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The task of writer verification is to provide a likelihood score for whether the queried and known handwritten image samples belong to the same writer or not. Such a task calls for the neural network to make it's outcome interpretable, i.e. provide a view into the network's decision making process. We implement and integrate cross-attention and soft-attention mechanisms to capture the highly correlated and salient points in feature space of 2D inputs. The attention maps serve as an explanation premise for the network's output likelihood score. The attention mechanism also allows the network to focus more on relevant areas of the input, thus improving the classification performance. Our proposed approach achieves a precision of 86% for detecting intra-writer cases in CEDAR cursive "AND" dataset. Furthermore, we generate meaningful explanations for the provided decision by extracting attention maps from multiple levels of the network.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Writer independent verification is the task to measure the similarity of two given handwritten samples as how likely is it that the samples were written by the same, without having any knowledge of writers identity. There have been many efforts in this field to provide an automated hint to streamline the job of manual handwriting examiners, making this an interesting research problem.</p><p>A general intuition is that samples from a single source tend to be similar while samples from different sources tend to show bigger variances. The premise for finding unique characteristics is based on the hypothesis that every individual has a unique way of writing <ref type="bibr" target="#b0">[1]</ref>. Further studies show that two different writers may also happen to have a similar writing style. This makes the problem of handwriting verification challenging.</p><p>With the advent of automated pattern learning methods; especially with models that can be trained to focus on key areas, it is possible to design a robust system to assist a Forensic Document Examiner (FDE). In this work we propose attention based approaches for the task of handwriting verification. We produce two kind of attention maps 1 which highlights (i) the important corresponding pixel regions between two images that the network deemed similar (ii) the high correlated pixel regions, in the feature space of given samples, that the network attends to provide its decision. Such visualizations render the desired interpretability for forensic verification systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>The task of verification is prevalent across bio-metric domains, viz. face verification in DeepFace <ref type="bibr" target="#b1">[2]</ref>, fingerprint verification <ref type="bibr" target="#b2">[3]</ref>, handwritten evidence verification <ref type="bibr" target="#b3">[4]</ref>, iris verification <ref type="bibr" target="#b4">[5]</ref>, speaker verification <ref type="bibr" target="#b5">[6]</ref>. Our paper focuses on handwritten evidence verification and proposes a novel approach for writer independent verification.</p><p>Earlier, researchers in the field of handwriting verification used handcrafted features coupled with classic learning techniques, where, Rjean et al <ref type="bibr" target="#b6">[7]</ref> present an overview of preprocessing techniques and feature extraction methods from handwritten text, Srihari et al introduced Gradient Structural Concavity (GSC) feature extraction technique <ref type="bibr" target="#b0">[1]</ref> for writer identification and verification and present CEDAR-FOX tool, which is a noteworthy contribution in this field. L Bovino et al <ref type="bibr" target="#b7">[8]</ref> present a multi-expert system based on stroke-oriented description for dynamic verification, Marius Bulacu et al <ref type="bibr" target="#b8">[9]</ref> present statistical methods that operate on the texture level and the character-shape (allograph) level, AA Brink <ref type="bibr" target="#b9">[10]</ref> prove that slantness as a feature for handwritten text is overrated which shows the robustness of handcrafted features, K Tselios et al <ref type="bibr" target="#b10">[11]</ref> present automated feature extraction method based for handwritten text, D. Bertolini et al <ref type="bibr" target="#b11">[12]</ref> discuss the use of texture descriptors to perform writer verification, M. N. Abdi et al <ref type="bibr" target="#b12">[13]</ref> propose a grapheme-based approach to offline Arabic writer identification and verification, Manabu Okawa et al <ref type="bibr" target="#b13">[14]</ref> propose a text and user generic model for writer verification that uses a combination of pen pressure information from ink intensity and writing indentations.</p><p>However, the recently proposed deep learning models enabled automatic extraction of generic features. Ameur Bensefia <ref type="bibr" target="#b14">[15]</ref> use Levenshtein edit distance based on Fisher-Wagner algorithm to estimate the cost of transforming one handwritten word into another, Shaikh et al <ref type="bibr" target="#b15">[16]</ref> present a Hybrid Deep Learning architecture combining handcrafted features and Convolutional Neural Network (CNN) based features, Chu et al <ref type="bibr" target="#b16">[17]</ref> propose an end-to-end deep learning method based on statistical features extracted on set-of-samples level.</p><p>Nevertheless, verification is still a challenge, as lack of interpretability still exists in these current widely used models. This makes it tough for the FDE to rely on model's binary decision. There is a demand to provide visual or textual explanations for a models decision which can aid the FDE's in verification of the samples confidently. Chauhan et al <ref type="bibr" target="#b17">[18]</ref> generate explanations for the confidence provided by CNN by mapping the input image to 15 annotated features provided by experts. However, the generation of such annotated data using using manual labor takes significant effort and is time consuming.</p><p>Hence, there is a need for a model to display what location is it attending to, while making a decision. Attention models proposed by Dzmitry Bahdanau et al <ref type="bibr" target="#b18">[19]</ref> automatically search for parts of a source input that are relevant to predicting the output, Luong et al <ref type="bibr" target="#b19">[20]</ref> proposed hard and soft attention wherein hard attention allows the output to be influenced by exactly one most relevant input, and soft attention uses a less strict criteria by only boosting the most relevant input while still allowing a subset of other inputs to contribute towards the models decision. Vaswani et al <ref type="bibr" target="#b20">[21]</ref> presented a very powerful model in Natural Language Processing (NLP) based solely on multi headed attention mechanisms, dispensing with recurrence and convolutions entirely. Han Zhang et al <ref type="bibr" target="#b21">[22]</ref> extend the concept of attention to 2d images to capture non-local dependencies and demonstrate the models efficacy generating images. Furthermore, Naofumi Tomita et al <ref type="bibr" target="#b22">[23]</ref> leverage 3D CNN to perform object localization using attention.</p><p>In this work, we propose to tackle the writer verification task with Multi-Head Cross Attention (MHCA) mechanism, where the model compares the two input images and focuses on the corresponding and relevant pixel in their feature space. The network integrates a Soft Attention (SA) mechanism designed using 3D CNN to help it attend more on the important correlated features for classification. We then generate two attention maps, (i) to show which locations in the two images are highly correlated, (ii) to show at which locations the model focuses for classification. We conduct extensive experiments on two datasets to show the effectiveness of cross attention combined with soft attention. Our experiments demonstrate that the approach performs at par or better than the existing state of the art methods on widely used datasets while also providing insightful explanation on model's decision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DATASET</head><p>We perform experiments on "AND" Dataset <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b17">[18]</ref> and test the performance of best model on CEDAR Signature <ref type="bibr" target="#b23">[24]</ref> dataset. Samples in both the datasets were handwritten on a paper and then scanned to create images of the manuscripts. For both datasets, after appropriate square padding corresponding to the maximum width and height of the sample, we resize each of them to have consistent size of 64 × 64 using bi-cubic interpolation. Moreover, we invert the pixels such that all the background pixels are 0. We apply a threshold on the images in CEDAR Signature dataset and change any pixel values which less than 30 to 0. Finally, we normalize the images in both datasets by dividing each pixel with 255 so that the range of values of the pixels stay between 0 and 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Cursive "AND"</head><p>The dataset is formed by cropping segments containing the word "AND" from the full letter CEDAR dataset <ref type="bibr" target="#b0">[1]</ref>. Post cropping, some non-"AND" words were removed manually.</p><p>After cleaning, we have a dataset of 1533 writers, constituting around 14000 samples variable shapes.</p><p>We define the samples written by the same and different writer as "intra-writer" and "inter-writer" pairs respectively. Each writer on an average has 9 samples of the word "AND". Hence we have around 9 2 = 36 samples of similar pair per writer. Therefore, we have around 1533 × 36 = 55188 pairs of intra-writer samples. Furthermore, we shuffle all the samples and use K-Fold cross validation <ref type="bibr" target="#b24">[25]</ref> with K = 5. We choose 4 folds as training set and 1 fold as testing set. Within the respective folds, we randomly generate 10 times more inter-writer pairs than intra-writer pairs, to make the training more effective as done in <ref type="bibr" target="#b25">[26]</ref>. Thus, the ratio of intrawriter pairs to inter-writer pairs in our training and testing sets is 1:10 for all experiments with this dataset.</p><p>B. CEDAR signature CEDAR signature database <ref type="bibr" target="#b23">[24]</ref> contains signatures from 55 individuals. Each of these signers signed 24 genuine signatures. For each signer there are 24 forgery samples from about 20 skillful forgers. Hence the dataset contains 1,320 genuine signatures as well as 1,320 forged signatures. All images in this dataset are available in gray scale mode.</p><p>We divide this dataset in 11 folds based on writer ids. Next, we consider random 10 parts as training set and remaining 1 part as test set. Thus, there are 50 writers are in training set and 5 writers in testing set. Furthermore, we have </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. METHOD</head><p>We employ cross attention (CA) and soft attention (SA) mechanism in conjunction with Inception-Resnet-v2 (IRv2) <ref type="bibr" target="#b26">[27]</ref>, a powerful feature extractor. The overall network is displayed in <ref type="figure">Fig. 4</ref>. We first process the input through the original stem block of IRv2 or two convolutional blocks of VGG16 (we consider this as VGG16's stem). The stem block extracts d features while scaling down the input height H and width W to h and w respectively. The higher level modules are discussed below:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Shared Weights</head><p>To process two input images simultaneously we train two copies of the stem block in a Siamese setting, such that both the branches have shared weights. The stem block outputs a 3D tensor R h ×w ×d . The stem is the replica of the IRv2 stem block, with only difference that inputs are gray scaled and of size 64 × 64.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Cross Attention</head><p>To cross attend f 1 , f 2 the feature outputs of stem 1 and stem 2 respectively, we first generate the key k, value v and query are 1×1 convolution functions. Evidently in <ref type="figure" target="#fig_1">Fig. 2</ref>, image 2 is considered as key input and image 1 as the query input. The k, q and v are then reshaped to tensor ∈ R h * w ×d = R t×d . Cross Attention (CA) is inspired by the self-attention presented in <ref type="bibr" target="#b21">[22]</ref> and is demonstrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. We calculate the relevance between q and k, which are features of two distinct inputs.</p><formula xml:id="formula_0">q. Where k = M (f 2 ); v = N (f 2 ); q = L(f 1 ) and M, N, L</formula><formula xml:id="formula_1">β i,j = exp(z ij ) t j=1 exp(z ij ) , where z ij = k q T ; z ij ∈ R t×t (1)</formula><p>Here, represents matrix multiplication; z ij calculates the relevance between every i th and j th location in k and q respectively. β ij , the attention map represents softmax normalization across every i th row as Eq. 1 boosts the most relevant j th value and suppresses the non-relevant values, for each corresponding i th row . Next, we calculate the enhanced representation r, of image 1 with infused contextual representation of image 2.</p><formula xml:id="formula_2">r i = t j=1 β i,j v j , where r i ∈ {r 1 , r 2 , . . . , r t } (2)</formula><p>r is then reshaped to tensor ∈ R h ×w ×d and the final output vector o is calculated. Here, o = V (r), where V is 1 × 1 convolution function. Furthermore, we swap the inputs and perform the aforementioned calculations again. This recalculation makes sure that features of image 1 have the context of image 2 and viceversa. Finally, the two outputs are concatenated on channel axis such that the resultant output ∈ R h ×w ×2d</p><p>The above operations constitute two attention heads, that is one set of weights for each output. We can scale this module to have n attention heads. In our experiments n = 8, such that first four heads are setup with image 1 as key and remaining four have image 2 as key. Multiple attention heads help the network to identify more than one relevant feature locations of one image with respect to a given index in another.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Soft Attention</head><p>The foreground pixels that contain a handwritten stroke are useful pixels. Each sample in the data contains only 7% percent of such useful pixels, as rest of the pixels are background with no information. Inspired by the work done by <ref type="bibr" target="#b22">[23]</ref> we propose a soft attention technique that uses 3Dconvolution <ref type="bibr" target="#b27">[28]</ref> to attend and identify only the most important features responsible for classification. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convolving a given three dimension output</head><formula xml:id="formula_3">f x ∈ R h x ×w x ×d x with one 3D kernel g of size 3×3×d x , generates a feature map f 3d ∈ R h x ×w x ×1 .</formula><p>Having K such kernels represents K attention heads and generates a feature map f 3d ∈ R h x ×w x ×K . Next, we normalize each of the K feature maps and aggregate them to calculate the soft attention scores ζ:</p><formula xml:id="formula_4">ζ = K k=1 exp(f 3dij ) w x i=1 h x j=1 exp(f 3dij ) ; where f 3d = g(f x ) (3)</formula><p>The normalization assigns an importance score for relevant locations in the feature map. Next we multiply f x with ζ and obtain f x ζ , to scale the values of salient locations. Finally, o ζ is calculated by adding f x with the product of f x ζ and ω, where ω is a learnable scalar.</p><formula xml:id="formula_5">o ζ = f x + ωf x ζ (4)</formula><p>This enables the network to decide how much attention should be applied over specific locations of the feature map.  <ref type="figure">Fig. 4</ref>. End to end architecture using VGG16 <ref type="bibr" target="#b28">[29]</ref> or Inception-Resnet-v2 (IRv2) <ref type="bibr" target="#b26">[27]</ref> blocks, multiple Cross Attention (CA) and Soft Attention (SA) modules. ⊗ indicates concatenation on channel axis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stem</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Loss Function</head><p>This problem is set up as a two class classification problem. Hence, we optimize the network using categorical crossentropy (CCE) function</p><formula xml:id="formula_6">L CCE = − log(p t )<label>(5)</label></formula><p>However, since there is a large skew between inter and intraclass samples, we also experiment with Focal Loss (FL) <ref type="bibr" target="#b29">[30]</ref> and implement the categorical focal loss L f l as</p><formula xml:id="formula_7">L F L = −α t (1 − p t ) γ log(p t );<label>(6)</label></formula><p>where</p><formula xml:id="formula_8">p t = p if y = 1 1 − p otherwise ; α t = α if y = 1 1 − α otherwise p ∈ [0, 1]</formula><p>is the model's estimated probability, and y ∈ [0, 1] is the ground truth label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS AND EVALUATION A. Setup</head><p>The initial building blocks of our feature extraction model uses the feature extraction blocks from VGG16 <ref type="bibr" target="#b28">[29]</ref> or Inveption-ResNetv2 (IRv2) model <ref type="bibr" target="#b26">[27]</ref>. These networks are the state-of-the-art image feature extractors trained on Ima-geNet <ref type="bibr" target="#b30">[31]</ref> dataset for classification. We train the model in two settings of input: (1) Two images concatenated on channel axis (Concat), (2) Two images in parallel(Siamese). In Concat setting, there is only one input to the network as we pre-process the two input images to overlay on one another. Soft-Attention mechanism is applicable to both the settings, and Cross-Attention is applicable only in case 2, when the two inputs are in a Siamese setting. Siamese setting is as displayed in <ref type="figure">Fig. 4</ref>.</p><p>The CA blocks are laid in parallel to extract key point correspondence between the input images. Next, a block, of SA layer followed by a 2 × 2 max-pool layer, is used alongside a standard feature reduction 2 × 2 max-pool block. The outputs of the SA max-pool and standard max-pool are then concatenated on channel axis. This helps the network attend more on important features for classification and reduce adherence to noisy part of data. To introduce some data augmentation and regularization we introduce a dropout of 0.5% probability after each convolutional block.</p><p>We train the network for 100 epochs with an early stopping patience of 20 epochs. Validation loss is monitored at every epoch to checkpoint the weights of the best model. For all the experiments we use Adam optimizer with learning rate, lr = 0.0001 and a decay, lr decay = 1.0 −6 . Both the customized VGG16 and IRv2 networks contain around 23 million parameters respectively. We utilized three 11 GB Nvidia 1080Ti GPUs in parallel for all the experiments which were written in Keras framework on Tensorflow backend.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation metrics</head><p>We evaluate our models using F-1, Precision (P), Recall (R), False Acceptance Rate (FAR), False Rejection Rate (FRR) and Accuracy (Acc). FRR, also known as "Type I" error, is the measure of the likelihood that the model will incorrectly classify an intra-writer sample as inter-writer. FAR, also known as "Type II" error, is the measure of the likelihood that the model will predict an inter-writer sample as intra-writer. <ref type="table" target="#tab_1">Table I</ref> shows the performance of baseline models for "Concat" and "Siamese" setting without any attention modules. In <ref type="table" target="#tab_1">Table I</ref> when γ is set to 0, FL is same as CCE. We set α = 0.75 and γ = 2.0 and adopt Siamese setting for our further experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results</head><p>Next, we experiment on the proposed attention based models and show the results in <ref type="table" target="#tab_1">Table II</ref>. We also managed to replicate the work done in <ref type="bibr" target="#b17">[18]</ref> and report their metrics. As for work done by <ref type="bibr" target="#b15">[16]</ref> we could only report their accuracy on this dataset. The MHCA combined with SA achieves the best results in Siamese setting. Moreover, the network is able to lower the FRR considerably as compared to vanilla settings. The difference in FAR and FRR values are due to the low number and high imbalance of samples per writer. We also experiment the application of MHCA on various levels of feature maps and display it's optimum location in <ref type="table" target="#tab_1">Table III</ref>.</p><p>We observe that as we increase the feature map size the performance improves. This is because as size reduces the features in the map become more abstract and there are less evidences to relate features of two images. <ref type="bibr" target="#b21">[22]</ref>  Furthermore, we train our model on CEDAR Signature <ref type="bibr" target="#b23">[24]</ref> to demonstrate that the proposed MHCA-SA mechanism is effective across different datasets. As displayed in <ref type="table" target="#tab_1">Table IV</ref> our method achieves comparable potential with most widely used methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Discussion and Ablation Analysis</head><p>Writer verification data is inherently biased with significantly more negative samples overwhelming positive samples. We mainly rely on Precision, Recall and F-1 score due to their robustness to such imbalanced data. Accuracy and FAR on the other hand are sensitive to the ratio of negative samples. This may be the case in our experiments where an increase in the number of easy negative samples in test set can lead to a drastic drop in FAR and increase accuracy. Based on our observations, using VGG16 blocks performed better than employing IRv2 blocks, and hence, we report the results with VGG16 blocks. Furthermore, we use softmax in our classification layer rather than sigmoid as softmax coupled with CCE tend to distribute the probability distribution far apart without requiring any constant margin. Moreover, CA finds the corresponding related pixels between two given images using cosine similarity score in feature space which leads to further improvement in feature generation. Additionally, SA boosts the network's ability of important feature selection and hence performance.</p><p>We extract the attention maps from the CA and SA modules which provides interpretation on the models learning process and finding evidence of writer similarities. <ref type="figure" target="#fig_4">Fig. 5</ref> displays attention energy maps from one of the CA heads. The images in <ref type="figure" target="#fig_4">Fig. 5a and Fig. 5b</ref> show the cross attention correspondence maps, when the images were from the same/different writer respectively. In both the subsections the images on the left are considered as the query and the adjacently right images are considered as key images to attend on. As shown in 5a, when a pixel on the curve of "a" is used as a query, two corresponding points from the key image are returned by the model. Visibly, these two points extracted from the key image are similar in shape and stroke. We hypothesize that the model uses this information to further strengthen it's belief that the images are from same writer.</p><p>Furthermore, as seen in <ref type="figure" target="#fig_4">Fig. 5b</ref> the model could not identify similar strokes, edges or contours, from the key image for the given query points, in the two images towards bottom right. This perhaps enhances the ability of the model to identify dissimilar writings.</p><p>We observe the pattern, of matching corresponding key points in similar alphabets, uniform across intra-writer samples. Also, the phenomena of query pixel matching an exact same black/non-important pixel in the image, is consistent for inter-writer samples. This provides a hint to the FDE's for the reason behind the models decision.</p><p>Next, we extract the attention maps from the SA layers, to display the areas which the network has jointly attended. 6 displays energy maps from the SA layer. These maps are calculated from the output of convolutional layer connected post the CA features are extracted concatenated. Since, this SA layer was connected after the layer that outputs feature maps of shape 16×16, the shape of this SA map is 16×16. We resize these maps to 64×64 using linear interpolation and mask it on the inputs. The red and green contours show the areas that the network finds most useful for classification. Specifically, the red areas have received highest attention and the green areas have received lower attention, while the dark blue areas are mere background regions. As evident from <ref type="figure" target="#fig_5">Fig. 6a</ref> top row, the energy is concentrated around the loop of "a" and "d" and the tent of "n" which are very visually similar in the samples, where as in the top row of <ref type="figure" target="#fig_5">Fig. 6b</ref> the energy concentrations are around dissimilar portions of "n" and dissimilar loops of "d". This shows that the network is able to indicate pixel regions relevant to the classification of inter and intra-class samples respectively. We experimented with VGG16 and IRv2 and observed better results and faster convergence with VGG16. Also applying batch-normalization after each attention layer and ReLU activation after each concatenation layer improved the performance and also led to faster convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In the domain of handwriting verification, it is required to explain the model's decision which can assist the Forensic Document Examiner (FDE). In this work, we have displayed the potential of Cross Attention and Soft Attention mechanism for this task which not only performs at par or better than the state-of-the-art methods but also provides insightful explanations on our model's decision. The CA modules can be easily set up to work with multi-modality data, i.e. to find crossrelevance in data modalities. Here, CA modules extract the intra-writer relevancies very effectively, while the SA module successfully extracts the most crucial pixel locations in the joint input. In future work we plan to apply CA to multi-modal datasets and also extend the current approach to handwritten full page datasets comprising of multiple words. We also plan to test the feasibility of SA on larger datasets for classification and related tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>(a) Samples from the CEDAR cursive "AND" dataset. (b) Samples from CEDAR Signatures dataset. The green arrows indicate pairing of samples from the same writer. The red arrows indicate pairing of samples from different writers. The character X ∈ {1, 2, 3}</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>24 2 =</head><label>2</label><figDesc>276 genuine pairs per writer with 50 × 276 = 13800 and 5 × 276 = 1380 genuine-genuine pairs in training and in testing set respectively. Furthermore, we consider all the 24 × 24 × 50 = 28800 and 24 × 24 × 5 = 2880 genuineforgery pairs for training and testing set respectively. Hence, the ratio of genuine-genuine-writer pairs to forgery-genuinewriter pairs in our training and testing sets is almost 1:2 for this dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Internal working of a Cross Attention module. L, M, N and V are separate 1 × 1 convolutions. R is the reshape operation. represents matrix multiplication. S denotes softmax which is performed per row to generate Attention Map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Internal working of a Soft Attention module. ⊕ denotes aggregation and * represents convolution operation. S denotes softmax which is performed over one feature map to generate one attention map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>CA Maps for input images. (a) CA Maps when inputs are are from the same writer, (b) CA Maps when inputs are from different writers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>SA Map for input images. (a) SA Maps when inputs are are from same writer, (b) SA Map when inputs are from different writers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I ESTABLISHING</head><label>I</label><figDesc>BASELINE ON "AND" DATASET AND PARAMETER SELECTION FOR FL</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Concat Setting</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Siamese Setting</cell><cell></cell></row><row><cell>α</cell><cell></cell><cell>γ</cell><cell>F-1</cell><cell>P</cell><cell>R</cell><cell>FAR</cell><cell>FRR</cell><cell>Acc</cell><cell>F-1</cell><cell>P</cell><cell>R</cell><cell>FAR</cell><cell>FRR</cell><cell>Acc</cell></row><row><cell></cell><cell></cell><cell>0.00</cell><cell>0.72</cell><cell>0.85</cell><cell>0.62</cell><cell>1.12</cell><cell>37.53</cell><cell>95.56</cell><cell>0.78</cell><cell>0.83</cell><cell>0.74</cell><cell>1.69</cell><cell>25.99</cell><cell>95.74</cell></row><row><cell></cell><cell></cell><cell>0.10</cell><cell>0.71</cell><cell>0.85</cell><cell>0.61</cell><cell>1.05</cell><cell>39.11</cell><cell>95.48</cell><cell>0.77</cell><cell>0.91</cell><cell>0.66</cell><cell>0.94</cell><cell>33.63</cell><cell>96.82</cell></row><row><cell></cell><cell></cell><cell>0.20</cell><cell>0.73</cell><cell>0.83</cell><cell>0.65</cell><cell>1.38</cell><cell>34.79</cell><cell>95.58</cell><cell>0.77</cell><cell>0.88</cell><cell>0.69</cell><cell>1.07</cell><cell>31.48</cell><cell>96.92</cell></row><row><cell cols="2">0.50</cell><cell>0.50</cell><cell>0.74</cell><cell>0.83</cell><cell>0.67</cell><cell>1.35</cell><cell>33.09</cell><cell>95.76</cell><cell>0.79</cell><cell>0.89</cell><cell>0.71</cell><cell>1.10</cell><cell>29.06</cell><cell>97.10</cell></row><row><cell></cell><cell></cell><cell>1.00</cell><cell>0.73</cell><cell>0.88</cell><cell>0.62</cell><cell>0.83</cell><cell>37.78</cell><cell>95.80</cell><cell>0.81</cell><cell>0.93</cell><cell>0.72</cell><cell>1.71</cell><cell>27.99</cell><cell>97.14</cell></row><row><cell></cell><cell></cell><cell>2.00</cell><cell>0.74</cell><cell>0.82</cell><cell>0.67</cell><cell>1.47</cell><cell>33.31</cell><cell>95.63</cell><cell>0.79</cell><cell>0.88</cell><cell>0.73</cell><cell>1.45</cell><cell>27.38</cell><cell>96.97</cell></row><row><cell></cell><cell></cell><cell>5.00</cell><cell>0.73</cell><cell>0.77</cell><cell>0.68</cell><cell>2.00</cell><cell>31.69</cell><cell>95.29</cell><cell>0.75</cell><cell>0.83</cell><cell>0.69</cell><cell>1.02</cell><cell>31.19</cell><cell>96.63</cell></row><row><cell></cell><cell></cell><cell>0.10</cell><cell>0.71</cell><cell>0.83</cell><cell>0.62</cell><cell>1.33</cell><cell>37.56</cell><cell>95.37</cell><cell>0.77</cell><cell>0.88</cell><cell>0.68</cell><cell>0.67</cell><cell>32.54</cell><cell>96.71</cell></row><row><cell></cell><cell></cell><cell>0.20</cell><cell>0.73</cell><cell>0.85</cell><cell>0.64</cell><cell>1.13</cell><cell>36.09</cell><cell>95.69</cell><cell>0.79</cell><cell>0.91</cell><cell>0.69</cell><cell>0.92</cell><cell>31.03</cell><cell>97.03</cell></row><row><cell cols="2">0.75</cell><cell>0.50 1.00</cell><cell>0.74 0.72</cell><cell>0.83 0.83</cell><cell>0.67 0.64</cell><cell>1.42 1.31</cell><cell>32.66 35.69</cell><cell>95.73 95.56</cell><cell>0.79 0.79</cell><cell>0.88 0.89</cell><cell>0.72 0.71</cell><cell>1.07 1.70</cell><cell>28.01 28.91</cell><cell>97.07 96.89</cell></row><row><cell></cell><cell></cell><cell>2.00</cell><cell>0.74</cell><cell>0.81</cell><cell>0.68</cell><cell>1.59</cell><cell>32.41</cell><cell>95.60</cell><cell>0.81</cell><cell>0.87</cell><cell>0.76</cell><cell>1.77</cell><cell>24.33</cell><cell>96.94</cell></row><row><cell></cell><cell></cell><cell>5.00</cell><cell>0.72</cell><cell>0.86</cell><cell>0.62</cell><cell>1.00</cell><cell>38.46</cell><cell>95.58</cell><cell>0.79</cell><cell>0.92</cell><cell>0.69</cell><cell>1.36</cell><cell>30.86</cell><cell>96.92</cell></row><row><cell></cell><cell></cell><cell cols="2">TABLE II</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">EXPERIMENT RESULTS OF VARIOUS MODELS ON "AND" DATASET</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>F-1</cell><cell>P</cell><cell>R</cell><cell>FAR</cell><cell>FRR</cell><cell>Acc</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>HDL [16]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">92.16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DAAM SAE [18]</cell><cell>0.70</cell><cell>0.85</cell><cell>0.59</cell><cell>3.69</cell><cell>40.57</cell><cell cols="2">95.23</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Concat Baseline</cell><cell>0.72</cell><cell>0.85</cell><cell>0.62</cell><cell>1.59</cell><cell>37.53</cell><cell cols="2">95.60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Concat SA</cell><cell>0.73</cell><cell>0.85</cell><cell>0.64</cell><cell>1.73</cell><cell>36.36</cell><cell cols="2">95.69</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Siamese Baseline</cell><cell>0.78</cell><cell>0.83</cell><cell>0.74</cell><cell>1.77</cell><cell>26.00</cell><cell cols="2">96.18</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Siamese CA SA</cell><cell>0.79</cell><cell>0.84</cell><cell>0.74</cell><cell>1.93</cell><cell>25.82</cell><cell cols="2">96.34</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Siamese MHCA SA</cell><cell>0.81</cell><cell>0.86</cell><cell>0.76</cell><cell>1.73</cell><cell>24.03</cell><cell cols="2">96.39</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III EXPERIMENTS</head><label>III</label><figDesc>TO LOCATE THE OPTIMUM FEATURE MAP SIZE TO ADD CA</figDesc><table><row><cell></cell><cell></cell><cell cols="2">MODULE</cell><cell></cell><cell></cell><cell></cell></row><row><cell>FeatureMap Size</cell><cell>F-1</cell><cell>P</cell><cell>R</cell><cell>FAR</cell><cell>FRR</cell><cell>Acc</cell></row><row><cell>16 × 16</cell><cell>0.70</cell><cell>0.78</cell><cell>0.64</cell><cell>1.84</cell><cell>35.81</cell><cell>95.06</cell></row><row><cell>32 × 32</cell><cell>0.81</cell><cell>0.86</cell><cell>0.76</cell><cell>1.73</cell><cell>24.03</cell><cell>96.39</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV EXPERIMENT</head><label>IV</label><figDesc>RESULTS OF VARIOUS METHODS ON CEDAR SIGNATURE DATASET</figDesc><table><row><cell>Method</cell><cell>FAR</cell><cell>FRR</cell><cell>Acc</cell></row><row><cell>Graph matching [32]</cell><cell>8.20</cell><cell>7.70</cell><cell>92.10</cell></row><row><cell>SigNet [33]</cell><cell>0.00</cell><cell>0.00</cell><cell>100.00</cell></row><row><cell>SigNet-F (SVM) [34]</cell><cell>4.63</cell><cell>4.63</cell><cell>-</cell></row><row><cell>Siamese MHCA SA</cell><cell>5.70</cell><cell>6.30</cell><cell>92.37</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code is publicly available on: https://github.com/mshaikh2/AttentionHandwritingVerification</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Individuality of handwriting: a validation study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Srihari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Sixth International Conference on Document Analysis and Recognition</title>
		<meeting>Sixth International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<date type="published" when="2001-09" />
			<biblScope unit="page" from="106" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">DeepFace: Closing the Gap to Human-Level Performance in Face Verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="1063" to="6919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On-line fingerprint verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bolle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="1997-04" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="302" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Handwritten Signature Forgery Detection using Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Jerome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kandulna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Kujur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Raimond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Procedia Computer Science</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="page" from="978" to="987" />
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Iris Recognition With Off-the-Shelf CNN Features: A Deep Learning Perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fookes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="18" to="848" />
			<date type="published" when="2018" />
			<publisher>IEEE Access</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Text-independent speaker verification using 3d convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torfi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dawson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Nasrabadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic signature verification and writer identification the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Plamondon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lorette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="107" to="131" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-expert verification of hand-written signatures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bovino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Impedovo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pirlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sarcinella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICDAR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="932" to="936" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Text-Independent Writer Identification and Verification Using Textural and Allographic Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bulacu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schomaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="701" to="717" />
			<date type="published" when="2007-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Towards robust writer verification by correcting unnatural slant</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Brink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M J</forename><surname>Niels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Van Batenburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Van Den Heuvel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R B</forename><surname>Schomaker</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0167865510003569" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="449" to="457" />
			<date type="published" when="2011-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automated off-line writer verification using short sentences and grid features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tselios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zois</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nassiopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karabetsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Economou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in AFHA</title>
		<imprint>
			<biblScope unit="volume">768</biblScope>
			<biblScope unit="page" from="21" to="25" />
			<date type="published" when="2011-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Texturebased descriptors for writer identification and verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bertolini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Justino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sabourin</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S095741741201130X" />
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2069" to="2080" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A model-based approach to offline text-independent Arabic writer identification and verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Abdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khemakhem</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0031320314004440" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1890" to="1903" />
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Text and User Generic Model for Writer Verification Using Combined Pen Pressure Information From Ink Intensity and Indented Writing on Paper</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">conference Name: IEEE Transactions on Human-Machine Systems</title>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="339" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Writer verification based on a single handwriting word samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bensefia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Paquet</surname></persName>
		</author>
		<idno type="DOI">10.1186/s13640-016-0139-0</idno>
		<ptr target="https://doi.org/10.1186/s13640-016-0139-0" />
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Image and Video Processing</title>
		<imprint>
			<biblScope unit="volume">2016</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">34</biblScope>
			<date type="published" when="2016-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hybrid Feature Learning for Handwriting Verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Shaikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chauhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Srihari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 16th International Conference on Frontiers in Handwriting Recognition (ICFHR)</title>
		<imprint>
			<date type="published" when="2018-08" />
			<biblScope unit="page" from="187" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Writer Verification using CNN Feature Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Shaikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chauhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Srihari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 16th International Conference on Frontiers in Handwriting Recognition (ICFHR)</title>
		<imprint>
			<date type="published" when="2018-08" />
			<biblScope unit="page" from="181" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Explanation based Handwriting Verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chauhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Shaikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Srihari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.02548</idno>
		<idno>arXiv: 1909.02548</idno>
		<ptr target="http://arxiv.org/abs/1909.02548" />
		<imprint>
			<date type="published" when="2019-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1409.0473" />
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<editor>Y. Bengio and Y. LeCun</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Effective Approaches to Attention-based Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D15-1166" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin ; I. Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08318</idno>
		<idno>arXiv: 1805.08318</idno>
		<ptr target="http://arxiv.org/abs/1805.08318" />
		<title level="m">Self-Attention Generative Adversarial Networks</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention-Based Deep Neural Networks for Detection of Cancerous and Precancerous Esophagus Tissue on Histopathological Slides</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tomita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Abdollahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Suriawinata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hassanpour</surname></persName>
		</author>
		<ptr target="https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2753982" />
	</analytic>
	<monogr>
		<title level="j">JAMA Network Open</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2019-11" />
			<publisher>American Medical Association</publisher>
		</imprint>
	</monogr>
	<note>e1 914 645-e1 914 645</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Offline signature verification and identification using distance statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Meenakshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Srihari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Aihua</surname></persName>
		</author>
		<idno type="DOI">10.1142/S0218001404003630</idno>
		<ptr target="https://doi.org/10.1142/S0218001404003630" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Pattern Recognition and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">07</biblScope>
			<biblScope unit="page" from="1339" to="1360" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A study of cross-validation and bootstrap for accuracy estimation and model selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
		<idno>ser. IJCAI95</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 14th International Joint Conference on Artificial Intelligence<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1995" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11371143</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The influence of negative training set size on machine learning-based virtual screening</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kurczab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Smusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Bojarski</surname></persName>
		</author>
		<ptr target="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4061540/" />
	</analytic>
	<monogr>
		<title level="j">Journal of Cheminformatics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-ResNet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017-02" />
			<biblScope unit="page" from="4278" to="4284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning Spatiotemporal Features with 3D Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="page" from="2380" to="7504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Focal Loss for Dense Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020-02" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="318" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A new off-line signature verification method based on graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srihari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th International Conference on Pattern Recognition (ICPR&apos;06)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="869" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Signet: Convolutional siamese network for writer independent offline signature verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">I</forename><surname>Toledo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Llados</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Pal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning features for offline handwritten signature verification using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Hafemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sabourin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Oliveira</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2017.05.012</idno>
		<ptr target="http://dx.doi.org/10.1016/j.patcog.2017.05.012" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page">163176</biblScope>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
