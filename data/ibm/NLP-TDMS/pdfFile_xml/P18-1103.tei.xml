<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-06T23:30+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Turn Response Selection for Chatbots with Deep Attention Matching Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15 -20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Zhou</surname></persName>
							<email>zhouxiangyang@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxiang</forename><surname>Dong</surname></persName>
							<email>dongdaxiang@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Chen</surname></persName>
							<email>chenying04@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianhai</forename><surname>Yu</surname></persName>
							<email>yudianhai@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
							<email>wuhua@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Turn Response Selection for Chatbots with Deep Attention Matching Network</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1118" to="1127"/>
							<date type="published">July 15 -20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1118</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Human generates responses relying on semantic and functional dependencies, including coreference relation, among dialogue elements and their context. In this paper, we investigate matching a response with its multi-turn context using dependency information based entirely on attention. Our solution is inspired by the recently proposed Transformer in machine translation (Vaswani et al., 2017) and we extend the attention mechanism in two ways. First, we construct representations of text segments at different granularities solely with stacked self-attention. Second, we try to extract the truly matched segment pairs with attention across the context and response. We jointly introduce those two kinds of attention in one uniform neural network. Experiments on two large-scale multi-turn response selection tasks show that our proposed model significantly outperforms the state-of-the-art models.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Building a chatbot that can naturally and consistently converse with human-beings on opendomain topics draws increasing research interests in past years. One important task in chatbots is response selection, which aims to select the bestmatched response from a set of candidates given the context of a conversation. Besides playing a critical role in retrieval-based chatbots ( <ref type="bibr" target="#b6">Ji et al., 2014)</ref>, response selection models have been used in automatic evaluation of dialogue generation * Equally contributed.</p><p>† Work done as a visiting scholar at Baidu. Wayne Xin Zhao is an associate professor of Renmin University of China and can be reached at batmanfly@ruc.edu.cn.</p><p>( <ref type="bibr" target="#b12">Lowe et al., 2017)</ref> and the discriminator of GANbased (Generative Adversarial Networks) neural dialogue generation ( ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conversation Context</head><p>Speaker A: Hi I am looking to see what packages are installed on my system, I don't see a path, is the list being held somewhere else? Speaker B: Try dpkg -get-selections Speaker A: What is that like? A database for packages instead of a flat file structure?</p><p>Speaker B: dpkg is the debian package manager -get-selections simply shows you what packages are handed by it</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Response of Speaker A:</head><p>No clue what do you need it for, its just reassurance as I don't know the debian package manager <ref type="figure">Figure 1</ref>: Example of human conversation on Ubuntu system troubleshooting. Speaker A is seeking for a solution of package management in his/her system and speaker B recommend using, the debian package manager, dpkg. But speaker A does not know dpkg, and asks a backchannel-question ( <ref type="bibr" target="#b18">Stolcke et al., 2000</ref>), i.e., "no clue what do you need it for?", aiming to double-check if dpkg could solve his/her problem.</p><p>Text segments with underlines in the same color across context and response can be seen as matched pairs.</p><p>Early studies on response selection only use the last utterance in context for matching a reply, which is referred to as single-turn response selection ( <ref type="bibr" target="#b23">Wang et al., 2013)</ref>. Recent works show that the consideration of a multi-turn context can facilitate selecting the next utterance ( <ref type="bibr" target="#b26">Wu et al., 2017</ref>). The reason why richer contextual information works is that human generated responses are heavily dependent on the previous dialogue segments at different granularities (words, phrases, sentences, etc), both semantically and functionally, over multiple turns rather than one turn ( <ref type="bibr" target="#b9">Lee et al., 2006;</ref><ref type="bibr">Traum and Hee- man, 1996</ref>). <ref type="figure">Figure 1</ref> illustrates semantic connectivities between segment pairs across context and response. As demonstrated, generally there are two kinds of matched segment pairs at different granularities across context and response: (1) surface text relevance, for example the lexical overlap of words "packages"-"package" and phrases "debian package manager"-"debian pack-age manager". (2) latent dependencies upon which segments are semantically/functionally related to each other. Such as the word "it" in the response, which refers to "dpkg" in the context, as well as the phrase "its just reassurance" in the response, which latently points to "what packages are installed on my system", the question that speaker A wants to double-check.</p><p>Previous studies show that capturing those matched segment pairs at different granularities across context and response is the key to multiturn response selection ( <ref type="bibr" target="#b26">Wu et al., 2017)</ref>. However, existing models only consider the textual relevance, which suffers from matching response that latently depends on previous turns. Moreover, Recurrent Neural Networks (RNN) are conveniently used for encoding texts, which is too costly to use for capturing multi-grained semantic representations <ref type="bibr" target="#b13">(Lowe et al., 2015;</ref><ref type="bibr" target="#b26">Wu et al., 2017)</ref>. As an alternative, we propose to match a response with multi-turn context using dependency information based entirely on attention mechanism. Our solution is inspired by the recently proposed Transformer in machine translation ( <ref type="bibr" target="#b21">Vaswani et al., 2017)</ref>, which addresses the issue of sequence-to-sequence generation only using attention, and we extend the key attention mechanism of Transformer in two ways:</p><p>self-attention By making a sentence attend to itself, we can capture its intra word-level dependencies. Phrases, such as "debian package manager", can be modeled with wordlevel self-attention over word-embeddings, and sentence-level representations can be constructed in a similar way with phraselevel self-attention. By hierarchically stacking self-attention from word embeddings, we can gradually construct semantic representations at different granularities.</p><p>cross-attention By making context and response attend to each other, we can generally capture dependencies between those latently matched segment pairs, which is able to provide complementary information to textual relevance for matching response with multi-turn context.</p><p>We jointly introduce self-attention and crossattention in one uniform neural matching network, namely the Deep Attention Matching Network (DAM), for multi-turn response selection. In practice, DAM takes each single word of an utterance in context or response as the centric-meaning of an abstractive semantic segment, and hierarchically enriches its representation with stacked self-attention, gradually producing more and more sophisticated segment representations surrounding the centric-word. Each utterance in context and response are matched based on segment pairs at different granularities, considering both textual relevance and dependency information. In this way, DAM generally captures matching information between the context and the response from word-level to sentence-level, important matching features are then distilled with convolution &amp; maxpooling operations, and finally fused into one single matching score via a single-layer perceptron.</p><p>We test DAM on two large-scale public multiturn response selection datasets, the Ubuntu Corpus v1 and Douban Conversation Corpus. Experimental results show that our model significantly outperforms the state-of-the-art models, and the improvement to the best baseline model on R 10 @1 is over 4%. What is more, DAM is expected to be convenient to deploy in practice because most attention computation can be fully parallelized ( <ref type="bibr" target="#b21">Vaswani et al., 2017)</ref>. Our contributions are two-folds: (1) we propose a new matching model for multi-turn response selection with selfattention and cross-attention. (2) empirical results show that our proposed model significantly outperforms the state-of-the-art baselines on public datasets, demonstrating the effectiveness of selfattention and cross-attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Conversational System</head><p>To build an automatic conversational agent is a long cherished goal in Artificial Intelligence (AI) <ref type="bibr" target="#b20">(Turing, 1950</ref>). Previous researches include taskoriented dialogue system, which focuses on completing tasks in vertical domain, and chatbots, which aims to consistently and naturally converse with human-beings on open-domain topics. Most modern chatbots are data-driven, either in a fashion of information-retrieval ( <ref type="bibr" target="#b6">Ji et al., 2014;</ref><ref type="bibr" target="#b4">Banchs and Li, 2012;</ref><ref type="bibr" target="#b15">Nio et al., 2014;</ref><ref type="bibr" target="#b0">Ameixa et al., 2014</ref>) or sequence-generation ( <ref type="bibr" target="#b17">Ritter et al., 2011</ref>  candidate that best matches the current context. The generation-based models, on the other hand, learn patterns of responding from dialogues and can directly generalize new responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Response Selection</head><p>Researches on response selection can be generally categorized into single-turn and multi-turn. Most early studies are single-turn that only consider the last utterance for matching response ( <ref type="bibr" target="#b23">Wang et al., 2013</ref><ref type="bibr" target="#b24">Wang et al., , 2015</ref>. Recent works extend it to multiturn conversation scenario, <ref type="bibr" target="#b13">Lowe et al.,(2015)</ref> and  use RNN to read context and response, use the last hidden states to represent context and response as two semantic vectors, and measure their relevance. Instead of only considering the last states of RNN, <ref type="bibr" target="#b26">Wu et al.,(2017)</ref> take hidden state at each time step as a text segment representation, and measure the distance between context and response via segment-segment matching matrixes. Nevertheless, matching with dependency information is generally ignored in previous works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Attention</head><p>Attention has been proven to be very effective in Natural Language Processing (NLP) ( <ref type="bibr" target="#b3">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b29">Yin et al., 2016;</ref><ref type="bibr" target="#b11">Lin et al., 2017</ref>) and other research areas ( <ref type="bibr" target="#b27">Xu et al., 2015)</ref>. Recently, <ref type="bibr" target="#b21">Vaswani et al.,(2017)</ref> propose a novel sequenceto-sequence generation network, the Transformer, which is entirely based on attention. Not only Transformer can achieve better translation results than convenient RNN-based models, but also it is very fast in training/predicting as the computation of attention can be fully parallelized. Previous works on attention mechanism show the superior ability of attention to capture semantic dependencies, which inspires us to improve multi-turn response selection with attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Deep Attention Matching Network</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formalization</head><p>Given a dialogue data set D = {(c, r, y) Z } N Z=1 , where c = {u 0 , ..., u n−1 } represents a conversation context with {u i } n−1 i=0 as utterances and r as a response candidate. y ∈ {0, 1} is a binary label, indicating whether r is a proper response for c. Our goal is to learn a matching model g(c, r) with D, which can measure the relevance between any context c and candidate response r. <ref type="figure" target="#fig_0">Figure 2</ref> gives an overview of DAM, which generally follows the representation-matchingaggregation framework to match response with multi-turn context. For each utterance</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Overview</head><formula xml:id="formula_0">u i = [w u i ,k ] nu i −1 k=0</formula><p>in a context and its response candidate r = [w r,t ] nr−1 t=0 , where n u i and n r stand for the numbers of words, DAM first looks up a shared word embedding table and represents u i and r as sequences of word embeddings, namely</p><formula xml:id="formula_1">U 0 i = [e 0 u i ,0 , ..., e 0 u i ,nu i −1 ] and R 0 = [e 0 r,0 , ..., e 0 r,nr−1 ] respectively, where e ∈ R d denotes a d-dimension word embedding.</formula><p>A representation module then starts to construct semantic representations at different granularities for u i and r. Practically, L identical layers of self-attention are hierarchically stacked, each l th self-attention layer takes the output of the l − 1 th layer as its input, and composites the input semantic vectors into more sophisticated representations based on self-attention. In this way, multigrained representations of u i and r are gradually constructed, denoted as</p><formula xml:id="formula_2">[U l i ] L l=0 and [R l ] L l=0 re- spectively. Given [U 0 i , ..., U L i ] and [R 0 , ..., R L ]</formula><p>, utterance u i and response r are then matched with each other in a manner of segment-segment similarity matrix. Practically, for each granularity l ∈ [0...L], two kinds of matching matrixes are constructed, i.e., the self-attention-match M u i ,r,l self and cross-attention-match M u i ,r,l cross , measuring the relevance between utterance and response with textual information and dependency information respectively.</p><p>Those matching scores are finally merged into a 3D matching image Q 1 . Each dimension of Q represents each utterance in context, each word in utterance and each word in response respectively. Important matching information between segment pairs across multi-turn context and candidate response is then extracted via convolution with max-pooling operations, and further fused into one matching score via a single-layer perceptron, representing the matching degree between the response candidate and the whole context.</p><p>Specifically, we use a shared component, the Attentive Module, to implement both selfattention in representation and cross-attention in matching. We will discuss in detail the implementation of Attentive Module and how we used it to implement both self-attention and cross-attention in following sections. <ref type="figure" target="#fig_2">Figure 3</ref> shows the structure of Attentive Module, which is similar to that used in Transformer ( <ref type="bibr" target="#b21">Vaswani et al., 2017)</ref>. Attentive Module has three input sentences: the query sentence, the key sentence and the value sentence, namely</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Attentive Module</head><formula xml:id="formula_3">Q = [e i ] n Q −1 i=0 , K = [e i ] n K −1 i=0 , V = [e i ] n V −1 i=0 respec- 1</formula><p>We refer to it as Q because it is like a cube.  tively, where n Q , n K and n V denote the number of words in each sentence and e i stands for a ddimension embedding, n K is equal to n V . The Attentive Module first takes each word in the query sentence to attend to words in the key sentence via Scaled Dot-Product Attention ( <ref type="bibr" target="#b21">Vaswani et al., 2017)</ref>, then applies those attention results upon the value sentence, which is defined as:</p><formula xml:id="formula_4">Att(Q, K) = sof tmax( Q[i] · K T √ d ) n Q −1 i=0</formula><p>(1)</p><formula xml:id="formula_5">V att = Att(Q, K) · V ∈ R n Q ×d (2)</formula><p>where  <ref type="bibr">et al., 2015)</ref> activation is then applied upon the normalization result, in order to further process the fused embeddings, defined as:</p><formula xml:id="formula_6">FFN(x) = max(0, xW 1 + b 1 )W 2 + b 2 (3)</formula><p>where, x is a 2D-tensor in the same shape of query sentence Q and W 1 , b 1 , W 2 , b 2 are learnt parameters. This kind of activation is empirically useful in other works, and we also adapt it in our model. The result FFN(x) is a 2D-tensor that has the same shape as x, FFN(x) is then residually added ( <ref type="bibr" target="#b5">He et al., 2016</ref>) to x, and the fusion result is then normalized as the final outputs. We refer to the whole Attentive Module as:</p><formula xml:id="formula_7">AttentiveModule(Q, K, V)<label>(4)</label></formula><p>As described, Attentive Module can capture dependencies across query sentence and key sentence, and further use the dependency information to composite elements in the query sentence and the value sentence into compositional representations. We exploit this property of the Attentive Module to construct multi-grained semantic representations as well as match with dependency information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Representation</head><p>Given U 0 i or R 0 , the word-level embedding representations for utterance u i or response r, DAM takes U 0 i ro R 0 as inputs and hierarchically stacks the Attentive Module to construct multi-grained representations of u i and r, which is formulated as:</p><formula xml:id="formula_8">U l+1 i = AttentiveModule(U l i , U l i , U l i )<label>(5)</label></formula><formula xml:id="formula_9">R l+1 = AttentiveModule(R l , R l , R l )<label>(6)</label></formula><p>where l ranges from 0 to L − 1, denoting the different levels of granularity. By this means, words in each utterance or response repeatedly function together to composite more and more holistic representations, we refer to those multi-grained representations as</p><formula xml:id="formula_10">[U 0 i , ..., U L i ] and [R 0 , ..., R L ] here- after.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Utterance-Response Matching</head><formula xml:id="formula_11">Given [U l i ] L l=0 and [R l ] L l=0</formula><p>, two kinds of segmentsegment matching matrixes are constructed at each level of granularity l, i.e., the self-attention-match</p><formula xml:id="formula_12">M u i ,r,l self and cross-attention-match M u i ,r,l cross . M u i ,r,l self</formula><p>is defined as:</p><formula xml:id="formula_13">M u i ,r,l self = {U l i [k] T · R l [t]} nu i ×nr<label>(7)</label></formula><p>in which, each element in the matrix is the dotproduct of</p><formula xml:id="formula_14">U l i [k] and R l [t]</formula><p>, the k th embedding in U l i and the t th embedding in R l , reflecting the textual relevance between the k th segment in u i and t th segment in r at the l th granularity. The crossattention-match matrix is based on cross-attention, which is defined as:</p><formula xml:id="formula_15">U l i = AttentiveModule(U l i , R l , R l ) (8) R l = AttentiveModule(R l , U l i , U l i )<label>(9)</label></formula><formula xml:id="formula_16">M u i ,r,l cross = { U l i [k] T · R l [t]} nu i ×nr<label>(10)</label></formula><p>where we use Attentive Module to make U l i and R l crossly attend to each other, constructing two</p><note type="other">new representations for both of them, written as U l i and R l respectively. Both U l i and R l implicitly</note><p>capture semantic structures that cross the utterance and response. In this way, those inter-dependent segment pairs are close to each other in representations, and dot-products between those latently inter-dependent pairs could get increased, providing dependency-aware matching information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Aggregation</head><p>DAM finally aggregates all the segmental matching degrees across each utterance and response into a 3D matching image Q, which is defined as:</p><formula xml:id="formula_17">Q = {Q i,k,t } n×nu i ×nr<label>(11)</label></formula><p>where each pixel Q i,k,t is formulated as:</p><formula xml:id="formula_18">Q i,k,t = [M u i ,r,l self [k, t]] L l=0 ⊕ [M u i ,r,l cross [k, t]] L l=0<label>(12)</label></formula><p>⊕ is concatenation operation, and each pixel has 2(L + 1) channels, storing the matching degrees between one certain segment pair at different levels of granularity. DAM then leverages twolayered 3D convolution with max-pooling operations to distill important matching features from the whole image. The operation of 3D convolution with max-pooling is the extension of typical 2D convolution, whose filters and strides are 3D cubes 2 . We finally compute matching score g(c, r) based on the extracted matching features f match (c, r) via a single-layer perceptron, which is formulated as:</p><formula xml:id="formula_19">g(c, r) = σ(W 3 f match (c, r) + b 3 )<label>(13)</label></formula><p>where W 3 and b 3 are learnt parameters, and σ is sigmoid function that gives the probability if r is a proper candidate to c. The loss function of DAM is the negative log likelihood, defined as:  contexts, where each context is provided with one positive response and nine negative replies. The Douban corpus is constructed in a similar way to the Ubuntu Corpus, except that its validation set contains 50k instances with 1:1 positive-negative ratios and the testing set of Douban corpus is consisted of 10k instances, where each context has 10 candidate responses, collected via a tiny invertedindex system (Lucene 3 ), and labels are manually annotated.</p><formula xml:id="formula_20">p(y|c, r) = g(c, r)y + (1 − g(c, r))(1 − y) (14) L(·) = − (c,</formula><note type="other">0.926 0.726 0.847 0.961 0.529 0.569 0.397 0.233 0.396 0.724 DAM 0.938 0.767 0.874 0.969 0.550 0.601 0.427 0.254 0.410 0.757 DAM f irst 0.927 0.736 0.854 0.962 0.528 0.579 0.400 0.229 0.396 0.741 DAM last 0.932 0.752 0.861 0.965 0.539 0.583 0.408 0.242 0.407 0.748 DAM self 0.931 0.741 0.859 0.964 0.527 0.574 0.382 0.221 0.403 0.750 DAMcross 0.932 0.749 0.863 0.966 0.</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Metric</head><p>We use the same evaluation metrics as in previous works ( <ref type="bibr" target="#b26">Wu et al., 2017)</ref>. Each comparison model is asked to select k best-matched response from n available candidates for the given conversation context c, and we calculate the recall of the true positive replies among the k selected ones as the main evaluation metric, denoted as</p><formula xml:id="formula_21">R n @k = k i=1 y i n i=1 y i</formula><p>, where y i is the binary label for each candidate. In addition to R n @k, we use MAP (Mean Average Precision) (Baeza-3 https://lucenent.apache.org/ <ref type="bibr" target="#b2">Yates et al., 1999</ref>), MRR (Mean Reciprocal Rank) ( <ref type="bibr" target="#b22">Voorhees et al., 1999</ref>), and Precision-at-one P @1 especially for Douban corpus, following the setting of previous works (Wu et al., 2017). Ablation : To verify the effects of multi-grained representation, we setup two comparison models, i.e., DAM f irst and DAM last , which dispense with the multi-grained representations in DAM, and use representation results from the 0 th layer and L th layer of self-attention instead. Moreover, we setup DAM self and DAM cross , which only use self-attention-match or cross-attention-match respectively, in order to examine the effectiveness of both self-attention-match and cross-attention-match.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RNN-based models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Model Training</head><p>We copy the reported evaluation results of all baselines for comparison. DAM is implemented in tensorflow <ref type="bibr">4</ref> , and the used vocabularies, word em-bedding sizes for Ubuntu corpus and Douban corpus are all set as same as the SMN ( <ref type="bibr" target="#b26">Wu et al., 2017)</ref>. We consider at most 9 turns and 50 words for each utterance (response) in our experiments, word embeddings are pre-trained using training sets via word2vec ( <ref type="bibr" target="#b14">Mikolov et al., 2013)</ref>, similar to previous works. We use zero-pad to handle the variable-sized input and parameters in FFN are set to 200, same as word-embedding size. We test stacking 1-7 self-attention layers, and reported our results with 5 stacks of self-attention because it gains the best scores on validation set. The 1 st convolution layer has 32 <ref type="bibr">[3,</ref><ref type="bibr">3,</ref><ref type="bibr">3]</ref> filters with [1,1,1] stride, and its max-pooling size is <ref type="bibr">[3,</ref><ref type="bibr">3,</ref><ref type="bibr">3]</ref> with <ref type="bibr">[3,</ref><ref type="bibr">3,</ref><ref type="bibr">3]</ref> stride. The 2 nd convolution layer has 16 <ref type="bibr">[3,</ref><ref type="bibr">3,</ref><ref type="bibr">3]</ref> filters with [1,1,1] stride, and its maxpooling size is also <ref type="bibr">[3,</ref><ref type="bibr">3,</ref><ref type="bibr">3]</ref> with <ref type="bibr">[3,</ref><ref type="bibr">3,</ref><ref type="bibr">3]</ref> stride. We tune DAM and the other ablation models with adam optimizer <ref type="bibr" target="#b7">(Le et al., 2011</ref>) to minimize loss function defined in Eq 15. Learning rate is initialized as 1e-3 and gradually decreased during training, and the batch-size is 256. We use validation sets to select the best models and report their performances on test sets. <ref type="table">Table 1</ref> shows the evaluation results of DAM as well as all comparison models. As demonstrated, DAM significantly outperforms other competitors on both Ubuntu Corpus and Douban Conversation Corpus, including SMN dynamic , which is the state-of-the-art baseline, demonstrating the superior power of attention mechanism in matching response with multi-turn context. Besides, both the performances of DAM f irst and DAM self decrease a lot compared with DAM, which shows the effectiveness of self-attention and cross-attention. Both DAM f irst and DAM last underperform DAM, which demonstrates the benefits of using multigrained representations. Also the absence of self-attention-match brings down the precision, as shown in DAM cross , exhibiting the necessity of jointly considering textual relevance and dependency information in response selection. One notable point is that, while DAM f irst is able to achieve close performance to SMN dynamic , it is about 2.3 times faster than SMN dynamic in our implementation as it is very simple in computation. We believe that DAM f irst is more suitable to the scenario that has limitations in computation time or memories but requires high precise, such as industry application or working as an component in other neural networks like GANs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Experiment Result</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>We use the Ubuntu Corpus for analyzing how selfattention and cross-attention work in DAM from both quantity analysis as well as visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Quantity Analysis</head><p>We first study how DAM performs in different utterance number of context. The left part in <ref type="figure" target="#fig_4">Fig- ure 4</ref> shows the changes of R 10 @1 on Ubuntu Corpus across contexts with different number of utterance. As demonstrated, while being good at matching response with long context that has more than 4 utterances, DAM can still stably deal with short context that only has 2 turns.  Moreover, the right part of <ref type="figure" target="#fig_4">Figure 4</ref> gives the comparison of performance across different contexts with different average utterance text length and self-attention stack depth. As demonstrated, stacking self-attention can consistently improve matching performance for contexts having different average utterance text length, implying the stability advantage of using multi-grained semantic representations. The performance of matching short utterances, that have less than 10 words, is obviously lower than the other longer ones. This is because the shorter the utterance text is, the fewer information it contains, and the more difficult for selecting the next utterance, while stacking self-attention can still help in this case. However for long utterances like containing more than 30 words, stacking self-attention can significantly improve the matching performance, which means that the more information an utterance contains, the more stacked self-attention it needs to capture its intra semantic structures. prior-match posterior-match self-attention cross-attention self-attention-match in stack 0 self-attention-match in stack 2 self-attention-match in stack 4 cross-attention-match in stack 4 self-attention-match cross-attention-match <ref type="figure">Figure 5</ref>: Visualization of self-attention-match, cross-attention-match as well as the distribution of self-attention and crossattention in matching response with the first utterance in <ref type="figure">Figure 1</ref>. Each colored grid represents the matching degree or attention score between two words. The deeper the color is, the more important this grid is.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Visualization</head><p>We study the case in <ref type="figure">Figure 1</ref> for analyzing in detail how self-attention and cross-attention work. Practically, we apply a softmax operation over self-attention-match and cross-attention-match, to examine the variance of dominating matching pairs during stacking self-attention or applying cross-attention. <ref type="figure">Figure 5</ref> gives the visualization results of the 0 th , 2 nd and 4 th self-attention-match matrixes, the 4 th cross-attention-match matrix, as well as the distribution of self-attention and crossattention in the 4 th layer in matching response with the first utterance (turn 0) due to space limitation. As demonstrated, important matching pairs in selfattention-match in stack 0 are nouns, verbs, like "package" and "packages", those are similar in topics. However matching scores between prepositions or pronouns pairs, such as "do" and "what", become more important in self-attention-match in stack 4. The visualization results of self-attention show the reason why matching between prepositions or pronouns matters, as demonstrated, selfattention generally capture the semantic structure of "no clue what do you need package manager" for "do" in response and "what packages are installed" for "what" in utterance, making segments surrounding "do" and "what" close to each other in representations, thus increases their dot-product results.</p><p>Also as shown in <ref type="figure">Figure 5</ref>, self-attentionmatch and cross-attention-match capture complementary information in matching utterance with response. Words like "reassurance" and "its" in response significantly get larger matching scores in cross-attention-match compared with self-attention-match. According to the visualization of cross-attention, "reassurance" generally depends on "system" "don't" and "held" in utterance, which makes it close to words like "list", "installed" or "held" of utterance. Scores of crossattention-match trend to centralize on several segments, which probably means that those segments in response generally capture structure-semantic information across utterance and response, amplifying their matching scores against the others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Error Analysis</head><p>To understand the limitations of DAM and where the future improvements might lie, we analyze 100 strong bad cases from test-set that fail in R 10 @5. We find two major kinds of bad cases: (1) fuzzycandidate, where response candidates are basically proper for the conversation context, except for a few improper details. (2) logical-error, where response candidates are wrong due to logical mismatch, for example, given a conversation context A: "I just want to stay at home tomorrow.", B: "Why not go hiking? I can go with you.", response candidate like "Sure, I was planning to go out tomorrow." is logically wrong because it is contradictory to the first utterance of speaker A. We believe generating adversarial examples, rather than randomly sampling, during training procedure may be a good idea for addressing both fuzzy-candidate and logical-error, and to capture logic-level information hidden behind conversation text is also worthy to be studied in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we investigate matching a response with its multi-turn context using dependency information based entirely on attention. Our solution extends the attention mechanism of Transformer in two ways: (1) using stacked selfattention to harvest multi-grained semantic representations. (2) utilizing cross-attention to match with dependency information. Empirical results on two large-scale datasets demonstrate the effectiveness of self-attention and cross-attention in multi-turn response selection. We believe that both self-attention and cross-attention could benefit other research area, including spoken language understanding, dialogue state tracking or seq2seq dialogue generation. We would like to explore in depth how attention can help improve neural dialogue modeling for both chatbots and taskoriented dialogue systems in our future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of Deep Attention Matching Network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Attentive Module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>average number of words in each turn number of turns in context</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: DAM's performance on Ubuntu Corpus across different contexts. The left part shows the performance in different utterance number of context. The right part shows performance in different average utterance text length of context as well as self-attention stack depth.</figDesc><graphic url="image-15.png" coords="7,307.28,320.88,142.17,102.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>). The retrieval-based systems enjoy the advantage of informative and fluent responses because it searches a large dialogue repository and selects</figDesc><table>Input 

Representation 

Matching 
Aggregation 

Word 
Embedding 

Representation Module 

! 

Word-word Matching 
with Cross-Attention 

" # 

" $ 

" % 

Matching 
Score 

g(c,r) 

3D Matching 
Image Q 

U i 

R 

Multi-grained 
Representations 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>:</head><label></label><figDesc></figDesc><table>Previous best performing 
models are based on RNNs, we choose 
representative models as baselines, includ-
ing SMN dynamic (Wu et al., 2017), Multi-
view(Zhou et al., 2016), DualEncoder lstm 
and DualEncoder bilstm (Lowe et al., 2015), 
DL2R (Yan et al., 2016), Match-LSTM 
(Wang and Jiang, 2017) and MV-LSTM 
(Pang et al., 2016), where SMN dynamic 
achieves the best scores against all the other 
published works, and we take it as our state-
of-the-art baseline. 

</table></figure>

			<note place="foot" n="2"> https://www.tensorflow.org/api docs/python/tf/nn/conv3d</note>

			<note place="foot" n="4"> https://www.tensorflow.org. Our code and data will be available at https://github.com/baidu/Dialogue/DAM</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We gratefully thank the anonymous reviewers for their insightful comments. This work is supported by the National Basic Research Program of China (973 program, No. 2014CB340505).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Luke, i am your father: dealing with out-of-domain requests by using movies subtitles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ameixa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Coheur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Fialho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paulo</forename><surname>Quaresma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Virtual Agents</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="13" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint/>
	</monogr>
<note type="report_type">ton. 2016. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Modern information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berthier</forename><surname>Ribeiro-Neto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>ACM press New York</publisher>
			<biblScope unit="volume">463</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>international conference on learning representations</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Iris: a chatoriented dialogue system based on the vector space model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rafael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haizhou</forename><surname>Banchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2012 System Demonstrations</title>
		<meeting>the ACL 2012 System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="37" to="42" />
		</imprint>
		<respStmt>
			<orgName>Association for Computational Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">An information retrieval approach to short text conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongcheng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.6988</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On optimization methods for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhik</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bobby</forename><surname>Lahiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Prochnow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on International Conference on Machine Learning</title>
		<meeting>the 28th International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="265" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Complexity of dependencies in discourse: Are dependencies in discourse more complex than in syntax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Dinesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Webber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Workshop on Treebanks and Linguistic Theories</title>
		<meeting>the 5th International Workshop on Treebanks and Linguistic Theories<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adversarial learning for neural dialogue generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="372" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cicero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>international conference on learning representations</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Towards an automatic turing test: Learning to evaluate dialogue responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Noseworthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Iulian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Angelard-Gontier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="372" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nissan</forename><surname>Pow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Vlad Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<title level="m">The ubuntu dialogue corpus: A large dataset for research in unstructured multiturn dialogue systems. annual meeting of the special interest group on discourse and dialogue</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="285" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Developing non-goal dialog system based on examples of drama television</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasguido</forename><surname>Nio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sakriani</forename><surname>Sakti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoki</forename><surname>Toda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirna</forename><surname>Adriani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural Interaction with Robots, Knowbots and Smartphones</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="355" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Text matching as image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengxian</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2793" to="2799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Data-driven response generation in social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="583" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dialogue act modeling for automatic tagging and recognition of conversational speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Ries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Coccaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Shriberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carol</forename><surname>Van Ess-Dykema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie</forename><surname>Meteer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="339" to="373" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Utterance units in spoken dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Traum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter A Heeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Dialogue Processing in Spoken Language Systems</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="125" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Computing machinery and intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">M</forename><surname>Turing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mind</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">236</biblScope>
			<biblScope unit="page" from="433" to="460" />
			<date type="published" when="1950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The trec-8 question answering track report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ellen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Trec</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="77" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A dataset for research on short-text conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="935" to="945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Syntax-based deep matching of short texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Joint Conferences on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Machine comprehension using match-lstm and answer pointer. international conference on learning representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sequential match network: A new architecture for multi-turn response selection in retrieval-based chatbots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoujun</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="372" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning to respond with deep neural networks for retrievalbased human-computer conversation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval</title>
		<meeting>the 39th International ACM SIGIR conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Abcnn: Attention-based convolutional neural network for modeling sentence pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Wenpeng Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Schütze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association of Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="259" to="272" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-view response selection for human-computer conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxiang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianhai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="372" to="381" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
