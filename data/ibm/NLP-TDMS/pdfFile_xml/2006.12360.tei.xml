<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Don&apos;t Wait, Just Weight: Improving Unsupervised Representations by Learning Goal-Driven Instance Weights</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linus</forename><surname>Ericsson</surname></persName>
							<email>linus.ericsson@ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh Edinburgh</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Gouk</surname></persName>
							<email>hgouk@inf.ed.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh Edinburgh</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
							<email>t.hospedales@ed.ac.uk</email>
							<affiliation key="aff2">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh Edinburgh</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Don&apos;t Wait, Just Weight: Improving Unsupervised Representations by Learning Goal-Driven Instance Weights</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the absence of large labelled datasets, self-supervised learning techniques can boost performance by learning useful representations from unlabelled data, which is often more readily available. However, there is often a domain shift between the unlabelled collection and the downstream target problem data. We show that by learning Bayesian instance weights for the unlabelled data, we can improve the downstream classification accuracy by prioritising the most useful instances. Additionally, we show that the training time can be reduced by discarding unnecessary datapoints. Our method, BetaDataWeighter is evaluated using the popular self-supervised rotation prediction task on STL-10 and Visual Decathlon. We compare to related instance weighting schemes, both hand-designed heuristics and meta-learning, as well as conventional self-supervised learning. BetaDataWeighter achieves both the highest average accuracy and rank across datasets, and on STL-10 it prunes up to 78% of unlabelled images without significant loss in accuracy, corresponding to over 50% reduction in training time.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the modern internet age, obtaining vast amounts of data is easier than ever and, as curating and annotating large datasets is expensive and labour-intensive, a promising direction is training deep models from more readily available unlabelled data. In recent years, self-supervised pre-training methods have been developed that are beginning to approach the performance of their supervised counterparts <ref type="bibr" target="#b23">[24]</ref>, thus making the training of machine learning models far less labour-intensive. However, when training such models on uncurated data, it is by definition impossible to know if all the unlabelled data is useful or relevant to the downstream task. The target domain is often shifted or narrower compared to the source domain, which could result in self-supervised learning damaging performance rather than helping it <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b36">37]</ref>. Compared to the rich literature of dealing with domain shift in supervised learning <ref type="bibr" target="#b35">[36]</ref>, the problem of domain shift in unsupervised visual representation learning has received little attention. Proposed solutions have focused on ad hoc approaches that have marginal benefits <ref type="bibr" target="#b36">[37]</ref> or require significant resources that are not available to the majority of practitioners <ref type="bibr" target="#b6">[7]</ref>.</p><p>We approach this problem from the perspective of learning instance weights, a set of parameters determining the importance of each example during training. The best weights are the ones that minimise the expected loss on the downstream supervised target domain, when re-using the representation learned in the weighted source domain. Towards this end, we develop a meta-learning algorithm <ref type="bibr" target="#b42">[43]</ref> termed BetaDataWeighter that infers the optimal weights for self-supervised learning on unlabelled source data, in order to optimise performance on a relatively small labelled problem downstream in the target domain. As the weights are learned, the most helpful source data will be given priority, while other source data that would be unnecessary or even actively damaging in terms of target domain performance will be down-weighted.</p><p>Since this down-weighted data contributes little to learning, there is potential to accelerate training by pruning such data from the source set. However, realising this is challenging, as a datapoint that seems useless now might become useful at a later stage of training. Therefore, pruning apparently useless datapoints too early could be a false efficiency. In order to manage pruning-based acceleration with the potential future value of the data, we introduce a Bayesian approach, termed BetaDataWeighter, that meta-learns a beta distribution over the value of each datapoint. This enables us to prune instances more effectively and safely, based on both their their estimated value as well as the certainty of that estimation.</p><p>To evaluate our method on a variety of target task types, we introduce a new cross-domain self-supervised learning benchmark based on the data from the Visual Decathlon challenge <ref type="bibr" target="#b37">[38]</ref>. Our results show that goal-driven self-supervised learning with BetaDataWeighter proves more effective than related heuristic and meta-learning alternatives.</p><p>Our main contributions are fourfold. Firstly, we study the novel problem of learning instance weights for self-supervised pre-training in support of a supervised target task in a different domain. Secondly, we introduce BetaDataWeighter, an effective meta-algorithm for goal-driven self-supervised instance weighting. This underpins a Bayesian dataset pruning mechanism that can prune up to 78% of unlabelled images in STL-10 [9] without significant loss in accuracy, leading to over 50% reduction in training time. Lastly, we show that when learning CNN features using BetaDataWeighter to re-weight ImageNet, we can improve downstream classification performance on a variety of tasks from Visual Decathlon, with similar outcomes shown on STL-10. Our method outperforms recent related weighting schemes based on both hand-designed heuristics and meta-learning, as well as vanilla non-weighted self-supervision-achieving both the highest average accuracy and rank across datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Unsupervised Learning The most common approaches to learning without labels include clustering <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b5">6]</ref> and autoencoding <ref type="bibr" target="#b22">[23]</ref>. In self-supervised learning, labels are generated automatically from the data itself <ref type="bibr" target="#b20">[21]</ref> and the model is trained to predict them, via a 'pretext task'. Many recent computer vision works have defined such tasks, like context prediction of image patches <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b32">33]</ref>, colouring in grayscale images <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b46">47]</ref> or predicting the image given transformations of it to encourage invariant representations <ref type="bibr" target="#b11">[12]</ref>. Kolesnikov et al. <ref type="bibr" target="#b23">[24]</ref> evaluate a range of these algorithms and find that the rotation-prediction pretext task <ref type="bibr" target="#b15">[16]</ref> is the most effective on natural image datasets like ImageNet <ref type="bibr" target="#b9">[10]</ref>. The model for this task, termed RotNet, is trained to predict which rotation has been applied to an image from the range {0 • , 90 • , 180 • , 270 • }. We build on RotNet by learning instance weights on the source data to improve both the performance when transferring learned features to a target domain, and the training time of self-supervised learning. Our goal is similar to that of <ref type="bibr" target="#b6">[7]</ref>, who also study learning from uncurated datasets, by hand-designing a robust unsupervised pre-training algorithm. In contrast, we meta-learn the pre-training strategy (paramaterised by instance weights) to optimise the downstream task, as well as the pruning strategy to accelerate self-supervised pre-training.</p><p>Instance-weighting and Curricula Our strategy is based on instance re-weighting, which has been widely studied in the context of boosting <ref type="bibr" target="#b14">[15]</ref>, hard negative mining <ref type="bibr" target="#b28">[29]</ref>, focal loss <ref type="bibr" target="#b27">[28]</ref> and more recently in meta-learning <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b40">41]</ref>. In particular, instance-weighing approaches have been successfully applied to deal with noisy labels and class imbalance in supervised learning <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b38">39]</ref>, by down-weighting mis-labelled images and over-represented categories respectively. In contrast, we study instance weighting for the purpose of self-supervised learning on uncurated data. In this case there is no label noise or class imbalance, but source images may have varying relevance to the downstream target problem. The challenges of learning on uncurated data have been highlighted for semi-supervised learning <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b33">34]</ref>. We focus on closing the loop between self-supervised pre-training and downstream supervised learning by meta-learning the optimal self-supervised instance weights. We ameliorate the computational overhead of such instance-wise meta-learning by introducing an effective data pruning strategy to eliminate unhelpful source data from consideration. This is in contrast to other approaches to dealing with data of varying relevance <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b19">20]</ref> which all still pay the cost to train on the full data.</p><p>Instance pruning has been studied in the literature <ref type="bibr" target="#b24">[25]</ref>, notably in the context of core-set construction <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b43">44]</ref>. However, with the notable exception of <ref type="bibr" target="#b39">[40]</ref>, few contemporary studies have attempted to prune the data used for deep networks online during model training.</p><p>Meta Learning Our weight learning strategy is an instantiation of meta-learning <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b18">19]</ref>, which has seen a large number of works in recent years <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b1">2]</ref>. The meta-learning algorithms most related to our method are L2RW <ref type="bibr" target="#b38">[39]</ref> and MetaWeightNet <ref type="bibr" target="#b40">[41]</ref>. We learn weights on the data in a transductive way, akin to L2RW, while MetaWeightNet learns an inductive model for weights. All these methods focus on within-domain learning, while we uniquely look at meta-learning instance weighing to facilitate domain-transfer between unlabelled source data and a downstream supervised learning task. Compared to L2RW <ref type="bibr" target="#b38">[39]</ref>, we do not normalise weights per batch (since for our unlabelled data, we cannot guarantee the proportion of good and bad data in each batch). More importantly, we achieve greater efficacy due to our Bayesian weight that enables continual weight learning without premature convergence, eliminating the need to reset weights after each batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background</head><p>Our goal is to solve a machine learning problem in a target domain for which we have a small set of clean labelled data. The labelled set is too small for training a deep network from scratch, so we aim to exploit self-supervised pre-training on a large unlabelled dataset. The auxiliary data is potentially uncurated, meaning that we do not know its composition and relevance to the target problem.</p><formula xml:id="formula_0">Let D target = {D train target ∪ D val target ∪ D test</formula><p>target } be the small labelled and curated dataset, and D source be the large unlabelled dataset with unknown and varying relevance to D target including partial or no overlap in categories and low-level statistics. Our goal is to train an unsupervised feature extractor on D source such that the accuracy of a classifier using this feature extractor and trained on D target is maximised. To deal with the unknown and varying usefulness of the source data, we adaptively re-weight source instances according to their contribution to target model performance. Furthermore, to reduce the cost of this process, we will discard the most unhelpful points from the source set online during training.</p><p>Let f θ be a neural network feature extractor and L ss be a self-supervised loss on the unlabelled source data, D source = (x <ref type="bibr" target="#b0">(1)</ref> s , x (2) s , . . . , x (n) s ). The downstream target task consists of labelled data</p><formula xml:id="formula_1">D train target = ((x (1) min w 1 m m j L meta ((x ( j) t , y ( j) t ), θ * ) s. t. θ * = arg min θ 1 n n i w (i) L ss ( f θ (x (i) s ))<label>(1)</label></formula><p>In the outer optimisation level, the instance weights w are updated so as to minimise the loss of the downstream task. In the inner level the feature extractor, θ, is updated to minimise the weighted loss on the source task. Next we will introduce an algorithm for approximately solving this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Learning goal-driven instance weights</head><p>We now introduce BetaDataWeighter (BDW), an online solution to the problem defined in Section 3. It maintains a distribution over the weights for each example, from which stochastic weights are generated at each update. These instance-wise parameters determine the importance of their corresponding datapoints throughout training. As they are only used during training, they do not affect inference run-time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">BetaDataWeighter</head><p>Instance-Weighted Feature Learning During training on the source data, the parameters θ of our feature extractor f θ are updated using gradient descent on the self-supervised loss, L ss . For each datapoint, x (i) s , we associate a data-weight parameter, w (i) . This parameter is used to scale the loss on </p><formula xml:id="formula_2">(i) s ∈ D source initialise a (i) = 1, b (i) = 1 4: Initialise model parameters θ 5: for epoch t from 1 to T do 6: for sampled mini-batch {x (i) s } k i=0 from D source,t do 7: Sample w (i) ∼ Beta(a (i) , b (i) ) by reparameterisation trick 8: θ ← θ − α ∇ θ 1 k k i w (i) L ss (x (i) s ; θ)</formula><p>Get new model params from update <ref type="bibr">9:</ref> a ← a − η ∇ a L meta (D train target , θ ) Update a using meta-gradient</p><formula xml:id="formula_3">10: b ← b − η ∇ b L meta (D train target , θ )</formula><p>Update b using meta-gradient <ref type="bibr">11:</ref> θ ← θ <ref type="bibr">12:</ref> end for 13:</p><formula xml:id="formula_4">D source,t+1 = {x (i) s ∈ D source,t | CDF(λ; a (i) , b (i) ) &gt; ρ}</formula><p>Prune datapoints <ref type="bibr">14:</ref> end for</p><formula xml:id="formula_5">x (i)</formula><p>s during training. A mini-batch update of size k, given the corresponding data-weights, is given by</p><formula xml:id="formula_6">θ ← θ − α ∇ θ 1 k k i=0 w (i) L ss ( f θ (x (i) s )),<label>(2)</label></formula><p>where α is the learning rate for the network parameters. By setting w (i) = 1 for all i, we obtain a standard stochastic gradient update. By tuning weights w (i) , we can prioritise data that contribute more to downstream performance, and reduce the influence of data that is unhelpful or damaging.</p><p>Bayesian Instance-Weighting Instead of point estimating the weights, w (i) , we can store the parameters defining a probability distribution over each weight. The weight of a datapoint can be viewed as the probability p (i) that an oracle data selector would select this datapoint for training. One can directly attempt to learn this value in a deterministic fashion (i.e. w (i) = p (i) ). This is how many existing re-weighting methods operate. Alternatively, because p (i) can be interpreted as a Bernoulli parameter, we can model our belief about its value with a beta distribution. This distribution is itself parameterised by two scalars, a (i) and b (i) . In our algorithm these will be the parameters that the outer loop optimises.</p><p>When sampling from a beta distribution, w (i) ∼ Beta(a (i) , b (i) ), we obtain a data-weight in the range [0, 1]. This means that no weight can be negative and neither can any point have a massive weight, which will help stabilise training. In order to define a beta distribution, the parameters a (i) and b (i) must be strictly positive. To learn these using unconstrained optimisation, we paramaterise them as log(a (i) ) and log(b (i) ). Initially, we have no knowledge of whether a particular datapoint is relevant or not, so we initialise p (i) to a uniform prior by setting a (i) and b (i) to one. As training progresses, we will learn whether the datapoint is useful or not and thus update our distribution over its probability by updating the parameters a (i) and b (i) . Like many meta-learning algorithms, we use our target to validate the model at each iteration of training. The target loss will provide higher order gradients for updating a (i) and b (i) . As solving the entire inner optimisation problem from Eq. (1) is prohibitively expensive and can lead to gradient stability problems, we approximate it by performing a single inner step.</p><p>Learning Algorithm We describe our algorithm with batch-size one for illustration. At each iteration we sample an instance weight w (i) ∼ Beta(a (i) , b (i) ), and then speculatively update model parameters θ given this weight</p><formula xml:id="formula_7">θ ← θ − α ∇ θ w (i) L ss ( f θ (x (i) s )).<label>(3)</label></formula><p>After this speculative update, we evaluate the resulting model parameters θ on the D train target set to get a meta-loss that measures how well the new model generalises to unseen data. If labelled data is available, this meta-loss, L meta , can be a measure of classification performance, such as the cross entropy loss. However, it is also possible to use a self-supervised loss instead. This is discussed further in Section 4.2. As w (i) was obtained via a sampling process, we are unable to get gradients for the distribution parameters. But by reparameterising the beta distribution according to <ref type="bibr">Figurnov et al. [13]</ref> or by using the tools available in recent deep learning libraries <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b0">1]</ref>, we can sample from it in a differentiable way. We can thus acquire gradients for a (i) , and b (i) during our backward pass,</p><formula xml:id="formula_8">a (i) ← a (i) − η ∇ a (i) L meta (D train target , θ ) (4) b (i) ← b (i) − η ∇ b (i) L meta (D train target , θ ).<label>(5)</label></formula><p>where η is learning rate for the instance weights, which is set independently of the model learning rate.</p><p>Each iteration of our algorithm consists of a model update followed by an update on the beta distribution parameters. Note that only the distributions corresponding to the datapoints within the current batch can be updated. Thus it takes a full epoch to update all of the distributions once. In this sense, the outer loop of Eq. (1) is being solved with a block coordinate descent method, whereas the inner loop is solved with stochastic gradient descent. Pseudocode for the full BetaDataWeighter method is provided in Algorithm 1. We give the details of a deterministic version, DataWeighter, in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Meta-loss</head><p>The loss, L meta , used to evaluate the feature extractor θ on D train target does not have to be the same loss used for training the feature extractor. When we have labels for this set, we prefer to use a loss that estimates the performance of a classifier. One could train a linear model g φ and then measure performance based on predictions of g φ ( f θ (·)). However, this inner model fitting should be as efficient as possible since a new model will be needed for each update. As such, we define a meta-loss that constructs a nearest centroid classifier <ref type="bibr" target="#b16">[17]</ref> (NCC) and measures the performance of θ using cross entropy,</p><formula xml:id="formula_9">L meta (D, θ) = 1 |D| (x (i) ,y (i) )∈D L CE (y (i) , g φ * ( f θ (x (i) ))) s. t. φ * = arg min φ L NCC (φ; D, θ),<label>(6)</label></formula><p>where L CE and L NCC are the cross entropy loss and nearest centroid classifier loss, respectively. The simple nature of NCC permits a closed form solution to the arg min operation, as popularised by Snell et al. <ref type="bibr" target="#b41">[42]</ref>. In initial experiments we found this to produce higher downstream classification accuracy compared to using a self-supervised meta-loss. At each iteration we sample a K-way-N-shot episode from the target set as in Prototypical Networks <ref type="bibr" target="#b41">[42]</ref> and use this to train the NCC and evaluate the meta-loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Pruning Unhelpful Data</head><p>Similar to the L2RW algorithm of Ren et al. <ref type="bibr" target="#b38">[39]</ref>, our method introduces computational overhead. We perform one extra forward pass and backward pass on the target set batch, plus an additional backward pass when updating the weight parameters. Although our use of NCC lowers computation overhead, we still find that our method is 3× to 6× slower than standard training.</p><p>The goal of learning instance weights is to improve downstream model performance, but a useful side-effect is that it can also be used to reduce the training time by discarding irrelevant data. We thus introduce a pruning process between every epoch where unhelpful data is discarded and no longer revisited in future epochs.</p><p>Arising from our Bayesian approach, we are able to incorporate uncertainty about the data-weights in the pruning strategy. We wish to ensure that discarding data does not change the model objective significantly, as this could impact the stability of the training process-a phenomenon that leads to overfitting <ref type="bibr" target="#b3">[4]</ref>, and that we do not suffer from premature pruning (e.g., of low mean but high variance distributions). However, if a large proportion of an instance's weight density lies below a small value, then we are highly likely to sample a low data-weight so we can be confident about discarding it. We introduce two hyperparameters: the density threshold ρ, and the CDF threshold λ. A datapoint will be pruned if more than ρ of the probability density lies below λ. Given source data for epoch t, D source,t , the data for training the BetaDataWeighter on epoch t + 1 is therefore</p><formula xml:id="formula_10">D source,t+1 = {x (i) s ∈ D source,t | CDF(λ; a (i) , b (i) ) &gt; ρ},<label>(7)</label></formula><p>where a (i) , b (i) are the beta distribution parameters of x (i) s and CDF(·) is the cumulative density function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section we describe the series of experiments we conduct to evaluate our algorithm, the Beta-DataWeighter, referred to as BDW. We also include results from running its deterministic version DW.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Analysing instance weights</head><p>We start by applying our method in a simple setup where we can track the weights of data which we know to be in-domain. A minimal variational auto-encoder (VAE) <ref type="bibr" target="#b22">[23]</ref> is trained on three domains: MNIST <ref type="bibr" target="#b26">[27]</ref>, FashionMNIST <ref type="bibr" target="#b44">[45]</ref> and KMNIST <ref type="bibr" target="#b7">[8]</ref>. Each dataset is chosen in turn to serve as the target domain, with its 10,000 test images serving as the target test set D test target . We select another 10,000 images from its training set to serve as the target train set D train target . The remaining training images are combined with the images from the training sets of the other two domains to form the unlabelled source set, D source . This creates a mixed source set of 170,000 images (i.e., 50,000 from MNIST, 60,000 from FashionMNIST, and 60,000 from KMNIST). No labels are used in this experiment as all losses are based on image reconstruction. Source set domain labels are also not used, and the goal is for our BDW VAE to select the most relevant source data for learning in order to encode each target problem.</p><p>The architecture consists of a fully-connected encoder that maps the input to 100 hidden units and then to a single hidden unit bottleneck. The decoder mirrors the structure of the encoder. The model is trained for 100 epochs with SGD using a learning rate of 1 × 10 −4 and a batch-size of 64.</p><p>We evaluate two baselines: VAE, which trains on all three domains with all data-weights set to 1, and Oracle VAE, which only trains on the images from the target domain. Our BetaDataWeighter (BDW), and its deterministic ablation DataWeighter (DW), are trained on all of the data. <ref type="table" target="#tab_1">Table 1</ref> shows that our methods improve the test loss compared to the baseline VAE which suffers from equally weighting source data of mixed relevance. Our methods perform similarly in terms of test loss, with both tending to improve on the VAE baseline. BDW manages to reduce the training time more significantly with better pruning.  <ref type="figure" target="#fig_0">Figure 1a</ref> plots the distribution of the learned data weights for the BetaDataWeighter with FashionM-NIST target. It is clear that the images from FashionMNIST are prioritised over the other two domains.</p><p>For the same run we also show the distribution of Beta parameters for all datapoints in <ref type="figure" target="#fig_0">Fig. 1b</ref>. Here, a large number of FashionMNIST images tend to have higher a and lower b values. There is also a subset of FashionMNIST images that are have their density concentrated around lower weights, putting them in the same cluster of pruned datapoints as the MNIST and KMNIST data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Goal-Driven Self-Supervised Learning: Experiment Setup</head><p>We now investigate whether our method can improve the quality of visual features learned from a large dataset of unlabelled images, for transfer to a downstream classification task on a different domain. We use the Visual Decathlon (VD) <ref type="bibr" target="#b37">[38]</ref> collection of vision datasets. It consists of 10 very different image classification tasks; Aircraft, CIFAR100, DPed, DTD, Flowers, GTSRB, ImageNet, Omniglot, SVHN and UCF101. In our experiments on VD, the feature extractor is always trained on the 1.28 million images from the ImageNet domain as D source , and we use the remaining nine domains as downstream target tasks. As the test labels have not been released, we use the competition's validation splits as our test sets. For the nine target domains we create our own two sets from the competition train splits; D train target which is used to drive instance weight learning, and D val target which is used for model selection before evaluation on the test set. See details in the appendix for the size of the target splits. This benchmark is substantially larger than the previous experiment. As we saw in <ref type="table" target="#tab_1">Table 1</ref>, the BetaDataWeighter is often substantially faster than DataWeighter due to improved uncertainty-based pruning. We therefore focus on BetaDataWeighter on the VD experiments.</p><p>Implementation details: Meta-Training We follow RotNet <ref type="bibr" target="#b15">[16]</ref> which is trained to predict which of four 2d rotations has been applied to an input image, from (0 • , 90 • , 180 • , 270 • ). Our network architecture is a ResNet34 <ref type="bibr" target="#b17">[18]</ref>. At each iteration, we sample a batch for the prototypical <ref type="bibr" target="#b41">[42]</ref> style meta-loss. We use the common 20-way 5-shot design (20 classes with 5 examples of each) with some exceptions. See appendix for further details. sets. For fine-tuning evaluation: We take the network which performed best on logistic regression and replace its classification head for the downstream task and finetune the entire network on the combined data from D train target and D val target . Baselines We compare our BetaDataWeighter to training the same rotation-prediction network on all data without any re-weighting. This baseline is referred to as RotNet. The best feature extractor is selected by early stopping on the target validation accuracy computed with logistic regression.</p><p>We also implement the nearest neighbour weighting scheme in <ref type="bibr" target="#b36">[37]</ref>. We refer to this baseline as the NN-Weighter. Details of our implementation can be found in the appendix. The last baseline we compare to is the related meta-learning method L2RW <ref type="bibr" target="#b38">[39]</ref>. L2RW is designed for within-domain supervised learning, but we adapt the algorithm for our task and use the same prototypical style meta-loss loss on the D train target batch.  STL-10 experiment We also evaluate our method on STL-10 <ref type="bibr" target="#b8">[9]</ref>, which contains 10 target supervised classes and has a larger amount of unlabelled out-of-domain images for training. The 5000 labelled training images are split into our D train target and D val target sets, with 2500 images in each. For the meta-loss we use a 10-way 10-shot batch design. We train a ResNet18 using rotation-prediction on the unlabelled STL-10 images with a batch-size of 128 and extract features from the final pre-logit layer of the network. Random crops of 84 × 84 are extracted during training and centred crops of the same size for testing and finetuning. Other hyperparameters are identical to those in the ImageNet/VD benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results</head><p>Goal Driven Self-Supervised Learning The results of both experiments are shown in <ref type="table" target="#tab_2">Table 2</ref>. From the results we can see that: (i) Weighting based methods usually improve on the vanilla self-supervised baseline of RotNet. Notably, (ii) BDW generally performs better than competitors. (iii) BDW is effective in both the scenario where a fixed feature is used to train a linear classifier, and where end-to-end fine-tuning is performed on the target problem.</p><p>To find out if the differences in performance of the models are statistically significant, we perform a Friedman test with p = 0.05 over all 10 domains in <ref type="table" target="#tab_2">Table 2</ref>. The test passes so we reject the null hypothesis that all algorithms perform according to the same distribution. We follow this by a post-hoc Nemenyi test on the average ranks of the algorithms. The resulting CD diagrams in <ref type="figure" target="#fig_2">Fig. 2</ref> confirm that the difference between RotNet and BetaDataWeighter is statistically significant both for the logistic regression results and finetuning scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Measuring the Effectiveness of Pruning</head><p>To study the impact of pruning on accuracy and training time, we evaluate different pruning hyperparameters: outer learning rate (η), density threshold (ρ) and the CDF threshold (λ), on the STL-10 (100,000 unlabelled and 5000 labelled images) experiment. BetaDataWeighter is trained on the unlabelled images with labelled D train target and D val target . From the results in <ref type="table" target="#tab_3">Table 3</ref> we can see that settings leading to both conservative and aggressive pruning achieve good accuracy. In particular, the finetuned test accuracies for all but one model are higher than the competing methods as seen in <ref type="table" target="#tab_2">Table 2</ref>. This shows that a high pruning rate can reduce the training time while still boosting performance. However, unlike in our experiments from Section 5.1, the training time never gets as low as the RotNet baseline. BDW does however improve upon the training time of the related meta-learning algorithm L2RW, showing the benefit of pruning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Self-supervised representation pre-training on large unlabelled datasets is a popular strategy to boost supervised learning on smaller downstream tasks. We highlight the challenge posed by mixed-relevance source data, and introduce a meta-learning method to train self-supervised source instance-weights in support of the downstream goal. Our Bayesian approach of modelling distributions over weights leads to improved meta-learning efficacy and enables reliable source data pruning, which limits the computational cost of such meta-learning. Results across 10 target tasks show that our method outperforms alternatives. In future work we plan to apply this framework to also optimise the weighting of multiple self-supervised tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>In common with other unsupervised and self-supervised learning methods, our contribution promises to reduce the amount of manual effort required to annotate datasets for training machine learning models. These techniques can potentially benefit society by enabling small organisations with less data to more easily compete with large institutions holding huge datasets. Our method is particularly oriented at the most realistic case where auxiliary data is unknown and of mixed relevance. While reducing manual labour is beneficial, in common with other unsupervised pre-training methods, we do trade-off this annotation effort for compute effort expended on the unlabelled set, which does entail some energy and environmental cost, and advantage those with greater access to compute resources. Our method ameliorates this issue to some extent with highly effective pruning techniques. These costs could be further reduced by future development in efficient meta-learning such as first order approximations and implicit differentiation. In future it may ultimately be possible to perform instance weighing with pruning more quickly than vanilla unweighted self-supervised learning.</p><p>Implementation Details: NN-Weighter This section describes our implementation of the nearest neighbour algorithm introduced by Peng et al. <ref type="bibr" target="#b36">[37]</ref>. For every image x (i) in the training set, the method finds the Euclidean distance d (i) to the single nearest D train target image. It converts this distance into a weight, w (i) = exp(−βd (i) ), given hyperparameter β which controls how quickly the weight approaches zero as the distance increases. As we use larger image spaces and datasets than the original authors we perform approximate nearest neighbour search using the HNSW method <ref type="bibr" target="#b29">[30]</ref> within the NMSLIB Python package <ref type="bibr" target="#b4">[5]</ref>. We tune β on CIFAR100 and use the optimal value of 1 × 10 −5 across the whole range of domains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) A histogram of the expected values of Beta distributions on source points on the FashionMNIST experiment. Data falling below the 0.5 line has been pruned. Blue: FashionMNIST, Orange: MNIST, Green: KMNIST. The majority of FashionMNIST images are still used in training and have high expected values while a majority of images from MNIST and KMNIST are pruned. (b) Density estimate of Beta parameters a and b. A high b value means the density is concentrated towards 0, while a high a means it's concentrated towards 1. Both figures were produced from the BetaDataWeighter run from Table 1 after epoch 25.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Implementation details: Evaluation For final evaluation, we consider two cases: (i) Training a linear classifier on the fixed pre-trained features, and (ii) Fine-tuning the pre-trained representation on the target problem. For linear classifier evaluation: Features are extracted after the second residual block and a logistic regression model is fit using L-BFGS with the data from both the D train target and D val target</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Critical difference diagrams from the Nemenyi test for the 10 datasets from VD and STL-10. Left: logistic regression on fixed features. Right: Finetuned. Each algorithm is represented by their average rank across all datasets. Bold lines connecting a group of algorithms indicate they are not significantly different as their average ranks differ by less than the CD value.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Source set D source , meta-set D train target , learning rates α, η, batch-size k, max epochs T 2: Output: Model parameters θ</figDesc><table><row><cell>Algorithm 1 BetaDataWeighter</cell></row></table><note>1: Input:3: For all x</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Autoencoding MNIST, FashionMNIST (FMNIST) and KMNIST with mixed source data. Lowest losses and training times are in bold, ignoring the Oracle VAE as it knows the domains.</figDesc><table><row><cell></cell><cell cols="2">MNIST</cell><cell cols="2">FMNIST</cell><cell cols="2">KMNIST</cell></row><row><cell></cell><cell cols="6">Test loss Time (s) Test loss Time (s) Test loss Time (s)</cell></row><row><cell>Oracle VAE</cell><cell>178.95</cell><cell cols="2">1,387 150.05</cell><cell>2,256</cell><cell>292.19</cell><cell>2,161</cell></row><row><cell>VAE</cell><cell>209.69</cell><cell cols="2">6,945 171.46</cell><cell>6,876</cell><cell>306.43</cell><cell>5,409</cell></row><row><cell>DW VAE (ours)</cell><cell cols="3">193.26 15,856 171.05</cell><cell>9,943</cell><cell>301.11</cell><cell>5,137</cell></row><row><cell>BDW VAE (ours)</cell><cell>192.64</cell><cell cols="2">6,005 171.13</cell><cell>1,424</cell><cell>302.70</cell><cell>4,081</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Test accuracies (%) of ResNet34 on Visual Decathlon domains after meta-learning instance weights for self-supervised training on ImageNet. Similarly for STL-10. Best results are in bold. . C100 DPed DTD Flwr GTSR OGlt SVHN UCF STL-10 Avg. Log. reg. RotNet 16.27 11.36 83.34 11.43 18.71 71.17 20.86 50.67 17.30 45.65 34.68 NN-Weighter [37] 17.45 12.66 83.59 11.54 17.13 72.10 24.15 54.72 15.64 45.68 35.47 L2RW [39] 16.70 15.72 85.36 14.44 30.27 79.79 18.71 57.64 22.89 44.68 38.62 BDW (ours) 21.13 17.53 86.38 18.87 24.48 77.49 23.49 57.74 22.49 46.41 39.60</figDesc><table><row><cell>RotNet NN-Weighter [37] L2RW [39] AircFinetuned 19.74 27.64 95.00 25.27 16.67 82.71 71.47 80.95 29.25 19.62 21.39 95.82 26.01 37.65 89.17 72.78 78.12 28.43 17.28 15.45 95.29 18.67 34.71 89.65 66.31 79.05 31.67 BDW (ours) 25.08 24.90 96.11 28.72 41.37 93.96 70.67 82.21 33.61</cell><cell>68.19 51.69 69.15 53.81 63.13 51.12 71.12 56.78</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>BetaDataWeighter pruning on STL-10. A range of pruning levels achieve higher finetuned accuracy than the RotNet baseline. Training time can be significantly reduced compared to L2RW.</figDesc><table><row><cell>Learning rate</cell><cell></cell><cell>10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>100</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Density threshold ρ</cell><cell>0.5</cell><cell></cell><cell>0.9</cell><cell></cell><cell>0.5</cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CDF threshold λ</cell><cell>0.1</cell><cell>0.25</cell><cell>0.1</cell><cell>0.25</cell><cell>0.1</cell><cell>0.25</cell><cell>0.1</cell><cell cols="3">0.25 RotNet L2RW</cell></row><row><cell>Finetuned</cell><cell>71.53</cell><cell>71.12</cell><cell>70.74</cell><cell>70.09</cell><cell>69.40</cell><cell>66.11</cell><cell>70.96</cell><cell cols="2">69.41 68.19</cell><cell>63.13</cell></row><row><cell>% pruned</cell><cell>35.06</cell><cell>49.69</cell><cell>0.67</cell><cell>10.45</cell><cell>78.26</cell><cell>83.56</cell><cell>47.49</cell><cell>68.38</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>Training time (s)</cell><cell>106,058</cell><cell>99,192</cell><cell>115,874</cell><cell>119,040</cell><cell>70,125</cell><cell>53,931</cell><cell>101,552</cell><cell cols="3">86,143 21,093 116,992</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t , y (1) t ), (x<ref type="bibr" target="#b1">(2)</ref> t , y (2) t ), . . . , (x (m) t , y (m) t )) with loss L meta , (which is typically a supervised loss but could potentially be unsupervised). We want to solve the following bi-level optimisation problem for instance weights w = (w<ref type="bibr" target="#b0">(1)</ref> , w<ref type="bibr" target="#b1">(2)</ref> , . . . , w (n) ):</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head><p>This work was supported in part by the EPSRC Centre for Doctoral Training in Data Science, funded by the UK Engineering and Physical Sciences Research Council (grant EP/L016427/1) and the University of Edinburgh.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Algorithm 2 Deterministic DataWeighter (DW) <ref type="bibr">1:</ref> Input: Source set D source , meta-set D train target , learning rates α, η, batch-size k, max epochs T 2: Output: Model parameters θ <ref type="bibr">3:</ref> For all x (i) s ∈ D source initialise w (i) = 0 4: Initialise model parameters θ <ref type="bibr">5:</ref> for epoch t from 1 to T do <ref type="bibr">6:</ref> for sampled mini-batch</p><p>Get new model params from update 8:</p><p>Update w using meta-gradient <ref type="bibr">9:</ref> θ ← θ 10:</p><p>end for 11:  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>Deterministic DataWeighter (DW) We present a deterministic version of our algorithm, where the data weights are point estimates, in contrast to the Bayesian approach in BDW described in Section 4.1.</p><p>The data weights, w, are clipped to the range [0, 1], similar to BDW. They are initialised to zero, like L2RW <ref type="bibr" target="#b38">[39]</ref>.</p><p>During training these weights are directly optimised by using D train target to validate the model. The update for a single data weight w (i) becomes</p><p>where η is learning rate for the data weights. The other properties of this algorithm are identical to BDW. Pseudocode for DW can be found in Algorithm 2.</p><p>Pruning in DW As DW does not have access to uncertainty of estimates, it prunes data solely based on the pruning parameter λ. If a data weight falls below this value, its associated datapoint is discarded. The source set at epoch t + 1 therefore consists of</p><p>Implementation Details: Size of Target Domain Splits For most domains we assign roughly 1000 images to each of these sets. However, in Flowers there are only 1020 images available so so we split them evenly across the two sets. In Omniglot there are a high number of classes (1623), so we assign 5 examples per class to each set. The combined sizes of these sets are given in <ref type="table">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details: Meta-Training on VD domains</head><p>We train each rotation-prediction model, as recommended by <ref type="bibr" target="#b23">[24]</ref>, for 35 epochs with SGD. In each batch, all four rotations of the image are included, effectively increasing the batch-size by a factor of four. It uses an initial learning rate of 0.1 which is decayed by 0.1 at epochs 15 and 25 and has a momentum parameter of 0.9. The batch-size is 256 (64 images × 4 rotations). All VD images have been resized to 72 × 72 and during training we take random crops of 64 × 64 pixels and reflect the image with a 50% probability. On some benchmarks we need to adapt the batch design as there are too few classes or too few examples of each class; on Daimler Pedestrian Classification we use 2-way 50-shot, on Omniglot 50-way 2-shot and on SVHN we use 10-way 10-shot classification.</p><p>Implementation Details: Meta-Testing on VD domains For linear classifier training, we use the hyperparameters suggested by <ref type="bibr" target="#b23">[24]</ref>, with 800 maximum iterations and the regularisation coefficient set to 100</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CM</head><p>where C is the number of classes for this task and M is the dimension of the extracted features.</p><p>For fine-tuning, the optimiser is Adam <ref type="bibr" target="#b21">[22]</ref> with a batch-size of 64. We tune the initial learning rate, learning rate schedule and weight decay parameters using a held out set before re-tuning the network with the chosen hyperparameters. For both conditions we take a center crop of 64 × 64 and do not use any data augmentation.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/.Softwareavailablefromtensorflow.org" />
		<editor>Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng</editor>
		<imprint>
			<date type="published" when="2015" />
			<pubPlace>Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever</pubPlace>
		</imprint>
	</monogr>
	<note>Dandelion Mané</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning to learn by gradient descent by gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><forename type="middle">Gómez</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando De</forename><surname>Freitas</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06476</idno>
		<title level="m">Practical Coreset Constructions for Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Stability and generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">André</forename><surname>Elisseeff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Engineering efficient and effective non-metric space library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Boytsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilegsaikhan</forename><surname>Naidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SISAP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep Clustering for Unsupervised Learning of Visual Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised pre-training of image features on non-curated data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep Learning for Classical Japanese Literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tarin</forename><surname>Clanuwat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Bober-Irizar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asanobu</forename><surname>Kitamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuaki</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<editor>CoRR</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An Analysis of Single-Layer Networks in Unsupervised Feature Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised Visual Representation Learning by Context Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Implicit Reparameterization Gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Figurnov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A decision-theoretic generalization of on-line learning and an application to boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Unsupervised Representation Learning by Predicting Image Rotations. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The elements of statistical learning: data mining, inference, and prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antreas</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Micaelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05439</idno>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: A Survey</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Invariant information clustering for unsupervised image classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Henriques</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Self-supervised Visual Feature Learning with Deep Neural Networks: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longlong</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingli</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Revisiting Self-Supervised Visual Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Àgata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoya</forename><surname>Bylinskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1311.6510</idno>
		<title level="m">Are all training examples equally valuable? arXiv preprint</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning representations for automatic colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C J</forename><surname>Burges</surname></persName>
		</author>
		<title level="m">MNIST handwritten digit database</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal Loss for Dense Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ensemble of exemplar-SVMs for object detection and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><forename type="middle">A</forename><surname>Malkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Yashunin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Munteanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Schwiegelshohn</surname></persName>
		</author>
		<title level="m">Coresets-Methods and History: A Theoreticians Design Pattern for Approximation and Streaming Algorithms. KI -Künstliche Intelligenz</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Impact of noisy labels in learning techniques: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitika</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanima</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hari</forename><forename type="middle">Prabhat</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDIS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Realistic Evaluation of Deep Semi-Supervised Learning Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekin</forename><surname>Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>De-Vito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<editor>Lu Fang, Junjie Bai, and Soumith Chintala</editor>
		<meeting><address><addrLine>Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Visual Domain Adaptation: A survey of recent advances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Magazine</title>
		<imprint>
			<date type="published" when="2015" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Investigating the effect of novel classes in semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">Yuxuan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><forename type="middle">Sing</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricia</forename><surname>Riddle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Pfahringer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACML</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Learning multiple visual domains with residual adapters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Sylvestre-Alvise Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning to Reweight Examples for Robust Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Active Learning for Convolutional Neural Networks: A Core-Set Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixuan</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanping</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongben</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Prototypical Networks for Few-shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Learning to Learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lorien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pratt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Springer US</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Core Vector Machines: Fast SVM Training on Very Large Data Sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ivor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">T</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pak-Ming</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine LearningAlgorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
		<editor>CoRR</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A Comprehensive Survey of Clustering Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongkuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingjie</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Data Science</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Colorful Image Colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
