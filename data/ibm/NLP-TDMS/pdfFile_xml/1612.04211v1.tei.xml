<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Perspective Context Matching for Machine Comprehension</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
							<email>zhigwang@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM T.J. Watson Research Center</orgName>
								<address>
									<addrLine>1101 Kitchawan Rd</addrLine>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM T.J. Watson Research Center</orgName>
								<address>
									<addrLine>1101 Kitchawan Rd</addrLine>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wael</forename><surname>Hamza</surname></persName>
							<email>whamza@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM T.J. Watson Research Center</orgName>
								<address>
									<addrLine>1101 Kitchawan Rd</addrLine>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
							<email>raduf@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM T.J. Watson Research Center</orgName>
								<address>
									<addrLine>1101 Kitchawan Rd</addrLine>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Perspective Context Matching for Machine Comprehension</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Previous machine comprehension (MC) datasets are either too small to train endto-end deep learning models, or not difficult enough to evaluate the ability of current MC techniques. The newly released SQuAD dataset alleviates these limitations, and gives us a chance to develop more realistic MC models. Based on this dataset, we propose a Multi-Perspective Context Matching (MPCM) model, which is an end-to-end system that directly predicts the answer beginning and ending points in a passage. Our model first adjusts each word-embedding vector in the passage by multiplying a relevancy weight computed against the question. Then, we encode the question and weighted passage by using bi-directional LSTMs. For each point in the passage, our model matches the context of this point against the encoded question from multiple perspectives and produces a matching vector. Given those matched vectors, we employ another bi-directional LSTM to aggregate all the information and predict the beginning and ending points. Experimental result on the test set of SQuAD shows that our model achieves a competitive result on the leaderboard.</p><p>To address the weakness of the previous MC datasets, <ref type="bibr" target="#b9">Rajpurkar et al. (2016)</ref> developed the Stanford Question Answering dataset (SQuAD). Comparing with other datasets, SQuAD is more</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Machine Comprehension (MC) is a compelling yet challenging task in both natural language processing and artificial intelligent research. Its task is to enable machine to understand a given passage and then answer questions related to the passage.</p><p>In recent years, several benchmark datasets have been developed to measure and accelerate the progress of MC technologies. RCTest <ref type="bibr" target="#b10">(Richardson et al., 2013)</ref> is one of the representative datasets. It consists of 500 fictional stories and 4 multiple choice questions per story (2,000 questions in total). A variety of MC methods were proposed based on this dataset. However, the limited size of this dataset prevents researchers from building end-to-end deep neural network models, and the state-of-the-art performances are still dominated by the methods highly relying on hand-crafted features <ref type="bibr" target="#b11">(Sachan et al., 2015;</ref><ref type="bibr">Wang and McAllester, 2015)</ref> or employing additional knowledge <ref type="bibr" target="#b17">(Wang et al., 2016a)</ref>. To deal with the scarcity of large scale supervised data, <ref type="bibr" target="#b1">Hermann et al. (2015)</ref> proposed to create millions of Cloze style MC examples automatically from news articles on the CNN and Daily Mail websites. They observed that each news article has a number of bullet points, which summarise aspects of the information in the article. Therefore, they constructed a corpus of (passage, question, answer) triples by replacing one entity in these bullet points at a time with a placeholder. Then, the MC task is converted into filling the placeholder in the question with an entity within the corresponding passage. Based on this large-scale corpus, several end-to-end deep neural network models are proposed successfully <ref type="bibr" target="#b1">(Hermann et al., 2015;</ref><ref type="bibr" target="#b3">Kadlec et al., 2016;</ref><ref type="bibr" target="#b13">Shen et al.,</ref> realistic and challenging for several reasons: (1) it is almost two orders of magnitude larger than previous manually labeled datasets; (2) all the questions are human-written, instead of the automatically generated Cloze style questions; (3) the answer can be an arbitrary span within the passage, rather than a limited set of multiple choices or entities; (4) different forms of reasoning is required for answering these questions.</p><p>In this work, we focus on the SQuAD dataset and propose an end-to-end deep neural network model for machine comprehension. Our basic assumption is that a span in a passage is more likely to be the correct answer if the context of this span is very similar to the question. Based on this assumption, we design a Multi-Perspective Context Matching (MPCM) model to identify the answer span by matching the context of each point in the passage with the question from multiple perspectives. Instead of enumerating all the possible spans explicitly and ranking them, our model identifies the answer span by predicting the beginning and ending points individually with globally normalized probability distributions across the whole passage. Ablation studies show that all components in our MPCM model are crucial. Experimental result on the test set of SQuAD shows that our model achieves a competitive result on the leaderboard.</p><p>In following parts, we start with a brief definition of the MC task (Section 2), followed by the details of our MPCM model (Section 3). Then we evaluate our model on the SQuAD dataset (Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Definition</head><p>Generally, a MC instance involves a question, a passage containing the answer, and the correct answer span within the passage. To do well on this task, a MC model need to comprehend the question, reason among the passage, and then identify the answer span. <ref type="table" target="#tab_1">Table 1</ref> demonstrates three examples from SQuAD. Formally, we can represent the SQuAD dataset as a set of tuples (Q, P, A), where Q = (q 1 , ..., q i , ..., q M ) is the question with a length M , P = (p 1 , ..., p j , ..., p N ) is the passage with a length N , and A = (a b , a e ) is the answer span, a b and a e are the beginning and ending points and 1 ≤ a b ≤ a e ≤ N . The MC task can be represented as estimating the conditional probability Pr (A|Q, P ) based on the training set, and  predicting answers for testing instances by</p><formula xml:id="formula_0">A * = arg max A∈A(P ) Pr(A|Q, P ),<label>(1)</label></formula><p>where A(P ) is a set of answer candidates from P . As the size of A(P ) is in the order of O(N 2 ), we make a simple independent assumption of predicting the beginning and endding points, and simplify the model as</p><formula xml:id="formula_1">A * = arg max 1≤a b ≤ae≤N Pr(a b |Q, P ) Pr(a e |Q, P ),<label>(2)</label></formula><p>where Pr(a b |Q, P ) (or Pr(a e |Q, P )) is the probability of the a b -th (or a e -th) position (point) of P to be the beginning (or ending) point of the answer span.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Multi-Perspective Context Matching Model</head><p>In this section, we propose a Multi-Perspective Context Matching (MPCM) model to estimate probability distributions Pr(a b |Q, P ) and Pr(a e |Q, P ). <ref type="figure" target="#fig_0">Figure 1</ref> shows the architecture of our MPCM model. The predictions of Pr(a b |Q, P ) and Pr(a e |Q, P ) only differentiate at the last prediction layer. And all other layers below the prediction layer are shared.</p><p>Given a pair of question Q and passage P , the MPCM model estimates probability distributions through the following six layers.</p><p>Word Representation Layer. The goal of this layer is to represent each word in the question and passage with a d-dimensional vector. We construct the d-dimensional vector with two components: word embeddings and character-composed</p><formula xml:id="formula_2">" # $ % …... …... " # ' ( …... …...</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word Representation Layer</head><p>Filter Layer embeddings. The word embedding is a fixed vector for each individual word, which is pre-trained with GloVe <ref type="bibr" target="#b8">(Pennington et al., 2014)</ref> or word2vec <ref type="bibr" target="#b7">(Mikolov et al., 2013)</ref>. The character-composed embedding is calculated by feeding each character (also represented as a vector) within a word into a Long Short-Term Memory Network (LSTM) <ref type="bibr" target="#b2">(Hochreiter and Schmidhuber, 1997)</ref>. The output of this layer is word vector sequences for question Q : [q 1 , ..., q M ], and passage P : [p 1 , ..., p N ]. Filter Layer. In most cases, only a small piece of the passage is needed to answer the question (see examples in <ref type="table" target="#tab_1">Table 1</ref>). Therefore, we define the filter layer to filter out redundant information from the passage. First, we calculate a relevancy degree r j for each word p j in passage P . Inspired from <ref type="bibr" target="#b18">Wang et al. (2016b)</ref>, we compute the relevancy degree r i,j between each word pair q i ∈ Q and p j ∈ P by calculating the cosine similarity</p><formula xml:id="formula_3">'$ " Multi-Perspective Context Matching Layer ... ... ... ... ... ... ... ... …... …... …... …... …... …... …... …...</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context Representation Layer</head><formula xml:id="formula_4">r i,j = q T i p j q i · p j</formula><p>, and get the relevancy degree by r j = max i∈M r i,j . Second, we filter each word vector by p j = r j · p j , and pass p j to the next layer. The main idea is that if a word in the passage is more relevant to the question, more information of the word should be considered in the subsequent steps.</p><p>Context Representation Layer. The purpose of this layer is to incorporate contextual informa-tion into the representation of each time step in the passage and the question. We utilize a bidirectional LSTM (BiLSTM) to encode contextual embeddings for each question word.</p><formula xml:id="formula_5">− → h q i = − −−− → LSTM( − → h q i−1 , q i ) i = 1, ..., M ← − h q i = ← −−− − LSTM( ← − h q i+1 , q i ) i = M, ..., 1<label>(3)</label></formula><p>Meanwhile, we apply the same BiLSTM to the passage:</p><formula xml:id="formula_6">− → h p j = − −−− → LSTM( − → h p j−1 , p j ) j = 1, ..., N ← − h p j = ← −−− − LSTM( ← − h p j+1 , p j ) j = N, ..., 1<label>(4)</label></formula><p>Multi-Perspective Context Matching Layer. This is the core layer within our MPCM model. The goal of this layer is to compare each contextual embedding of the passage with the question with multi-perspectives. We define those multiperspective matching functions in following two directions:</p><p>First, dimensional weighted matchings with</p><formula xml:id="formula_7">m = f m (v 1 , v 2 ; W )<label>(5)</label></formula><p>where v 1 and v 2 are two d-dimensional vectors, W ∈ l×d is a trainable parameter, l is the number of perspectives, and the returned value m is a l-dimensional vector m = [m 1 , ..., m k , ..., m l ]. Each element m k ∈ m is a matching value from the k-th perspective, and it is calculated by the cosine similarity between two weighted vectors</p><formula xml:id="formula_8">m k = cosine(W k • v 1 , W k • v 2 ) (6)</formula><p>where • is the elementwise multiplication, and W k is the k-th row of W , which controls the k-th perspective and assigns different weights to different dimensions of the d-dimensional space. Second, on the orthogonal direction of f m , we define three matching strategies to compare each contextual embedding of the passage with the question:</p><p>(1) Full-Matching: each forward (or backward) contextual embedding of the passage is compared with the forward (or backward) representation of the entire question.</p><formula xml:id="formula_9">− → m f ull j = f m ( − → h p j , − → h q M ; W 1 ) ← − m f ull j = f m ( ← − h p j , ← − h q 1 ; W 2 )<label>(7)</label></formula><p>(2) Maxpooling-Matching: each forward (or backward) contextual embedding of the passage is compared with every forward (or backward) contextual embeddings of the question, and only the maximum value is retained.</p><formula xml:id="formula_10">− → m max j = max i∈(1...M ) f m ( − → h p j , − → h q i ; W 3 ) ← − m max j = max i∈(1...M ) f m ( ← − h p j , ← − h q i ; W 4 )<label>(8)</label></formula><p>(3) Meanpooling-Matching: This is similar to the Maxpooling-Matching, but we replace the max operation with the mean operation.</p><formula xml:id="formula_11">− → m mean j = 1 M M i=1 f m ( − → h p j , − → h q i ; W 5 ) ← − m mean j = 1 M M i=1 f m ( ← − h p j , ← − h q i ; W 6 )<label>(9)</label></formula><p>Thus, the matching vector for each position of the passage is the concatenation of all the matching vectors</p><formula xml:id="formula_12">m j = [ − → m f ull j ; ← − m f ull j ; − → m max j ; ← − m max j ; − → m mean j ; ← − m mean j ].</formula><p>For the examples in <ref type="table" target="#tab_1">Table 1</ref>, the forward Full-Matching vector is extremely useful for question #1, because we only need to match the left context to the entire question. Similarly, the backward Full-Matching vector is very helpful for question #2. However, for question #3, we have to utilize the Maxpooling-Matching and Meanpooling-Matching strategies, because both the left and right contexts need to partially match the question.</p><p>Aggregation Layer. This layer is employed to aggregate the matching vectors, so that each time step of the passages can interactive with its surrounding positions. We incorporate the matching vectors with a BiLSTM, and generate the aggregation vector for each time step.</p><p>Prediction Layer. We predict the probability distributions of Pr(a b |Q, P ) and Pr(a e |Q, P ) separately with two different feed-forward neural networks (shown in <ref type="figure" target="#fig_0">Figure 1</ref>, solid-lines for Pr(a b |Q, P ), dotted-lines for Pr(a e |Q, P )). We feed the aggregation vector of each time step into the feed-forward neural network individually, calculate a value for each time step, then normalize the values across the entire passage with sof tmax operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Settings</head><p>We evaluate our model with the SQuAD dataset. This dataset includes 87,599 training instances, 10,570 validation instances, and a large hidden test set 1 . We process the corpus with the tokenizer from Stanford CorNLP . To evaluate the experimental results, we employ two metrics: Exact Match (EM) and F1 score <ref type="bibr" target="#b9">(Rajpurkar et al., 2016)</ref>.</p><p>To initialize the word embeddings in the word representation layer, we use the 300-dimensional GloVe word vectors pre-trained from the 840B Common Crawl corpus <ref type="bibr" target="#b8">(Pennington et al., 2014)</ref>. For the out-of-vocabulary (OOV) words, we initialize the word embeddings randomly. We set the hidden size as 100 for all the LSTM layers, and set the number of perspectives l of our multiperspective matching function (Equation (5)) as 50. We apply dropout to every layers in <ref type="figure" target="#fig_0">Figure  1</ref>, and set the dropout ratio as 0.2. To train the model, we minimize the cross entropy of the be-  ginning and end points, and use the ADAM optimizer <ref type="bibr" target="#b4">(Kingma and Ba, 2014)</ref> to update parameters. We set the learning rate as 0.0001. For decoding, we enforce the end point is equal or greater than the beginning point. <ref type="table" target="#tab_3">Table 2</ref> summarizes the performance of our models and other competing models. Our single MPCM model achieves the EM of 65.5, and the F1 score of 75.1. We also build an ensemble MPCM model by simply averaging the probability distributions of 5 models, where all the models have the same architecture but initialized with different seeds. With the help of the simple ensemble strategy, our MPCM model improves about 3% in term of EM, and 2% in term of F1 score. Comparing the performance of other models, our MPCM models achieve competitive results in both single and ensemble scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results on the Test Set</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Influence of the Multi-Perspective Matching Function</head><p>In this sub-section, we study the influence of our multi-perspective matching function in Eq.(5). We built a baseline model vanilla-cosine by replacing Eq.(5) with the vanilla cosine similarity function. We also varied the number of perspectives l among {1, 10, 30, 50}, and kept the other options un-   changed. <ref type="table" target="#tab_5">Table 3</ref> shows the performance on the dev set. We can see that, even if we only utilize one perspective, our multi-perspective matching function works better than the vanilla-cosine baseline. When increasing the number of perspectives, the performance improves significantly. Therefore, our multi-perspective matching function is really effective for matching vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Layer Ablation</head><p>In this sub-section, we evaluate the effectiveness of various layers in our MPCM model. We built several layer ablation models by removing one layer at a time. For the Multi-Perspective Context Matching Layer, we cannot remove it entirely. Instead, we built three models (w/o Full-Matching, w/o Maxpooling-Matching, w/o Meanpooling-Matching) by eliminating each matching strategy individually. <ref type="table" target="#tab_6">Table 4</ref> shows the performance of all ablation models and our full MPCM model on the dev set. We can see that removing any components from the MPCM model decreases the performance significantly. Among all the layers, the Aggregation Layer is the most crucial layer. Among all the matching strategies, Maxpooling-Matching has the biggest effect. who <ref type="formula" target="#formula_0">(1059)</ref> what is <ref type="formula" target="#formula_11">(966)</ref> when <ref type="formula" target="#formula_11">(696)</ref> how many <ref type="formula" target="#formula_5">(543)</ref> what was <ref type="formula" target="#formula_1">(542)</ref> which <ref type="formula" target="#formula_6">(454)</ref> where <ref type="formula" target="#formula_5">(433)</ref> what did <ref type="formula" target="#formula_5">(375)</ref> in what <ref type="formula" target="#formula_1">(243)</ref> what does <ref type="formula" target="#formula_1">(220)</ref> what type <ref type="formula" target="#formula_0">(193)</ref> what are <ref type="formula" target="#formula_0">(189)</ref> why <ref type="formula" target="#formula_0">(151)</ref> how much <ref type="formula" target="#formula_0">(138)</ref> what do <ref type="formula" target="#formula_0">(122)</ref> what kind <ref type="formula" target="#formula_10">(88)</ref> how did <ref type="formula" target="#formula_9">(79)</ref> what year <ref type="formula" target="#formula_0">(71)</ref> how long <ref type="formula" target="#formula_7">(65)</ref> in which <ref type="formula">(60)</ref> what were <ref type="formula" target="#formula_1">(52)</ref> what has <ref type="formula" target="#formula_6">(44)</ref> what can <ref type="formula" target="#formula_6">(44)</ref> F1 EM <ref type="figure">Figure 2</ref>: Performance for different answer length. how did <ref type="formula" target="#formula_9">(79)</ref> what has <ref type="formula" target="#formula_6">(44)</ref> what were <ref type="formula" target="#formula_1">(52)</ref> why <ref type="formula" target="#formula_0">(151)</ref> what did <ref type="formula" target="#formula_5">(375)</ref> what do <ref type="formula" target="#formula_0">(122)</ref> what can <ref type="formula" target="#formula_6">(44)</ref> what was <ref type="formula" target="#formula_1">(542)</ref> what does <ref type="formula" target="#formula_1">(220)</ref> where <ref type="formula" target="#formula_5">(433)</ref> what type <ref type="formula" target="#formula_0">(193)</ref> which <ref type="formula" target="#formula_6">(454)</ref> what is <ref type="formula" target="#formula_11">(966)</ref> what are <ref type="formula" target="#formula_0">(189)</ref> how much <ref type="formula" target="#formula_0">(138)</ref> who <ref type="formula" target="#formula_0">(1059)</ref> how many <ref type="formula" target="#formula_5">(543)</ref> what kind <ref type="formula" target="#formula_10">(88)</ref> how long <ref type="formula" target="#formula_7">(65)</ref> when <ref type="formula" target="#formula_11">(696)</ref> in what <ref type="formula" target="#formula_1">(243)</ref> in which <ref type="formula">(60)</ref> what year (71) F1 EM <ref type="figure">Figure 3</ref>: Performance for different question types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Result Analysis</head><p>To better understand the behavior of our MPCM model, we conduct some analysis of the result on the dev set. <ref type="figure">Figure 2</ref> shows the performance changes based on the answer length. We can see that the performance drops when the answer length increases, and the EM drops faster than the F1 score. The phenomenon reveals that longer answers are harder to find, and it is easier to find the approximate answer region than identify the precise boundaries. <ref type="figure">Figure 3</ref> shows the performances of different types of questions. The numbers inside the brackets are the frequency of that question type on the dev set. We can see that the performances for " <ref type="bibr">when", "what year", "in what", and "in which"</ref> questions are much higher than the others. The possible reason is that the temporal expressions are easier to detect for "when" and "what year" questions, and there is an explicit boundary word "in" for "in what" and "in which" questions. Our model works poorly for the "how did" question. Because "how did" questions usually require longer answers, and the answers could be any type of phrases. <ref type="figure">Figure 4</ref> visualizes the probability distributions produced by our MPCM model for an example question from the dev set, where the upper subfigure is the probabilities for the beginning point and the lower one is the probabilities for the ending point. We can see that our model assigns most mass of the probability to the correct beginning and ending points.</p><p>To conduct the error analysis, we randomly select 50 incorrect questions from the dev set. We found that predictions for 16% questions are acceptable (even though they are not in the correct answer list) and 22% overlap with the correct answer. 14% of the questions require reasoning across multiple sentences, and most of the remaining questions require external knowledge or complex reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Many deep learning based models were proposed since the release of the SQuAD dataset. Based on the method of identifying the answer spans, most of the models can be roughly categorized into the following two classes:</p><p>Chunking and Ranking. In this kind of methods, a list of candidate chunks (answers) are extracted firstly. Then, models are trained to rank the correct chunk to the top of the list. <ref type="bibr" target="#b9">Rajpurkar et al. (2016)</ref> proposed to collect the candidate chunks from all constituents of parse trees, and designed some hand-crafted features to rank the chunks with logistic regression model ("Logistic Regression" in <ref type="table" target="#tab_3">Table 2</ref>). However, over 20% of the questions do not have any correct answers within the candidate list. To increase the recall, <ref type="bibr" target="#b20">Yu et al. (2016)</ref> extracted candidate chunks based on some part-of-speech patterns, which made over 90% of the questions answerable. Then, they employed an attention-based RNN model to rank all the chunks ("Dynamic Chunk Reader" in <ref type="table" target="#tab_3">Table 2</ref>). <ref type="bibr" target="#b5">Lee et al. (2016)</ref>   <ref type="figure">Figure 4</ref>: Probability distributions for the question "What did Luther consider Christ 's life ?", where the correct answer is "an illustration of the Ten Commandments", the upper sub-figure is for the beginning point and the lower one is for the ending point. grams) within the passage, learned a fixed length representations for each chunk with a multi-layer BiLSTM model, and scored each chunk based on the fixed length representations.</p><p>Boundary Identification. Instead of extracting a list of candidate answers, this kind of methods learns to identify the answer span directly. Generally, some kinds of question-aware representations are learnt for each time step of the passage, then the beginning and ending points are predict based on the representations. <ref type="bibr" target="#b15">Wang and Jiang (2016)</ref> proposed a match-LSTM model to match the passage with the question, then the Pointer Network <ref type="bibr" target="#b14">(Vinyals et al., 2015)</ref> was utilized to select a list of positions from the passage as the final answer ("Match-LSTM (Sequence)" in Table 2). However, the returned positions are not guaranteed to be consecutive. They further modified the Pointer Network to only predict the beginning or ending points ("Match-LSTM (Boundary)" and "Match-LSTM with Bi-Ptr" in <ref type="table" target="#tab_3">Table  2</ref>). <ref type="bibr" target="#b19">Xiong et al. (2016)</ref> introduced the Dynamic Coattention Network ("Dynamic Coattention" in <ref type="table" target="#tab_3">Table 2</ref>). Their model first captured the interactions between the question and the passage with a co-attentive encoder, then a dynamic pointing decoder was used for predicting the beginning and ending points. <ref type="bibr" target="#b12">Seo et al. (2016)</ref> proposed a similar model with <ref type="bibr" target="#b19">Xiong et al. (2016)</ref>. This model employed a bi-directional attention flow mechanism to achieve a question-aware context representations for the passage, then the beginning and ending points were predict based on the representations. Our model also belongs to this category. However, different from all the previous models, our model generates the question-aware representations by explicitly matching contextual embeddings of the passage with the question from multiple perspectives, and no lexical or word vector information is passed to the boundary identification layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we proposed the Multi-Perspective Context Matching (MPCM) model for machine comprehension task. Our model identifies the answer span by matching each time-step of the passage with the question from multiple perspectives, and predicts the beginning and ending points based on globally normalizing probability distributions. Ablation studies show that all aspects of matching inside the MPCM model are crucial. Experimental result on the test set of SQuAD shows that our model achieves a competitive result on the leaderboard.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Architecture for Multi-Perspective Context Matching Model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Question #1 :</head><label>#1</label><figDesc>Who is Welsh medium education available to ? Passage: ...... Welsh medium education is available to all age groups through nurseries , schools , colleges ...... What type of musical instruments did the Yuan bring to China ? Passage: Western musical instruments were introduced to enrich Chinese performing arts ...... Passage: ......, Pulitzer Prize winning novelist Philip Roth , ...... and American writer and satirist Kurt Vonnegut are notable alumni .</figDesc><table><row><cell>Question #2: Question #3: What is the name of the Pulitzer Prize novelist</cell></row><row><cell>who was also a university alumni?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>: Examples from SQuAD, where only the</cell></row><row><cell>relevant content of the original passage is retained,</cell></row><row><cell>and the blue underlined spans are the correct an-</cell></row><row><cell>swers.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Results on the SQuAD test set. All the results here reflect the SQuAD leaderboard as of Dec. 9, 2016.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Influence of the multi-perspective matching function in Eq.(5) .</figDesc><table><row><cell>Models</cell><cell>EM</cell><cell>F1</cell></row><row><cell>w/o character</cell><cell cols="2">62.8 73.0</cell></row><row><cell>w/o Filter Layer</cell><cell cols="2">64.0 74.0</cell></row><row><cell>w/o Full-Matching</cell><cell cols="2">64.3 74.8</cell></row><row><cell>w/o Maxpooling-Matching</cell><cell cols="2">63.1 73.7</cell></row><row><cell cols="3">w/o Meanpooling-Matching 64.1 74.9</cell></row><row><cell>w/o Aggregation Layer</cell><cell cols="2">61.0 72.3</cell></row><row><cell>MPCM (single)</cell><cell cols="2">66.1 75.8</cell></row><row><cell>MPCM (ensemble)</cell><cell cols="2">69.4 78.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Layer ablation on the dev set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>enumerated all possible chunks (up to 30-</figDesc><table><row><cell>0.6 0.5 0.4 0.3 0.2 0.1 0.0 0.0 0.2 0.4 0.6 0.8 1.0</cell><cell>an an</cell><cell>example example</cell><cell>, ,</cell><cell>is is</cell><cell>nothing nothing</cell><cell>more more</cell><cell>than than</cell><cell>an an</cell><cell>illustration illustration</cell><cell>of of</cell><cell>the the</cell><cell>Ten Commandments , Ten Commandments ,</cell><cell>which which</cell><cell>a a</cell><cell>Christian should Christian should</cell><cell>follow follow</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2016">). However, did a careful hand-analysis of this dataset, and concluded that this dataset is not difficult enough to evaluate the ability of current MC techniques.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">To evaluate on the hidden test set, we have to submit the executable system to the leaderboard (https://rajpurkar.github.io/SQuAD-explorer/)</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A thorough examination of the cnn/daily mail reading comprehension task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02858</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bajgar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01547</idno>
		<title level="m">Text understanding with the attention sum reader network</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning recurrent span representations for extractive question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01436</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mc-Closky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL) System Demonstrations</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mctest: A challenge dataset for the open-domain machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Renshaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning answerentailing structures for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrinmaya</forename><surname>Sachan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinava</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="239" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01603</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.05284</idno>
		<title level="m">Reasonet: Learning to stop reading in machine comprehension</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.07905</idno>
		<title level="m">Machine comprehension using match-lstm and answer pointer</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Machine comprehension with syntax, frames, and semantics</title>
		<editor>Hai Wang and Mohit Bansal Kevin Gimpel David McAllester</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">700</biblScope>
			<pubPlace>Short Papers</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Employing external rich knowledge for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangmin</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sentence similarity learning by lexical decomposition and composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><surname>Ittycheriah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceddings of Coling</title>
		<meeting>eddings of Coling</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Dynamic coattention networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01604</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">End-to-end answer chunk extraction and ranking for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazi</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09996</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
