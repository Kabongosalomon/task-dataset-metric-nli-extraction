<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Time Masking: Leveraging Temporal Information in Spoken Dialogue Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rylan</forename><surname>Conway</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lambert</forename><forename type="middle">Mathias</forename><surname>Amazon</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexa</forename><surname>Ai</surname></persName>
						</author>
						<title level="a" type="main">Time Masking: Leveraging Temporal Information in Spoken Dialogue Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In a spoken dialogue system, dialogue state tracker (DST) components track the state of the conversation by updating a distribution of values associated with each of the slots being tracked for the current user turn, using the interactions until then. Much of the previous work has relied on modeling the natural order of the conversation, using distance based offsets as an approximation of time. In this work, we hypothesize that leveraging the wall-clock temporal difference between turns is crucial for finer-grained control of dialogue scenarios. We develop a novel approach that applies a time mask, based on the wall-clock time difference, to the associated slot embeddings and empirically demonstrate that our proposed approach outperforms existing approaches that leverage distance offsets, on both an internal benchmark dataset as well as DSTC2.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Modern spoken dialogue systems -such as Intelligent Personal Digital Assistants (IPDAs) like Google Assistant, Siri, and Alexa -provide users a natural language interface to help complete tasks such as reserving restaurants, checking the weather, playing music etc. Spoken language understanding (SLU) is a central component in such dialogue systems, and is responsible for parsing the natural language text to semantic frames. In task-oriented spoken dialogue systems, a key challenge is tracking entities the user introduced in previous dialogue turns. For example, if a user request for what's the weather in arlington is followed by how about tomorrow, the dialogue system has to keep track of the entity arlington being referenced. Typically, this is formulated as a dialogue state tracking (DST) task <ref type="bibr" target="#b4">(Henderson et al., 2014b;</ref><ref type="bibr" target="#b7">Mrkšić et al., 2016)</ref>.</p><p>Previous approaches to dialogue state tracking have mostly focused on dialogue representa-tions <ref type="bibr" target="#b7">(Mrkšić et al., 2016)</ref>, dealing with noisy input <ref type="bibr" target="#b2">(Henderson et al., 2012;</ref><ref type="bibr" target="#b6">Mesnil et al., 2015)</ref>, or tracking slots from multiple domains <ref type="bibr" target="#b4">(Henderson et al., 2014b;</ref><ref type="bibr" target="#b9">Rastogi et al., 2017;</ref><ref type="bibr" target="#b8">Naik et al., 2018)</ref>. In this paper, we focus on temporal information associated with each dialogue turn. Although the dialogue representations -typically encoded using LSTMs -are able to implicitly capture the temporal order in the sequence of dialogue turns, we hypothesize that explicitly and accurately encoding temporal information is essential for resolving ambiguity in dialogue state tracking. Recently, <ref type="bibr" target="#b8">(Naik et al., 2018)</ref> presented work that models the slot distance offset from the current turn using a one-hot representation input to the DST module. Alternatively, <ref type="bibr" target="#b11">(Su et al., 2018)</ref> leverage the distance offset in an attention mechanism. We posit that the notion of time based on distance offset relative to the current turn is too coarse-grained and often insufficient for resolving ambiguities associated with more complex multi-turn dialogues. For example, in a dialogue "how far is issaquah?" followed by "what is the weather like?" we could have two possible interpretations -a follow-up utterance issued within 10 seconds would indicate that the user is referring to the city slot of "Issaquah" from the previous turn, whereas, if the follow-up utterance is more than 30 seconds apart there is a good chance that the user was just inquiring about the weather in their current location. In this case, a dialogue system that only encodes the distance offset will be unable to correctly disambiguate the aforementioned situation. Based on this intuition, we develop a novel approach for incorporating temporal information in dialogue state tracking by using a time mask over the slots.</p><p>To summarize, we introduce the notion of a time mask to incorporate temporal information into the embedding for slots. In contrast to previous ap-proaches using distance offsets, we propose leveraging the wall-clock time difference between the current turn and the previous turns in the dialogue to explicitly model temporal information. Furthermore, we demonstrate how domain and intent information can be mixed in with the temporal information in this framework to improve DST accuracy. We demonstrate empirically that our proposed approach improves over the baseline that only encodes distance offsets as temporal information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Slot Carryover Task Description</head><p>In this paper, we build on the approach in <ref type="bibr" target="#b8">(Naik et al., 2018)</ref>. For completeness, we define the carryover task formulation here, but refer readers to the original work for architecture details. A dialogue turn at time t is defined as the tu-</p><formula xml:id="formula_0">ple {a t , S t , w t }, where w t ∈ W is a sequence of words {w it } Nt i=1</formula><p>; a t ∈ A is the dialogue act; and S t is a set of slots, where each slot s is a key-value pair s = {k, v}, with k ∈ K being the slot name (or slot key), and v ∈ V being the slot value. A user turn is represented by</p><formula xml:id="formula_1">u t = {a u t , S u t , w u t } and a system turn is repre- sented by v t = {a v t , S v t , w v t }.</formula><p>Given a sequence of D user turns {u t−D , . . . , u t−2 , u t−1 }; and their associated system turns {v t−D , . . . , v t−2 , v t−1 } 1 ; and the current user turn u t , we construct a candidate set of slots from the context as</p><formula xml:id="formula_2">C(S) = t j=t−D i∈u,v S i j .<label>(1)</label></formula><p>For a candidate slot s ∈ C(S), for the dialogue turn at time t, the probability to carryover the slot is defined as</p><formula xml:id="formula_3">P (+|s, d(s), u t , u t−1 t−D , v t−1 t−D ),<label>(2)</label></formula><p>where d(s) ∈ [0, D] is an integer value describing the offset of the candidate slot from the current turn u t . The final carryover decision is determined by comparing the carryover probability to a tunable decision threshold An encoder-decoder model is used to evaluate each slot candidate, as shown in <ref type="figure">Figure 1</ref>. The current turn, past user turns, and past system turns are all encoded using an LSTM layer with atten-tion. Each slot (key and value) and intent are also encoded by averaging the word embeddings contained in each. Finally, the slot distance is encoded by counting the number of turns back that the slot appeared in (this would equal zero for slots from the current turn) and one-hot encoding that value. This is shown by the "Recenty One-Hot" input in the diagram. The final encoded slot candidate is passed to the decoder which produces a final carryover probability that determines whether or not the slot should be carried over to the current turn 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Simple Time Mask (STM)</head><p>Inspired by <ref type="bibr" target="#b5">(Li et al., 2018)</ref>, we introduce the concept of masked embeddings so that irrelevant dimensions are suppressed in the embedding of the slots. We start by constructing a time embedding based on the temporal distance, d ∆t , of each candidate slot 3 . This is shown in <ref type="figure">Fig. 1</ref> as the bottom input, in the red box. The time embedding is given by</p><formula xml:id="formula_4">d t = φ(W t d ∆t + b t ),<label>(3)</label></formula><p>where d t is a nonlinear transformation implemented as a single layer feedforward neural network with weight matrix W t ∈ R Nt×1 and N t is dimensionality of the time embedding vector. The time mask, m t , is computed by passing the time embedding, d t , through another feedforward neural network</p><formula xml:id="formula_5">m t = σ(W dt d t + b dt ),<label>(4)</label></formula><p>where W dt ∈ R Ns×Nt and N s is the dimensionality of the candidate slot embedding, h s . Finally, we apply the time mask to the encoded slot embedding:</p><formula xml:id="formula_6">h s = h s m dt ,<label>(5)</label></formula><p>The updated candidate slot embedding, h s , is now passed to the decoder in the exact same way as in the baseline slot carryover model as described in <ref type="bibr" target="#b8">(Naik et al., 2018)</ref>. Temporal dialogue behavior can vary by domain. <ref type="figure" target="#fig_0">Figure 2</ref> shows how much the distribution of d ∆t can differ between three different domains in an internal IPDA dataset (described in more detail in 3.1). Therefore, we consider two exten-  <ref type="figure">Figure 1</ref>: Slot carryover architecture from <ref type="bibr" target="#b8">(Naik et al., 2018)</ref> augmented with a temporal component using domainspecific time masking as described in Section 2.2.2. sions of the time masking approach that take into account the multi-domain nature of IPDAs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Intent Specific Time Mask (ITM)</head><p>We leverage the dialogue act or intent associated with the current turn in the time mask model. In this formulation the time embedding is now given by</p><formula xml:id="formula_7">d t = φ(W t d ∆t,a + b t ),<label>(6)</label></formula><p>where d ∆t,a = d ∆t ⊕ h a is just the temporal distance concatenated with the existing intent embedding h a and now W t ∈ R Nt×(Na+1) , where N a is the number of dimensions used in the intent embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Domain Specific Time Mask (DTM)</head><p>We also try more coarse-grained, domain-level information as input to the time embedding. Here we use a one-hot encoded representation of the domains, which gives us:</p><formula xml:id="formula_8">d t = φ(W t d ∆t,D + b t ),<label>(7)</label></formula><p>where d ∆t,D = d ∆t ⊕ 1 D is the concatenation of the temporal distance with the one-hot-encoded domain, 1 D , and W t ∈ R Nt×(N D +1) , where N D is the number of dimensions used in the one-hotencoded Domain representation. This architecture is shown in in <ref type="figure">Figure 1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Time-Decay Attention (TDA)</head><p>For comparison, we re-implemented the timedecay attention (TDA) model introduced in <ref type="bibr" target="#b11">(Su et al., 2018)</ref>. However, the original work does not actually use time as a feature input but rather the ordinal distance (equivalent to slot distance in our formulation) of each dialogue turn from the current utterance. To compare with our methods we use the actual temporal difference between dialogue turns in our implementation of the TDA model. The parameters are learned in the end-toend training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>We present the results on 2 datasets. The IPDA dataset, in <ref type="table">Table 1</ref>, is an internal benchmark dataset collected from an IPDA for the en-US locale based on real usage. It consists of interactions over 7 domains -Music, Weather, LocalSearch, SmartHome, Video, MovieShowTimes, and Question Answering. The data is transformed into individual candidate slots that are presented to the model, which determines whether or not they are relevant for the given turn. For benchmarking against a public corpora, we also measure performance on DSTC2 <ref type="bibr" target="#b3">(Henderson et al., 2014a)</ref> dataset. We post-process the dataset similar to the internal dataset and only consider the top ASR and SLU hypothesis in addition to the system turn, dialogue acts and the associated slots.  <ref type="figure">Figure 3</ref> shows the distribution of time between turns for both datasets. If a slot candidate came from a context turn that was spoken 20 seconds before the current turn then d ∆t = 20. Based on human judged ground-truth, the slots that should be carried over to the current turn are shown in orange and the slots that should not are shown in blue. One clear difference between the two distributions in the IPDA dataset is the long tail of the non-carryover distribution, indicating carryovers are more likely from a recent turn. The domain specific distributions further indicate that leveraging dialogue time could be useful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results on IPDA Dataset</head><p>From <ref type="table" target="#tab_3">Table 2</ref>, we can see that the TDA models offer a slight improvement over the baseline model. Both models incorporate slot distance offset but the attention mechanism provides an additional boost. The time mask models show additional gains demonstrating that leveraging dialogue time from each turn is important. Moreover, the time information provides complementary information over the distance offset based measure, as shown by the improvements of the time masking models over the baseline model. The DTM model performs the best overall in terms of F1, which suggests that adding domain information into the time mask provides additional disambiguation power. Interestingly, we see that the ITM model does not improve much over the STM model, possibly because the intent embeddings do not necessarily distinguish between temporal behavior, and are already being leveraged by the slot carryover model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Investigating longer temporal distance</head><p>Here, we investigate the ability of the models to maintain higher accuracy over longer time windows in the dialogue context. The overall F1 scores for each model are binned by d ∆t for each candidate slot. The results are shown in <ref type="table" target="#tab_3">Table 2</ref>. The domain specific time mask model performs the best in each d ∆t bin. The effect of adding dialogue time information significantly improves performance in the largest d ∆t range. This is likely due to the model learning that older slots are less relevant to the current turn, which is impossible for the baseline model to do. Additionally, we can see that the TDA model performs comparably to the STM model in the range 0 &lt; d ∆t ≤ 30 but in the highest bin (30 &lt; d ∆t ≤ 60) we see that it falls well short of all of the time-masked models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results on DSTC2 Dataset</head><p>Since there is only one domain in DSTC2, we chose to only implement the STM model. From the last column in <ref type="table" target="#tab_3">Table 2</ref>, we can see that the STM model produces the best result. The TDA model, contrary to previously reported results on DSTC4, does not perform as well. Our hypothesis is that the temporal distribution across turns is not monotonically decaying, which is an assumption made in their approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Previous work on leveraging temporal information for dialogue state tracking has focused mostly on using distance offsets. <ref type="bibr" target="#b0">(Chen et al., 2017)</ref> presented a time-aware attention network to leverage both contextual and ordinal distance information (i.e. the number of turns back from the current turn) and saw significant improvement. Subsequently, <ref type="bibr" target="#b11">(Su et al., 2018)</ref> improved upon this by (a) IPDA dataset.</p><p>(b) DSTC2 dataset. <ref type="figure">Figure 3</ref>: Distribution of temporal distance, d ∆t , for all candidate slots. The "Carryover Slots", shown in orange, represent slot candidates found in the context that should be carried over to the current turn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Overall d∆t = (0, 15] d∆t = (15, 30] d∆t = (30, 60] Overall DSTC2 F1 Baseline <ref type="bibr" target="#b8">(Naik et al., 2018)</ref> 87  designing a more flexible data-driven time attention mechanism that applied continuously decaying weights to past utterances before being fed into a contextual encoder. The attention weight was determined based on the distance offset relative to the current turn. However, distance offset is unable to capture complex dialogue scenarios, and our work improves upon this by modeling the actual wall-clock time difference between the current turn and the contextual turns. This is particularly important in a multi-domain setting where a few second pause between consecutive user turns can be interpreted very differently depending on the dialogue scenario, and our experiments support our hypothesis. Embedding masks have been explored in machine translation. <ref type="bibr" target="#b1">(Choi et al., 2016)</ref> showed that contextualized word embeddings could be constructed from static word embeddings by applying a learned context mask. This context mask allows the word to have different representations depending on the source sentence context around the word that is being translated, and the authors demonstrated improvements in machine translation tasks with this approach. The approach of masking word representations was also explored in <ref type="bibr" target="#b10">(Ruseti et al., 2016)</ref> for categorizing words into their wordnet classes. We extend this masking concept to dialogue state tracking task, where we encode the temporal information in the dialogue as the masking operation over slots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work we presented a novel approach for incorporating dialogue time information in multidomain large-scale SLU systems. We showed that our proposed time masking strategy provided gains over baseline systems that simply encode dialogue distance. We presented several methods for incorporating additional information such as domain and intents into the time mask, and showed that this approach improved over competing approaches that indirectly incorporate time, particularly for multi-domain dialogues. In the future, we want to investigate more contextualized representations of the domain and intent in order to capture more subtle variations in the dialogue for multi-domain scenarios.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Distribution of temporal distance, d ∆t , for all carryover slots from three of the largest domains in the IPDA dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>User Utterance History System Utterance History User Current Utterance</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>ENCODER</cell><cell></cell><cell cols="2">DECODER</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Word Attention</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>how far away is issaquah</cell><cell>LSTM Encoder</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>issaquah is 18.3 miles away what's the weather like Candidate Slot (Key, Value) City issaquah</cell><cell>LSTM Encoder LSTM Encoder Word Attention Word Attention Slot Embedding</cell><cell>Stream Attention</cell><cell>Dense Layer</cell><cell>Softmax</cell><cell>0.75 0.25</cell><cell>No Carry Carry</cell></row><row><cell>Current Intent</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GetWeatherIntent</cell><cell>Intent Embedding</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Recency One-Hot</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>010</cell><cell>Dense Layer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Domain One-hot</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0000010</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Temporal Distance</cell><cell>Time Mask</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">DIALOGUE TIME</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Overall F1 scores on the IPDA and DSTC2 dataset as well as F1 scores binned by d ∆t for the IPDA dataset, which is measured in seconds. Note: the DSTC2 dataset only contains a single domain</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For simplicity we assume a turn taking model -a user turn and system turn alternate.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The inputs shown in red are not part of the original formulation in<ref type="bibr" target="#b8">(Naik et al., 2018)</ref>.3 Defined as number of seconds in the past that the turn which contains the slot occured relative to the current utterance.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dynamic time-aware attention to speaker roles and contexts for spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ta-Chung</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shang-Yu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="554" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Context-dependent word representation for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heeyoul</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1607.00578</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Discriminative spoken language understanding using word confusion networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gašić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pirros</forename><surname>Tsiakoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="176" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The second dialog state tracking challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL)</title>
		<meeting>the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Word-based dialog state tracking with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL)</title>
		<meeting>the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="292" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Timedependent representation for neural event sequence prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Conference on Learning Representations</title>
		<meeting>the Sixth International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Using recurrent neural networks for slot filling in spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grégoire</forename><surname>Mesnil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gokhan</forename><surname>Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="530" to="539" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrkšić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Diarmuid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Hsien</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03777</idno>
		<title level="m">Neural belief tracker: Data-driven dialogue state tracking</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Contextual slot carryover for disparate schemas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chetan</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arpit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hancheng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lambert</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scalable multi-domain dialogue state tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tür</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="561" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Using embedding masks for word categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Ruseti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Traian</forename><surname>Rebedea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Trausan-Matu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Representation Learning for NLP</title>
		<meeting>the 1st Workshop on Representation Learning for NLP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="201" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">How time matters: Learning time-decay attention for contextual spoken language understanding in dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shang-Yu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Chieh</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Long Papers)</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Long Papers)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2133" to="2142" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
