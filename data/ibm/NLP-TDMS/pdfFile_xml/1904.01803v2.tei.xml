<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gated Fully Fusion for Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangtai</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houlong</forename><surname>Zhao</surname></persName>
							<email>houlongzhao@deepmotion.ai</email>
							<affiliation key="aff1">
								<orgName type="laboratory">DeepMotion 3 Tecent AI lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Han</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhai</forename><surname>Tong</surname></persName>
							<email>yhtong@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Tan</surname></persName>
							<email>tan@cis.pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
							<email>kuiyuanyang@deepmotion.ai</email>
							<affiliation key="aff1">
								<orgName type="laboratory">DeepMotion 3 Tecent AI lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Gated Fully Fusion for Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic segmentation generates comprehensive understanding of scenes through densely predicting the category for each pixel. High-level features from Deep Convolutional Neural Networks already demonstrate their effectiveness in semantic segmentation tasks, however the coarse resolution of highlevel features often leads to inferior results for small/thin objects where detailed information is important. It is natural to consider importing low level features to compensate for the lost detailed information in high-level features. Unfortunately, simply combining multi-level features suffers from the semantic gap among them. In this paper, we propose a new architecture, named Gated Fully Fusion (GFF), to selectively fuse features from multiple levels using gates in a fully connected way. Specifically, features at each level are enhanced by higher-level features with stronger semantics and lowerlevel features with more details, and gates are used to control the propagation of useful information which significantly reduces the noises during fusion. We achieve the state of the art results on four challenging scene parsing datasets including Cityscapes, Pascal Context, COCO-stuff and ADE20K.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Semantic segmentation densely predicts the semantic category for every pixel in an image, such comprehensive image understanding is valuable for many vision-based applications such as medical image analysis <ref type="bibr" target="#b35">(Ronneberger, Fischer, and Brox 2015)</ref>, remote sensing <ref type="bibr" target="#b17">(Kampffmeyer, Salberg, and Jenssen 2016)</ref> and autonomous driving <ref type="bibr" target="#b39">(Xu et al. 2017</ref>). However, precisely predicting label for every pixel is challenging as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, since pixels can be from tiny or large objects, far or near objects, and inside object or object boundary.</p><p>As a semantic prediction problem, the basic task of semantic segmentation is to generate high-level representation for each pixel, i.e., a high-level and high-resolution feature map. Given the ability of ConvNets in learning high-level representation from data, semantic segmentation has made much progress by leveraging such high-level representation. However, high-level representation from ConvNets is generated along lowering the resolution, thus high-resolution and Copyright c 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. high-level feature maps are distributed in two ends in a Con-vNet.</p><p>To get a feature map that is both high-resolution and highlevel, which is not readily available in a ConvNet, it is natural to consider fusing high-level feature maps from top layers and high-resolution feature maps from bottom layers. These feature maps are with different properties, that highlevel feature map can correctly predict most of the pixels on large patterns in a coarse manner, which is widely used in the current semantic segmentation approaches, while low-level feature maps can only predict few pixels on small patterns.</p><p>Thus, simply combining high-level feature maps and high-resolution feature maps will drown useful information in massive useless information, and cannot reach an informative high-level and high-resolution feature map. Therefore, an advanced fusion mechanism is required to collect information selectively from different feature maps. To achieve this, we propose Gated Fully Fusion (GFF) which uses gating mechanism, a kind of operation commonly used for information extraction from time series, to pixelwisely measure the usefulness of each feature vector, and control information propagation through gates accordingly. The principle of the gate at each layer is designed to either send arXiv:1904.01803v2 [cs.CV] 24 Feb 2020 out useful information to other layers or receive information from other layers when the information in the current layer is useless. Using gate to control information propagation, redundancies can also be effectively minimized in the network, allowing us to fuse multi-level feature maps in a fully-connected manner. <ref type="figure" target="#fig_0">Fig 1 compares</ref> the results of GFF and PSPNet <ref type="bibr" target="#b46">(Zhao et al. 2017)</ref>, where GFF can handle finelevel details such as poles and traffic lights in a much better way.</p><p>In addition, contextual information in large receptive field is also very important for semantic segmentation as proved by PSPNet <ref type="bibr" target="#b46">(Zhao et al. 2017)</ref>, ASPP <ref type="bibr" target="#b5">(Chen et al. 2018b</ref>) and DenseASPP <ref type="bibr" target="#b41">(Yang et al. 2018)</ref>. Therefore, we also model contextual information after GFF to further improve the performance. Specifically, we propose a dense feature pyramid (DFP) module to encode context information into each feature map. DFP reuses the contextual information for each feature level and aims to enhance the context modeling part while GFF operates on the backbone network to capture more detailed information. Combining both components in a single end-to-end network, we achieve state-of-the-art results on four scene parsing datasets.</p><p>The main contributions of our work can be summarized as three points: Firstly, we propose Gated Fully Fusion to generate high-resolution and high-level feature map from multilevel feature maps, and Dense Feature Pyramid to enhance the semantic representation of multi-level feature maps. Secondly, detailed analysis with visualization of gates learned in different layers intuitively shows the information regulation mechanism in GFF. Finally, The proposed method is extensively verified on four standard semantic segmentation benchmarks including Cityscapes, Pascal Context, COCOstuff and ADE20K, where our method achieves state-of-theart performance on all four tasks. In particular, our models achieve 82.3% mIoU on Cityscapes test set with ResNet101 as backbone, 83.3% mIoU with WiderResNet as backbone which are trained only on the fine labeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Context modeling Though high-level feature maps in Con-vNets have shown promising results on semantic segmentation <ref type="bibr" target="#b30">(Long, Shelhamer, and Darrell 2015)</ref>, their receptive field sizes are still not large enough to capture contextual information for large objects and regions. Thus, context modeling becomes a practical direction in semantic segmentation. PSPNet <ref type="bibr" target="#b46">(Zhao et al. 2017</ref>) uses spatial pyramid pooling to aggregate multi-scale contextual information. Deeplab series <ref type="bibr" target="#b2">(Chen et al. 2015;</ref> develop atrous spatial pyramid pooling (ASPP) to capture multi-scale contextual information by dilated convolutional layers with different dilation rates. Instead of parallel aggregation as adopted in PSPNet and Deeplab, <ref type="bibr" target="#b41">Yang et al. (Yang et al. 2018</ref>) and <ref type="bibr" target="#b0">Bilinski et al. (Bilinski and Prisacariu 2018)</ref> follow the idea of the dense connection <ref type="bibr" target="#b14">(Huang et al. 2017</ref>) to encode contextual information in a dense way. In <ref type="bibr" target="#b34">(Peng et al. 2017)</ref>, factorized large filters are directly used to increase the receptive field size for context modeling. SVCNet <ref type="bibr" target="#b10">(Ding et al. 2019)</ref> generates a scale and shape-variant semantic mask for each pixel to confine its contextual region.In PSANet <ref type="bibr" target="#b47">(Zhao et al. 2018</ref>), contextual information is collected from all positions according to the similarities defined in a projected feature space. Similarly, DANet <ref type="bibr" target="#b11">(Fu et al. 2018)</ref>, CCNet , EMAnet <ref type="bibr" target="#b23">(Li et al. 2019</ref>) and ANN <ref type="bibr" target="#b49">(Zhu et al. 2019</ref>) use non-local style operator ) to aggregate information from the whole image based on pixelto-pixel affinities. Multi-level feature fusion In addition to lack of contextual information, the top layer also lacks of fine detailed information. To address this issue, in FCN <ref type="bibr" target="#b30">(Long, Shelhamer, and Darrell 2015)</ref>, predictions from middle layers are used to improve segmentation for detailed structures, while hypercolumns (Hariharan et al. 2015) directly combines features from multiple layers for prediction. The U-Net <ref type="bibr" target="#b35">(Ronneberger, Fischer, and Brox 2015)</ref> adds skip connections between the encoder and decoder to reuse low level features, <ref type="bibr" target="#b45">(Zhang et al. 2018b</ref>) improves U-Net by fusing high-level features into low-level features. Feature Pyramid Network (FPN) <ref type="bibr" target="#b27">(Lin et al. 2017b</ref>) uses the structure of U-Net with predictions from each level of the feature pyramid. DeepLabV3+ <ref type="bibr" target="#b6">(Chen et al. 2018c</ref>) refines the decoder of its previous version by combing low-level features. In  and <ref type="bibr" target="#b9">(Ding et al. 2018)</ref>, they proposed to locally fuse every two adjacent feature maps in the feature pyramid into one feature map until only one feature map is left. These fusion methods operate locally in the feature pyramid without awareness of the usefulness of all feature maps to be fused, which limits the propagation of useful features. Gating mechanism In deep neural networks, especially for recurrent networks, gates are commonly utilized to control information propagation. For example, LSTM <ref type="bibr" target="#b13">(Hochreiter and Schmidhuber 1997)</ref> and GRU ) are two typical cases where different gates are used to handle longterm memory and dependencies. The highway network (Srivastava, Greff, and Schmidhuber 2015) uses gates to make training deep network possible. To improve multi-task learning for scene parsing and depth estimation, PAD-Net <ref type="bibr" target="#b40">(Xu et al. 2018</ref>) is proposed to use gates to fuse multi-modal features trained from multiple auxiliary tasks. DepthSeg(Kong and Fowlkes 2018) proposes depth-aware gating module which uses depth estimates to adaptively modify the pooling field size in high-level feature map. GSCNN <ref type="bibr" target="#b37">(Takikawa et al. 2019</ref>) uses gates to learn the precise boundary information by including another shape stream to encode edge feature into final representation.</p><p>Our method is related and inspired by the above methods, and differs from them in that multi-level feature maps are fused simultaneously through gating mechanism, and the resulting method surpasses the state-of-the-art approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>In this section, we first overview the basic setting of multilevel feature fusion and three baseline fusion strategies. Then, we introduce the proposed multi-level fusion module (GFF) and the whole network with the context modeling module (DFP). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-level Feature Fusion</head><p>Given L feature maps {X i ∈ R Hi×Wi×Ci } L i=1 extracted from some backbone networks such as ResNet <ref type="bibr" target="#b12">(He et al. 2016)</ref>, where feature maps are ordered by their depth in the network with increasing semantics but decreasing details, H i , W i and C i are the height, width and number of channels of the ith feature map respectively, feature maps of higher levels are with lower resolution due to the downsampling operations, i.e., H i+1 ≤ H i , W i+1 ≤ W i . In semantic segmentation, the top feature map X L with 1/8 resolution of the raw input image is mostly used for its rich semantics. The major limitation of X L is its low spatial resolution without detailed information, because the outputs need to be with the same resolution as the input image. In contrast, feature maps of low level from shallow layers are with high resolution, but with limited semantics. Intuitively, combining the complementary strengths of multiple level feature maps would achieve the goal of both high resolution and rich semantics, and this process can be abstracted as a fusion process f , i.e.,</p><formula xml:id="formula_0">{X 1 , X 2 · · · X L } f → {X 1 ,X 2 · · ·X L }<label>(1)</label></formula><p>whereX l is the fused feature map for the lth level. To simplify the notations in following equations, bilinear sampling and 1 × 1 convolution are ignored which are used to reshape the feature maps at the right hand side to let the fused feature maps have the same size as those at the left hand side. Concatenation is a straightforward operation to aggregate all the information in multiple feature maps, but it mixes the useful information with large amount of non-informative features. Addition is another simple way to combine feature maps by adding features at each position, while it suffers from the similar problem as concatenation. FPN <ref type="bibr" target="#b27">(Lin et al. 2017b</ref>) conducts the fusion process through a top-down pathway with lateral connections. The three fusion strategies can be formulated as,</p><formula xml:id="formula_1">Concat:X l = concat(X 1 , ..., X L ),<label>(2)</label></formula><p>Addition:</p><formula xml:id="formula_2">X l = L i=1 X i ,<label>(3)</label></formula><formula xml:id="formula_3">FPN:X l =X l+1 + X l , whereX L = X L . (4)</formula><p>The problem of these basic fusion strategies is that feature maps are fused together without measuring the usefulness of each feature vector, and massive useless features are mixed with useful feature during fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gated Fully Fusion</head><p>GFF module design: The basic task in multi-level feature fusion is to aggregate useful information together under interference of massive useless information. Gating is a mature mechanism to measure the usefulness of each feature vector in a feature map and aggregates information accordingly. In this paper, Gated Fully Fusion (GFF) is designed based on the simple addition-based fusion by controlling information flow with gates. Specifically, each level l is associated with a gate map G l ∈ [0, 1] H l ×W l . With these gate maps, the addition-based fusion is formally defined as</p><formula xml:id="formula_4">X l = (1 + G l ) · X l + (1 − G l ) · L i=1,i =l G i · X i ,<label>(5)</label></formula><p>where · denotes element-wise multiplication broadcasting in the channel dimension, each gate map G l = sigmoid(w i * X i ) is estimated by a convolutional layer parameterized with w i ∈ R 1×1×Ci . There are totally L gate maps where L equals to the number of feature maps. The detailed operation can be seen in <ref type="figure" target="#fig_1">Fig 2.</ref> GFF involves duplex gating mechanism: A feature vector at position (x, y) from level i, (where i = l) can be fused to l only when the value of G i (x, y) is large and the value of G l (x, y) is small, i.e., information is sent when level i has the useful information that level l is missing. Besides that useful information can be regulated to the right place through gates, useless information can also be effectively suppressed on both the sender and receiver sides, and information redundancy can be avoided because the information is only received when the current position has useless features.</p><p>More visualization examples can be seen in experiments parts.</p><p>Comparison with Other Gate module: The work <ref type="bibr" target="#b9">(Ding et al. 2018</ref>) also used gates for information control between adjacent layers. GFF differs in using gates to fully fuse features from every level instead of adjacent levels, and richer information in all levels with large usability variance motivates us to design the duplex gating mechanism, which filters out useless information more effectively with gates at both sides of the sender and receiver. Experimental results in the experiment section demonstrate the advantage of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dense Feature Pyramid</head><p>Context modeling aims to encode more global information, and it is orthogonal to the proposed GFF becasue GFF is designed for backbone level. Therefore, we further design a module to encode more contextual information from outputs of both PSPNet <ref type="bibr" target="#b46">(Zhao et al. 2017</ref>) and GFF. Motivated by that dense connections can strengthen feature propagation <ref type="bibr" target="#b14">(Huang et al. 2017)</ref>, we also densely connect the feature maps in a top-down manner starting from feature map outputted from the PSPNet, and high-level feature maps are reused multiple times to add more contextual information to low levels, which was found important in our experiments for correctly segmenting large pattern in objects. This process is shown as follows:</p><formula xml:id="formula_5">y i = H i ([y 0 ,X 1 , ...,X i−1 ])<label>(6)</label></formula><p>Consequently, the j-th feature pyramid receives the featuremaps of all preceding pyramids, y 0 ,X 1 ,...X i−1 as input and outputs current pyramid y i : where x 0 is the output of PSP-Net andX i is the output of i-th GFF module. Fusion function H i is implemented by a single convolution layer. Since the feature pyramid is densely connected, we denote this module as Dense Feature Pyramid (DFP). The collections of DFP's outputs y i are used for final prediction. Both GFF and DFP can be plugged into existing FCNs for end-to-end training with only slightly extra computation cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network Architecture and Implementation</head><p>Our network architecture is designed based on previous state-of-the-art network PSPNet <ref type="bibr" target="#b46">(Zhao et al. 2017)</ref> with ResNet <ref type="bibr" target="#b12">(He et al. 2016)</ref> as backbone for basic feature extraction, the last two stages in ResNet are modified with dilated convolution to make both strides to 1 and keep spatial information. of backbone are used as the input for GFF module, and all feature maps are reduced to 256 channels with 1×1 convolutional layers. The output feature maps from GFF are further fused with two 3×3 convolutional layers in each level before feeding into the DFP module. All convolutional layers are followed by batch normalization <ref type="bibr" target="#b16">(Ioffe and Szegedy 2015)</ref> and ReLU activation function. After DFP, all feature maps are concatenated for final semantic segmentation. Compared with the basic PSPNet, the proposed method only slightly increases the number of parameters and computations. The entire network is trained in an end-to-end manner driving by cross-entropy loss defined on the segmentation benchmarks. To facilitate the training process, an auxiliary loss together with the main loss are used to help optimization following <ref type="bibr" target="#b21">(Lee et al. 2015)</ref>, where the main loss is defined on the final output of the network and the auxiliary loss is defined on the output feature map at stage3 of ResNet with weight of 0.4 <ref type="bibr" target="#b46">(Zhao et al. 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment</head><p>In this section, we analyze the proposed method on Cityscapes <ref type="bibr" target="#b8">(Cordts et al. 2016</ref>)dataset and report results on other datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>Our implementation is based on PyTorch <ref type="bibr" target="#b33">(Paszke et al. 2017</ref>). The weight decay is set to 1e-4. Standard SGD is used for optimization, and "poly" learning rate scheduling policy is used to adjust learning rate, where initial learning rate is set to 1e-3 and decayed by (1 − iter total iter ) power with power = 0.9. Synchronized batch normalization <ref type="bibr" target="#b44">(Zhang et al. 2018a</ref>) is used. For Cityscapes, crop size of 864 × 864 is used, 100K training iterations with mini-batch size of 8 is carried for training. For ADE20K, COCO-stuff and Pascal Context, crop size of 512 × 512 is used (images with side smaller than the crop size are padded with zeros), 150K training iterations are used with mini-batch size of 16. As </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments on Cityscapes Dataset</head><p>Cityscapes is a large-scale dataset for semantic urban scene understanding. It contains 5000 fine pixel-level annotated images, which is divided into 2975, 500, and 1525 images for training, validation and testing respectively, where labels of training and validation are publicly released and labels of testing set are held for online evaluation. It also provides 20000 coarsely annotated images. 30 classes are annotated and 19 of them are used for pixel-level semantic labeling task. Images are in high resolution with the size of 1024 × 2048. The evaluation metric for this dataset is the mean Intersection over Union (mIoU). Strong Baseline We choose PSPNet <ref type="bibr" target="#b46">(Zhao et al. 2017</ref>) as our baseline model which achieved state-of-the-art performance for semantic segmentation. We re-implement PSPNet on Cityscapes and achieve similar performance with mIoU of 78.6% on validation set. All results are reported by using sliding window crop prediction. Ablation Study on Feature Fusion Methods First, we compare several methods introduced in Method part. To speed up the training process, we use weights from the trained PSPNet to initialize the parameters in each fusion method. We use train-fine data for training and report performance on validation set. For fair comparison with concatenation and addition, we also reduce the channel dimension of feature maps to 256 and use two 3×3 convolutional layers to refine the fused feature map. As for FPN, we implement the original FPN for semantic segmentation following <ref type="bibr" target="#b19">(Kirillov et al. 2017</ref>) and we add it to PSPNet. Note that FPN based on PSPNet fuses 5 feature maps, where one is context feature map from pyramid pooling module and others are from the backbone.</p><p>All the results are shown in <ref type="table">Table 1</ref>. As expected, concatenation and addition only slightly improve the baseline, and FPN achieves the best performance among the three base fusion methods, while the proposed GFF obtains even more improvement with mIoU of 80.4%. Since GFF is a gated version of addition-based fusion, the results demonstrate the effectiveness of the used gating mechanism. For further comparison, we also add the proposed gating mechanism into top-down pathway of FPN and observe slight improvement, which is reasonable since most high-level features are useful for low levels. This demonstrates the advantage of fully fusing multi-level feature maps, and the importance of gating mechanism especially during fusing low-level features to high levels. <ref type="figure">Fig 4 shows</ref> results after using GFF, where the accuracies of predictions for both distant objects and object boundaries are significantly improved.</p><p>Ablation Study for Improvement Strategies We perform two strategies to further boost the performance of our model:, (1) DFP: Dense Feature Pyramid is used after the output of GFF module; and (2) MS: multi-scale inference is adopted, where the final segmentation map is averaged from segmentation probability maps with scales {0.75, 1, 1.25, 1.5, 1.75} for evaluation. Experimental results are shown in <ref type="table" target="#tab_2">Table 2</ref>, and DFP further improves the performance by 0.8% mIoU. <ref type="figure" target="#fig_4">Fig. 5</ref> shows several visual comparisons, where DFP generates more consistent segmentation inside large objects and demonstrates the effectiveness in using contextual information for resolving local ambiguities. With multi-scale inference, our model achieves 81.8% mIoU, which significantly outperforms previous state-ofthe-art model DeepLabv3+ (79.55% on Cityscapes validation set) by 2.25%. Ablation Study for other architectures we also perform experiments two different backbone architectures <ref type="bibr" target="#b41">(Yang et al. 2018)</ref>. One is another strong baseline and the other is PSPNet with lightweight backbone. Results are shown in table 4. It shows that both GFF and DFP show their generality on improving model results. In particular, resnet18 based PSPnet improve 5.9% point from the baseline. Computation Cost In <ref type="table" target="#tab_3">Table 3</ref>, we also study the computational cost of using our modules, where our method spends 7.7% more computational cost and 6.3% more parameters compared with the baseline PSPNet which indicates our method can be easily plugged in existing state-of-art segmentation methods with little extra computation cost.</p><p>Comparison to the State-of-the-Art As a common practice toward best performance, we average the predictions of multi-scaled images for inference. For fair comparison, all methods are only trained using fine annotated dataset and evaluated on test set by the evaluation server.       comparisons are reported in <ref type="table" target="#tab_8">Table 7</ref>, where our method achieves the highest IoU on 15 out of 19 categories, and large improvements are from small/thin categories such as pole, street light/sign, person and rider. Moreover, we further apply our methods on state-ofthe-art method Deeplabv3+ <ref type="bibr" target="#b6">(Chen et al. 2018c</ref>) with more stronger backbone Wider-ResNet (Zagoruyko and Komodakis 2016) pretrained on Mapillary <ref type="bibr" target="#b32">(Neuhold et al. 2017</ref>) dataset which shares the same setting with GSCNN <ref type="bibr" target="#b37">(Takikawa et al. 2019)</ref>. <ref type="table" target="#tab_9">Table 8</ref> shows the detailed results of our methods with previous state-of-art methods which also use corase data annotations. For both experiments, we don't use coarse data. More detailed analysis will be given by gate visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualization of Gates</head><p>In this section, we visualize what gates have learned and analyze how gates control the information propagation. <ref type="figure" target="#fig_0">Fig 10</ref> shows the gates learned from ADE20K and <ref type="figure" target="#fig_8">Fig 9(a)</ref> shows the gates learned from Cityscapes respectively. For each input image, we show the learned gate map of each level. As expected, we find that the higher-level features (e.g., G 3 , G 4 ) are more useful for large structures with explicit semantics, while the lower-level features (e.g., G 1 and G 2 ) are mainly useful for local details and boundaries.</p><p>Functionally, we find that the higher level features always spread information to other layers and only receive sparse feature signals. For example, the gate from stage 4 (in G 4 of <ref type="figure" target="#fig_8">Fig 9)</ref> shows that almost all pixels are of high-confidence. Higher-level features cover a large receptive field with fewer details, and they can provide a ground scope of the main semantics.</p><p>In contrast, the lower level layers prefer to receive information while only spreading a few sparse signals. This verifies that lower level representations generally vary frequently along the spatial dimension and they require additional features as semantic supplement, while a benefit is that lower features can provide precise information for details and object boundaries (G 2 in <ref type="figure" target="#fig_0">Fig 10 and G 1 in Fig 9(a)</ref>).</p><p>Method road swalk build wall fence pole tlight sign veg. terrain sky person rider car truck bus train mbike bike mIoU PSPNet <ref type="bibr" target="#b46">(Zhao et al. 2017</ref><ref type="bibr">) 98.6 86.2 92.9 50.8 58.8 64.0 75.6 79.0 93.4 72.3 95.4 86.5 71.3 95.9 68.2 79.5 73.8 69.5 77.2 78.4 AAF (Ke et al. 2018</ref> 98.5 85.6 93.0 53.8 58.9 65.9 75.0 78.4 93.7 72.4 95.6 86.4 70.5 95.9 73.9 82.7 76.9 68.7 76.4 79.1 DenseASPP <ref type="bibr" target="#b41">(Yang et al. 2018</ref><ref type="bibr">) 98.7 87.1 93.4 60.7 62.7 65.6 74.6 78.5 93.6 72.5 95.4 86.2 71.9 96.0 78.0 90.3 80.7 69.7 76.8 80.6 DANet (Fu et al. 2018</ref> 98.6 87.1 93.    To further verify the effectiveness of the learned gates, we set the value of each gate G i to zero and compare the segmentation results with learned gate values. <ref type="figure" target="#fig_8">Fig 9 (b)</ref> shows the comparison results, where wrongly predicted pixels after setting G i to zero are highlighted. Information through G 1 and G 2 is mainly help for object boundaries, while information through G 3 and G 4 is mainly help for large patterns such as cars. Additional visualization examples for the gates can be found in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on Other Datasets</head><p>ADE20K is a challenging scene parsing dataset annotated with 150 classes, and it contains 20K/2K images for training and validation. Images in this dataset are from more different scenes with more small scale objects, and are with varied sizes including max side larger than 2000 and min side smaller than 100. Following the standard protocol, both mIoU and pixel accuracy evaluated on validation set are used as the performance metrics.In table6, with backbone ResNet101, our method outperforms state-of-the-art methods with considerable margin in terms of both mIoU and <ref type="figure">Figure 8</ref>: Visualization results on ADE20K validation dataset (ResNet101 as backbone). Comparing with PSPNet, our method captures more detailed information, and finds missing small objects (e.g., lights in first two examples) and generates "smoother" on object boundaries (e.g., figures on the wall in last example). Best view in color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>BackBone mIoU(%) EncNet <ref type="formula">(</ref>   shown in <ref type="table" target="#tab_10">Table 9</ref>. Our method achieves the state-of-the-art results on both ResNet50 and ResNet101 backbone and outperforms the existing methods by a large margin. COCO Stuff <ref type="bibr" target="#b1">(Caesar, Uijlings, and Ferrari 2018)</ref> contains 10000 images from Microsoft COCO dataset <ref type="bibr" target="#b25">(Lin et al. 2014)</ref>, out of which 9000 images are for training and 1000 images for testing. This dataset contains 171 categories including objects and stuff annotated to each pixel. The results of COCO Stuff are shown in <ref type="table" target="#tab_11">Table 10</ref>. Our method outperforms the existing methods and achieves top performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this work, we propose Gated Fully Fusion (GFF) to fully fuse multi-level feature maps controlled by learned gate maps. The novel module bridges the gap between high resolution with low semantics and low resolution with high semantics. We explore the proposed GFF for the task of semantic segmentation and achieve new state-of-the-art results four challenging scene parsing dataset. In particular, we find that the missing low-level features can be fused into each feature level in the pyramid, which indicates that our module can well handle small and thin objects in the scene.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of challenges in semantic segmentation. (a) Input Image. (b) Ground Truth. (c) PSPNet result. (d) Our result. Our method performs much better on small patterns such as distant poles and traffic lights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The proposed gated fully fusion module, where G l is the gate map generated from X l , and features corresponding high gate values are allowed to send out and regions with low gate values are allowed to receive.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of the overall architecture. (a) Backbone Network(e.g. ResNet (He et al. 2016)) with pyramid pooling module (PPM) (Zhao et al. 2017) on the top. The backbone provides a pyramid of features at different levels. (b), Feature pyramid through gated fully fusion (GFF) modules. The detail of the GFF module is illustrated in Fig 2 . (c), Then the final features containing context information are obtained from a dense feature pyramid (DFP) module. Best view in color and zoom in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Fig 3 shows the overall framework including both GFF and DFP. PSPNet forms the bottom-up pathway with backbone network and pyramid pooling module (PPM), where PPM is at the top to encode contextual information. Feature maps from last residual blocks in each stage Figure 4: Visualization of segmentation results of two images us-ing GFF and PSPNet. The first column shows two input images zoomed in regions marked with red dash rectangles. The second column shows results of PSPNet, and the third column shows results of using GFF. The fourth column lists the ground truth. The last column shows the refined parts by GFF. It shows that GFF can handle distant missing objects like poles, traffic lights and object boundaries. Best view in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>DFP enhances segmentation results on large scale objects and generates more consistent results. Best view in color and zoom in. a common practice to avoid , data augmentation including random horizontal flipping, random cropping, random color jittering within the range of [−10, 10], and random scaling in the range of [0.75, 2] are used during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Visualization of learned gate maps on ADE20K dataset. G i represents the gate map of the ith layer. Best view in color and zoom in for detailed information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>(a) Visualization of learned gate maps on Cityscapes dataset, where G i represents the gate map of the ith layer. (b) Wrongly classified pixels are highlighted after setting G i to 0 comparing with using original gate values. Best view in color and zoom in for detailed information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc><ref type="bibr" target="#b44">Zhang et al. 2018a</ref>) ResNet-50 49.0 DANet<ref type="bibr" target="#b11">(Fu et al. 2018</ref></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>More visualization of learned gate maps on Cityscapes dataset. G i represents the output of i th layer's gate. It shows the gate control the information propagation. Best view in color and zoom in for detailed information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>More more visualization of learned gate maps on ADE20K dataset. G i represents the gate map of the ith layer. Best view in color and zoom in for detailed information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>summarizes the comparisons, our method achieves 80.9%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison experiments on Cityscapes validation set, where PSPNet serves as the baseline method.</figDesc><table><row><cell>Method</cell><cell cols="3">mIoU(%) FLOPS(G) Params(M)</cell></row><row><cell>PSPNet(Baseline)</cell><cell>78.6</cell><cell>580.1</cell><cell>65.6</cell></row><row><cell>PSPNet + GFF</cell><cell>80.4</cell><cell>600.1</cell><cell>69.7</cell></row><row><cell cols="2">PSPNet + GFF + DFP 81.2</cell><cell>625.5</cell><cell>70.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Computational cost comparison, where PSPNet serves as the baseline with image of size 512 × 512 as input.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>mIoU%</cell></row><row><cell>DenseASPP</cell><cell cols="2">DenseNet121 78.9</cell></row><row><cell>DenseASPP+GFF</cell><cell cols="2">DenseNet121 80.1(1.2↑)</cell></row><row><cell cols="3">DenseASPP+GFF+DFP DenseNet121 80.9(2.0↑)</cell></row><row><cell>PSP</cell><cell>ResNet18</cell><cell>73.0</cell></row><row><cell>PSP + GFF</cell><cell>ResNet18</cell><cell>76.6 (3.6↑)</cell></row><row><cell>PSP + GFF+DFP</cell><cell>ResNet18</cell><cell>78.9 (5.9↑)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Ablation study on two different models, where mIoU is evaluated on Cityscapes validation set.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>mIoU(%)</cell></row><row><cell>PSPNet (Zhao et al. 2017) †</cell><cell cols="2">ResNet101 78.4</cell></row><row><cell>PSANet (Zhao et al. 2018) †</cell><cell cols="2">ResNet101 78.6</cell></row><row><cell>GFFNet(Ours) †</cell><cell cols="2">ResNet101 80.9</cell></row><row><cell>AAF (Ke et al. 2018) ‡</cell><cell cols="2">ResNet101 79.1</cell></row><row><cell>PSANet (Zhao et al. 2018) ‡</cell><cell cols="2">ResNet101 80.1</cell></row><row><cell>DFN (Yu et al. 2018) ‡</cell><cell cols="2">ResNet101 79.3</cell></row><row><cell cols="3">DepthSeg (Kong and Fowlkes 2018) ‡ ResNet101 78.2</cell></row><row><cell>DenseASPP (Yang et al. 2018)  ‡</cell><cell cols="2">DenseNet161 80.6</cell></row><row><cell>SVCNet (Ding et al. 2019)  ‡</cell><cell cols="2">ResNet101 81.0</cell></row><row><cell>DANet (Fu et al. 2018)  ‡</cell><cell cols="2">ResNet101 81.5</cell></row><row><cell>GFFNet(Ours) ‡</cell><cell cols="2">ResNet101 82.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>State-of-the-art comparison experiments onCityscapes test set. †means only using the train-fine dataset.‡means both the train-fine and val-fine data are used.</figDesc><table><row><cell>Method</cell><cell cols="2">Backbone mIoU(%) Pixel Acc.(%)</cell></row><row><cell cols="2">RefineNet (Lin et al. 2017a) ResNet101 40.20</cell><cell>-</cell></row><row><cell>PSPNet (Zhao et al. 2017)</cell><cell>ResNet101 43.29</cell><cell>81.39</cell></row><row><cell cols="2">PSANet (Zhao et al. 2018) ResNet101 43.77</cell><cell>81.51</cell></row><row><cell cols="2">EncNet (Zhang et al. 2018a) ResNet101 44.65</cell><cell>81.69</cell></row><row><cell cols="2">GCUNet (Li and Gupta 2018) ResNet101 44.81</cell><cell>81.19</cell></row><row><cell>GFFNet(Ours)</cell><cell>ResNet101 45.33</cell><cell>82.01</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>State-of-the-art comparison experiments on ADE20K validation set. Our models achieve top performance measured by both mIoU and pixel accuracy.</figDesc><table /><note>mIoU by only using train-fine dataset and outperforms PSANet (Zhao et al. 2018) by 2.3%. By fine-tuning the model on both train-fine and val-fine datasets, our method achieves the best mIoU of 82.3%. Detailed per-category</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>5 56.1 63.3 69.7 77.3 81.3 93.9 72.9 95.7 87.3 72.9 96.2 76.8 89.4 86.5 72.2 78.2 81.5 GFFNet(Ours) 98.7 87.2 93.9 59.6 64.3 71.5 78.3 82.2 94.0 72.6 95.9 88.2 73.9 96.5 79.8 92.2 84.7 71.5 78.8 82.3</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Per-category results on Cityscapes test set. Note that all the models are trained with only fine annotated data. Our method outperforms existing approaches on 15 out of 19 categories, and achieves 82.3% mIoU.</figDesc><table><row><cell>Method</cell><cell>Coarse road swalk build. wall fence pole tlight sign veg terrain sky person rider car truck bus train motor bike mIoU</cell></row><row><cell>PSP-Net (Zhao et al. 2017)</cell><cell>98.7 86.9 93.5 58.4 63.7 67.7 76.1 80.5 93.6 72.2 95.3 86.8 71.9 96.2 77.7 91.5 83.6 70.8 77.5 81.2</cell></row><row><cell>DeepLabV3 (Chen et al. 2017)</cell><cell>98.6 86.2 93.5 55.2 63.2 70.0 77.1 81.3 93.8 72.3 95.9 87.6 73.4 96.3 75.1 90.4 85.1 72.1 78.3 81.3</cell></row><row><cell>DeepLabV3+ (Chen et al. 2018c)</cell><cell>98.7 87.0 93.9 59.5 63.7 71.4 78.2 82.2 94.0 73.0 95.8 88.0 73.3 96.4 78.0 90.9 83.9 73.8 78.9 81.9</cell></row><row><cell>AutoDeepLab-L (Liu et al. 2019)</cell><cell>98.8 87.6 93.8 61.4 64.4 71.2 77.6 80.9 94.1 72.7 96.0 87.8 72.8 96.5 78.2 90.9 88.4 69.0 77.6 82.1</cell></row><row><cell>DPC (Chen et al. 2018a)</cell><cell>98.7 87.1 93.8 57.7 63.5 71.0 78.0 82.1 94.0 73.3 95.4 88.2 74.5 96.5 81.2 93.3 89.0 74.1 79.0 82.7</cell></row><row><cell>G-SCNN (Takikawa et al. 2019)</cell><cell>98.7 87.4 94.2 61.9 64.6 72.9 79.6 82.5 94.3 74.3 96.2 88.3 74.2 96.0 77.2 90.1 87.7 72.6 79.4 82.8</cell></row><row><cell>GFFNet(ours)</cell><cell>98.8 87.9 94.3 64.7 65.8 71.9 78.9 82.4 94.2 74.3 96.1 88.4 74.9 96.5 79.2 92.8 90.2 73.4 79.1 83.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Per-category results on Cityscapes test set. Note that our methods and G-SCNN are trained with only fine annotated data.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Results on Pascal Context testing set. pixel accuracy. Several visual comparison results are shown in Fig 8, where our method performs much better at details and object boundaries. Pascal Context (Mottaghi et al. 2014) provides pixel-wise segmentation annotation for 59 classes. There are 4998 training images and 5105 testing images. The results are</figDesc><table><row><cell>Method</cell><cell>BackBone mIoU(%)</cell></row><row><cell>RefineNet (Lin et al. 2017a)</cell><cell>ResNet101 33.6</cell></row><row><cell cols="2">DSSPN (Liang, Zhou, and Xing 2018) ResNet101 36.2</cell></row><row><cell>CCLNet (Ding et al. 2018)</cell><cell>ResNet101 35.7</cell></row><row><cell>GFFNet(Ours)</cell><cell>ResNet101 39.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Results on COCO stuff testing set.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dense decoder shortcut connections for single-pass semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bilinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Prisacariu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Coco-stuff: Thing and stuff classes in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1209" to="1218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<title level="m">Semantic image segmentation with deep convolutional nets and fully connected CRFs. ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Searching for efficient multi-scale architectures for dense image prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
		<editor>Bengio, S.</editor>
		<editor>Wallach, H.</editor>
		<editor>Larochelle, H.</editor>
		<editor>Grauman, K.</editor>
		<editor>Cesa-Bianchi, N.</editor>
		<editor>and Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8699" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Context contrasted feature and gated multi-scale aggregation for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Qun Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">cvpr</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semantic correlation promoted shape-variant context for segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02983</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Dual attention network for scene segmentation</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11721</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<title level="m">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semantic segmentation of small objects and modeling of uncertainty in urban remote sensing images using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kampffmeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-B</forename><surname>Salberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adaptive affinity fields for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-W</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A unified architecture for instance and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recurrent scene parsing with perspective understanding in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deeply-supervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Beyond grids: Learning graph representations for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Expectation-maximization attention networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dynamic-structured semantic propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-scale context intertwining for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02985</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-G</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The mapillary vistas dataset for semantic understanding of street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rota Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4990" to="4999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS-W</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Large kernel matters improve semantic segmentation by global convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<title level="m">U-net: Convolutional networks for biomedical image segmentation. MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Training very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Gatedscnn: Gated shape cnns for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Takikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Acuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>In CVPR</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">End-to-end learning of driving models from large-scale video datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pad-net: Multitasks guided prediction-and-distillation network for simultaneous depth estimation and scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Denseaspp for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning a discriminative feature network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<title level="m">Wide residual networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Context encoding for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Exfuse: Enhancing feature fusion for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Psanet: Point-wise spatial attention network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.05442</idno>
		<title level="m">Semantic understanding of scenes through the ADE20K dataset</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Asymmetric non-local neural networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV. More visualization of Gates Here we give more visualization examples on Cityscapes (Cordts et al. 2016) and ADE20k (Zhou et al. 2016) dataset shown in Fig 9 and Fig 10 respectively</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
