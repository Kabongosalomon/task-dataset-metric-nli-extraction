<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semi-supervised Sequence-to-sequence ASR using Unpaired Speech and Text</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-08-20">20 Aug 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murali</forename><surname>Karthick Baskar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Brno University of Technology</orgName>
								<orgName type="institution" key="instit2">Czech Republic</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<region>Υ MERL</region>
									<country>USA, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<region>Υ MERL</region>
									<country>USA, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramon</forename><surname>Astudillo</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IBM T. J. Watson Research Center</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Brno University of Technology</orgName>
								<orgName type="institution" key="instit2">Czech Republic</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukáš</forename><surname>Burget</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Brno University of Technology</orgName>
								<orgName type="institution" key="instit2">Czech Republic</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jančernocký</forename></persName>
						</author>
						<title level="a" type="main">Semi-supervised Sequence-to-sequence ASR using Unpaired Speech and Text</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-08-20">20 Aug 2019</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: Sequence-to-sequence</term>
					<term>end-to-end</term>
					<term>ASR</term>
					<term>TTS</term>
					<term>semi-supervised</term>
					<term>unsupervised</term>
					<term>cycle consistency</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sequence-to-sequence automatic speech recognition (ASR) models require large quantities of data to attain high performance. For this reason, there has been a recent surge in interest for unsupervised and semi-supervised training in such models. This work builds upon recent results showing notable improvements in semi-supervised training using cycle-consistency and related techniques. Such techniques derive training procedures and losses able to leverage unpaired speech and/or text data by combining ASR with Text-to-Speech (TTS) models. In particular, this work proposes a new semi-supervised loss combining an end-to-end differentiable ASR→TTS loss with TTS→ASR loss. The method is able to leverage both unpaired speech and text data to outperform recently proposed related techniques in terms of %WER. We provide extensive results analyzing the impact of data quantity and speech and text modalities and show consistent gains across WSJ and Librispeech corpora. Our code is provided in ESPnet to reproduce the experiments.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Sequence-to-sequence (seq2seq) ASR training directly maps a speech input to an output character sequence using a neural network <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>, similar to those used in machine translation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. The model requires a considerable amount of paired speech and text to learn alignment and classification <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>, which limits its use in under-resourced domains. On the other hand, unpaired speech and text can be obtained for most domains in large quantities making training with unpaired data particularly relevant for seq2seq models.</p><p>Recent works have shown that the problem of seq2seq training in low-resource conditions can be tackled using unpaired data. These methods can be classified into three categories according to the type of data used. First, methods utilizing only unpaired speech for unsupervised training. In this category, <ref type="bibr" target="#b7">[8]</ref> proposes an end-to-end differentiable loss integrating ASR and TTS models by the straight-through estimator. The work in <ref type="bibr" target="#b8">[9]</ref> also proposes an end-to-end differentiable loss integrating ASR and a Text-to-Encoder (TTE) model. Both works, showed that connecting ASR with TTS/TTE can handle unpaired speech data as well as reduce ASR recognition errors. The second category concerns methods leveraging unpaired text data, which focus on enhancing the decoder component of seq2seq ASR <ref type="bibr" target="#b9">[10]</ref> or moving the encoder representation closer to text representation <ref type="bibr" target="#b10">[11]</ref>. The former approach <ref type="bibr" target="#b9">[10]</ref> is implemented by feeding the text data to TTE-to-ASR <ref type="bibr" target="#b9">[10]</ref> model, where the TTE converts the text directly to encoder representations and then is fed to the decoder. In the latter model, an encoder component is shared to have common representation across both speech and text data <ref type="bibr" target="#b10">[11]</ref>. In addition to these works, <ref type="bibr" target="#b11">[12]</ref> used a language model (LM) built with unpaired text-only data to jointly train with an ASR. In the third category, both unpaired speech and text data are exploited. Examples of this are the speech-chain framework <ref type="bibr" target="#b12">[13]</ref> and adversarial training schemes <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. The former proposes a dual pipeline of ASR→TTS and TTS→ASR systems. Here, the backpropagation only takes place in the secondary stage of the chain. The latter utilizes, adversarial learning objectives to match the speech and text distributions.</p><p>Outside of the speech processing area, cycle-consistency can be related to successful progress in machine translation <ref type="bibr" target="#b15">[16]</ref>, which derive end-to-end differentiability by using N-best approach. Also, improvements are observed in image processing tasks using cycle-consistency with an adversarial objective <ref type="bibr" target="#b17">[17]</ref>.</p><p>In this paper, we borrow ideas from the above mentioned techniques to handle unpaired data using cycle-consistency. The basic idea of this approach is that if a model converts an input data to output data and another model reconstructs the input data from the output data, then the input data and its reconstruction should be similar. We use this idea to derive a new loss able to use both unpaired speech and text data. The contributions of this paper are the following • A speech-only end-to-end differentiable loss is proposed using ASR→TTS. This improves the TTE cycle loss in <ref type="bibr" target="#b8">[9]</ref> and is related to the straight-through loss in <ref type="bibr" target="#b7">[8]</ref> • We complement this with a text-only pipeline loss using TTS→ASR. This improves TTE backtranslation <ref type="bibr" target="#b9">[10]</ref> and is related to the speech-chain in <ref type="bibr" target="#b12">[13]</ref> • We combine this and a shallow integrated RNNLM <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b19">19]</ref> to obtain a high performance on unpaired data. Our experimental analysis using WSJ and Librispeech corpus described in section 4 substantiate our hypothesis that unpaired speech and text can individually improve ASR performance and combining them provides additional gains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Cycle consistency training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">ASR and TTS with seq2seq models</head><p>Sequence-to-sequence ASR systems <ref type="bibr" target="#b1">[2]</ref> model directly a probability distribution over C-length character or phoneme</p><formula xml:id="formula_0">sequence Y = {yc} C c=1 given T -length speech features X = {xt} T t=1 i.e. Mel-filterbank as pASR(Y | X) = C c p(yc | y1:c−1, X).<label>(1)</label></formula><p>Each character prediction depends on the entire input and all previously generated characters. To account for this complex relation, a neural network with attention mechanism is used <ref type="bibr" target="#b3">[4]</ref>. This is composed of an encoder network, typically one or more bi-directional LSTMs layers <ref type="bibr" target="#b20">[20]</ref>, generating an encoder sequence H = {ht} T t=1 as H = encoder(X).</p><p>A decoder, also modeled by one or more LSTM networks, utilizes a matching function to select the relevant encoded input features{ht} t ′ c t=tc for each step c given its internal state. This matching function is normalized to resemble a probability distribution over the input features and is termed attention. Decoder internal state, attended input and previously produced character are used to predict the current character.</p><p>When paired data is available, the seq2seq models are trained with the cross-entropy (CE) criterion given correct output y * :</p><formula xml:id="formula_2">LASR = −log pASR(y * | X) (3) = − l=1 log pASR(y * l | y * 1:l−1 , X).</formula><p>TTS seq2seq models use a very similar architecture, but receive the characters as input Y = {yc} C c=1 and predict the speech features with a regression layer X = {xt} T t=1 . There are some needed modifications with respect to ASR, such as the need to predict continuous data and the end of utterance. In this work, we used the Tacotron2 architecture, detailed in <ref type="bibr" target="#b21">[21]</ref>. The loss of the TTS system is divided into three terms:</p><formula xml:id="formula_3">LTTS = LMSE + LL1 + LBCE<label>(4)</label></formula><p>where the mean square error (squared L2) and L1 norms are over a regression estimateX and BCE is the binary cross entropy loss for the end of sentence prediction. MSE and L1 components of the loss can be interpreted as the minus log-probability of the speech features for Gaussian and Laplace distributions for constant scale parameters i.e.</p><formula xml:id="formula_4">LTTS = − log pTTS(X * | Y) (5) = − t=1 log pTTS(X * t | X * 1:t−1 , Y)</formula><p>considering the MSE only, we have</p><formula xml:id="formula_5">− log pTTS(X * t | X * 1:t−1 , Y) ∝ X * t − g(Y, X * 1:t−1 ) 2 (6) whereXt = g(Y, X * 1:t−1 )</formula><p>is the Tacotron2 auto-regressive estimate at time t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Unsupervised training with unpaired speech data</head><p>The previous formulations concern the case in which both paired speech X and text Y are available. In this section we propose an improvement over recent work in <ref type="bibr" target="#b8">[9]</ref> that provides an unsupervised loss for ASR while being more performant. A central problem to the ASR→TTS pipeline is the fact that the text bottleneck eliminates a lot of information from speech e.g. speaker identity. The work in <ref type="bibr" target="#b8">[9]</ref> solves this by training the TTS model to predict encoded speech, H in eq. (2), instead of speech X directly. Since the encoded speech keeps some speech characteristics, it is possible to define the following loss based on the Cycle-Consistency (CC) criterion  <ref type="figure" target="#fig_0">Figure 1a</ref> denotes the ASR→TTS pipeline where Y k is the k th randomly sampled ASR output. <ref type="figure" target="#fig_0">Figure 1b</ref> shows the TTS→ASR pipeline.</p><formula xml:id="formula_6">LASR→TTE = E p ASR (Y|X) {LTTE} (7)</formula><p>where LTTE is the same as LTTS but replacing X by the encoded speech H. This loss penalizes the ASR system for transcriptions that, once transformed back into an estimate of the encoded speechĤ by the TTE, differ from the original encoded speech H. Such loss does not require to have the correct text output y * and is end-to-end differentiable.</p><p>TTE-CC has the disadvantage of having to train an specific network to predict H. The encoded speech, may also already eliminate some of the speech characteristics making the CC loss less powerful. In this work, we propose to use the TTS loss instead of the TTE loss, to realize</p><formula xml:id="formula_7">LASR→TTS = E p ASR (Y|X) {LTTS}.<label>(8)</label></formula><p>Aside from being computationally more intensive, this requires solving the problem of passing speaker characteristics through the text bottleneck, see <ref type="figure" target="#fig_0">Figure 1</ref>(a). In order to do so, inspired by <ref type="bibr" target="#b12">[13]</ref>, we augment the TTS model with speaker vectors obtained from a x-vector network <ref type="bibr" target="#b22">[22]</ref>. For example, for the MSE component of LTTS we have</p><formula xml:id="formula_8">LMSE = − log pTTS(X * | Y, f (X))<label>(9)</label></formula><p>where f implements the x-vector. Note that x-vectors are designed to retain speaker characteristics but not general structure of the speech signal. In that sense, the model can not learn to copy directly X from input to output. Similarly to <ref type="bibr" target="#b8">[9]</ref> we use policy-gradient to back-propagate through the expectation in the loss and update the ASR parameters. This yields the following gradient update formula</p><formula xml:id="formula_9">∇ θ LASR→TTS ≈ Y n ∼p ASR 1 N R(Y n , X)∇ θ log pASR(Y n | X) (10)</formula><p>for N samples. The reward R is obtained by subtracting a bias term from the TTS loss to reduce variance R(Y n , X) = LTTS − B(X). The bias term is calculated as the mean value of X for each sample. In practical terms, using policy gradient amounts to sampling multiple sentences from the ASR distribution and backpropagating each of them as if they were the ground truth, but weighted by the reconstruction loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Unsupervised training with unpaired text data</head><p>As with speech, unsupervised training of ASR using only text and cycle-consistency is possible by using a TTS→ASR chain, see <ref type="figure" target="#fig_0">Figure 1</ref>(b). Since our target in this work is to increase ASR performance, the resulting computation graph is simpler than in the ASR→TTS case. There is no need to backpropagate into the TTS module and thus the chain can be realized by forming a non-differentiable TTS→ASR pipeline as</p><formula xml:id="formula_10">LTTS→ASR = −logpASR(Y * |X).<label>(11)</label></formula><p>The point estimate is obtained by providing the TTS system with the input text Y along with randomly chosen x-vectors to generateX = arg max</p><formula xml:id="formula_11">X {pTTS(X | Y)}.<label>(12)</label></formula><p>Thus the pipeline can act as an autoencoder and allows training of unpaired text only data. Using a non-differentiable pipeline where TTS generates synthetic speech sentences for which we only have a transcription is akin to back-translation in machine translation <ref type="bibr" target="#b23">[23]</ref>. The work in <ref type="bibr" target="#b9">[10]</ref> proposes a similar idea utilizing TTE instead of TTS. The TTS→ASR pipeline is also used in <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Unsupervised training with both unpaired speech and text data</head><p>In the case in which both paired speech and text are available, both ASR→TTS and TTS→ASR can be trained jointly, see <ref type="figure" target="#fig_0">figure 1</ref>. The final loss L both is a linear interpolation of the loss functions defined in equations <ref type="formula" target="#formula_0">(10)</ref> and <ref type="bibr" target="#b7">(8)</ref>.</p><formula xml:id="formula_12">L both = α * LASR→TTS + (1 − α) * LTTS→ASR<label>(13)</label></formula><p>where α is a hyper-parameter set by default to α = 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental Setup</head><p>An extensive experimental setup leveraging the Librispeech <ref type="bibr" target="#b24">[24]</ref> and Wall Street Journal (WSJ) <ref type="bibr" target="#b25">[25]</ref> corpora was used in this work. For each corpus, a sub-set of the data was used to simulate unpaired speech and text conditions.</p><p>Training utilized 83-dimensional filter-bank with pitch features as input. The encoder-decoder network utilized location aware attention <ref type="bibr" target="#b1">[2]</ref>. For WSJ and Librispeech experiments, the encoder comprises 8 bi-directional LSTM layers <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b26">26]</ref> each with 320 units and the decoder comprises 1 (uni-directional) LSTM layer with 320 units. The CE objective is optimized using AdaDelta <ref type="bibr" target="#b27">[27]</ref> with an initial learning rate set to 1.0. The training batch size is 30 and the number of training epochs is 20. The learning rate decay is based on the validation performance computed using the character error rate (min. edit distance). ESPnet <ref type="bibr" target="#b28">[28]</ref> is used to implement and execute all our experiments. For TTS, Tacotron2 <ref type="bibr" target="#b21">[21]</ref> model is used and is configured as described in our previous work <ref type="bibr" target="#b8">[9]</ref> differing only with the output targets. Here, we use 83 dimensional log-Mel filterbank features as targets and x-vectors <ref type="bibr" target="#b22">[22]</ref> are fed as auxiliary information to provide speaker characteristics of each utterance. Detailed experimental setup can be obtained from our github codebase in ESPnet. Unsupervised training was performed after conventional supervised training of each model. Cycle-consistency utilized five samples in Eq. 10. A small amount of paired data was also used to regularize the model during the unsupervised stage. For WSJ, 81 hours were split into 2, 5, 10 and 14 hours of paired data and the remaining 67 hours was used as unpaired data. <ref type="table" target="#tab_0">Table 1</ref> shows the effect of the amount of parallel data on WSJ and Librispeech using conventional supervised training regime. The eval92 test set was kept for evaluations. In Librispeech, the 100 hours is used as paired data and the 360 hours set as unpaired data as in <ref type="bibr" target="#b8">[9]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Analysis and Results</head><p>The initial experiments compared the performance of the TTS loss introduced in this work, to the TTE loss from previous work <ref type="bibr" target="#b8">[9]</ref>. The analysis involved Librispeech setup using 100 hours of paired data with 180 and 360 hours of unpaired data for a fair comparison. The results shows that TTS approach improves %WER over TTE by a 2.5% relative (20.7% to 20.1%) on 360 hours of unpaired data and 2.9% relative (19.9% to 19.4%) on 180 hours set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Impact of amount of paired data (semi-supervision)</head><p>In this section, the model is tested with varying amounts of unpaired data such as 14 hours and 67 hours along with certain amount of semi-supervision such as 2, 5, 10 and 14 hours of data.     <ref type="bibr" target="#b15">16</ref>.4%. The oracle result is our baseline performance using WSJ-SI284. Note that the performance on WSJ-SI284 is inferior to our previously reported baseline <ref type="bibr" target="#b29">[29]</ref>. This is due to a difference in architecture necessary to fit ASR and TTS models into the GPU. In case of Librispeech, the <ref type="table" target="#tab_3">table 3</ref> shows that training with unpaired audio and text data can achieve a %WER of 17.5 leading to 32.5% relative difference with the oracle performance. <ref type="table" target="#tab_3">Table 3</ref> also shows that our approach only using unpaired text gains 18.0% relative improvement over backtranslation-TTE <ref type="bibr" target="#b9">[10]</ref> approach. Complementary gains of absolute 0.9% were observed by integrating RNNLM with this approach and the result is compared with crictizing-LM <ref type="bibr" target="#b11">[12]</ref>. The effectiveness of our approach using unpaired speech only data is shown in table 3 by obtaining 16.8% WER. Jointly training unpaired speech and text provided modest gains with a %WER improvement from 16.8 to 16.6. From these results, one can infer that training with unpaired speech only data has major benefits over text only data in large corpus such as Librispeech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related Work</head><p>As indicated in the previous sections, the work here proposed improvements over recent related approaches and integrated some of them into a single loss. Back-translation style TTE <ref type="bibr" target="#b9">[10]</ref> synthesized encoder can be related to the TTS→ASR loss here used, as both utilize a pipeline that generates synthetic train data. The latter has the advantage of utilizing directly speech and has higher performance overall. The speech-chain framework <ref type="bibr" target="#b12">[13]</ref> was the first work to integrate ASR and TTS to train using unpaired speech and text. It also utilizes a TTS→ASR pipeline loss but this is not learnt jointly with the end-to-end differentiable ASR→TTS loss. One limitation of the speech-chain is the fact that is not end-to-end differentiable, having to rely in the alternate training of a TTS→ASR and ASR→TTS loss to update both models. This limitation was addressed in <ref type="bibr" target="#b8">[9]</ref> proposing an end-to-end differentiable loss for speech only and based on TTE. The work presented here improves upon this by extending TTE to TTS with x-vectors and combining this with a TTS→ASR point-estimate loss. Finally <ref type="bibr" target="#b7">[8]</ref> is similar to <ref type="bibr" target="#b8">[9]</ref> but applies straight-through estimators to construct end-to-end differentiable ASR→TTS losses based on argmax and a expected loss similar to Eq. 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>This paper presented a new approach to exploit the information in unpaired speech and/or unpaired text to improve the performance of seq2seq ASR systems. We show that under low-resource conditions such as WSJ corpus the performance improvements are relatively higher compared to corpus with sufficient amount of data such as Librispeech. We show as well that integrating unpaired speech and text, both as a pipeline loss and through shallow integration with a RNN language model, provides additional gains and competitive results. Future work will be focused on cycle-consistency approaches where ASR and TTS do not have matching conditions. Preliminary experiments show that this is a limitation of current systems.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Simplified representation of the cycle-consistency training for unpaired speech and text. The flow of the diagram is bottom-to-top. The red dotted lines denotes the gradient propagation and the updated parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Baseline</figDesc><table><row><cell cols="4">performance of paired data (semi-supervised)</cell></row><row><cell cols="4">across WSJ-SI84 and Librispeech by using pre-trained and</cell></row><row><cell cols="4">randomly initialized ASR model on eval-92 test for WSJ and</cell></row><row><cell cols="2">test-clean for Librispeech</cell><cell></cell><cell></cell></row><row><cell cols="3">Paired data Corpus info</cell><cell></cell></row><row><cell>Name</cell><cell># Hours</cell><cell>% CER</cell><cell>% WER</cell></row><row><cell></cell><cell>2</cell><cell>27.7</cell><cell>68.2</cell></row><row><cell>WSJ</cell><cell>5 10</cell><cell>13.2 10.8</cell><cell>41.5 33.7</cell></row><row><cell></cell><cell>14</cell><cell>10.2</cell><cell>31.5</cell></row><row><cell>Librispeech</cell><cell>100</cell><cell>8.9</cell><cell>21.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>, shows the %WER obtained by using varying amounts of unpaired speech and text data and different amounts of paired data. The experiments are done starting from 2 hours of parallel data and 14 hours of unpaired data. With only 2 hours of parallel data, the model %WER performance improves from 68.2 inTable 1to 49.8 by only aiding 14 hours of unpaired speech and improved to 51.9 with 67 hours of unpaired speech.Overfitting is observed in the case of unpaired speech, while in case of text only data, the model improved consistently from 63 (pair 14 hours) to 39.6 (pair 67 hours). This showcases the importance of text only data under very low-resource scenario. Interestingly, including both unpaired speech and text of 14 hours, the %WER improved to 43.7 and with 67 hours of data the model obtained 41.4 %WER. The pattern emerging here is that, under very low-resource scenario, the model benefits from large amounts of text. But, adding more speech only data leads to slight degradation in performance. This pattern is not observed with 5, 10 and 14 hours of parallel data conditions as increasing the amount of speech data from 14 hours to 67 hours improved by ∼1% absolute WER. With 14 hours of supervision and 67 hours of unpaired speech, text and both, the %WER improves to 28.0, 27.1 and 26.2 respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>%WER performance analysis on varying the amount of paired data (semi-supervision) and unpaired data on eval-92 test set using WSJ. The Type refers to type unpaired data used and it can be either "speech/text/both"</figDesc><table><row><cell cols="2">Unpaired data</cell><cell></cell><cell cols="2">Paired data (#hrs)</cell><cell></cell></row><row><cell>#hrs</cell><cell>Type</cell><cell>2</cell><cell>5</cell><cell>10</cell><cell>14</cell></row><row><cell>14</cell><cell>Speech</cell><cell>49.8</cell><cell cols="2">39.9 29.8</cell><cell>-</cell></row><row><cell>14</cell><cell>Text</cell><cell>63.0</cell><cell cols="2">43.6 34.6</cell><cell>-</cell></row><row><cell>14</cell><cell>Both</cell><cell>43.7</cell><cell cols="2">35.5 28.3</cell><cell>-</cell></row><row><cell>67</cell><cell>Speech</cell><cell>51.9</cell><cell cols="2">38.8 28.4</cell><cell>28.0</cell></row><row><cell>67</cell><cell>Text</cell><cell>39.6</cell><cell cols="2">36.8 29.6</cell><cell>27.1</cell></row><row><cell>67</cell><cell>Both</cell><cell>41.4</cell><cell cols="2">34.2 27.7</cell><cell>26.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Unsupervised</figDesc><table><row><cell cols="5">ASR performance across best results in</cell></row><row><cell cols="5">literature. The Type refers to type unpaired data used and it can</cell></row><row><cell cols="2">be either "speech/text/both"</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">WSJ-SI84 (parallel) + WSJ-SI284 (unpaired)</cell><cell></cell></row><row><cell>Model</cell><cell>Type</cell><cell cols="2">RNNLM %CER</cell><cell>%WER</cell></row><row><cell>Speech chain [13]</cell><cell>Both</cell><cell>-</cell><cell>9.9</cell><cell>-</cell></row><row><cell>Adversarial [14]</cell><cell>Both</cell><cell>yes</cell><cell>-</cell><cell>24.9</cell></row><row><cell>this work</cell><cell>Both</cell><cell>-</cell><cell>9.1</cell><cell>26.1</cell></row><row><cell>this work</cell><cell>Both</cell><cell>yes</cell><cell>7.8</cell><cell>20.3</cell></row><row><cell>oracle</cell><cell>-</cell><cell>-</cell><cell>5.5</cell><cell>16.4</cell></row><row><cell>oracle [29]</cell><cell>-</cell><cell>yes</cell><cell>2.0</cell><cell>4.8</cell></row><row><cell cols="4">Librispeech 100 h (parallel) + 360 h (unpaired)</cell><cell></cell></row><row><cell>Backtranslation-TTE [10]</cell><cell>Text</cell><cell>-</cell><cell>10.0</cell><cell>22.0</cell></row><row><cell>this work</cell><cell>Text</cell><cell>-</cell><cell>8.0</cell><cell>17.9</cell></row><row><cell>Crictizing-LM [12]</cell><cell>Text</cell><cell>yes</cell><cell>9.1</cell><cell>17.3</cell></row><row><cell>this work</cell><cell>Text</cell><cell>yes</cell><cell>8.0</cell><cell>17.0</cell></row><row><cell>Cycle-TTE [9]</cell><cell>Speech</cell><cell>yes</cell><cell>9.9</cell><cell>19.5</cell></row><row><cell>this work</cell><cell>Speech</cell><cell>yes</cell><cell>7.8</cell><cell>16.8</cell></row><row><cell>Adversarial-AE [15]</cell><cell>Both</cell><cell>yes</cell><cell>8.4</cell><cell>18.0</cell></row><row><cell>this work</cell><cell>Both</cell><cell>-</cell><cell>7.6</cell><cell>17.5</cell></row><row><cell>this work</cell><cell>Both</cell><cell>yes</cell><cell>7.6</cell><cell>16.6</cell></row><row><cell>oracle [9]</cell><cell>-</cell><cell>-</cell><cell>4.6</cell><cell>11.8</cell></row><row><cell cols="3">4.2. Model comparison with literature</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 ,</head><label>3</label><figDesc>shows the results of using both unpaired speech and text data from WSJ-SI284 and Librispeech 360 hours across literature. In WSJ corpus, the model achieves a %WER of 20.3 with RNNLM and 26.1 without RNNLM. This leaves a relative difference of 37.1% compared to oracle performance of</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards end-to-end speech recognition with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1764" to="1772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">End-to-end attention-based large vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="4945" to="4949" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Listen, attend and spell: A neural network for large vocabulary conversational speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="4960" to="4964" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep speech 2: End-to-end speech recognition in english and mandarin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Anubhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A comparison of sequence-to-sequence models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="939" to="943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">End-to-end feedback loss in speech chain framework via straight-through estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tjandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sakti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nakamura</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.13107</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cycle-consistency training for end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Astudillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Back-translation-style data augmentation for end-to-end asr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Toda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Astudillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Takeda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in SLT</title>
		<imprint>
			<biblScope unit="page" from="426" to="433" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-modal data augmentation for end-to-end asr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Renduchintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wiesner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in Interspeech</title>
		<imprint>
			<biblScope unit="page" from="2394" to="2398" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adversarial training of end-to-end speech recognition using a criticizing language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Listening while speaking: Speech chain by deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tjandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sakti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nakamura</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="301" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Combining end-to-end and adversarial training for low-resource speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Drexler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in SLT</title>
		<imprint>
			<biblScope unit="page" from="361" to="368" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sequence training of encoder-decoder model using policy gradient for end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delcroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nakatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICASSP</title>
		<imprint>
			<biblScope unit="page" from="5839" to="5843" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dual learning for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="820" to="828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Advances in joint ctc-attention based end-to-end speech recognition with a deep cnn encoder and rnn-lm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="949" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An analysis of incorporating an external language model into a sequence-to-sequence model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICASSP</title>
		<imprint>
			<biblScope unit="page" from="1" to="5828" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Natural tts synthesis by conditioning wavenet on mel spectrogram predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Skerrv-Ryan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICASSP</title>
		<imprint>
			<biblScope unit="page" from="4779" to="4783" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">X-vectors: Robust dnn embeddings for speaker recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garcia-Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICASSP</title>
		<imprint>
			<biblScope unit="page" from="5329" to="5333" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06709</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Librispeech: an asr corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="5206" to="5210" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The design for the Wall Street Journal-based CSR corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the workshop on Speech and Natural Language</title>
		<meeting>of the workshop on Speech and Natural Language</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page" from="357" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">ADADELTA: an adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<idno>abs/1212.5701</idno>
		<ptr target="http://arxiv.org/abs/1212.5701" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Espnet: End-to-end speech processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nishitoba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Unno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">Enrique</forename><surname>Yalta Soplin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heymann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wiesner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Renduchintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ochiai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in Interspeech</title>
		<imprint>
			<biblScope unit="page" from="2207" to="2211" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Promising accurate prefix boosting for sequence-to-sequence asr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Baskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Černockỳ</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.02770</idno>
		<ptr target="http://arxiv.org/ps/1905.01152v2" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>This figure &quot;chart_new.png&quot; is available in &quot;png</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">This figure &quot;tts.png&quot; is available in &quot;png</title>
		<ptr target="http://arxiv.org/ps/1905.01152v2" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
