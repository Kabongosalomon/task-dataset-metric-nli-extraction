<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pattern Recognition Letters Leveraging Recent Advances in Deep Learning for Audio-Visual Emotion Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Schoneveld</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Powder AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Othmani</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Université Paris-Est</orgName>
								<orgName type="institution" key="instit2">LISSI</orgName>
								<orgName type="institution" key="instit3">UPEC</orgName>
								<address>
									<postCode>94400</postCode>
									<settlement>Vitry sur Seine</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazem</forename><surname>Abdelkawy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Powder AI Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Université Paris-Est</orgName>
								<orgName type="institution" key="instit2">LISSI</orgName>
								<orgName type="institution" key="instit3">UPEC</orgName>
								<address>
									<postCode>94400</postCode>
									<settlement>Vitry sur Seine</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pattern Recognition Letters Leveraging Recent Advances in Deep Learning for Audio-Visual Emotion Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1 journal homepage: www.elsevier.com</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Emotional expressions are the behaviors that communicate our emotional state or attitude to others. They are expressed through verbal and non-verbal communication. Complex human behavior can be understood by studying physical features from multiple modalities; mainly facial, vocal and physical gestures. Recently, spontaneous multi-modal emotion recognition has been extensively studied for human behavior analysis. In this paper, we propose a new deep learning-based approach for audio-visual emotion recognition. Our approach leverages recent advances in deep learning like knowledge distillation and high-performing deep architectures. The deep feature representations of the audio and visual modalities are fused based on a model-level fusion strategy. A recurrent neural network is then used to capture the temporal dynamics. Our proposed approach substantially outperforms state-of-the-art approaches in predicting valence on the RECOLA dataset. Moreover, our proposed visual facial expression feature extraction network outperforms state-of-the-art results on the AffectNet and Google Facial Expression Comparison datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Darwin concluded through his observations and descriptions of human emotional expressions that emotions adapt to evolution, are biologically innate, and universal across all human and even non-human primates <ref type="bibr" target="#b15">(Matsumoto [2001]</ref>). Formal, systematic research studies have since been realized on the universality of emotions. This work demonstrated: (i) the universality of six basic emotions (anger, disgust, fear, happiness, sadness and surprise) and (ii) the cultural differences in spontaneous emotional expressions <ref type="bibr" target="#b5">(Ekman et al. [1987]</ref>).</p><p>A human's emotion resulting from an interaction with stimuli is referred to as an affect. In psychology, an affect refers to the mental counterparts of internal bodily representations associated with emotions. In fact, humans express affect through facial, vocal or gestural behaviors. The notion of affect is subjective, and in the literature it is represented by two alternative views: the categorical view where affects are represented as discrete states with a wide variety of affective displays and the dimensional view, where we suppose that affects might not be culturally universal and alternatively, should be represented in a continuous arousal-valence space.</p><p>Recently, a trend in the scientific community has emerged towards developing new technologies for processing, interpreting or simulating human emotions through Affective Computing or through Artificial Emotional Intelligence. Consequently, a broad range of applications have been developed in Human-Computer Interaction, health informatics and assistive technologies. Initial research on affect recognition focused mainly on unimodal approaches, with speech emotion recognition (SER) and facial expression recognition (FER) <ref type="bibr" target="#b25">(Rouast et al. [2019]</ref>) treated as separate problems. More recently however, work in affective computing has paid more attention to multimodal emotion recognition by developing approaches to multimodal data fusion.</p><p>Research on affect recognition has seen considerable progress as the focus has shifted from the study of laboratorycontrolled databases to databases covering real-world scenarios. In traditional emotion recognition databases, subjects posed a particular basic emotion in laboratory-controlled conditions. In more recent databases, videos are obtained from reallife scenarios with in-the-wild environmental conditions and less constrained settings, which exhibit characteristics like illumination variation, noise, occlusion, non-frontal head poses, and so on. Today, automatic emotion recognition of the six basic emotions in acted visual and/or audio expressions can be performed with high accuracy. However, in-the-wild emotion recognition is a more challenging problem due to the fact that spontaneously occurring behavior varies more widely in its audio profile, visual aspects, and timing.</p><p>In the present era, deep learning-based approaches are revolutionizing many areas of technology. Automatic emotion recognition likewise can benefit from the effectiveness of deep learning. In this paper, we propose a new approach for audiovisual emotion recognition (AVER). This approach is based on pre-training separate audio and visual deep convolutional neural network (CNN) recognition modules. A fusion module is then trained on the specific audio-visual dataset of interest. The fusion module is trained with the combination of generic emotion recognition features extracted by our pre-trained audio and visual components. The remainder of the paper is organized as follows. Section 2 reviews the literature on AVER and presents the contributions of our paper. Section 3 describes the proposed approach. Section 4 presents the experiments then reports and discusses the results. Finally, Section 5 concludes the paper and suggests future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works and paper contributions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Related Work</head><p>Multimodal fusion for emotion recognition concerns the family of machine learning approaches that integrate information from multiple modalities in order to predict an outcome measure. Such is usually either a class with a discrete value (e.g., happy vs. sad), or a continuous value (e.g., the level of arousal/valence). Several literature review papers survey existing approaches for multimodal emotion recognition <ref type="bibr" target="#b25">(Rouast et al. [2019]</ref>; <ref type="bibr" target="#b1">Baltrušaitis et al. [2018]</ref>; <ref type="bibr" target="#b31">Zeng et al. [2008]</ref>; <ref type="bibr" target="#b21">Poria et al. [2017]</ref>). There are three key aspects to any multimodal fusion approach: (i) which features to extract, (ii) how to fuse the features, and (iii) how to capture the temporal dynamics. Extracted features: several handcrafted features have been designed for AVER. These low-level descriptors concern mainly geometric features like facial landmarks. Meanwhile, commonly-used audio signal features include spectral, cepstral, prosodic, and voice quality features. Recently, deep neural network-based features have become more popular for AVER. These deep learning-based approaches fall into two main categories. In the first, several handcrafted features are extracted from the video and audio signals and then fed to the deep neural network <ref type="bibr" target="#b23">(Ringeval et al. [2015]</ref>; <ref type="bibr" target="#b9">He et al. [2015]</ref>; <ref type="bibr" target="#b19">Othmani et al. [2019]</ref>; <ref type="bibr" target="#b22">Rejaibi et al. [2019]</ref>; <ref type="bibr" target="#b18">Muzammel et al. [2020]</ref>). In the second category, raw visual and audio signals are fed to the deep network <ref type="bibr" target="#b28">(Tzarakis et al. [2017]</ref>; <ref type="bibr" target="#b29">Tzirakis et al. [2018]</ref>; <ref type="bibr" target="#b3">Basnet et al. [2019]</ref>). Deep convolutional neural networks (CNNs) have been observed as outperforming other AVER methods <ref type="bibr" target="#b25">(Rouast et al. [2019]</ref>). Multimodal features fusion: An important consideration in multimodal emotion recognition concerns the way in which the audio and visual features are fused together. Four types of strategy are reported in the literature: feature-level fusion, decisionlevel fusion, hybrid fusion and model-level fusion ; <ref type="bibr" target="#b21">Poria et al. [2017]</ref>). Feature-level fusion also called early-fusion concerns approaches where features are immediately integrated after extraction via simple concatenation into a single highdimensional feature vector. Such is the most common strategy for multimodal emotion recognition. Decision-level fusion or late fusion concerns approaches that perform fusion after an independent prediction is made by a separate model for each modality. In the audio-visual case, this typically means taking the predictions from an audio-only model, and the prediction from a visual-only model, and applying an algebraic combination rule of the multiple predicted class labels such as 'min', 'sum', and so on. Score-level fusion is a subfamily of the decision-level family that employs an equally weighted summation of the individual unimodal predictors. Hybrid fusion combines outputs from early fusion and from individual classification scores of each modality. Model-level fusion aims to learn a joint representation of the multiple input modalities by first concatenating the input feature representations, and then passing these through a model that computes a learned, internal representation prior to making its prediction. In this family of approaches, multiple kernel learning <ref type="bibr" target="#b4">(Chen et al. [2014]</ref>), and graphical models <ref type="bibr" target="#b1">(Baltrušaitis et al. [2018</ref><ref type="bibr" target="#b2">(Baltrušaitis et al. [ , 2013</ref>) have been studied, in addition to neural network-based approaches. Modelling temporal dynamics: audio-visual data represents a dynamic set of signals across both spatial and temporal dimensions. <ref type="bibr" target="#b25">Rouast et al. [2019]</ref> identify three distinct methods by which deep learning is typically used to model these signals: Spatial feature representations: concerns learning features from individual images or very short image sequences, or from short periods of audio. Temporal feature representations: where sequences of audio or image inputs serve as the model's input. It has been demonstrated that deep neural networks and especially recurrent neural networks are capable of capturing the temporal dynamics of such sequences <ref type="bibr" target="#b13">(Kim et al. [2017]</ref>). Joint feature representations: in these approaches, the features from unimodal approaches are combined. Once features are extracted from multiple modalities at multiple time points, they are fused using one of strategies of modality fusion <ref type="bibr" target="#b23">(Ringeval et al. [2015]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Contributions of this work</head><p>In this work, a higher-performing deep neural network-based approach for AVER is presented. The proposed model is a fusion of two deep neural networks: (i) a deep CNN model, trained with knowledge distillation, for FER and (ii) a modified and fine-tuned VGGish model for SER. A model-level fusion based approach is employed to fuse the audio and the visual feature representations. To model the temporal dynamics, the spatial and temporal representations are processed using recurrent neural networks. The contributions of this work can be summarized as follows:</p><p>• A new high-performing deep neural network-based approach for AudioVisual Emotion Recognition (AVER)</p><p>• Learning two independent feature extractors -one for audio and one for face images -that are specialised for emotion recognition, and that could be employed for any downstream audiovisual emotion recognition task or dataset</p><p>• Applying knowledge distillation (specifically, selfdistillation), alongside additional unlabeled data for FER</p><p>• Learning the spatio-temporal dynamics via a recurrent neural network for AVER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed multimodal deep CNN architecture</head><p>Our proposed multimodal deep CNN architecture is made up of three components:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Visual facial expression embedding network</head><p>The first component of our multimodal architecture is a deep convolutional neural network (CNN) for facial expression recognition. The input to this network is a single RGB face image, detected and cropped using Multi-Task Cascaded Convolutional Networks (MTCNN) <ref type="bibr" target="#b32">(Zhang et al. [2016]</ref>). The output of this network is a compact vector of dimension D face .</p><p>We refer to this network as our 'facial expression embedding network', and we train it using knowledge distillation (Hinton et al. <ref type="bibr">[2014]</ref>). Knowledge distillation is a two step process whereby a teacher network is trained on the task of interest, and then a (typically smaller) student network is trained on predictions made by the teacher. Specifically in this work, we leverage the benefits of self-distillation, whereby the student network has the same size as (or at least, is not smaller than) the teacher network. Self-distillation was recently leveraged to achieve state-of-the-art results on the well-known Imagenet classification dataset <ref type="bibr" target="#b30">(Xie et al. [2020]</ref>). It has also been shown theoretically that self-distillation can improve a model's performance via a regularization effect <ref type="bibr" target="#b16">(Mobahi et al. [2020]</ref>). We use self-distillation to improve the performance of our facial expression embedding network. The training procedure for this network thus consists of two phases:</p><p>1. Training a teacher model: our teacher model is a finetuned FaceNet <ref type="bibr" target="#b26">(Schroff et al. [2015]</ref>), trained simultaneously on two different visual facial expression recognition datasets (Section 3.1.1). 2. Training a student model: a second CNN is additionally trained to mimic the outputs of this fine-tuned FaceNet (Section 3.1.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">The teacher network</head><p>The starting point for our teacher model is a pre-trained FaceNet <ref type="bibr" target="#b26">(Schroff et al. [2015]</ref>). This model is then trained to learn specialised features for facial emotion recognition using two datasets:</p><p>• AffectNet <ref type="bibr" target="#b17">(Mollahosseini et al. [2019]</ref>), which consists of around 440,000 in-the-wild face crop images, each of which is human-annotated into one of eight facial expression categories (Neutral, Happy, Sad, Surprise, Fear, Disgust, Anger and Contempt).</p><p>• Google Facial Expression Comparison (FEC) (Agarwala and Vemulapalli <ref type="bibr">[2019]</ref>), which consists of around 700,000 triplets of unique face crop images. Annotations denoting the most similar pair of face expressions in each triplet are provided. The goal is to train a model that places the similar pair closer together in a learned embedding space.</p><p>Our teacher model's architecture <ref type="figure">(Fig. 1)</ref> is almost identical to the model proposed in <ref type="bibr" target="#b0">Agarwala and Vemulapalli [2019]</ref>, the only difference is that we add an additional output head for the AffectNet loss. A pre-trained FaceNet 1 is taken up until the Inception 4e block. This is followed by a 1x1 convolution and a series of five untrained DenseNet ) blocks. After this, another 1x1 convolution followed by global average pooling reduces this representation to a single D face dimensional vector. After pooling, two independent linear transformations serve as output heads. These heads take the D facedimensional facial expression representation vector as input and make separate predictions for the AffectNet and FEC tasks. A 32-dimensional embedding is used for the FEC triplets task, while an 8-dimensional output head produces class logits for AffectNet (which has 8 classes). The teacher network training procedure is detailed in Algorithm 1, while implementation details are provided in the supplementary materials.</p><p>To improve the regularization effects of self-distillation through model ensembling, we in fact train two teacher networks, and concatenate their outputs to serve as distillation targets (see Section 3.1.2 for details). The only difference between the two teachers are the random seeds used for initialization, and penultimate layer dimensionalities: we use D face = 128 for one teacher network, and D face = 256 for the other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Student network</head><p>Our student network is a DenseNet201 pre-trained on Imagenet. <ref type="bibr">2</ref> The student network training procedure is essentially the same as described in Algorithm 1, except that we additionally sample batches of unlabeled data from an internal dataset, which we refer to as PowderFaces. The Pow-derFaces dataset was created by downloading approximately 20,000 short, publicly-available videos from various online sources such as YouTube. To increase the frequency of faces in the dataset, specific search terms and topics were used when searching for videos, such as 'podcast', 'interview', or 'monologue'. MTCNN face detection was then applied to the extracted frames from those videos, producing approximately 1 million individual face crops. The sampled batches of face crops from the Google FEC, AffectNet, and Powder-Faces datasets are passed through our two teacher networks. Each of the two teacher networks produces predictions for the Google FEC task (32-dimensional) and AffectNet class logits <ref type="bibr">(140,</ref><ref type="bibr">140,</ref><ref type="bibr">3)</ref> Input face-cropped Image Inception-Resnet-V1 up to 4e Block </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FEC-Triplets Loss</head><p>AffectNet Classification Loss <ref type="figure">Fig. 1</ref>. Our facial expression recognition neural network architecture, before distillation (i.e. the teacher network). Faces are detected and cropped using MTCNN. The resulting 140x140 RGB images are then fed to an Inception Resnet V1 until the Inception 4e block. This is followed by a 1x1 convolution layer (1x1 Conv), batch normalization (BN) and a ReLU activation. Then, five DenseNet blocks are applied. Finally, another set of 1x1 Conv, BN and ReLU is applied. The output is then averaged over the spatial dimensions, resulting in a vector of size D face (in the figure, D face = 128). Two separate linear layers then give us the final model outputs -a vector for the Google FEC triplets task, and class logits for AffectNet. The model is trained to minimize both the AffectNet and Google FEC losses simultaneously. The numbers over each block represent the tensor's output shape after applying that block.   <ref type="figure">Fig. 3</ref>. Our audio-visual fusion network architecture. The audio and the visual embedding vectors are each fed to a small, independent convolutional network. This results in one tensor of size (9, 64) for each modality. We concatenate these two to give a tensor of size (9, 128), which is fed to a two-layer LSTM network with dimensionality of 256. Taking the final time step's output of this LSTM gives a single vector of size 256, which is pass through a single fully-connected layer with two outputs, which after a tanh activation gives our predictions between -1 and 1 for arousal and valance.</p><p>(8-dimensional). These four vectors (i.e., two vectors from two teacher networks) are individually L2-normalized The four normalized vectors are then concatenated, producing one long vector of dimension 80. A knowledge distillation loss (we specifically use 'Relational Knowledge Distillation' <ref type="bibr" target="#b20">(Park et al. [2019]</ref>) for our loss function) is then calculated by comparing the output of a third output head in the student network to this 80-dimensional target vector. This knowledge distillation loss is then added to the standard AffectNet and Google FEC losses, which are calculated as per the teacher network training procedure. Implementation details for our student network are provided in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Audio embedding network for emotion recognition</head><p>This section details our proposed deep learning-based approach for recognizing emotions from audio segments. The approach is based on fine-tuning the VGGish model <ref type="bibr" target="#b10">(Hershey et al. [2017]</ref>) on the RECOLA dataset <ref type="bibr" target="#b24">(Ringeval et al. [2013]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Audio pre-processing</head><p>For each input audio file, a set of Mel-spectrogram representations (R) are created. The input audio files are down-sampled at a 16KHz sampling rate. Then, the short-time Fourier transform (STFT) is performed to create windows with length (l) of 40 milliseconds and a hop length of 40 milliseconds. To create the latter windows, a set of 128 Mel filters (M f ) are applied with a Mel frequency range of 125-7500 Hz. Finally, for each audio file, a tensor of shape [R, l, M f ] is generated to create a compatible pre-processed data input for the VGGish backbone network.</p><p>Algorithm 1: Visual model: training the teacher network. Given feature extractor network f Θ , Google FEC output head g φ , AffectNet output head h θ , number of training steps N, AffectNet loss weight α.</p><p>for iteration in range(N) do (X FEC , y FEC ) ← batch of Google FEC triplets and labels (X Aff , y Aff ) ← batch of AffectNet images and class labels e FEC ← f Θ (X FEC )</p><p>Face embeddings for FEC images e Aff ← f Θ (X Aff )</p><p>Face embeddings for AffectNet</p><formula xml:id="formula_1">images v FEC ← g φ (e FEC )</formula><p>Predict vectors for triplet loss p Aff ← h θ (e Aff ) Predict class probabilities for AffectNet</p><formula xml:id="formula_2">L FEC = triplet_loss(v FEC , y FEC ) L Aff = cross_entropy_loss(p Aff , y Aff ) L = L FEC + α * L Aff</formula><p>Total loss for training step Obtain all gradients ∆ all = ( ∂L ∂Θ , ∂L ∂φ , ∂L ∂θ ) (Θ, φ, θ) ← SGD(∆ all ) Update feature extractor and output heads' parameters simultaneously end</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">VGGish backbone network</head><p>Our deep model for audio-based emotion recognition is based on a modified version of the VGGish model <ref type="bibr" target="#b10">(Hershey et al. [2017]</ref>). Our starting point is the original VGGish model, pre-trained on the Audio Set dataset ). The VGGish backbone consists of 6 convolutional layers that output 64, 128, 256, and 512 feature maps ( f ) respectively. For each convolution layer, a kernel (k) with size 3x3, and stride (s) of 1x1 is used. A max pooling layer with a kernel (k) of size 2x2, and stride (s) 2x2 is then applied. We take this VGGish backbone, but replace its last convolution and max pooling layers with a global average pooling layer. The resulting model produces an output vector of dimension 256. After this, we add three randomly-initialized, fully-connected layers, with output dimensionalities of 4096, 4096, and 128. These layer sizes were chosen to mimic the fully-connected penultimate layers of the original VGGish architecture. The aim of these layers is to extract a standard embedding vector with size of 128 that reflects the emotional characteristics of the input audio segment.</p><p>We take this expanded VGGish backbone architecture and fine-tune it end-to-end on the RECOLA dataset. Two separate VGGish networks are fine-tuned: one to predict arousal and the other to predict valence. We pass inputs of size <ref type="bibr">[480,</ref><ref type="bibr">128]</ref> to the VGGish model, which are the mel-spectrogram representations of 30 seconds of audio from one of the videos in the RECOLA dataset. The target used for fine tuning is then the average ground truth arousal or valence for the target values corresponding to the input 30 seconds of audio. We predict this target by passing the 128-dimensional audio representation through a fully-connected layer f φ with a tanh activation. The training procedure for fine tuning our audio feature extraction model is detailed in Algorithm 2.</p><p>Algorithm 2: VGGish fine-tuning algorithm for predicting arousal. Given the VGGish feature extractor network f Θ , arousal prediction head f φ , number of training steps N.</p><p>for iteration in range(N) do (X, y) ← batch of RECOLA spectrograms and targets e ← f Θ (X) Calculate VGGish embeddings for batch p ← f θ (e) Predict arousal for all elements in batch Loss = −concordance_correlation_coeff(p, y) Obtain all gradients ∆ all = ( ∂Loss ∂Θ , ∂Loss ∂θ ) (Θ, θ) ← Adam(∆ all ) Update VGGish model, output head end</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Audio-visual fusion model</head><p>A model-level fusion based approach is considered as shown in <ref type="figure">Fig. 3</ref>. Visual features are extracted by taking face crops from the video sequence of interest using MTCNN and passing them to our student network (Section 3.1.2). Audio features are extracted using our fine-tuned VGGish backbone (the temporal granularity of these features is increased by removing the global average pooling over the temporal dimension from our fine-tuned VGGish model). To have the same reduced size, audio and visual features are passed through small, independent pre-transform networks consisting of 1D convolutions, where the convolution filters slide over the temporal dimension. The pre-transform networks are designed to give an output tensor of shape <ref type="bibr">[9,</ref><ref type="bibr">64]</ref>. These are concatenated into a single audio-visual features tensor, which is passed to a two-layer LSTM network with hidden and output dimensionality of 256. The final output of the LSTM network is then taken, and passed to a simple linear transform with two outputs. These outputs are passed through a tanh activation to produce the final arousal and valence predictions. Our loss function is the negative CCC. Further implementation details for our fusion network are provided in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>The performances of the proposed approach have been evaluated using the REmote COLlaborative and affective interactions (RECOLA) corpus <ref type="bibr" target="#b24">(Ringeval et al. [2013]</ref>). In RECOLA, participants' spontaneous interactions were collected while being engaged in a remote discussion that aimed to manipulate their moods. Then, six annotators measured the emotional state present in all sequences continuously on the valence and arousal dimensions. 27 audio-visual recordings of 5 minutes of interaction, which includes 9 videos for training and 9 for validation, are publicly available. In order to perform a fair comparison, the test set annotations (for the last 9 videos) of the AVEC challenge are not given. We use the training videos to train our models, validate on the validation videos, and submitted our results on the test set to the RECOLA dataset managers for evaluation. We also evaluate our visual feature extraction network on held-out evaluation data from the AffectNet and Google FEC datasets, which were introduced in Section 3.1.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation metric</head><p>The Concordance Coefficient Correlation (CCC) <ref type="bibr" target="#b14">(Lawrence and Lin [1989]</ref>) is used to evaluate the performance of the proposed approach on RECOLA, as it is standard metric for emotion recognition on the RECOLA dataset. The CCC (Equation 1) measures the agreement between a vector of predicted (Pred) and true (T rue) values for a continuous variable:</p><formula xml:id="formula_3">CCC(T rue, Pred) = 2 * Corr(T rue, Pred) * σ T rue * σ Pred σ 2 T rue + σ 2 Pred + (µ T rue − µ Pred ) 2</formula><p>(1) Where µ x is the mean of x, σ x is the standard deviation of x, and Corr(x, y) returns Pearson's correlation coefficient between x and y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Visual facial expression embedding network performance</head><p>We evaluate our visual facial expression embedding network on the standard evaluation subsets of the two datasets it was trained on: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Accuracy <ref type="bibr" target="#b7">Georgescu et al. [2019]</ref> 59.6% <ref type="bibr" target="#b27">Siqueira et al. [2020]</ref> 59.3% Ours <ref type="table">(Teacher model)</ref> 61.3% Ours <ref type="bibr">(Student, no distillation)</ref> 58.8% Ours (Distilled student, no PowderFaces) 61.1% Ours (Distilled student) 61.6% [2019], we evaluate using triplet accuracy on the Google FEC test set. Using this metric, we find our approach substantially improves on state-of-the-art on the FEC test set, with an accuracy of 86.5% <ref type="table" target="#tab_3">(Table 2)</ref>.</p><p>To experimentally verify the importance of the different components of our approach, we perform an ablation study. Training the student model architecture without the distillation loss component reveals the importance of distillation. Without distillation, our model's performance dropped substantially: to 58.8% on AffectNet and 85% on Google FEC.</p><p>Similarly, to determine the importance of the unlabeled Pow-derFaces dataset, we again train the student model with distillation, but without the additional distillation targets provided by using this unlabeled data. The results suggest that the additional unlabeled data may not be so important to our results: accuracy on AffectNet dropped only slightly to 61.1%, and performance on Google FEC reduced by only 0.1% to 86.4%. We discuss these ablation results further in Section 4.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Performance on RECOLA in the visual-only and audioonly context</head><p>To test performance on RECOLA of our visual and audio feature extractors separately, we retrain the same fusion architecture, but disable either the audio or visual feature inputs. Visual-only: feeding the embeddings from our visual feature extractor into the visual-only version of our fusion model performs well on the RECOLA dataset <ref type="table">(Table 3)</ref>. Such reaches a CCC of 0.55 for predicting valence and 0.57 for predicting arousal on the validation set, while on the test set our CCC reaches 0.66 for valence and 0.57 for arousal. This result illustrates the robustness of our visual feature extractor: when predicting valence, our method achieves state-of-the-art performance when compared to other multimodal approaches, even though we use only visual features as input. Audio-only: our results <ref type="table">(Table 3)</ref> show that our modified VG-Gish backbone feature extractor for audio segments performs well, CCCs of 0.52 and 0.70 for valence and arousal, respectively on the RECOLA test set. The achieved results for arousal prediction match the existing state-of-the-art methods when only audio features are used <ref type="table">(Table 4</ref>). This shows that our approach to transfer learning from the acoustic events domain, and the VGGish architecture, provide a robust means to extracting audio-based features for emotion recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Fusion model performance and comparison with state-ofthe-art approaches</head><p>Our multimodal fusion model achieves a CCC of 0.740 in valence prediction, and 0.719 in arousal prediction in RECOLA test set <ref type="table" target="#tab_5">(Table 5</ref>). These results reflect the robustness of our learned visual and audio features extraction techniques, and the efficacy of the approach to fusing these modalities and accounting for temporal dynamics. Our proposed method, with a CCC of 0.740, substantially outperforms all existing methods in predicting valence, with the previous best performing approach of <ref type="bibr" target="#b28">Tzarakis et al. [2017]</ref> achieving a CCC of 0.612. Simultaneously, our approach achieves strong results in predicting arousal with a CCC of 0.719, compared to the state-of-the-art performance of <ref type="bibr" target="#b23">Ringeval et al. [2015]</ref>, who obtained a CCC of 0.796.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Discussion</head><p>The results in Section 4.3 suggest that the unlabeled Powder-Faces dataset only provides a marginal benefit in performance, if any at all. This ran contrary to our expectations: we expected unlabeled data to help in this context, given a similar approach achieved state-of-the-art results on Imagenet <ref type="bibr" target="#b30">(Xie et al. [2020]</ref>). In that work however, the unlabeled dataset consisted of 300 million images -300 times more than the Imagenet dataset itself. In our case, our unlabeled dataset of one million images is only about twice the size of the number of faces in AffectNet and Google FEC combined. Thus, to truly conclude whether the benefits of unlabeled data as shown on Imagenet transfer well to facial expressions, many more unlabeled images are needed. We plan to address this point in future work.</p><p>On the other hand, it appears that self-distillation is indeed beneficial in the facial expressions domain. This is illustrated by the marked improvement in our accuracy on the Google FEC dataset when distillation is applied, compared to training without distillation. For AffectNet, the benefits of distillation are less pronounced, and it seems that the combination of using a pre-trained FaceNet model, plus training on Google FEC at the same time as AffectNet are all necessary components to achieving our results.</p><p>Finally, our results for valence on the RECOLA test set show a dramatic improvement over existing state-of-the-art approaches <ref type="table" target="#tab_5">(Table 5</ref>). As mentioned, our visual-only model for RECOLA also achieved state-of-the-art for valence <ref type="table">(Table 3)</ref>. This provides further evidence for the effectiveness of our visual feature extraction approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future work</head><p>This paper introduces a high-performing deep neural network-based approach for AVER that fuses a distilled visual feature extractor network with a modified VGGish backbone and a model-level fusion architecture. The proposed visual facial expression embedding network shows that end-toend training on both AffectNet and FEC in tandem is a highly effective method for learning robust facial expression representations. We have also demonstrated that knowledge distillation can provide further improvements for facial expression recognition. The performance of our modified VGGish backbone feature extractor presents a promising new direction for predicting emotion from audio. Moreover, our deep neural network approach to multimodal fusion has been shown to be effective in AVER, outperforming the state-of-art methods in predicting valence on the RECOLA dataset. For future work, we plan to investigate the best strategy for continuous emotion encoding: classification with coarse categories, regression, label distribution learning or even ranking. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Modified VGGish backbone feature extractor for Speech Emotion Recognition. The Mel-Spectogram is computed from the audio signal and then fed to the modified VGGish backbone network consisting of 6 convolutional layers followed by 3 fully connected layers of size (4096, 4096 and 128) to output an embedding vector of size 128. The size of the feature maps (f ) of each convolutional and fully-connected layer are shown above each block of operations. The kernel size (k) and stride (s) are specified below the convolution blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Performances of the proposed visual facial expression embedding network on the AffectNet validation set comparing to existing state-of-theart methods</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="2">. Triplet prediction performances of the proposed visual facial ex-</cell></row><row><cell cols="2">pression embedding network on the Google FEC test set comparing to ex-</cell></row><row><cell>isting state-of-the-art methods</cell><cell></cell></row><row><cell>Methods</cell><cell>Accuracy</cell></row><row><cell>Agarwala and Vemulapalli [2019]</cell><cell>81.8%</cell></row><row><cell>Ours (Teacher model)</cell><cell>84.5%</cell></row><row><cell>Ours (Student, no distillation)</cell><cell>85.0%</cell></row><row><cell cols="2">Ours (Distilled student, no PowderFaces) 86.4%</cell></row><row><cell>Ours (Distilled student)</cell><cell>86.5%</cell></row><row><cell cols="2">1. AffectNet: for AffectNet, which requires classifying faces</cell></row><row><cell cols="2">into eight discrete facial expression classes, we train a lo-</cell></row><row><cell cols="2">gistic regression model on the features extracted by our</cell></row><row><cell cols="2">student network for the entire AffectNet training set. 3 This</cell></row><row><cell cols="2">method achieves state-of-the-art results on the AffectNet</cell></row><row><cell cols="2">validation set, with an accuracy of 61.6% (Table 1).</cell></row><row><cell cols="2">2. Google FEC: following Agarwala and Vemulapalli</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>RECOLA dataset results (in terms of CCC) for predicting arousal and valence on train, development and test sets. Performances of the proposed audio embedding network on the RECOLA dataset comparing to existing state-of-the-art methods. In parenthesis are the performances obtained in the development set. --: no results reported in the original papers.</figDesc><table><row><cell></cell><cell>Valence</cell><cell></cell><cell></cell><cell>Arousal</cell><cell></cell><cell></cell></row><row><cell>CCC</cell><cell>Train</cell><cell cols="3">Dev Test Train</cell><cell cols="2">Dev Test</cell></row><row><cell>Visual only</cell><cell>.6</cell><cell>.55</cell><cell>.66</cell><cell>.49</cell><cell>.57</cell><cell>.57</cell></row><row><cell>Audio only</cell><cell>.55</cell><cell>.46</cell><cell>.52</cell><cell>.78</cell><cell>.80</cell><cell>.70</cell></row><row><cell cols="2">Audio-visual .69</cell><cell>.63</cell><cell>.74</cell><cell>.78</cell><cell>.81</cell><cell>.72</cell></row><row><cell>Methods</cell><cell></cell><cell>Arousal</cell><cell></cell><cell>Valence</cell><cell></cell><cell></cell></row><row><cell cols="3">Tzarakis et al. [2017] .70 (.75)</cell><cell></cell><cell>.31 (.41)</cell><cell></cell><cell></cell></row><row><cell cols="2">Han et al. [2017]</cell><cell>.67 (.76)</cell><cell></cell><cell>.36 (.48)</cell><cell></cell><cell></cell></row><row><cell>He et al. [2015]</cell><cell></cell><cell cols="3">--(.80) --(.40)</cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell></cell><cell>.70 (.80)</cell><cell></cell><cell>.52 (.46)</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>RECOLA dataset results (in terms of CCC) for predicting arousal and valence. S.M.: Strength modeling of SVR + BLSM</figDesc><table><row><cell>Methods</cell><cell>Audio features</cell><cell cols="2">Visual features Modality fusion</cell><cell cols="2">Arousal Valence</cell></row><row><cell cols="2">Ringeval et al. [2015] LLDs + BLSTM</cell><cell>LLDs</cell><cell>Feature-level</cell><cell>.761</cell><cell>.492</cell></row><row><cell cols="2">Ringeval et al. [2015] LLDs + BLSTM</cell><cell>LLDs</cell><cell>Decision-level</cell><cell>.796</cell><cell>.501</cell></row><row><cell>He et al. [2015]</cell><cell>LLDs</cell><cell>LLDs</cell><cell>model-based (BLSTM)</cell><cell>.747</cell><cell>.609</cell></row><row><cell>Han et al. [2017]</cell><cell>LDDs+S.M.</cell><cell>Geom. + S.M.</cell><cell>modality and model-based</cell><cell>.685</cell><cell>.554</cell></row><row><cell cols="2">Tzarakis et al. [2017] 1D CNN</cell><cell>ResNet50</cell><cell>Model-level (2 LSTM)</cell><cell>.714</cell><cell>.612</cell></row><row><cell>Proposed</cell><cell cols="2">Fine-tuned VGGish Distilled CNN</cell><cell>Model-based (LSTM)</cell><cell>.719</cell><cell>.740</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In this, we used a FaceNet pre-trained on the VGGFace2 dataset, as we found performance to be slightly improved. The pre-trained FaceNet model architecture and weights were obtained from https://github.com/timesler/facenetpytorch.2  We use the implementation and pre-trained Imagenet weights provided in the torchvision Python package.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">When training this logistic regression, we re-weight the classes in the Af-fectNet training set to have equal representation, as per the validation set.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by Powder, a deep tech startup. Powder is a video editing and sharing platform for gamers. https://powder.gg/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Materials</head><p>Supplementary material associated with this article can be found in the enclosed file.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A compact embedding for facial expression similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5676" to="5685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Multimodal machine learning: A survey and taxonomy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baltrušaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaitanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Louis-Philippe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="423" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dimensional affect recognition using continuous conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baltrušaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ntombikayise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Peter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference and Workshops on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Estimation of affective dimensions using cnn-based features of audiovisual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Howlader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M M</forename><surname>Rahmanb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hatzinakos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page" from="290" to="297" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Emotion recognition in the wild with feature fusion and multiple kernel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Multimodal Interaction</title>
		<meeting>the 16th International Conference on Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="508" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Universals and cultural differences in the judgments of facial expressions of emotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">V</forename><surname>Friesen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>O&amp;apos;sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Diacoyanni-Tarlatzis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Heider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lecompte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ayhan Pitcairn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Ricci-Bitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tzavaras</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page">712</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Audio set: An ontology and humanlabeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Local learning with deep and handcrafted features for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="64827" to="64836" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Strength modelling for real-world automatic continuous affect recognition from audiovisual signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ringeval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="76" to="86" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multimodal affective dimension prediction using deep bidirectional long short-term memory recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sahli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge</title>
		<meeting>the 5th International Workshop on Audio/Visual Emotion Challenge</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="73" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Slaney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wilson</surname></persName>
		</author>
		<title level="m">Cnn architectures for large-scale audio classification. International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning Workshop NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-objective based spatio-temporal feature representation learning robust to expression intensity variations for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Baddar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Ro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="223" to="236" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A concordance correlation coefficient to evaluate reproducibility</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="page" from="255" to="268" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The handbook of culture and psychology. chapter Culture and emotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matsumoto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="171" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Self-distillation amplifies regularization in hilbert space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Farajtabar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05715</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Affectnet: A database for facial expression, valence, and arousal computing in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mollahosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="18" to="31" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Audvowelconsnet: A phoneme-level based deep cnn architecture for clinical depression diagnosis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Muzammel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Salam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chetouani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Othmani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning with Applications</title>
		<imprint>
			<biblScope unit="page">100005</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Othmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kadoch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bentounes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rejaibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Alfred</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.00310</idno>
		<title level="m">Towards robust deep neural networks for affect and depression recognition from speech</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Relational knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3967" to="3976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A review of affective computing: From unimodal analysis to multimodal fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bajpai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="98" to="125" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Mfccbased recurrent neural network for automatic clinical depression recognition and assessment from speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rejaibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Komaty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meriaudeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agrebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Othmani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.07208</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Prediction of asynchronous dimensional emotion ratings from audiovisual and physiological data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ringeval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kroubi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Thiran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lalanne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="22" to="30" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Introducing the recola multimodal corpus of remote collaborative and affective interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ringeval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sonderegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lalanne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th IEEE international conference and workshops on automatic face and gesture recognition (FG</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep learning for human affect recognition: Insights and new developments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Rouast</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raymond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Affective Computing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Efficient facial feature learning with wide ensemble-based convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Siqueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Magg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wermter</surname></persName>
		</author>
		<idno>ArXiv abs/2001.06338</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Endto-end multimodal emotion recognition using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tzarakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Björn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Stefanos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1301" to="1309" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">End-to-end speech emotion recognition using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tzirakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiehao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bjorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5089" to="5093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10687" to="10698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A survey of affect recognition methods: Audio, visual, and spontaneous expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">I</forename><surname>Roisman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="39" to="58" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="DOI">10.1109/LSP.2016.2603342</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning affective features with a hybrid deep model for audio-visual emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technology</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="3030" to="3043" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
