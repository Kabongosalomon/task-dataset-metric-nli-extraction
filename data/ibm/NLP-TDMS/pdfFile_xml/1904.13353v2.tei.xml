<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Object Contour and Edge Detection with RefineContourNet</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">André</forename><forename type="middle">Peter</forename><surname>Kelm</surname></persName>
							<email>andre.kelm@hsu-hamburg.de</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Udo</forename><surname>Zölzer</surname></persName>
							<email>udo.zoelzer@hsu-hamburg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Signal Processing and Communications</orgName>
								<orgName type="institution">Helmut Schmidt University</orgName>
								<address>
									<addrLine>Holstenhofweg 85</addrLine>
									<postCode>22043</postCode>
									<settlement>Hamburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Mechatronics</orgName>
								<orgName type="institution">Hamburg University of Technology</orgName>
								<address>
									<addrLine>Am Schwarzenberg-Campus 1</addrLine>
									<postCode>21073</postCode>
									<settlement>Hamburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Object Contour and Edge Detection with RefineContourNet</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Object Contour Detection · Edge Detection · Multi-Path Refinement CNN</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A ResNet-based multi-path refinement CNN is used for object contour detection. For this task, we prioritise the effective utilization of the high-level abstraction capability of a ResNet, which leads to state-of-the-art results for edge detection. Keeping our focus in mind, we fuse the high, mid and low-level features in that specific order, which differs from many other approaches. It uses the tensor with the highest-levelled features as the starting point to combine it layer-by-layer with features of a lower abstraction level until it reaches the lowest level. We train this network on a modified PASCAL VOC 2012 dataset for object contour detection and evaluate on a refined PASCAL-val dataset reaching an excellent performance and an Optimal Dataset Scale (ODS) of 0.752. Furthermore, by fine-training on the BSDS500 dataset we reach state-of-the-art results for edge-detection with an ODS of 0.824.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Object contour detection extracts information about the object shape in images. Reliable detectors distinguish between desired object contours and edges from the background. Resulting object contour maps are very useful for supporting and/or improving various computer vision applications, like semantic segmentation <ref type="bibr" target="#b10">[5,</ref><ref type="bibr" target="#b35">31,</ref><ref type="bibr" target="#b39">35]</ref>, object proposal <ref type="bibr" target="#b28">[24]</ref> and object flow estimation <ref type="bibr" target="#b29">[25,</ref><ref type="bibr" target="#b35">31]</ref>.</p><p>Holistically-Nested Edge Detection (HED) <ref type="bibr" target="#b36">[32]</ref> has shown that it is beneficial to use features of a pre-trained classification network to capture desired image boundaries and suppressing undesired edges. Khoreva et al. <ref type="bibr" target="#b17">[13]</ref> have specifically trained the HED on object contour detection and proven the potential of HED for this task. Yang et al. have used a Fully Convolutional Encoder-Decoder Network (CEDN) to produce contour maps, in which the object contours of certain object classes are highlighted and other edges are suppressed more effectively than before <ref type="bibr" target="#b38">[34]</ref>. Convolutional Oriented Boundaries (COB) <ref type="bibr" target="#b25">[21]</ref> outperforms these results by using multi-scale oriented contours derived from a HED-like network architecture together with an efficient hierarchical image segmentation algorithm. A common feature in all this work is that a Very Deep Convolutional Network for Large-Scale Image Recognition (VGG) <ref type="bibr" target="#b32">[28]</ref> and its classifying ability is used as a backbone network. It is obvious that this backbone and its effective use are the major keys to the results achieved, but the new methods mentioned here do not use the latest classification networks, like Deep Residual Learning for Image Recognition (ResNet) <ref type="bibr" target="#b16">[12]</ref>, which show a higher classification ability than VGG. We use a ResNet as backbone and propose a strategy to prioritise the effective utilization of the high-level abstraction capability for object contour detection. Accordingly we choose a fitting architecture and a customized training procedure. We outperform the methods mentioned previously and achieve a very robust detector with an excellent performance on the validation data of a refined PASCAL VOC [10]. High-level edge detection is closely related to object contour detection, because object contours are often an important subset of the desired detection. Continuing, we will introduce the edge detection task and show that, unlike object contour detection, there is unexploited potential for using the high abstraction capability of classification networks.</p><p>Edge detection has a rich history and -as one of the classic vision problems -plays a role in almost any vision pipeline with applications like optical flow estimation <ref type="bibr" target="#b21">[17]</ref>, image segmentation <ref type="bibr" target="#b12">[7]</ref> or generative image inpainting <ref type="bibr" target="#b26">[22,</ref><ref type="bibr" target="#b37">33]</ref>. Classical low-level detectors, such as Canny <ref type="bibr" target="#b11">[6]</ref> and Sobel <ref type="bibr" target="#b33">[29]</ref>, or the recently applied edge-detection with the Phase Stretch Transform <ref type="bibr" target="#b6">[2]</ref>, filter the entire image and do not distinguish between semantic edges and the rest. Edge detection is no longer limited only to low-level computer vision problems. Even the evaluation method established to date -the Berkeley Segmentation Dataset and the Benchmark 500 (BSDS500) <ref type="bibr" target="#b5">[1]</ref> -requires high-level image processing algorithms for good results. Before Convolutional Neural Networks (CNNs) became popular, algorithms like the gPb <ref type="bibr" target="#b5">[1]</ref>, which uses contour detection together with hierarchical image segmentation, reached impressive results. In recent years, edge detectors, such as DeepNet <ref type="bibr" target="#b18">[14]</ref> and N 4 -Fields <ref type="bibr" target="#b15">[11]</ref> have begun to use operations from CNNs to reach a higherlevel detection. DeepEdge <ref type="bibr" target="#b8">[3]</ref> and DeepContour <ref type="bibr" target="#b31">[27]</ref> are CNN applications that use more high-level features to extract contours, and show that this capability improves the detection of certain edges. HED uses higher abstraction abilities than previous methods by combining multi-scale features and multi-level features extracted of a pre-learned CNN, and improves edge detection. Latest edge detectors such as the Crisp Edge Detector (CED) <ref type="bibr" target="#b35">[31]</ref>, Richer Convolutional Features (RCF) <ref type="bibr" target="#b24">[20]</ref>, COB and a High-for-Low features algorithm (HFL) <ref type="bibr" target="#b9">[4]</ref> make use of their backbone classification nets for edge detection in different ways. But some of these networks are based on older backbone CNNs like the VGG and/or use simple HED-like skip-layer architectures. Similarly for object contour detection, we assume recent work has unexploited potential in the utilization of pre-trained classification abilities, in terms of architecture, backbone network, training procedure and datasets. We contribute a simple but decisive strategy, a network architecture choice following our strategy and unconventional training methods to reach state-of-the-art. Section 2 will briefly summarize the closest related work, Section 3 contains main contributions, like concept, realization and special training procedures for the proposed detector, Section 4 compares the method with other relevant methods and Section 5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>AlexNet <ref type="bibr" target="#b20">[16]</ref> was a breakthrough for image classification and was extended to solve other computer vision tasks, such as image segmentation, object contour, and edge detection. The step from image classification to image segmentation with the Fully Convolutional Network (FCN) <ref type="bibr" target="#b30">[26]</ref> has favored new edge detection algorithms such as HED, as it allows a pixel-wise classification of an image. HED has successfully used the logistic loss function for the edge or non-edge binary classification. Our approach uses the same loss function, but differs in term of another weighting factor, network architecture, and backbone network. Another image segmentation network, Learning Deconvolution Network for Semantic Segmentation <ref type="bibr" target="#b27">[23]</ref>, has favored the development of the CEDN, demonstrating the strong relationship between image segmentation, object contour detection and edge detection. The good results of the CEDN inspired us to consider recent image segmentation networks for our task. Yang et al. created a new contour dataset using a Conditional-Random-Fields (CRF) <ref type="bibr" target="#b19">[15]</ref> refining method. CEDN and edge detector networks such as COB and HFL have an older backbone net and are outperformed by RCF, which is based on a ResNet and improved the edge detection. RCF has the same backbone network, but differs from our approach in using a different network architecture because it uses a skip-layer structure for feature concatenation like HED. We state that this simple concatenation is not effective enough for edge detection, and we propose to use a more advanced network structure. We have the hypothesis that an effective network architecture for edge detection should prioritise the high abstraction capability itself. As the deepest feature maps are the next ones to the classification layer, we propose to use them as the starting point to refine them layer-by-layer with features of a lower level until it reaches the level of classical edge detection algorithms. Our required properties are combined in RefineNet <ref type="bibr" target="#b22">[18]</ref> and that is why we have used the publicly available code from Guosheng Lin et al. as the basis of our approach. Parallelly to the implementation of our method, the CED from the work Learning to Predict Crisp Boundaries from Deng et al. <ref type="bibr" target="#b13">[8]</ref> has used a similar bottom-up architecture and surpasses RCF and achieves state-of-the-art. The work from Wang et al. Deep Crisp Boundaries <ref type="bibr" target="#b35">[31]</ref> further develops this method and improved state-of-the-art results. Our approach mainly differs from theirs in its conceptualization. They also focus on producing "crisp" (thinned) boundaries, as they have shown that this benefits their results. We assume in contrast, that by focusing on the effective utilization of the high abstraction capability of a backbone network, we could achieve better results. <ref type="figure">Fig. 1</ref>. RefineContourNet -modified RefineNet <ref type="bibr" target="#b22">[18]</ref>, where last layers are changed, such that there is one featue map at the end and a sigmoid activation function that predicts the probability of presence of contour 3 RefineContourNet</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Concept</head><p>Detecting edges with classical low-level methods can visualize the high amount of edges in many images. To distinguish between meaningful edges and undesired edges, a semantic context is required. Our selected contexts are the object contours of the 20 classes of the PASCAL VOC dataset. If context is clear and some low-level vision functions are available, the most important ability for an object contour detector is the high-level abstraction capability, so that edges can be distinguished in the sense of context. For this reason, our concept focuses on the effective use of the high-level abstraction ability of a modern classification network for object contour detection. With this strategy, we choose the architecture, backbone network, training procedure and datasets.</p><p>We hypothesize that an effective edge detection network architecture should prioritize the above mentioned capability. For this we propose to give preference to the deepest feature maps of the backbone network and to use them as the starting point in a refinement architecture. To connect the high-level classification ability with the pixel-wise detection stage, we assume, that a step-by-step refinement, where deep features are fused with features of the next shallower level until the shallowest level is reached, should be more effective than skip-layers with a simple feature concatination architecture. In most classification networks, features of different abstraction levels have different resolutions. To merge these features, a multi-resolution fusion is necessary. The RefineNet from Lin et al. <ref type="bibr" target="#b22">[18]</ref> provides the desired multi-path refinement and we base our application upon that and name our application in reference to this network RefineContourNet (RCN).</p><p>The training procedure has to accomplish two main goals: To effectively use the pre-trained features to form a specific abstraction capability for identifying desired object contours learned from data on the one hand and to connect this to the pixel-wise detection stage on the other hand. Because training data of object contours is limited, both training goals can be enhanced with data augmentation methods. For a similar reason, we do some experiments with a modified Microsoft Common Objects in Context (COCO) <ref type="bibr" target="#b23">[19]</ref> dataset, to create an additional object contour dataset, usable for a pre-training. For fine-training on edge-detection, we offer a simple and unconventional training method that considers the individuality of BSDS500's hand-drawn labels. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Image Segmentation Network for Contour Detection</head><p>The main difference between an image segmentation network and a contour detection network lies in the definition of the objective function. Instead of defining a multi-label segmentation, an object contour can be defined binary. We use the logistic regression loss function</p><formula xml:id="formula_0">L(h Θ (x), y) = −y · β log(h Θ (x)) − (1 − y) · log(1 − h Θ (x)),<label>(1)</label></formula><p>with h Θ (x) ∈ [ 0, 1], y ∈ { 0, 1} and β = 10, where h Θ (x) is the prediction for a pixel x with the corresponding binary label y. Θ symbolizes the learned parameters and β is a weighting factor for enhancing the contour detection due to the large imbalance between the contour and the noncontour pixels. Changing the loss function results in a change of the last layer of the RefineNet according to the binary loss function. The 21 feature map layers previously used to segment 20 PASCAL-VOC classes, including background class, will be replaced by a single feature map sufficient for binary classification of contours. <ref type="figure">Figure 1</ref> shows the RefineContourNet. For clarity, the connections between the blocks specifies the resolution of the feature maps and the size of the feature channel dimension. The Residual Blocks (RB) are part of the ResNet-101. RefineNet has introduced three different refinement path blocks: The Residual Convolution Unit (RCU), the Multi-Resolution Fusion (MRF) and the Chained Residual Pooling (CRP). They are arranged in a row to use the higher-level features as input to combine them with the lower-level features of the RB at the same level. The RCU in <ref type="figure" target="#fig_0">Fig. 2 (a)</ref> has residual convolutional layers and enriches the network with more parameters. It can adjust and modify the input for MRF. The MRF block adapts the input first by performing a convolution operation, in order to adjust the channel dimension of the feature space corresponding to the higher-level ones with the lower level. Then, the smaller resolution feature maps get upsampled to have same tensor dimensions as the larger ones, after which they are added, as shown in <ref type="figure" target="#fig_0">Fig.  2 (b)</ref>. The goal of the CRP is to gather more context from the feature maps than a normal max pooling layer. Several pooling blocks are concatenated and each block consists of a max-pooling with higher stride length and a convolutional operation. Illustration of CRP with two max-pooling operations is shown in <ref type="figure" target="#fig_0">Fig. 2 (c)</ref>. In the final refinement step, we use the original image as input for an extra path with 3 RCUs, which improves the results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Network Architecture</head><formula xml:id="formula_1">(a) Original (b) GT (c) CEDN (d) RCN-VOC (e) RCN</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>We have done various experiments with different combinations of refinement blocks per multipath, and we have always observed the best results by placing the three blocks sequentially in a row, as shown in <ref type="figure">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training</head><p>For each epoch, 1000 random images are selected from the training set, and the data is augmented by random cropping, vertical flipping, and scaling between 0.7 and 1.3. To find an optimal training method, we have examined the following training variants:</p><p>-RCN-VOC is trained only on the CRF-refined object contour dataset proposed by Yang et al. <ref type="bibr" target="#b38">[34]</ref>. -RCN-COCO is pre-trained on a modified COCO dataset, where we have considered only the 20 PASCAL VOC classes and have produced contours. COCO segmentation masks and from those generated contours are not accurate, so we enrich them with additional contours. For this we use our own object contour detector RCN-VOC and set a high threshold to add only confident contour detections. -RCN is pre-trained on the modified COCO and trained on the refined PASCAL VOC.</p><p>Training the network for edge detection involves fine-training on the validation and train sets of the BSDS500 dataset. The BSDS contains individual, hand-drawn contours for the same images created by different people. We take the subjective decisions into account of which edge is a desired edge and which is not, by simply using all individual labels, and let the CNN form a compromise. To give an indication of how such training affects the results, we fine-train one of the networks only on the drawings of one single person, called RCN-VOC-1. All trainings and modifications are done in MatConvNet <ref type="bibr" target="#b34">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Object Contour Detection Evaluation</head><p>For evaluation, we use Piotr's Computer Vision Matlab Toolbox <ref type="bibr" target="#b14">[9]</ref>, the included Non-Maximum-Suppression (NMS) algorithm for thinning the soft object contour maps and a subset of 1103 images of a CRF-refined PASCAL val2012. We calculate the Precision and Recall (PR) curve for the RCN  models, CEDN, HED and COB in <ref type="figure" target="#fig_2">Fig.4</ref>. In Tab. 1 the Optimal Dataset Scale (ODS), Optimal Image Scale (OIS) and the Average Precision (AP) for the methods are noted. The quantitative analysis reveals that the RCN models significantly perform better in comparison to the other methods on all three metrics. This is also reflected in the visual results, cf. <ref type="figure" target="#fig_1">Fig. 3</ref>. The RCN-VOC and RCN have upper hand in suppressing the undesired edges, such as inner contours of the objects. At the same time, they also can recognize object contours more clearly. A disadvantage is that the contour predictions are thicker than in CEDN, which is due to the halved resolution owed by the network architecture. Nevertheless, the detection is very robust and the NMS can effectively calculate 1-pixel thinned object contours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Edge Detection Evaluation</head><p>The results of a quantitative evaluation of the RCN on unseen test images of BSDS500 are represented in the PR-curves in <ref type="figure">Fig 6.</ref> ODS, OIS and AP from methods such as RCN, CED, RCF, COB and HED are listed in Tab. 2. The proposed RCN achieves the state-of-the-art with a higher ODS than recent methods, closely followed by CED. In <ref type="figure">Fig. 5</ref> results of CED and RCN are visualized for some test images. Careful analysis of the results reveals that RCN detects some relevant edges, cf. inner contours of the snowshoes (1st row), face of the young man (2nd row), snout of the llama (4th row), which CED no longer recognizes. As for the object contour detection task, the disadvantage of the thicker edge predictions persists for the RCN. However, the NMS works more precisely for edge prediction maps from RCN, as the bit depth per pixel is increased from 8 to 16 bits. The difference is evident in the results for background edges in the image of the young man (2nd row), since an absolute maximum of the CED prediction could not be clearly distinguished, RCN edges are thinned more effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>The strategy, of using the high abstraction capability for object contour and edge detection more effectively than previous methods, has given us very good results in object contour detection and state-of-the-art results in edge detection. Our concept that RefineNet <ref type="bibr" target="#b22">[18]</ref> provides a very useful bottom-up multipath refinement architecture for edge detection is supported by these results. With the unconventional training methods, like the pre-training with a modified COCO dataset or by simply using all individual labels for fine-training on BSDS500, we have been able to improve the respective task.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Block diagrams of refinement path operations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Visualization of object contour detection methods</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>PR-curves on refined PASCAL val2012</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Visualization of edge detection methods PR-curves on BSDS500</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison</figDesc><table><row><cell></cell><cell>on re-</cell></row><row><cell cols="2">fined PASCAL val2012</cell></row><row><cell>Net</cell><cell>ODS OIS AP</cell></row><row><cell>RCN</cell><cell>.752 .773 .641</cell></row><row><cell cols="2">RCN-VOC .721 .746 .613</cell></row><row><cell cols="2">RCN-COCO .716 .741 .719</cell></row><row><cell>CEDN</cell><cell>.654 .657 .679</cell></row><row><cell>COB</cell><cell>.624 .657 .593</cell></row><row><cell>HED</cell><cell>.587 .598 .568</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison on BSDS500</figDesc><table><row><cell>Net</cell><cell>ODS OIS AP</cell></row><row><cell cols="2">RCN-VOC .824 .839 .837</cell></row><row><cell>RCN</cell><cell>.823 .838 .853</cell></row><row><cell>CED</cell><cell>.822 .840 .895</cell></row><row><cell cols="2">RCN-VOC-1 .812 .827 .822</cell></row><row><cell>RCF</cell><cell>.811 .830 .846</cell></row><row><cell>Human</cell><cell>.803 .803 -</cell></row><row><cell>COB</cell><cell>.793 .819 .849</cell></row><row><cell>HED</cell><cell>.788 .808 .840</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rcn-Voc</surname></persName>
		</author>
		<idno>F=.823] RCN (2019) [F=.822] CED</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deepcontour</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deepedge</surname></persName>
		</author>
		<idno>F=.729] gPb-UCM</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Canny</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sobel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2010.161</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2010.161" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="898" to="916" />
			<date type="published" when="2011-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Physics-inspired image edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Asghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jalali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Global Conference on Signal and Information Processing (GlobalSIP)</title>
		<imprint>
			<date type="published" when="2014-12" />
			<biblScope unit="page" from="293" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/GlobalSIP.2014.7032125</idno>
		<ptr target="https://doi.org/10.1109/GlobalSIP.2014.7032125" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deepedge: A multi-scale bifurcated deep network for top-down contour detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7299067</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2015.7299067" />
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="4380" to="4389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">High-for-low and low-for-high: Efficient boundary detection from deep object features and its applications to high-level vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.65</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2015.65" />
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="page" from="504" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semantic segmentation with boundary neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.392</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.392" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="3602" to="3610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A computational approach to edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="679" to="698" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with task-specific edge detection using cnns and a discriminatively trained domain transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.492</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.492" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="4545" to="4554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to predict crisp boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018</title>
		<editor>Weiss, Y.</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="570" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Piotr&apos;s Computer Vision Matlab Toolbox (PMT)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html" />
	</analytic>
	<monogr>
		<title level="m">The PAS-CAL Visual Object Classes Challenge 2012 (VOC2012) Results</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">N 4 -Fields: Neural network nearest neighbor fields for image transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ACCV 2014</title>
		<editor>Cremers, D., Reid, I., Saito, H., Yang, M.H.</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="536" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.90" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Weakly supervised object boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.27</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.27" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Visual Boundary Prediction: A Deep Neural Prediction Network and Quality Dissection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kivinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v33/kivinen14.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics. Proceedings of Machine Learning Research</title>
		<editor>Kaski, S., Corander, J.</editor>
		<meeting>the Seventeenth International Conference on Artificial Intelligence and Statistics. Machine Learning Research<address><addrLine>PMLR, Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04-25" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="512" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/4296-efficient-inference-in-fully-connected-crfs-with-gaussian-edge-potentials.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Weinberger, K.Q.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Weinberger, K.Q.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Boundary flow: A siamese network that predicts boundary motion without training on motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00346</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2018.00346" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="3282" to="3290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.549</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.549" />
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="5168" to="5177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2014</title>
		<editor>Fleet, D., Pajdla, T., Schiele, B., Tuytelaars, T.</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Richer convolutional features for edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2018.2878849</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2018.2878849" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Convolutional oriented boundaries: From image segmentation to high-level tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="819" to="833" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Edgeconnect: Generative image inpainting with adversarial edge learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nazeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Z</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ebrahimi</surname></persName>
		</author>
		<idno>abs/1901.00212</idno>
		<ptr target="http://arxiv.org/abs/1901.00212" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.178</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2015.178" />
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Multiscale combinatorial grouping for image segmentation and object proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00848</idno>
		<imprint>
			<date type="published" when="2015-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Epicflow: Edge-preserving interpolation of correspondences for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7298720</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2015.7298720" />
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="1164" to="1172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2016.2572683</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2016.2572683" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="640" to="651" />
			<date type="published" when="2017-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deepcontour: A deep convolutional feature learned by positive-sharing loss for contour detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7299024</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2015.7299024" />
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="3982" to="3991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Camera models and machine perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sobel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1970" />
		</imprint>
		<respStmt>
			<orgName>Stanford Univ Calif Dept of Computer Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Matconvnet -convolutional neural networks for matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the ACM Int. Conf. on Multimedia</title>
		<meeting>eeding of the ACM Int. Conf. on Multimedia</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep crisp boundaries: From boundaries to higher-level tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2018.2874279</idno>
		<ptr target="https://doi.org/10.1109/TIP.2018.2874279" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1285" to="1298" />
			<date type="published" when="2019-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.164</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2015.164" />
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="page" from="1395" to="1403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Edge-guided generative adversarial network for image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="DOI">10.1109/VCIP.2017.8305138</idno>
		<ptr target="https://doi.org/10.1109/VCIP.2017.8305138" />
	</analytic>
	<monogr>
		<title level="m">IEEE Visual Communications and Image Processing (VCIP)</title>
		<imprint>
			<date type="published" when="2017-12" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Object contour detection with a fully convolutional encoder-decoder network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.28</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.28" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="193" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Discriminative feature learning for video semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICVRV.2014.65</idno>
		<ptr target="https://doi.org/10.1109/ICVRV.2014.65" />
	</analytic>
	<monogr>
		<title level="m">2014 International Conference on Virtual Reality and Visualization</title>
		<imprint>
			<date type="published" when="2014-08" />
			<biblScope unit="page" from="321" to="326" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
