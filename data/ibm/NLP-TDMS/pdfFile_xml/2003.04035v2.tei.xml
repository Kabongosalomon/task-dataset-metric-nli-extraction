<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transformation-based Adversarial Video Prediction on Large-Scale Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pauline</forename><surname>Luc</surname></persName>
							<email>paulineluc@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><surname>Clark</surname></persName>
							<email>aidanclark@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>De</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Las</forename><surname>Casas</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yotam</forename><surname>Doron</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albin</forename><surname>Cassirer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Transformation-based Adversarial Video Prediction on Large-Scale Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent breakthroughs in adversarial generative modeling have led to models capable of producing video samples of high quality, even on large and complex datasets of real-world video. In this work, we focus on the task of video prediction, where given a sequence of frames extracted from a video, the goal is to generate a plausible future sequence. We first improve the state of the art by performing a systematic empirical study of discriminator decompositions and proposing an architecture that yields faster convergence and higher performance than previous approaches. We then analyze recurrent units in the generator, and propose a novel recurrent unit which transforms its past hidden state according to predicted motion-like features, and refines it to handle dis-occlusions, scene changes and other complex behavior. We show that this recurrent unit consistently outperforms previous designs. Our final model leads to a leap in the state-of-the-art performance, obtaining a test set Fréchet Video Distance of 25.7, down from 69.2, on the large-scale Kinetics-600 dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The ability to anticipate future events is a crucial component of intelligence. Video prediction, the task of generating a plausible future sequence of frames given initial conditioning frames, has been increasingly studied as a proxy task towards developing this ability, as video is a modality that is cheap to acquire, available in abundant quantities and extremely rich in information. It has been shown to be useful for representation learning <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b68">69]</ref> and in the context of reinforcement learning, to define intrinsic rewards <ref type="bibr" target="#b6">[7]</ref>, or for planning and control <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b73">74]</ref>.</p><p>Recently, large-scale adversarial generative modeling of images has led to highly realistic generations, even at high resolutions <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b33">34]</ref>. Modeling of video <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b56">57]</ref> has also seen impressive advances. Clark et al. <ref type="bibr" target="#b11">[12]</ref> showed strong results on class-conditional generation of Kinetics-600, but the setting of video prediction was found to be surprisingly difficult in comparison. This may be due to the mode dropping behavior of GANs. Indeed, generating a video from scratch allows the network to learn just a few plausible types of sequences instead of needing to infer a plausible continuation for any sequence.</p><p>In this work we focus on improving DVD-GAN-FP <ref type="bibr" target="#b11">[12]</ref> in two ways. First, following the authors' observation that the architecture of the discriminator is key for efficient training on large-scale datasets, we propose alternative decompositions for the discriminator and conduct an empirical study, validating a new architecture that achieves faster convergence and yields improved video quality.</p><p>Second, we draw inspiration from recent state-of-the-art approaches for video prediction <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b20">21]</ref> that predict the parameters of transformations, use them to warp the conditioning frames, and combine the result with direct generation. On the one hand, transformation-based prediction can use information in the conditioning frames, bypassing the need to learn to decompress representations of the relevant elements. On the other, in the presence of dis-occlusion, of appearing objects, or of new regions of the scene unfolding due to camera motion, warping past information seems an overly complex task in comparison with generating these pixel values directly. The two approaches are hence highly complementary. We seek to incorporate such transformations in the DVD-GAN-FP architecture, to provide sufficient flexibility to model camera and object motion in a straightforward fashion, while retaining scalability of the overall architecture. For this purpose, we introduce the Transformationbased Spatial Recurrent Unit (TSRU), a novel recurrent unit that predicts motion-like features, and uses them to warp its past hidden state. It then refines the obtained predictions by generating features to account for newly visible areas of the scene, and finally fuses the two streams.</p><p>To summarize, our contributions are as follows:</p><p>• We conduct a systematic performance study of discriminator decompositions for DVD-GAN-FP and propose an architecture that achieves faster convergence and yields higher performance than previous approaches.</p><p>• We propose a novel transformation-based recurrent unit to make the generator more expressive. This module is better suited for large-scale training than existing transformation-based alternatives, and we show that it brings substantial performance improvements over classical recurrent units.</p><p>• Our final approach, combining these two improvements, leads to a large improvement on the Kinetics-600 video prediction benchmark. Qualitatively, our model yields highly realistic frames and motion patterns.</p><p>2 Background and related work <ref type="bibr" target="#b1">2</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.1 Video generation and GANs</head><p>Since the introduction of the next frame prediction task <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b61">62]</ref>, video prediction and generation have become increasingly popular research topics. An important line of work has focused on extending ideas from generative modeling of images to these tasks <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b31">32]</ref>.</p><p>Generative Adversarial Networks (GANs) <ref type="bibr" target="#b22">[23]</ref> define a minimax game between a discriminator D learning to distinguish real data from generated samples, and a generator G learning to minimize the likelihood of the discriminator classifying generated samples as generated. In this work, we base our network architecture off DVD-GAN-FP <ref type="bibr" target="#b11">[12]</ref>, which has built on the design principles combined in BigGAN for stable, large-scale training of image generative adversarial networks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b16">17]</ref>. The generator of DVD-GAN-FP is primarily a 2D convolutional residual network with convolutional recurrent units interspersed between blocks at multiple resolutions to model consistency between frames. The discriminator is composed of a spatial discriminator, assessing individual frames, and a spatio-temporal discriminator, assessing the concatenation of the conditioning and generated frames. We provide an overview of DVD-GAN-FP in the appendix.</p><p>Many GAN approaches decompose the discriminator into multiple subnetworks. This includes multi-resolution approaches used for images <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b26">27]</ref>, speech <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b37">38]</ref> and video <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b11">12]</ref>. Some of these methods decompose the discriminator in a way which requires the existence of multiple generators, potentially sharing some hidden layers <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b32">33]</ref>. While an important research direction, for the purposes of our analysis of discriminators, we do not consider such variants.</p><p>Progress on the task of video prediction has spanned multiple directions. A number of works advocate for disentangling the factors of variation in the representations used by the models through careful design of the loss and network architecture: for example, between motion and content <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b64">65]</ref>, pose and appearance <ref type="bibr" target="#b14">[15]</ref> or foreground and background <ref type="bibr" target="#b68">[69]</ref>. Object-centric representations have also been proposed <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b81">82]</ref>. Some focus on alleviating error accumulation via architectural improvements <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b67">68]</ref>. Several works also motivate the use of various reconstruction, ranking and perceptual losses <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b75">76]</ref>. Finally, another line of work reformulates video prediction in other feature spaces than the pixel space, such as high level image features <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b66">67]</ref>, optical flow <ref type="bibr" target="#b70">[71]</ref>, dense trajectories <ref type="bibr" target="#b71">[72]</ref> and segmentation maps <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref>. Jayaraman et al. <ref type="bibr" target="#b30">[31]</ref> operate in the pixel space, but choose not to commit to a fixed time offset between the conditioning frames and the prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Kernel-based and vector-based transformations</head><p>Tranformation-based models for video prediction rely on differentiable warping functions to generate future frames as a function of past frames. Following the terminology introduced by Reda et al. <ref type="bibr" target="#b53">[54]</ref>, they fall into one of two families of methods: vector-based or kernel-based.</p><p>Vector-based approaches consist in predicting the coordinates of a grid used to differentiably sample from the input <ref type="bibr" target="#b28">[29]</ref>. This kind of approach has been used in conjunction with optical flow estimation <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b53">54]</ref>. Liu et al. <ref type="bibr" target="#b43">[44]</ref> extend this idea, predicting a 3D pseudo-optical flow field used to sample from the entire sequence of conditioning frames via tri-linear interpolation.</p><p>Kernel-based approaches predict the parameters of dynamic, depth-wise locally connected layers <ref type="bibr" target="#b78">[79,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b53">54]</ref>. These predicted parameters can be shared across spatial locations, forcing each spatial position to undergo the same transformation. To allow for varying motion across locations, Xue et al. <ref type="bibr" target="#b78">[79]</ref> use several such transformations, and combine them using a predicted vector of weights at each position, to yield per-pixel filters. Predicting a fixed number of depth-wise transformation kernels alongside a per-spatial-location weighting over transformations can be seen as a factorized version of predicting a full depth-wise transformation at each position. We therefore refer to the two approaches as factorized and pixelwise. Finn et al. <ref type="bibr" target="#b19">[20]</ref> explore both and find them to perform similarly. We refer to the appendix for a more formal description and an illustration for each.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Spatial recurrent units for video processing</head><p>Shi et al. <ref type="bibr" target="#b58">[59]</ref> and Ballas et al. <ref type="bibr" target="#b3">[4]</ref> introduce convolutional variants of the Long Short-Term Memory (LSTM) <ref type="bibr" target="#b25">[26]</ref> and Gated Recurrent Unit (GRU) modules <ref type="bibr" target="#b10">[11]</ref>. Our proposed recurrent unit builds on the convolutional recurrent units employed by DVD-GAN-FP, Convolutional GRUs (ConvGRUs), which produce output h t based on input x t and previous output h t−1 as:</p><formula xml:id="formula_0">r = σ(W r k [h t−1 ; x t ] + b r ) (1) h t = r h t−1 (2) c = ρ(W c k [h t ; x t ] + b c ) (3) u = σ(W u k [h t−1 ; x t ] + b u ) (4) h t = u h t−1 + (1 − u) c<label>(5)</label></formula><p>Here σ is the sigmoid function, ρ is the chosen activation function, k represents a k × k convolution, and the operator represents elementwise multiplication.</p><p>With similar motivations to ours, Xu et al. <ref type="bibr" target="#b76">[77]</ref> propose a recurrent unit for structured video prediction, relying on dynamic prediction of the weights used in the Convolutional LSTM (ConvLSTM) update equations. To avoid over-parameterization, the authors propose a shared learned mapping of feature channel differences to corresponding 2D kernels. This requires processing O(C 2 ) inputs, where C is the number of channels of the hidden state, which is prohibitively expensive in our large-scale setting. We refer the interested reader to the appendix for a more detailed discussion.</p><p>Shi et al. <ref type="bibr" target="#b59">[60]</ref> introduce TrajGRU, which also warps the past hidden state to account for motion, using vector-based transformations, and provides the result as input to all gates of a ConvGRU. Vector-based transformations allow modeling of arbitrarily large motions with a fixed number of parameters, but they require random memory access, whereas modern accelerators are better suited for computation formulated in terms of matrix multiplications. As a result, in our work, we employ kernel-based transformations. We first propose a straightforward kernel-based extension of TrajGRU, called K-TrajGRU. We then formulate a novel transformation-based recurrent unit that is simpler and more directly interpretable.</p><p>3 Large-scale transformation-based video prediction</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Discriminator decompositions</head><p>Architectures that can effectively leverage a large dataset like Kinetics-600 require careful design. While the generator must create temporally consistent sequences of plausible frames, the discriminator must also be able to assess both temporal consistency and overall plausibility.  <ref type="figure">Figure 1</ref>: A visualization of the inputs each discriminator must judge. From left to right: the original DVD-GAN-FP discriminator decomposition processes full resolution frames and downsampled videos; our proposed faster decomposition processes downsampled frames and cropped videos; our proposed stronger decomposition additionally processes downsampled videos.</p><p>Both Tulyakov et al. <ref type="bibr" target="#b62">[63]</ref> and DVD-GAN-FP <ref type="bibr" target="#b11">[12]</ref> employ two discriminators: a Spatial Discriminator D S and a Temporal Discriminator D T , which respectively process individual frames and video clips. While Tulyakov et al. <ref type="bibr" target="#b62">[63]</ref> motivate the use of D S mainly by improved convergence, DVD-GAN-FP makes the two discriminators complementary in order to additionally decrease computational complexity. In DVD-GAN-FP, D S assesses single frame content and structure by randomly sampling K full-resolution frames and judging them individually, while D T is charged with assessing global temporal structure by processing videos spatially downsampled by a factor s.</p><p>To further improve efficiency, we propose an alternative split of the roles of the discriminators, with D S judging per-frame global structure, while D T critiques local spatio-temporal structure. We achieve this by downsampling the K randomly sampled frames fed to D S by a factor s, and cropping The potential blind spot of our new decomposition is global spatio-temporal coherence. For instance, if the motion of two spatially distant objects is correlated, then this decomposition will fail to teach G that one object must move based on the movement of the other. To address this limitation, we investigate a second configuration which consists in combining our proposed decomposition with a second temporal discriminator D T assessing the quality of the downsampled generated videos, like in DVD-GAN-FP, and call this decomposition DVD-GAN-FP stronger . We summarize these configurations in <ref type="figure">Figure 1</ref> and <ref type="table" target="#tab_0">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Transformation-based Recurrent Units</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Stacked kernel-based warping for efficient, multi-scale modeling of motion</head><p>As motivated in Section 2.3, we favour kernel-based transformations over vector-based ones. We rely on the stacking of multiple such transformations in the feature hierarchy of the generator to allow the modeling of large motion in a multi-scale manner, while keeping the number of parameters under control.</p><p>We first study a kernel-based extension of TrajGRU, whose equations we report in the appendix. Inspired by recent work that leverages complementary pixel-based generations and transformationbased predictions <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b20">21]</ref>, we also propose a novel recurrent unit whose design is simpler and more directly interpretable, which we describe next.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">TSRU</head><p>To combine transformation-based prediction with direct generation, we begin by observing that the final equation of ConvGRU, Eq (5), already combines past information h t−1 with newly generated content c. Similarly, our recurrent unit combinesh t−1 , obtained by warping h t−1 , with newly generated content c. Instead of computing the reset gate r and resetting h t−1 , we compute the parameters of a transformation θ, which we use to warp h t−1 . The rest of our model is unchanged (withh t−1 playing the role of h t in c's update equation, Eq <ref type="formula">(3)</ref>). Finally, our module is described by the following equations:</p><formula xml:id="formula_1">θ h,x = f (h t−1 , x t ) (6) h t−1 = w(h t−1 ; θ h,x ) (7) c = ρ(W c k [h t−1 ; x t ] + b c ) (8) u = σ(W u k [h t−1 ; x t ] + b u ) (9) h t = u h t−1 + (1 − u) c,<label>(10)</label></formula><p>where f is a shallow convolutional neural network, w is a warping function described next, and the rest of the notations follow from Eqs (1)- <ref type="bibr" target="#b4">(5)</ref>. We have designed this recurrent unit to closely follow the design of ConvGRU and call it TSRU c . We provide an illustration in <ref type="figure" target="#fig_3">Figure 2</ref>. We study two alternatives for the information flow between gates. TSRU p computes θ, u and c in parallel given x t and h t−1 , yielding the following replacement for Eq <ref type="bibr" target="#b7">(8)</ref>:</p><formula xml:id="formula_2">c = ρ(W c k [h t−1 ; x t ] + b c ).</formula><p>At the other end, TSRU s computes each intermediate output in a fully sequential manner: like in TSRU c , c is given access toh t−1 , but additionally, u is given access to both outputsh t−1 and c, so as to make an informed decision prior to mixing. This yields the following replacement for Eq <ref type="formula">(9)</ref>:</p><formula xml:id="formula_3">u = σ(W u k [h t−1 ; c] + b u ).</formula><p>We also illustrate these variants in the appendix.</p><p>The resulting modules are generic and can be used as drop-in replacements for other recurrent units, which are already widely used for generative modeling of video. Provided that the conditioning frame encodings are used as initialization of its hidden representation, as in the DVD-GAN-FP architecture, this unit can predict and account for motion in the use it makes of past information, if this is beneficial. When interleaved at different levels of the feature hierarchy in the generator, like in DVD-GAN-FP, motion can naturally be modeled in a multi-scale approach. The modules are designed in such a way that a pixel-level motion representation can emerge, but we do not enforce this. In particular, we do not provide any supervision (e.g. ground truth flow fields) for the predicted motion-like features, instead allowing the network to learn representations in a data-driven way.</p><p>We study both factorized and pixelwise kernel-based warping (c.f. Section 2.2 and appendix). Specifically, for factorized warping, we predict a sample-dependent set w of N 2D-convolution kernels of size k × k and a selection map S, where each spatial position holds a vector of probabilities over the N warpings, which we use as coefficients to linearly combine the basis weight vectors into per-pixel weights. This map is intended to act as a pseudo-motion segmentation map, splitting the feature space into regions of homogeneous motion. In practice, we choose N = k 2 and k = 3. Pixelwise kernel-based warping consists in predicting a k × k weight vector for each spatial position, which is used locally to weigh the surrounding values of the input in a weighted average. We denote this approach by Pixelwise Transformation-based Spatial Recurrent Unit (PTSRU).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental set up</head><p>Datasets Kinetics is a large dataset of YouTube videos intended for action recognition tasks. There are three versions of the dataset with increasing numbers of classes and samples: Kinetics-400 <ref type="bibr" target="#b34">[35]</ref>, Kinetics-600 <ref type="bibr" target="#b8">[9]</ref> and Kinetics-700 <ref type="bibr" target="#b9">[10]</ref>. Here we use the second, Kinetics-600, to enable fair comparison to prior work. This dataset contains around 500,000 10-second videos at 25 FPS spread across 600 classes. Kinetics is often used in the action recognition field, but has only recently been used for generative purposes. Li et al. <ref type="bibr" target="#b40">[41]</ref> and Balaji et al. <ref type="bibr" target="#b2">[3]</ref> used filtered and processed versions of the dataset, and more recently the entire unfiltered dataset has been used for generative modeling with GANs as well as autoregressive models <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b74">75]</ref>. Following these works, we resize all videos to 64 × 64 resolution and train on randomly-selected 16 frame sequences from the training set. Specifically, G is trained to predict 11 frames conditioned on 5 input frames. Unfortunately, the Kinetics dataset in general is not covered by a license which would allow for showing random data samples in a paper. For this reason, for all qualitative analysis and shown samples in this work, we use conditioning frames taken from the UCF-101 dataset <ref type="bibr" target="#b60">[61]</ref>, though the underlying models are trained completely on Kinetics-600. Finally, we extend our analysis on UCF-101 <ref type="bibr" target="#b60">[61]</ref> and BAIR Robot Pushing dataset <ref type="bibr" target="#b17">[18]</ref>, presented in the appendix.</p><p>Metrics Traditional measures for video prediction models such as Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM) <ref type="bibr" target="#b72">[73]</ref> rely on the existence of ground truth continuations of the conditioning frames and judge the generated frames by their similarity to the real ones. While this works for small time scales with near deterministic futures, these metrics fail when there are many potential future outcomes. Unterthiner et al. <ref type="bibr" target="#b63">[64]</ref> propose the Fréchet Video Distance (FVD), an adaption of the Fréchet Inception Distance (FID) metric <ref type="bibr" target="#b24">[25]</ref> which judges a generative model by comparing summary statistics of a set of samples with a set of real examples in a relevant embedding space. For FVD, the logits of an I3D network trained on Kinetics-400 <ref type="bibr" target="#b7">[8]</ref> are used as the embedding space. We adopt this metric for ease of comparison with prior work. We also report the Inception Score (IS).</p><p>Training details All models were trained on 32 to 512 TPUv3 accelerators using the Adam <ref type="bibr" target="#b35">[36]</ref> optimizer. In all our experiments, we validate the best model according to performance on the training set. Like BigGAN <ref type="bibr" target="#b5">[6]</ref>, we find it important to use cross-replica batch statistics where the per-batch mean and variance are calculated across all accelerators at each training step. More details on our training setup are provided in the appendix. We will make the code 1 available, as well as models trained on UCF-101 and BAIR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparing Discriminator Decompositions</head><p>We begin by comparing several discriminator decompositions, including a MoCoGAN-like decomposition <ref type="bibr" target="#b62">[63]</ref>, DVD-GAN-FP <ref type="bibr" target="#b11">[12]</ref>, and our proposed decompositions, on the Kinetics video prediction benchmark <ref type="bibr" target="#b74">[75]</ref>. We fix the number of frames K sampled for the spatial discriminator to 8. For MoCoGAN-like, we also evaluate the authors' original setting, where K = 1. Finally, for each of the discriminator decompositions, we evaluate a baseline where only the temporal discriminator is used, denoted respectively by Clip (for MoCoGAN-like), Downsampled (for DVD-GAN-FP), Cropped (for DVD-GAN-FP f aster ) and Cropped &amp; Downsampled (for DVD-GAN-FP stronger ).</p><p>Because we are interested in architectures which improve real experimentation time, we fix the step budget for each method according to its average step duration, measured over 30 seconds for a batch size of 512, and make sure that we do not account for any lag introduced by data loading or preprocessing. For these normalized step budgets, training takes approximately 103 hours, corresponding to 1M iterations for the fastest one. We summarize step budgets and results in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>For all four discriminator decompositions, the addition of a frame discriminator yields large improvements in FVD, along with improving convergence speed as previously noted by Tulyakov et al. <ref type="bibr" target="#b62">[63]</ref>. We also observe that the Downsampled and Cropped baselines obtain poor performance compared to the other approaches, consistent with our observation that the Downsampled baseline can only assess global spatio-temporal structure, and the Cropped baseline can only assess local structure. In contrast, a combination of the two (Cropped + Downsampled) largely improves on the Clip baseline, which has access to all conditioning and predicted frames, and obtains improved performance with respect to the two MoCoGAN-like variants, while being 29% faster to train. Next, we find that in this setting, DVD-GAN-FP yields an improvement in terms of average step duration over the MoCoGAN-like baselines, but lags behind in terms of prediction quality. As expected, the alternative decomposition we propose has a much shorter average step duration than the MoCoGAN-like and DVD-GAN-FP baselines, and additionally closes the performance gap with MoCoGAN-like. Adding a spatial discriminator to the Cropped &amp; Downsampled baseline yields our final proposed decomposition, DVD-GAN-FP stronger , which obtains a substantial improvement over previous approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparing Recurrent Units</head><p>We now report a comparison between recurrent units in <ref type="table" target="#tab_1">Table 2</ref>. Here, we use the fastest of our discriminator decompositions, DVD-GAN-FP f aster , so as to maximize the number of training steps which can be afforded for each configuration in this large-scale setting. We train all models for 500k steps. Our setup is otherwise identical to the one presented in Section 4.2. We find that all our proposed transformation-based approaches perform better than using ConvGRU or ConvLSTM, suggesting that the increased expressivity of the generator is beneficial for generation quality, with the best results observed for TSRU p in this setting. Interestingly, we observe that PTSRU performs slightly worse than its TSRU s counterpart (which is the least competitive of our TSRU designs); suggesting that the factorized version may be more effective at predicting structured, globally consistent motion-like features. As a result, we do not investigate other versions of the PTSRU design. Finally, we observe a small performance improvement for all TSRU compared with K-TrajGRU, suggesting that its simplified design facilitates learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Scaling up</head><p>Putting it all together, we combine our strongest decomposition DVD-GAN-FP stronger with TSRU p . We call this model TrIVD-GAN-FP, for "Transformation-based &amp; TrIple Video Discriminator GAN". We increase the channel multiplier (described in the appendix) from 48 to 120 and train this model for 700,000 steps. We report the results of this model evaluated on the test set of Kinetics 600 in <ref type="table" target="#tab_2">Table 3</ref>, together with the results reported by Weissenborn et al. <ref type="bibr" target="#b74">[75]</ref> and Clark et al. <ref type="bibr" target="#b11">[12]</ref>. We also provide unbiased estimates of standard deviation for 10 training runs. In comparison with the strong DVD-GAN-FP baseline, our contributions lead to a substantial decrease of FVD.   In <ref type="figure" target="#fig_4">Figure 3</ref>, we show predictions and corresponding motion fields from our final model TrIVD-GAN-FP. We select them to demonstrate the interpretability of these motion fields for a number of samples. We obtain them by interpreting the local predicted weights for a given spatial position as a probability distribution over flow vectors corresponding to each of the kernel grid positions and taking the resulting expected flow value. They can be combined across resolutions by iteratively upsampling a coarse flow field, multiplying it by the upsampling factor to account for the higher resolution, and refining it with the next lower level flow field. These predictions have been obtained on UCF-101 and also show strong transfer performance. Though this is not entirely surprising given the similarity between the classes and content, it further emphasizes the generalization properties of our model. We provide an extended qualitative analysis including additional samples in the appendix.</p><p>Finally, we perform additional experiments on the BAIR Robot Pushing dataset, and on UCF-101, to demonstrate the generality of our method. We further show that our model is able to generate diverse predictions on the BAIR dataset. We report the results in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Effectively training video prediction models on large datasets requires scalable and powerful network architectures. In this work we have systematically analyzed several approaches towards reducing the computational cost of GAN discriminators and proposed DVD-GAN-FP f aster and DVD-GAN-FP stronger , which improve wall-clock time and performance over the strong baseline DVD-GAN-FP.</p><p>We have further motivated and proposed a family of transformation-based recurrent units -drop-in replacements for standard recurrent units -which further improve performance. Combining these contributions led to our final model, TrIVD-GAN-FP, which obtains large performance improvements and is able to make diverse predictions. For future directions, we plan to investigate the use of these recurrent units for video processing tasks, as well as the usefulness of the representations that our models learn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>Video generative modeling is a well-established research problem, which can drive the development of general purpose technologies for modeling distributions of data arising from complex spatio-temporal phenomena. Kinetics-600 is a large, diverse and complex dataset, and as such, it is reasonable to expect that approaches that yield improvements on this dataset should generally prove useful in many other settings. Our proposed discriminator decomposition yields faster convergence and better performance. Our proposed transformation-based recurrent units provide additional flexibility to the generator, and are designed such that pixel-level motion representations can emerge in a data-driven way. Such advances could be applied to the acceleration of climate models. Extensions of video prediction models could replace components for which physical counterparts are too computationally intensive or too uncertain. For example, high resolution cloud simulation <ref type="bibr" target="#b21">[22]</ref> and prediction of short-term changes in the sea ice extent using satellite images have both been identified as a "high leverage" machine learning applications for tackling climate change <ref type="bibr" target="#b54">[55]</ref>. Our improvements could also generally benefit applications that involve decision-making in the real world, such as autonomous driving and robotics.</p><p>On a more pessimistic note, progress in the field of video generation models might be applied by malicious agents, to forge false, plausible elements in support of deceptive affirmations, thus contributing to the creation, confirmation and spread of beliefs. This can, in turn, have dramatic consequences.</p><p>As the field progresses, methods that can detect generated images or videos automatically, such as <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b77">78]</ref>, are therefore becoming increasingly important. Nguyen et al. <ref type="bibr" target="#b49">[50]</ref> provide a survey of such methods.</p><p>Using Kinetics as a benchmark might also inadvertently result in researchers overlooking privacy concerns, motivated by research incentives. Kinetics is a dynamic dataset, with a small number of videos occasionally removed, following their removal from YouTube <ref type="bibr" target="#b34">[35]</ref>. We believe that releasing frames or continuations for this dataset would therefore be unethical. The same concern applies to releasing generative models trained on this dataset, since we cannot guarantee that they will not be able to partially memorize some of the removed videos. To mitigate this risk, we hope that future research can continue to follow our evaluation procedure, when using this dataset, by presenting only aggregated metrics of performance, and by employing other datasets to demonstrate qualitative performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Overview of DVD-GAN-FP</head><p>Except for the improvements discussed in the main text, our model is identical to DVD-GAN-FP <ref type="bibr" target="#b11">[12]</ref>. This model consists of an encoder E, a generator G and a discriminator D. We give an overview of the architecture of each component here, and refer the reader to the paper for additional details. Both BigGAN and DVD-GAN were initially formulated in a class-conditioned setting. For simplicity, DVD-GAN-FP replaces the class label with a dummy constant zero class label wherever a class label is used.</p><p>G is primarily a 2D convolutional residual network based on the BigGAN architecture <ref type="bibr" target="#b5">[6]</ref>, with convolutional recurrent units interspersed between blocks at multiple resolutions to model consistency between frames. First, a feature vector is obtained by concatenating a latent sample z drawn from a normal distribution and features linearly obtained from the class label, both of dimension 128. Spatial features of resolution 8 × 8 are obtained via a reshaped affine transformation of this feature vector. These features are repeated along the time dimension, and passed for each time-step as input to the first convolutional recurrent unit. The features output at each time step then undergo framewise processing: a convolutional layer, followed by two identical BigGAN G residual blocks, the second of which performs nearest-neighbour spatial upsampling by 2. The recurrent unit and the framewise processing form the first stage of the architecture. Another two similar stages are employed, leading to features of resolution 64 × 64. At each stage, the recurrent unit's hidden state is initialized with the features extracted by the encoder at that resolution. Finally, the predicted images are obtained through a final BatchNorm-ReLU, a convolutional layer and a tanh. Batch normalization layers are also employed in each of the G residual blocks, independently for each time step, and conditioned on the 256-dimensional input feature vector containing the latent sample and the class information. All linear and convolutional layers in the entire model have Spectral Normalization applied to their weights <ref type="bibr" target="#b80">[81]</ref>, with the exception of the linear embedding mapping to the first spatial features.</p><p>All discriminators discussed in the main text are convolutional networks almost identical to BigGAN's discriminator, except in the input they take. Per-frame discriminators are entirely unchanged: they consist of five stages, each containing a single D residual block, the first four of which employ average pooling to downsample the features by 2. The final block is followed by a ReLU activation.</p><p>Features are then summed across the spatial dimensions, and passed to the projection head, further described in Section E.1. The spatio-temporal discriminators, which are responsible for assessing temporal consistency, process the inputs identically, with the exception that all convolutions in the first two residual blocks are replaced with 3D convolutions.</p><p>In order to condition G on initial frames, an encoder E, identical to the spatial discriminator network, is applied to the conditioning frames. For each RNN present in G, the features extracted by the encoder at the corresponding resolution are reshaped, by folding the time dimension inside the channel dimension. The result is compressed using a single convolutional layer, to form the initial state for the corresponding RNN. In this way, all the recurrent units in G are initialized with features that have knowledge of the conditioning frames.</p><p>We refer the reader to <ref type="bibr" target="#b11">[12]</ref> for the parametrization of the G and D residual blocks, and the corresponding hyperparameters, which our method leaves unchanged, unless stated otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Kernel-based warping</head><p>We give a more formal description of factorized and pixelwise kernel-based warping, as introduced in Section 2.3. Pixelwise kernel-based warping approaches employ a network f to predict a set of parameters θ = W ∈ R H×W ×k 2 , given input x. These parameters are then used in depthwise, dynamic, locally-connected layers of kernel size k, taking as input h ∈ R H×W ×C . In previous kernelbased video prediction approaches <ref type="bibr" target="#b78">[79,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b53">54]</ref>, h and x are respectively the last conditioning frame and the concatenated conditioning frames. In the case of our recurrent units, h represents the past hidden state h t−1 and the input x is formed by the concatenation of h t−1 and the current input to the unit x t . Formally, at a given spatial position (i, j), the c th dimension of the output vectorh i,j is given by:</p><formula xml:id="formula_4">h i,j [c] = (m,n)∈ 0,k−1 2 W i,j [mk + n] · h i+m−(k−1)//2,j+n−(k−1)//2 [c],<label>(11)</label></formula><p>where h has been padded to preserve spatial dimensions.</p><p>In the case of factorized kernel-based warping, f predicts θ = (w, S), where w ∈ R k 2 ×N and S ∈ R H×W ×N , which are combined to form a tensor W ∈ R H×W ×k 2 as follows. At (i, j), the q th dimension of W i,j is given by :</p><formula xml:id="formula_5">W i,j [q] = N l=1 S i,j [l] · w[q, l].<label>(12)</label></formula><p>W is then used as in the pixelwise warping case. We illustrate the procedure for pixelwise and factorized kernel-based warping in <ref type="figure">Figure 4</ref>.  <ref type="figure">Figure 4</ref>: Kernel-based warping. Given input x, a network f predicts transformation parameters: a tensor of weights W for pixelwise warping, or a set of weights w and a map of coefficients S for factorized warping, which are then combined to obtain W (Equation <ref type="formula" target="#formula_5">(12)</ref>). In both cases, the result W is used in a dynamic, locally connected, depth-wise layer, applied to h (Equation <ref type="formula" target="#formula_4">(11)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional experiments C.1 BAIR Robot Pushing</head><p>The BAIR Robot Pushing dataset of robotic arm motion <ref type="bibr" target="#b17">[18]</ref> has been used as a benchmark in prior work on video prediction <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b74">75]</ref>. The original BAIR Robot Pushing dataset contains 43,264 training samples and 256 test samples, each of 30 frames. Unterthiner et al. <ref type="bibr" target="#b63">[64]</ref> have shown that the expected FVD between two non-overlapping 256-sized subsets of 16-frame videos randomly drawn from this dataset is on the order of 10 2 . This suggests that on such a small test set, this metric has a very important bias, impeding quantitative comparison to state-of-the-art results. Instead, if we take two non-overlapping 30k-sized subsets, the expected FVD falls close to zero <ref type="bibr" target="#b63">[64]</ref>. We therefore wish to employ 30k samples to estimate the statistics of both ground truth and generated samples. To do so, we propose to take a more balanced split of the data, using only the first 70% of the train set for training, and the remaining 30% augmented with the original 256 test samples for evaluation.  We train TrIVD-GAN-FP, using a channel multiplier in G of 48, and in D of 92, and learning rates of 2 · 10 −5 for G and 3 · 10 −5 for D. Like previous work, we condition on one frame and generate 15 future frames. As on Kinetics, we validate models according to performance on the train set and report FVD on the test set for both splits. We use the original split to compare to Video Transformer and DVD-GAN-FP, and our proposed balanced split, as a baseline for future work.</p><p>We report the obtained results on this dataset in <ref type="table" target="#tab_3">Table 4</ref>. Despite substantial improvement over DVD-GAN-FP and Video Transformer <ref type="bibr" target="#b74">[75]</ref> on Kinetics, on BAIR, TrIVD-GAN-FP only sees slight improvement over DVD-GAN-FP, still higher than Video Transformer. Qualitatively, we do not observe a visual difference between the models, and echo comments from both papers that FVD may not be an adequate metric on BAIR, especially given the large bias of the metric resulting from the small sample size. Given access to 30K samples to estimate the FVD with the more balanced split, our model reaches 31.8 test FVD, estimated over 10 evaluation runs.</p><p>Finally, we report SSIM scores from BAIR, collected as in SAVP <ref type="bibr" target="#b38">[39]</ref>. This involves generating continuations of a single frame and selecting the one which maximizes the average frame SSIM, then reporting per-frame SSIMs for all "best continuations" of conditioning frames. The results of this experiment are in <ref type="figure" target="#fig_6">Figure 5</ref>. Notably, we see high correlation between SSIM and increasing , meaning that generating more samples leads to a meaningfully diverse variety of futures, such that some are closer to the ground truth than others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 UCF-101</head><p>UCF-101 <ref type="bibr" target="#b60">[61]</ref> is dataset of 13,320 videos of human actions across 101 classes. As a complement to the qualitative results we present, we also provide quantitative performance on this dataset. For the models with a channel multiplier of 48 trained for 1M iterations on Kinetics, TrIVD-GAN-FP improves upon DVD-GAN-FP from 117 test FVD to 95.8. The large model with a channel multiplier of 120 trained on Kinetics further improves this to 67.7. Finally, we train a model with channel multiplier of 92 on the UCF train set for 1M iterations, using the same hyperparameters as on Kinetics, and obtain our best results, of 55.2 ± 0.64 test FVD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Extended qualitative analysis</head><p>We start by showing in Section D.1 a qualitative comparison between recurrent units, using our medium-size models, described in Section 4.3 of the main paper. Next, in Sections D.2 and D.3, we provide further qualitative analysis of the samples predicted by our final large-scale TrIVD-GAN-FP model, described in Section 4.4 of the main paper. To support our analysis, we provide accompanying videos at https://drive.google.com/open?id=1fvmEm3gOWprWy2IuMFe4DuwEPahe9MwC.</p><p>All models have been trained on Kinetics-600 train set, and all predictions have been sampled randomly on the UCF-101 test set 1, due to restrictions on Kinetics-600.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Qualitative comparison between ConvGRU and TSRU</head><p>We now compare our proposed recurrent unit TSRU p with ConvGRU, using for each the best model obtained when trained for 1M steps. We employ the truncation trick <ref type="bibr" target="#b5">[6]</ref>, as we find that this significantly improves both models' predictions. Specifically, we employ moderate truncation of the latent representation, using threshold 0.8.</p><p>We refer the reader to the accompanying video in folder C.1., showing a side-by-side comparison of randomly sampled predictions, with the ConvGRU baseline on the left, and the TSRU model on the right. We find that our proposed module yields a slight but perceptible improvement over ConvGRU, in that the model using TSRU is more often able to conserve a plausible object shape across frame predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Influence of the latent representation on the predictions</head><p>Here, we provide a qualitative analysis of the influence of the latent representation on the predictions, using our large-scale TriVD-GAN-FP model. In the directory C.2., we provide a comparison of samples obtained using, on the left constant zero latent representations z (i.e. extreme truncation) and on the right random latent representations sampled from the same distribution as the one used during training (i.e. no truncation).</p><p>We find that in comparison with the constant zero latent representation samples, the non-truncated samples exhibit much more drastic changes in the predictions with respect to the input frames. Camera motion, zooming or object motion tend to be more important in a number of samples when using the non-truncated distribution. Effects like fade-to-black or novel object appearances, tend to appear regularly in the non-truncated examples, while we have not observed them in the samples that use a fixed zero latent representation. We point out examples of such effects in the annotated comparison (annotated_constantz_vs_notruncation); and also provide a non-annotated comparison (constantz_vs_notruncation). This points to a significant influence of the random variable on the predictions, as also evaluated quantitatively on the BAIR dataset in Section C.1.</p><p>Finally, for the interested reader, we also provide a larger batch of random samples in folder C.2./all, with the following levels of truncation: extreme truncation, where we sample all videos with the fixed zero latent representation; moderate truncation with 0.8 threshold; and no truncation at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Analysis of success and failure cases</head><p>As a result of the findings described in Section D.2, we find that our large-scale model also benefits from the truncation trick, in terms of generation quality. Focusing on the constant zero latent representation samples, overall, we find that our final model TRiVD-GAN-FP generates highly plausible motion patterns, including in highly complex cases such as intricate hand motions or water motion patterns. The model also makes plausible predictions of complex motion in a number of cases, where points move with a non-uniform acceleration across frames, suggesting that it has learned a good representation for physical phenomena such as gravity (eg. for people jumping or juggling) and for some forms of object interactions (eg. for demonstrations of knitting). We highlight some of these in the accompanying video C.3./annotated_success_constantz.</p><p>We also highlight some failure cases in C.3./annotated_failure_constantz_vs_notruncation, where we provide constant zero latent representation samples on the left, and non-truncated samples on the right. We find that when the truncation trick is not used, the model sometimes predicts very large motion, that leads to important object deformations. We also sometimes observe large artifacts across the entire scene. Occasionally, the model makes predictions of obviously implausible trajectories. In contrast, when using truncation, we see that these failures cases largely disappear.</p><p>Across both levels of truncation however, we find that the model struggles when globally coherent motion must be predicted for objects with fine spatial structure, eg. in the case of hoola loops, or horse legs. We also find that in certain cases, motion seems to simply stop, presumably since this is an easy way for the model to yield plausible predictions in an overly conservative manner. We note however that a trivial copy baseline, which outputs a copy of the last input frame for each time step, performs very poorly at 330 on the Kinetics validation set -showing that consistently predicting trivial motion patterns is heavily penalized by the FVD. On rare occasions, we also observe objects disappearing into the void. A final failure case comes up when the input sequence is blurry, which leads to poor predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Experimental Details E.1 Loss</head><p>The discriminator and generator are jointly trained with a geometric hinge loss <ref type="bibr" target="#b41">[42]</ref>, updating the discriminators two times for each generator update. Specifically, the discriminator optimizes the following empirical risk:</p><formula xml:id="formula_6">1 D D d=1 ρ(1 + D(G(z))) + ρ(1 − D(x)),<label>(13)</label></formula><p>where D is the discriminator output dimension, ρ is the ReLU function, and z and x represent respectively a latent sample and a video. The generator optimizes the following empirical risk:</p><formula xml:id="formula_7">− 1 D D d=1 D(G(z)).<label>(14)</label></formula><p>Following DVD-GAN, we employ a variant of the projection conditioning of the discriminator 2 , which we call mixed projection discriminator.</p><p>We first recall the formulation of the original projection discriminator <ref type="bibr" target="#b47">[48]</ref>. Calling h a batch of spatiotemporal features extracted from the videos fed to the discriminator, the projection discriminator maps these, on the one hand, to per-sample r b x scalars only dependent on the input videos. On the other, a dot product is taken between h and a learned vector, corresponding to a dummy zero class y (or to the conditioning class if appropriate), to obtain a per-sample r b</p><p>x,y scalar dependent on both the videos and the class. Next, these scalars are added together to form the discriminator output o b , consisting of a single scalar per sample b.</p><p>Instead, the mixed projection discriminator extracts scalars r b,t x and r b,t x,y for each batch sample b and each time step t and computes D = B × B × T outputs, where B represents the batch size and T the time dimension of h, obtained as follows:</p><formula xml:id="formula_8">o b,b ,t = r b,t x + t r b ,t x,y .<label>(15)</label></formula><p>We find that this variant improves over the original projection discriminator from 64.2 to 55.0 on the Kinetics validation set, for medium-sized models using a channel multiplier of 48 trained for 700k steps. As a result we employ it to train all our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Implementation Details</head><p>We preprocess the data as follows. Real video clips are first resized so that their smallest dimension is 64, then randomly cropped and finally scaled to the range [-1, 1]. For evaluation, real and generated videos are then resized to resolution 224 × 224 using bilinear interpolation, for input to the I3D network.</p><p>Spectral Normalization <ref type="bibr" target="#b80">[81]</ref> is applied to all weights in the generator, approximated by only the first singular value. All weights are initialized orthogonally <ref type="bibr" target="#b57">[58]</ref>, and during training we maintain an auxiliary loss on the generator which penalizes weights diverging from orthogonality. This loss is added to the GAN loss before optimization with weight 0.0001. The model is optimized using Adam <ref type="bibr" target="#b35">[36]</ref> with batch size 512, β 1 set to 0, β 2 set to 0.999 and a learning rate of 1 · 10 −4 and 5 · 10 −4 for G and D respectively.</p><p>All samples for evaluation (but not for training) use an exponential moving average of G's weights, which is accumulated with decay γ = 0.9999 and is initialized after 20,000 training steps with the value of weights at that point. In order to disambiguate the effect of the weights' moving averages from those present in the Batch Normalization layers, we always evaluate using "standing statistics", where we run a large number of forward passes of the model (100 batches of 256 samples), and record the average means and variances for all Batch Normalization layers. These averages are used as the "batch statistics" for all subsequent evaluations. This process is repeated for each evaluation.</p><p>Conditioning of the Batch Normalization layers <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17]</ref> is done by having the scale and offset parameters be an affine transformation of the conditioning vector. We note that the final batch normalization layer in G is not conditional, and uses a regular learned scale and offset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Estimating average step duration and memory consumption</head><p>We note that for calculating step time and maximum memory usage in <ref type="table" target="#tab_0">Table 1</ref> of the main paper, each training step corresponds to two D updates and a single G update, involving three passes of G with batch size 8 and two passes of D with batch size 16 alongside one with batch size 8. Furthermore, we emphasize that TPUs rely on the XLA intermediate compilation language, which automatically performs some re-materialization and optimization. As such, these numbers might change for other implementations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Further details on the recurrent units F.1 Implementation details</head><p>For the ConvGRU modules in DVD-GAN-FP, Clark et al. <ref type="bibr" target="#b11">[12]</ref> use ReLU activation functions, rather than the hyperbolic tangent (tanh). For the ConvLSTM module, we experiment with both tanh, which is traditionally used, and the ReLU activation. We find that ReLU significantly outperforms tanh early on, reaching 51.8 train FVD after 180K steps, against 62.7 for tanh. As a result, for our comparison of recurrent units, we report results using the ReLU activation function.</p><p>The hyper-network f used to predict the parameters of our transformations differs depending on the output dimension. In the case of PTSRU, we use a resolution-preserving, 2-hidden layer CNN. For both TSRU and K-TrajGRU, a first subnetwork consists of a single convolutional layer, followed by adaptive max-pooling, of output resolution 4 × 4, then by a single hidden-layer MLP, to predict the set of kernels w used across the input. The pseudo-segmentation map S is obtained using a single convolutional layer applied to the input. For all three architectures, ReLU activations are interleaved between the linear layers, and we use a softmax activation on the output. Each architecture has zero or two hidden layers, and we set the number of output hidden channels (or units in case of linear layers) to half the number of input channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 K-TrajGRU</head><p>Our proposed K-TrajGRU is a kernel-based extension of TrajGRU <ref type="bibr" target="#b59">[60]</ref>, which we further detail here. Formally, it is described by the following equations:</p><formula xml:id="formula_9">θ h,x = f (h t−1 , x t ),<label>(16)</label></formula><formula xml:id="formula_10">h t−1 = w(h t−1 ; θ h,x ),<label>(17)</label></formula><formula xml:id="formula_11">r = σ(W r k [h t−1 ; x t ] + b r ),<label>(18)</label></formula><formula xml:id="formula_12">h t = r h t−1 ,<label>(19)</label></formula><formula xml:id="formula_13">c = ρ(W c k [h t ; x t ] + b c ),<label>(20)</label></formula><formula xml:id="formula_14">u = σ(W u k [h t−1 ; x t ] + b u ),<label>(21)</label></formula><formula xml:id="formula_15">h t = u h t−1 + (1 − u) c,<label>(22)</label></formula><p>where h t−1 and x t denote respectively the past hidden features and the input features; f and w are respectively the shallow convolutional neural network and the factorized warping function presented in the main paper and used by our TSRU modules; k denotes a convolution operation with kernel size k, ρ is the activation function used (in our case, tanh), σ denotes the sigmoid function. denotes elementwise multiplication, and W o and b o are kernel and bias parameters for the convolutions used to produce each intermediate output o.</p><p>Conceptually, we see that K-TrajGRU and TSRU are related, in that they both warp the past hidden features to account for motion. TSRU however has a simpler design, which can be intuitively interpreted: it fuses the warped featuresh t−1 and novelly generated content c, using a predicted gate u. K-TrajGRU, instead, provides the warped features to equations <ref type="bibr" target="#b17">(18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21)</ref>, but still combines the input hidden features h t−1 unchanged, together with c.</p><p>We note that Shi et al. <ref type="bibr" target="#b59">[60]</ref> propose to predict L warped versions of the hidden state, and that our proposed recurrent unit specifically extends the case where L = 1. Increasing L, as well as the addition of a read gate to the TSRU module, are potential ideas for further improving our transformation-based recurrent unit's expressivity, which we leave to be explored in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3 ConvLSTM</head><p>For comparison with other recurrent units, the hidden state and memory cell of our ConvLSTM baseline each have half as many channels as the hidden state used in the other designs, that do not maintain a memory cell (ConvGRU, K-TrajGRU and TSRU variants). The hidden state and the memory cell are initialized with respectively the first and the second half of the conditioning sequence's encoding. Like for the other recurrent units, this encoding is obtained by passing the concatenated image-level features extracted by the encoder for each frame, through a 3x3 convolution layer, followed by a ReLU activation, to compress the representation to the right dimension. Then, the output hidden state and memory cell for each time step are concatenated and passed on to the subsequent residual blocks of the generator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.4 Dynamic ConvLSTM</head><p>The dynamic ConvLSTM proposed by Xu et al. <ref type="bibr" target="#b76">[77]</ref> is conceptually simple: for each sample, a set of kernels are predicted and used in the convolutional layers applied either on the recurrent unit's input features x or on its hidden features h, in the input, forget, output gate equations, as well as for the intermediate state's equation -totaling 8 predicted kernels. The network predicting these kernels, denoted by "Kernel CNN", employs a channel-wise softmax operation along the input channel to increase sparsity of the predicted kernels. We refer the reader to the Section 3.3 of the original paper for a more extensive presentation.</p><p>It is important to note that such dynamic convolutions require a different 2D kernel to be predicted for each input and output channel. To keep to a small number of parameters of the Kernel CNN, Xu et al. <ref type="bibr" target="#b76">[77]</ref> propose to share weights across the input and output channel dimensions, by mapping the difference between image features extracted from the two previous frames F t [i] − F t−1 [j] to the corresponding 2D kernel W (i, j), where F [i] denotes the i − th channel of features F , and where W (i, j) corresponds to the filter used for input channel i and output channel j. This relies on the fact that the generator is autoregressive, and encodes each prediction before making the next one. We adapt this to our architecture, as described later in this section for the sake of completeness.</p><p>Before going into more implementation details however, we draw the attention of the reader to the following important analysis. In contrast with dynamic convolutions, our proposed transformationbased recurrent units all essentially rely on dynamic, depth-wise, locally connected layers. Hence, while these recurrent units predict transformation parameters whose dimension scales with the spatial dimensions of the hidden features, the dimension of the Dynamic ConvLSTM transformation's parameters scales with the number of input and output channels. We find that this leads to a largely increased computational cost, even for medium sized models. Specifically, with extensive use of parallelisation across devices, when using Dynamic ConvLSTMs, our medium-size models, that use a channel multiplier of 48, run at an average step duration of 6.385s for a batch size of 512; while in the same set up, models using ConvGRU (resp. TSRU) run at 284ms (resp. 354ms) per step. In this context, the Dynamic ConvLSTM module hence only allows a very limited width of architectures. In this sense, we argue that it is not scalable, and therefore we do not consider it for comparison with other recurrent units for the purposes of our large-scale setting.</p><p>Implementation details To allow the use of their proposed recurrent unit in our architecture, at time step t, we treat the first c h channels of the previous and current features (x t−1 , x t ) as the input features to the Kernel CNN, where c h denotes the number of channels of the hidden representation of the recurrent unit. We note that this requires the number of channels c x of the inputs features x to be a multiple of c h , so that each difference of features can be mapped to q + 1 kernels, where q is such that c x = q × c h . For the first time step, we use the first c h channels of the sequence encoding to play the role of the previous features x t−1 .</p><p>A second difference is that their proposed unit additionally fuses the past hidden features, and past cell states to produce the hidden and cell states that are input to the module. This operation is denoted by F us − 4 in the paper. This might be redundant with the intended function of the hidden state and memory cell of a ConvLSTM. In practice, it is observed by the authors to bring only a marginal gain in general. Additionally, it is an orthogonal contribution, which could be implemented for any of the other recurrent units we have considered. As a result, we do not implement this aspect of their recurrent unit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.5 TSRU variants</head><p>We provide in <ref type="figure" target="#fig_8">Figure 6</ref> an illustration for all proposed TSRU variants described in Section 3.2.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>T × H/s × W/s clips inside the high resolution video fed to D T , where T , H, W correspond to time, height, and width dimension of the input. Compared with DVD-GAN-FP, this reduces the number of pixels to process per video, from K × H × W + T × H/s × W/s to (K + T ) × H/s × W/s. We call this decomposition DVD-GAN-FP f aster . In practice, we follow DVD-GAN-FP and choose s = 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Information flow for ConvGRU (left) and our proposed TSRU c (right); where α represents elementwise convex combination with coefficient provided by u; k , convolution with kernel size k;, concatenation and , elementwise multiplication. Other notations follow from Eqs (6)-<ref type="bibr" target="#b9">(10)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Qualitative evaluation on UCF-101 test set 1. Top: predictions. Second row: motion field obtained by combining the two lower levels of predicted motion-like features. All sequences are temporally subsampled by two for visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Per-frame SSIM for TrIVD-GAN-FP where SSIM is taken as the max over differing numbers of sampled continuations. Higher is better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Information flow for ConvGRU (top-left) and our proposed TSRU c (top-right), TSRU p (bottom-left) and TSRU s variants (bottom-right); where α represents elementwise convex combination of h t−1 and c with coefficient provided by u; k , convolution with kernel size k; , concatenation and , elementwise multiplication.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>A comparison of different discriminator decompositions for video prediction trained for a fixed time budget evaluated on the Kinetics 600 validation set. We describe each in terms of the types of sub-discriminators it contains: frame-level (F), downsampled frame-level (dF), clipped video (clV), downsampled video (dV) and cropped video (crV). Average step duration in milliseconds (ms) and maximum memory consumption (GB) are measured for a single forward-backward training step, averaged over 30 seconds. We report the step budget for each run in number of thousands of steps (K).</figDesc><table><row><cell>F DF CLV DV CRV</cell><cell>IS</cell><cell>FVD</cell><cell>K</cell><cell>MS</cell><cell>GB</cell></row><row><cell>CLIP</cell><cell>10.9</cell><cell>59.6</cell><cell>758</cell><cell cols="2">488 3.49</cell></row><row><cell>DOWNSAMPLED</cell><cell>8.9</cell><cell cols="4">122.1 1000 370 3.01</cell></row><row><cell>CROPPED</cell><cell>9.4</cell><cell>159.9</cell><cell>997</cell><cell cols="2">371 3.10</cell></row><row><cell>CROPPED + DOWNSAMPLED</cell><cell>11.5</cell><cell>44.1</cell><cell>792</cell><cell cols="2">467 3.30</cell></row><row><cell>MOCOGAN-LIKE K = 1</cell><cell>11.3</cell><cell>47.4</cell><cell>651</cell><cell cols="2">568 3.95</cell></row><row><cell>MOCOGAN-LIKE K = 8</cell><cell>11.4</cell><cell>46.2</cell><cell>561</cell><cell cols="2">660 3.95</cell></row><row><cell>DVD-GAN-FP (OURS)</cell><cell>10.9</cell><cell>55.0</cell><cell>689</cell><cell cols="2">537 3.53</cell></row><row><cell>DVD-GAN-FP f aster</cell><cell>11.7</cell><cell>46.1</cell><cell>817</cell><cell cols="2">453 3.54</cell></row><row><cell>DVD-GAN-FPstronger</cell><cell>11.8</cell><cell>39.3</cell><cell>675</cell><cell cols="2">548 3.65</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of recurrent unit designs for video prediction on Kinetics 600 validation set. All models are trained for 500k steps.</figDesc><table><row><cell></cell><cell cols="2">IS (↑) FVD (↓)</cell></row><row><cell>CONVLSTM</cell><cell>11.4</cell><cell>51.3</cell></row><row><cell>CONVGRU</cell><cell>11.4</cell><cell>49.4</cell></row><row><cell>K-TRAJGRU</cell><cell>11.6</cell><cell>45.9</cell></row><row><cell>TSRUc</cell><cell>11.6</cell><cell>44.2</cell></row><row><cell>TSRUp</cell><cell>11.5</cell><cell>43.7</cell></row><row><cell>TSRUs</cell><cell>11.6</cell><cell>45.8</cell></row><row><cell>PTSRUs</cell><cell>11.5</cell><cell>47.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Video prediction performance on Kinetics-600 test set, without frame skipping. FVD is computed using test set video statistics. We predict 11 frames conditioned on 5 input frames. We provide unbiased estimates of standard deviation over 10 runs.</figDesc><table><row><cell>METHOD</cell><cell>IS (↑)</cell><cell>FVD (↓)</cell></row><row><cell>VIDEO TRANSFORMER</cell><cell>-</cell><cell>170 ± 5</cell></row><row><cell>DVD-GAN-FP</cell><cell>-</cell><cell>69.15 ± 1.16</cell></row><row><cell>TRIVD-GAN-FP</cell><cell cols="2">12.54 ± 0.06 25.74 ± 0.66</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>FVD on BAIR Video Prediction, using the original (O) and balanced (B) splits of the BAIR dataset. On the original split, FVD is estimated using 256 samples, due to the small test set size, leading to a large bias for the FVD (on the order of 100.) We propose a more balanced split of the data, to allow the use of 30K samples to estimate the test FVD.</figDesc><table><row><cell>METHOD</cell><cell>SPLIT</cell><cell>FVD (↓)</cell></row><row><cell>SAVP</cell><cell>O</cell><cell>116.4</cell></row><row><cell>DVD-GAN-FP</cell><cell>O</cell><cell>109.8</cell></row><row><cell>TRIVD-GAN-FP</cell><cell>O</cell><cell>103.3</cell></row><row><cell>VIDEO TRANSFORMER</cell><cell>O</cell><cell>94 ± 2</cell></row><row><cell>TRIVD-GAN-FP</cell><cell>B</cell><cell>31.8 ± 0.2</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our training script is heavily tied to proprietary code, and there is no simple way to release it. However, upon acceptance, we will release functions relating to data processing, models, losses and metrics.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Communicated to us privately by the authors to help reproduce their results.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Jean-Baptiste Alayrac, Lucas Smaira, Jeff Donahue, Jacob Walker, Jordan Hoffman, Carl Doersch, Joao Carreira, Andy Brock, Yusuf Aytar, Matteusz Malinowski, Karel Lenc, Suman Ravuri, Jason Ramapuram, Viorica Patraucean, Simon Osindero and Chloe Rosenberg for their help, feedback and insights.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mesonet: a compact facial video forgery detection network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darius</forename><surname>Afchar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Nozick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junichi</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isao</forename><surname>Echizen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Workshop on Information Forensics and Security (WIFS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Stochastic variational video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Tfgan: Improving conditioning for text-to-video synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Martin Renqiang Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><forename type="middle">Peter</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Graf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Delving deeper into convolutional networks for learning video representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06432</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">High fidelity speech synthesis with adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikołaj</forename><surname>Bińkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Casagrande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><forename type="middle">C</forename><surname>Cobo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Exploration by random network distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harrison</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleg</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.12894</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? A new model and the Kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andras</forename><surname>Banki-Horvath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.01340</idno>
		<title level="m">A short note about Kinetics-600</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A short note on the kinetics-700 human action dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06987</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoderdecoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adversarial video generation on complex datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno>arxiv:1907.06571</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Modulating early visual processing by language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Harm De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jérémie</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Stochastic video generation with a learned prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised learning of disentangled representations from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Emily</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vighnesh</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Birodkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep generative image models using a Laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Emily L Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A learned representation for artistic style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kudlur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Self-supervised visual planning with temporal skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederik</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">X</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep visual foresight for planning robot motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised learning for physical interaction through video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Disentangling propagation and generation for video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhe</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi-Zhi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruth</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Could machine learning break the convection parameterization deadlock?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Gentine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Pritchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Rasp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gael</forename><surname>Reinaudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galen</forename><surname>Yacalis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Geophysical Research Letters</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5742" to="5751" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Controllable video generation with sparse trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">GANs trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multimodal unsupervised image-toimage translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Video prediction with appearance and motion conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunseok</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Time-agnostic prediction: Predicting predictable video frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinesh</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederik</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ivo Danihelka, Oriol Vinyals, Alex Graves, and Koray Kavukcuoglu. Video pixel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Progressive growing of GANs for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The Kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sequential attend, infer, repeat: Generative modelling of moving objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><forename type="middle">R</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingmar</forename><surname>Posner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Melgan: Generative adversarial networks for conditional waveform synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kundan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rithesh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Thibault De Boissiere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><forename type="middle">Zhen</forename><surname>Gestin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Teoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sotelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>De Brébisson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex X</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederik</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.01523</idno>
		<title level="m">Stochastic adversarial video prediction</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Flowgrounded spatial-temporal video prediction from still images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Video generation from text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Martin Renqiang Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Hyun</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><surname>Chul Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geometric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02894</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Future frame prediction for anomaly detection -a new baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongze</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Video frame synthesis using deep voxel flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aseem</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Agarwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Predicting deeper into the future of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pauline</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Predicting future instance segmentation by forecasting convolutional features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pauline</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">cGANs with projection discriminator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh</forename><forename type="middle">Thi</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cuong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dung</forename><forename type="middle">Tien</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11573</idno>
		<title level="m">Duc Thanh Nguyen, and Saeid Nahavandi. Deep learning for deepfakes creation and detection</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Folded recurrent neural networks for future video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oliu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Selva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Spatio-temporal video autoencoder with differentiable memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Viorica Pȃtrȃucean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Video (language) modeling: a baseline for generative models of natural videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelio</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chopra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6604</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Sdc-net: Video prediction using spatially-displaced convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitsum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tarjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Tackling climate change with machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Rolnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Priya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Donti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelly</forename><surname>Kaack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Kochanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Sankaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Slavin Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natasha</forename><surname>Milojevic-Dupont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Jaques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Waldman-Brown</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05433</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekraam</forename><surname>Sabir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Jaiswal</surname></persName>
		</author>
		<title level="m">Wael AbdAlmageed, Iacopo Masi, and Prem Natarajan. Recurrent convolutional strategies for face manipulation detection in videos. Interfaces (GUI)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">TGANv2: Efficient training of large models for video generation with multiple subsampling layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunta</forename><surname>Saito</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.09245</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">L</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhourong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai-Kin</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chun</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Deep learning for precipitation nowcasting: a benchmark and a new model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonard</forename><surname>Lausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai-Kin</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chun</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">MoCoGAN: Decomposing motion and content for video generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Sjoerd Van Steenkiste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Marinier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01717</idno>
		<title level="m">Towards accurate generative models of video: A new metric &amp; challenges</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Decomposing motion and content for natural video sequence prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xunyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Learning to generate long-term future via hierarchical prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungryull</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xunyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Anticipating the future by watching unlabeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Generating the future with adversarial transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Generating videos with scene dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">From pixels to torques: Policy learning with deep dynamical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niklas</forename><surname>Wahlström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">B</forename><surname>Schön</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">Peter</forename><surname>Deisenroth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Dense optical flow prediction from a static image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">An uncertain future: Forecasting from static images using variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero P</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">A locally linear latent dynamics model for control from raw images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Watter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joschka</forename><surname>Boedecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Scaling autoregressive video models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Learning to generate time-lapse videos using multi-stage dynamic generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Structure preserving video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zefan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">On the generalization of gan image forensics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinsheng</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chinese Conference on Biometric Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="134" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Visual dynamics: Probabilistic future frame synthesis via cross convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><forename type="middle">L</forename><surname>Bouman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Object-oriented dynamics predictor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangxiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongjie</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Per-ke</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
