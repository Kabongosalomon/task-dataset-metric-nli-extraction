<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Competitive Collaboration: Joint Unsupervised Learning of Depth, Camera Motion, Optical Flow and Motion Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
							<email>aranjan@tuebingen.mpg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
							<email>vjampani@nvidia.com</email>
							<affiliation key="aff1">
								<address>
									<settlement>NVIDIA 3 MIT</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Balles</surname></persName>
							<email>lballes@tuebingen.mpg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihwan</forename><surname>Kim</surname></persName>
							<email>kihwank@nvidia.com</email>
							<affiliation key="aff1">
								<address>
									<settlement>NVIDIA 3 MIT</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
							<email>deqings@nvidia.com</email>
							<affiliation key="aff1">
								<address>
									<settlement>NVIDIA 3 MIT</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
							<email>jwulff@tuebingen.mpg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
							<email>black@tuebingen.mpg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Competitive Collaboration: Joint Unsupervised Learning of Depth, Camera Motion, Optical Flow and Motion Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We address the unsupervised learning of several interconnected problems in low-level vision: single view depth prediction, camera motion estimation, optical flow, and segmentation of a video into the static scene and moving regions. Our key insight is that these four fundamental vision problems are coupled through geometric constraints. Consequently, learning to solve them together simplifies the problem because the solutions can reinforce each other. We go beyond previous work by exploiting geometry more explicitly and segmenting the scene into static and moving regions. To that end, we introduce Competitive Collaboration, a framework that facilitates the coordinated training of multiple specialized neural networks to solve complex problems. Competitive Collaboration works much like expectation-maximization, but with neural networks that act as both competitors to explain pixels that correspond to static or moving regions, and as collaborators through a moderator that assigns pixels to be either static or independently moving. Our novel method integrates all these problems in a common framework and simultaneously reasons about the segmentation of the scene into moving objects and the static background, the camera motion, depth of the static scene structure, and the optical flow of moving objects. Our model is trained without any supervision and achieves state-of-the-art performance among joint unsupervised methods on all sub-problems. .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep learning methods have achieved state-of-the-art results on computer vision problems with supervision using large amounts of data <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21]</ref>. However, for many vision problems requiring dense, continuous-valued outputs, it is ei-This project was formerly referred by Adversarial Collaboration: Joint Unsupervised Learning of Depth, Camera Motion, Optical Flow and Motion Segmentation ther impractical or expensive to gather ground truth data <ref type="bibr" target="#b5">[6]</ref>. We consider four such problems in this paper: single view depth prediction, camera motion estimation, optical flow, and motion segmentation. Previous work has approached these problems with supervision using real <ref type="bibr" target="#b4">[5]</ref> and synthetic data <ref type="bibr" target="#b3">[4]</ref>. However there is always a realism gap between synthetic and real data, and real data is limited or inaccurate. For example, depth ground truth obtained using LIDAR <ref type="bibr" target="#b5">[6]</ref> is sparse. Furthermore, there are no sensors that provide ground truth optical flow, so all existing datasets with real imagery are limited or approximate <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b12">13]</ref>. Motion segmentation ground truth currently requires manual labeling of all pixels in an image <ref type="bibr" target="#b25">[26]</ref>.</p><p>Problem. Recent work has tried to address the problem of limited training data using unsupervised learning <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b23">24]</ref>. To learn a mapping from pixels to flow, depth, and camera motion without ground truth is challenging because each of these problems is highly ambiguous. To address this, additional constraints are needed and the geometric relations between static scenes, camera motion, and optical flow can be exploited. For example, unsupervised learning of depth and camera motion has been coupled in <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b21">22]</ref>. They use an explainability mask to exclude evidence that cannot be explained by the static scene assumption. Yin et al. <ref type="bibr" target="#b35">[36]</ref> extend this to estimate optical flow as well and use forwardbackward consistency to reason about unexplained pixels. These methods perform poorly on depth <ref type="bibr" target="#b36">[37]</ref> and optical flow <ref type="bibr" target="#b35">[36]</ref> benchmarks. A key reason is that the constraints applied here do not distinguish or segment objects that move independently like people and cars. More generally, not all the data in the unlabeled training set will conform to the model assumptions, and some of it might corrupt the network training. For instance, the training data for depth and camera motion should not contain independently moving objects. Similarly, for optical flow, the data should not contain occlusions, which disrupt the commonly used photometric loss.</p><p>Idea. A typical real-world scene consists of static regions, which do not move in the physical world, and moving objects <ref type="bibr" target="#b34">[35]</ref>. Given depth and camera-motion, we can reason about the static scene in a video sequence. Optical flow, in contrast, reasons about all parts of the scene. Motion segmentation classifies a scene into static and moving regions. Our key insight is that these problems are coupled by the geometry and motion of the scene; therefore solving them jointly is synergistic. We show that by learning jointly from unlabeled data, our coupled networks can partition the dataset and use only the relevant data, resulting in more accurate results than learning without this synergy.</p><p>Approach. To address the problem of joint unsupervised learning, we introduce Competitive Collaboration (CC), a generic framework in which networks learn to collaborate and compete, thereby achieving specific goals. In our specific scenario, Competitive Collaboration is a three player game consisting of two players competing for a resource that is regulated by a third player, moderator. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, we introduce two players in our framework, the static scene reconstructor, R = (D, C), that reasons about the static scene pixels using depth, D, and camera motion, C; and a moving region reconstructor, F , that reasons about pixels in the independently moving regions. These two players compete for training data by reasoning about static-scene and moving-region pixels in an image sequence. The competition is moderated by a motion segmentation network, M , that segments the static scene and moving regions, and distributes training data to the players. However, the moderator also needs training to ensure a fair competition. Therefore, the players, R and F , collaborate to train the moderator, M , such that it classifies static and moving regions correctly in alternating phases of the training cycle. This general framework is similar in spirit to expectation-maximization (EM) but is formulated for neural network training.</p><p>Contributions. In summary our contributions are: 1) We introduce Competitive Collaboration, an unsupervised learning framework where networks act as competitors and collaborators to reach specific goals. 2) We show that jointly training networks with this framework has a synergistic effect on their performance. 3) To our knowledge, our method is the first to use low level information like depth, camera motion and optical flow to solve a segmentation task without any supervision. 4) We achieve state-of-the-art performance on single view depth prediction and camera motion estimation among unsupervised methods. We achieve state of art performance on optical flow among unsupervised methods methods that reason about the geometry of the scene, and introduce the first baseline for fully unsupervised motion segmentation. We even outperform competing methods that use much larger networks <ref type="bibr" target="#b35">[36]</ref> and multiple refinement steps such as network cascading <ref type="bibr" target="#b23">[24]</ref>. 5) We analyze the convergence properties of our method and give an intuition of its generalization using mixed domain learning on MNIST <ref type="bibr" target="#b18">[19]</ref> and SVHN <ref type="bibr" target="#b24">[25]</ref> digits. All our models and code are available at https://github.com/anuragranj/cc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Our method is a three-player game, consisting of two competitors and a moderator, where the moderator takes the role of a critic and two competitors collaborate to train the moderator. The idea of collaboration can also be seen as neural expectation maximization <ref type="bibr" target="#b7">[8]</ref> where one model is trained to distribute data to other models. For unsupervised learning, these ideas have been mainly used to model the data distribution <ref type="bibr" target="#b7">[8]</ref> and have not been applied to unsupervised training of regression or classification problems.</p><p>There is significant recent work on supervised training of single image depth prediction <ref type="bibr" target="#b4">[5]</ref>, camera motion estimation <ref type="bibr" target="#b15">[16]</ref> and optical flow estimation <ref type="bibr" target="#b3">[4]</ref>. However, as labeling large datasets for continuous-valued regression tasks is not trivial, and the methods often rely on synthetic data <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">23]</ref>. Unsupervised methods have tried to independently solve for optical flow <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b33">34]</ref> by minimizing a photometric loss. This is highly underconstrained and thus the methods perform poorly.</p><p>More recent works <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref> have approached estimation of these problems by coupling two or more problems together in an unsupervised learning framework. Zhou et al. <ref type="bibr" target="#b36">[37]</ref> introduce joint unsupervised learning of egomotion and depth from multiple unlabeled frames. To account for moving objects, they learn an explainability mask. However, these masks also capture model failures such as occlusions at depth discontinuities, and are hence not useful for motion segmentation. Mahjourian et al. <ref type="bibr" target="#b21">[22]</ref> use a more explicit geometric loss to jointly learn depth and camera motion for rigid scenes. Yin et al. <ref type="bibr" target="#b35">[36]</ref> add a refinement network to <ref type="bibr" target="#b36">[37]</ref> to also estimate residual optical flow. The estimation of residual flow is designed to account for moving regions, but there is no coupling of the optical flow network with The motion segmentation network, M , masks out static scene pixels from F to produce composite optical flow over the full image. A loss, E, using the composite flow is applied over neighboring frames to train all these models jointly. the depth and camera motion networks. Residual optical flow is obtained using a cascaded refinement network, thus preventing other networks from using flow information to improve themselves. Therefore, recent works show good performance either on depth and camera motion <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref> or on optical flow <ref type="bibr" target="#b23">[24]</ref>, but not on both. Zou et al. <ref type="bibr" target="#b37">[38]</ref> exploit consistency between depth and optical flow to improve performance. The key missing piece that we add is to jointly learn the segmentation of the scene into static and independently-moving regions. This allows the networks to use geometric constraints where they apply and generic flow where they do not. Our work introduces a framework where motion segmentation, flow, depth and camera motion models can be coupled and solved jointly to reason about the complete geometric structure and motion of the scene.</p><p>Competitive Collaboration can be generalized to problems in which the models have intersecting goals where they can compete and collaborate. For example, modeling multi-modal distributions can be accomplished using our framework, whereby each competitor learns the distribution over a mode. In fact, the use of expectation-maximization (EM) in computer vision began with the optical flow problem and was used to segment the scene into "layers" <ref type="bibr" target="#b14">[15]</ref> and was then widely applied to other vision problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Competitive Collaboration</head><p>In our context, Competitive Collaboration is formulated as a three-player game consisting of two players competing for a resource that is regulated by a moderator as illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>. Consider an unlabeled training dataset D = {D i : i ∈ N}, which can be partitioned into two disjoint sets. Two players {R, F } compete to obtain this data as a resource, and each player tries to partition D to minimize its loss. The partition is regulated by the moderator's output m = M (D i ), m ∈ [0, 1] Ω , and Ω is the output domain of the competitors. The competing players minimize their loss function L R , L F respectively such that each player optimizes for itself but not for the group. To resolve this problem, our training cycle consists of two phases. In the first phase, we train the competitors by fixing the moderator network M and minimizing</p><formula xml:id="formula_0">E 1 = i m · L R (R(D i )) + (1 − m) · L F (F (D i )), (1)</formula><p>where · is used to represent elementwise product throughout the paper. However, the moderator M also needs to be trained. This happens in the second phase of the training cycle. The competitors {R, F } form a consensus and train the moderator M such that it correctly distributes the data in the next phase of the training cycle. In the collaboration phase, we fix the competitors and train the moderator by minimizing,</p><formula xml:id="formula_1">E 2 = E 1 + i L M (D i , R, F )<label>(2)</label></formula><p>where L M is a loss that denotes a consensus between the competitors {R, F }. Competitive Collaboration can be applied to more general problems of training multiple task specific networks. In the Appendix A.1, we show the generalization of our method using an example of mixed domain learning on MNIST and SVHN digits, and analyze its convergence properties.</p><p>In the context of jointly learning depth, camera motion, optical flow and motion segmentation, the first player R = (D, C) consists of the depth and camera motion networks that reason about the static regions in the scene. The second player F is the optical flow network that reasons about the moving regions. For training the competitors, the motion segmentation network M selects networks (D, C) on pixels that are static and selects F on pixels that belong to moving regions. The competition ensures that (D, C) reasons only about the static parts and prevents moving pixels from corrupting its training. Similarly, it prevents any static pixels from appearing in the training loss of F , thereby improving its performance in the moving regions. In the second phase of the training cycle, the competitors (D, C) and F now collaborate to reason about static scene and moving regions by forming a consensus that is used as a loss for training the moderator, M . In the rest of this section, we formulate the joint unsupervised estimation of depth, camera motion, optical flow and motion segmentation within this framework.</p><p>Notation. We use {D θ , C φ , F ψ , M χ }, to denote the networks that estimate depth, camera motion, optical flow and motion segmentation respectively. The subscripts {θ, φ, ψ, χ} are the network parameters. We will omit the subscripts in several places for brevity. Consider an image sequence I − , I, I + with target frame I and temporally neighboring reference frames I − , I + . In general, we can have many neighboring frames. In our implementation, we use 5-frame sequences for C φ and M χ but for simplicity use 3 frames to describe our approach. We estimate the depth of the target frame as</p><formula xml:id="formula_2">d = D θ (I).<label>(3)</label></formula><p>We estimate the camera motion, e, of each of the reference frames I − , I + w.r.t. the target frame I as</p><formula xml:id="formula_3">e − , e + = C φ (I − , I, I + ).<label>(4)</label></formula><p>Similarly, we estimate the segmentation of the target image into the static scene and moving regions. The optical flow of the static scene is defined only by the camera motion and depth. This generally refers to the structure of the scene. The moving regions have independent motion w.r.t. the scene. The segmentation masks corresponding to each pair of target and reference image are given by</p><formula xml:id="formula_4">m − , m + = M χ (I − , I, I + ),<label>(5)</label></formula><p>where m − , m + ∈ [0, 1] Ω are the probabilities of regions being static in spatial pixel domain, Ω. Finally, the network F ψ estimates the optical flow. F ψ works with 2 images at a time, and its weights are shared while estimating u − , u + , the backward and forward optical flow 1 respectively.</p><formula xml:id="formula_5">u − = F ψ (I, I − ), u + = F ψ (I, I + ).<label>(6)</label></formula><p>Loss. We learn the parameters of the networks {D θ , C φ , F ψ , M χ } by jointly minimizing the energy</p><formula xml:id="formula_6">E = λ R E R + λ F E F + λ M E M + λ C E C + λ S E S ,<label>(7)</label></formula><p>where {λ R , λ F , λ M , λ C , λ S } are the weights on the respective energy terms. The terms E R and E F are the objectives that are minimized by the two competitors reconstructing static and moving regions respectively. The competition for data is driven by E M . A larger weight λ M will drive more pixels towards static scene reconstructor. The term E C drives the collaboration and E S is a smoothness regularizer. The static scene term, E R minimizes the photometric loss on the static scene pixels given by</p><formula xml:id="formula_7">E R = s∈{+,−} Ω ρ I, w c (I s , e s , d) · m s<label>(8)</label></formula><p>where Ω is the spatial pixel domain, ρ is a robust error function, and w c warps the reference frames towards the target frame according to depth d and camera motion e. Similarly, E F minimizes photometric loss on moving regions</p><formula xml:id="formula_8">E F = s∈{+,−} Ω ρ I, w f (I s , u s ) · (1 − m s )<label>(9)</label></formula><p>where w f warps the reference image using flow u. We show the formulations for w c , w f in the Appendix A.2 and A.3 respectively. We compute the robust error ρ(x, y) as</p><formula xml:id="formula_9">ρ(x,y)=λρ √ (x−y) 2 + 2 +(1−λρ) 1− (2µx µy+c 1 )(2µxy+c 2 ) (µ 2 x +µ 2 y +c 1 )(σx+σy+c 2 )<label>(10)</label></formula><p>where λ ρ is a fixed constant and = 0.01. The second term is also known as the structure similarity loss <ref type="bibr" target="#b32">[33]</ref> that has been used in previous work <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b35">36]</ref>, and µ x , σ x are the local mean and variance over the pixel neighborhood with c 1 = 0.01 2 and c 2 = 0.03 2 .</p><p>The loss E M minimizes the cross entropy, H, between the masks and a unit tensor regulated by λ M</p><formula xml:id="formula_10">E M = s∈{+,−} Ω H(1, m s ).<label>(11)</label></formula><p>A larger λ M gives preference to the static scene reconstructor R, biasing the scene towards being static.</p><p>Let ν(e, d) represent the optical flow induced by camera motion e and depth d, as described in the Appendix A.2. The consensus loss E C drives the collaboration and constrains the masks to segment moving objects by taking a consensus between flow of the static scene given by ν(e, d) and optical flow estimates from F ψ . It is given by</p><formula xml:id="formula_11">E C = s∈{+,−} Ω H I ρ R &lt;ρ F ∨ I ||ν(es,d)−us||&lt;λc , m s (12)</formula><p>where I ∈ {0, 1} is an indicator function and equals 1 if the condition in the subscript is true. The first indicator function favors mask assignments to the competitor that achieves lower photometric error on a pixel by comparing ρ R = ρ(I, w c (I s , e s , d)) and ρ F = ρ(I, w f (I s , u s )). In the second indicator function, the threshold λ c forces I = 1, if the static scene flow ν(e, d) is close to the optical flow u, indicating a static scene. The symbol ∨ denotes logical OR between indicator functions. The consensus loss E C encourages a pixel to be labeled as static if R has a lower photometric error than F or if the induced flow of R is similar to that of F . Finally, the smoothness term E S acts as a regularizer on depth, segmentations and flow,</p><formula xml:id="formula_12">E S = Ω ||λ e ∇d|| 2 + ||λ e ∇u − || 2 + ||λ e ∇u + || 2 +||λ e ∇m − || 2 + ||λ e ∇m + || 2 ,<label>(13)</label></formula><p>where λ e = e −∇I (elementwise) and ∇ is the first derivative along spatial directions <ref type="bibr" target="#b27">[28]</ref>. The term λ e ensures that smoothness is guided by edges of the images.</p><p>Inference. The depth d and camera motion e are directly inferred from network outputs. The motion segmentation m * is obtained by the output of mask network M χ and the consensus between the static flow and optical flow estimates from F χ . It is given by</p><formula xml:id="formula_13">m * = I m+·m−&gt;0.5 ∨ I ||ν(e+,d)−u+||&lt;λc .<label>(14)</label></formula><p>The first term takes the intersection of mask probabilities inferred by M χ using forward and backward reference frames. The second term takes a consensus between flow estimated from R = (D θ , C φ ) and F ψ to reason about the masks. The final masks are obtained by taking the union of both terms. Finally, the full optical flow, u * , between (I, I + ) is a composite of optical flows from the static scene and the independently moving regions given by</p><formula xml:id="formula_14">u * = I m * &gt;0.5 · ν(e + , d) + I m * ≤0.5 · u + .<label>(15)</label></formula><p>The loss in Eq. <ref type="formula" target="#formula_6">(7)</ref> is formulated to minimize the reconstruction error of the neighboring frames. Two competitors, the static scene reconstructor R = (D θ , C φ ) and moving region reconstructor F ψ minimize this loss. The reconstructor R reasons about the static scene using Eq. (8) and the reconstructor F ψ reasons about the moving regions using Eq. (9). The moderation is achieved by the mask network, M χ using Eq. <ref type="bibr" target="#b10">(11)</ref>. Furthermore, the collaboration between R, F is driven using Eq. (12) to train the network M χ . If the scenes are completely static, and only the camera moves, the mask forces (D θ , C φ ) to reconstruct the whole scene. However, (D θ , C φ ) are wrong in the independently moving regions of the scene, and these regions are reconstructed using F ψ . The moderator M χ is trained to segment static and moving regions correctly by taking a consensus from (D θ , C φ ) and F ψ to reason about static and moving parts on the scene, as seen in Eq. <ref type="bibr" target="#b11">(12)</ref>. Therefore, our training cycle has two phases. In the first phase, the moderator M χ drives competition between two models (D θ , C φ ) and F ψ using Eqs. <ref type="bibr" target="#b7">(8,</ref><ref type="bibr" target="#b8">9)</ref>. In the second phase, the competitors (D θ , C φ ) and F ψ collaborate together to train the moderator M χ using Eqs. <ref type="bibr" target="#b10">(11,</ref><ref type="bibr" target="#b11">12)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Network Architecture. For the depth network, we experiment with DispNetS <ref type="bibr" target="#b36">[37]</ref> and DispResNet where we replace convolutional blocks with residual blocks <ref type="bibr" target="#b9">[10]</ref>. The network D θ takes a single RGB image as input and outputs depth. For the flow network, F ψ , we experiment with both FlowNetC <ref type="bibr" target="#b3">[4]</ref> and PWC-Net <ref type="bibr" target="#b29">[30]</ref>. The PWC-Net uses the multi-frame unsupervised learning framework from Janai et al. <ref type="bibr" target="#b11">[12]</ref>. The network F ψ computes optical flow between a pair of frames. The networks C φ , M χ take a 5 frame sequence (I −− , I − , I, I + , I ++ ) as input. The mask network M χ has an encoder-decoder architecture. The encoder</p><formula xml:id="formula_15">Result: Trained Network Parameters, (θ, φ, ψ, χ) Define λ = (λ R , λ F , λ M , λ C ); Randomly initialize (θ, φ, ψ, χ); Update (θ, φ) by jointly training (D θ , C φ ) with λ = (1.0, 0.0, 0.0, 0.0); Update ψ by training F ψ with λ = (0.0, 1.0, 0.0, 0.0); Update χ by jointly training (D θ , C φ , F ψ , M χ ) with λ = (1.0, 0.5, 0.0, 0.0); Loop Competition Step Update θ, φ by jointly training (D θ , C φ , F ψ , M χ ) with λ = (1.0, 0.5, 0.05, 0) ; Update ψ by jointly training (D θ , C φ , F ψ , M χ )</formula><p>with λ = (0.0, 1.0, 0.005, 0) ; Network Training. We use raw KITTI sequences <ref type="bibr" target="#b5">[6]</ref> for training using Eigen et al.'s split <ref type="bibr" target="#b4">[5]</ref> that is consistent across related works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref>. We train the networks with a batch size of 4 and learning rate of 10 −4 using ADAM <ref type="bibr" target="#b16">[17]</ref> optimization. The images are scaled to 256 × 832 for training. The data is augmented with random scaling, cropping and horizontal flips. We use Algorithm 1 for training. Initially, we train (D θ , C φ ) with only photometric loss over static pixels E R and smoothness loss E S while other loss terms are set to zero. Similarly, we train F ψ independently with photometric loss over all pixels and smoothness losses. The models (D θ , C φ ), F ψ at this stage are referred to as 'basic' models in our experiments. We then learn M χ using the joint loss. We use λ R = 1.0, λ F = 0.5 for joint training because the static scene reconstructor R uses 4 reference frames in its loss, whereas the optical flow network F uses 2 frames. Hence, these weights normalize the loss per neighboring frame. We iteratively train (D θ , C φ ), F ψ , M χ using the joint loss while keeping the other network weights fixed. The consensus weight λ C = 0.3 is used only while training the mask network. Other constants are fixed with λ S = 0.005, and threshold in Eq. <ref type="bibr" target="#b13">(14)</ref>, λ c = 0.001. The constant λ ρ = 0.003 regulates the SSIM loss and is chosen empirically. We iteratively train the competitors (D θ , C φ ), F ψ and moderator M χ for about 100,000 iterations at each step until validation error saturates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Monocular Depth and Camera Motion Estimation.</head><p>We obtain state of the art results on single view depth prediction and camera motion estimation as shown in <ref type="table">Tables 1 and 3</ref>. The depth is evaluated on the Eigen et al. <ref type="bibr" target="#b4">[5]</ref> split of the raw KITTI dataset <ref type="bibr" target="#b5">[6]</ref> and camera motion is evaluated on the KITTI Odometry dataset <ref type="bibr" target="#b5">[6]</ref>. These evaluation frameworks are consistent with previous work <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b35">36]</ref>. All depth maps are capped at 80 meters. As shown in <ref type="table">Table 1</ref>, by training our method only on KITTI <ref type="bibr" target="#b5">[6]</ref>, we get similar or better performance than competing methods like <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b37">38]</ref> that use a much bigger Resnet-50 architecture <ref type="bibr" target="#b9">[10]</ref> and are trained on the larger Cityscapes dataset <ref type="bibr" target="#b2">[3]</ref>. Using Cityscapes in our training further improves our performance on depth estimation benchmarks (cs+k in <ref type="table">Table 1</ref>).</p><p>Ablation studies on depth estimation are shown in <ref type="table" target="#tab_1">Table  2</ref>. In the basic mode, our network architecture, DispNet for depth and camera motion estimation is most similar to <ref type="bibr" target="#b36">[37]</ref> and this is reflected in the performance of our basic model. We get some performance improvements by adding the SSIM loss <ref type="bibr" target="#b32">[33]</ref>. However, we observe that using the Competitive Collaboration (CC) framework with a joint loss results in larger performance gains in both tasks. Further improvements are obtained by using a better network architecture, DispResNet. Greater improvements in depth estimation are obtained when we use a better network for flow, which shows that improving on one task improves the performance of the other in the CC framework (row 4 vs 5 in <ref type="table" target="#tab_1">Table 2</ref>).</p><p>The camera motion estimation also shows similar performance trends as shown in <ref type="table">Table 3</ref>. Using a basic model, we achieve similar performance as the baseline <ref type="bibr" target="#b36">[37]</ref>, which improves with the addition of the SSIM loss. Using the CC framework leads to further improvements in performance.</p><p>In summary, we show that joint training using CC boosts performance of single view depth prediction and camera motion estimation. We show qualitative results in <ref type="figure" target="#fig_3">Figure 4</ref>. In the Appendix, we show additional evaluations using Make3D dataset <ref type="bibr" target="#b28">[29]</ref> (A.6) and more qualitative results (A.5).  Zhou et al. <ref type="bibr" target="#b36">[37]</ref> 0.016 ± 0.009 0.013 ± 0.009</p><p>Mahjourian et al. <ref type="bibr" target="#b21">[22]</ref> 0.013 ± 0.010 0.012 ± 0.011 Geonet <ref type="bibr" target="#b35">[36]</ref> 0.012 ± 0.007 0.012 ± 0.009 DF-Net <ref type="bibr" target="#b37">[38]</ref> 0.017 ± 0.007 0.015 ± 0.009 Basic (ours) 0.022 ± 0.010 0.018 ± 0.011 Basic + ssim (ours) 0.017 ± 0.009 0.015 ± 0.009</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CC + ssim (ours)</head><p>0.012 ± 0.007 0.012 ± 0.008 <ref type="table">Table 3</ref>: Results on Pose Estimation.</p><p>Optical Flow Estimation. We compare the performance of our approach with competing methods using the KITTI 2015 training set <ref type="bibr" target="#b5">[6]</ref> to be consistent with previous work <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b35">36]</ref>. We obtain state of the art performance among joint methods as shown in <ref type="table" target="#tab_3">Table 4</ref>. Unsupervised fine tuning (CC-uft) by setting λ M = 0.02 gives more improvements than CC as masks now choose best flow between R and F without being overconstrained to choose R. In contrast, UnFlow-CSS <ref type="bibr" target="#b23">[24]</ref> uses 3 cascaded networks to refine optical flow at each stage. Geonet <ref type="bibr" target="#b35">[36]</ref> and DF-Net <ref type="bibr" target="#b37">[38]</ref> are more similar to our architecture but use a larger ResNet-50 architecture. Back2Future <ref type="bibr" target="#b11">[12]</ref> performs better than our method in terms of outlier error, but not in terms of average end point error due to better occlusion reasoning.</p><p>In <ref type="table" target="#tab_4">Table 5</ref>, we observe that by training the static scene reconstructor R or moving region reconstructor F independently leads to worse performance. This happens because R can not reason about dynamic moving objects in the scene. Similarly F is not as good as R for reasoning about static parts of the scene, especially in occluded regions. Using them together, and compositing the optical flow from both as shown in Eq. (15) leads to a large improvement in performance. Moreover, using better network architectures further improves the performance under CC framework. We show qualitative results in <ref type="figure" target="#fig_3">Figure 4</ref> and in the Appendix A.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Train</head><p>Test Method EPE Fl Fl FlowNet2 <ref type="bibr" target="#b10">[11]</ref> 10.06 30.37 % -SPyNet <ref type="bibr" target="#b26">[27]</ref> 20.56 44.78% -UnFlow-C <ref type="bibr" target="#b23">[24]</ref> 8.80 28.94% 29.46% UnFlow-CSS <ref type="bibr" target="#b23">[24]</ref> 8.10 23.27% -Back2Future <ref type="bibr" target="#b11">[12]</ref> 6.59 -22.94% Back2Future* <ref type="bibr" target="#b11">[12]</ref> 7.04 24.21% -Geonet <ref type="bibr" target="#b35">[36]</ref> 10.81 --DF-Net <ref type="bibr" target="#b37">[38]</ref> 8    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Discussion</head><p>Typically, learning to infer depth from a single image requires training images with ground truth depth scans, and learning to compute optical flow relies on synthetic data, which may not generalize to real image sequences. For static scenes, observed by a moving camera, these two problems are related by camera motion; depth and camera motion completely determine the 2D optical flow. This holds true over several frames if the scene is static and only the camera moves. Thus by combining depth, camera, and flow estimation, we can learn single-image depth by using information from several frames during training. This is particularly critical for unsupervised training since both depth and optical flow are highly ill-posed. Combining evidence from multiple tasks and multiple frames helps to synergistically constrain the problem. This alone is not enough, however, as real scenes contain multiple moving objects that do not conform to static scene geometry. Consequently, we also learn to segment the scene into static and moving regions without supervision. In the independently moving regions, a generic flow network learns to estimate the optical flow.</p><p>To facilitate this process we introduce Competitive Collaboration in which networks both compete and cooperate. We demonstrate that this results in top performance among unsupervised methods for all subproblems. Additionally, the moderator learns to segment the scene into static and moving regions without any direct supervision.</p><p>Future Work. We can add small amounts of supervised training, with which we expect to significantly boost performance on benchmarks, cf. <ref type="bibr" target="#b23">[24]</ref>. We could use, for example, sparse depth and flow from KITTI and segmentation from Cityscapes to selectively provide ground truth to different networks. A richer segmentation network together with semantic segmentation should improve non-rigid segmentation. For automotive applications, the depth map formulation should be extended to a world coordinate system, which would support the integration of depth information over long image sequences. Finally, as shown in <ref type="bibr" target="#b34">[35]</ref>, the key ideas of using layers and geometry apply to general scenes beyond the automotive case and we should be able to train this method to work with generic scenes and camera motions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Competitive Collaboration as a General Learning Framework</head><p>Competitive collaboration (CC) can be seen as a general learning framework for training multiple task-specific networks. To showcase this generality, we demonstrate CC on a mixed-domain classification problem in Section A.1.1 and analyze CC convergence properties in Section A.1.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.1 Mixed Domain Classification</head><p>Digit classification is the task of classifying a given image I into one of the 10 digit classes t ∈ {0, 1, 2, .., 9}. Two most widely used datasets for digit classification include images of the postal code digits, MNIST <ref type="bibr" target="#b18">[19]</ref> and street view house numbers, SVHN <ref type="bibr" target="#b24">[25]</ref>. For our setup, we take the samples from both of the datasets, and shuffle them together. This means that, although an image and a target, (I i , t i ) form a pair, there is no information if the digits came from MNIST or SVHN.</p><p>We now train our model under Competitive Collaboration framework given the mixed-domain dataset MNIST+SVHN, a mixture of MNIST and SVHN. The model consists of two networks R x and F x that compete with each other regulated by a moderator M y which assigns training data to each of the competitors. Here, x denotes the combined weights of the two competitor networks (R, F ) and y denotes the weight of the moderator network M . The networks are trained using an alternate optimization procedure consisting of two phases. In the competition phase, we train the competitors by fixing the moderator M and minimizing,</p><formula xml:id="formula_16">E 1 = i m i · H(R x (I i ), t i ) + (1 − m i ) · H(F x (I i ), t i ) (16) where m i = M y (I i ) ∈ [0, 1]</formula><p>is the output of the moderator and is the probability of assigning a sample to R x . H(R x (I i ), t i ) is the cross entropy classification loss on the network R x and a similar loss is applied on network F x .</p><p>During the collaboration phase, we fix the competitors and train the moderator by minimizing,</p><formula xml:id="formula_17">E 2 = E 1 + i λ · − log(m i + ε) if L Ri &lt; L Fi , − log(1 − m i + ε) if L Ri ≥ L Fi .<label>(17)</label></formula><p>where L Ri = H(R x (I i ), t i ) is the cross entropy loss from network R x and similarly L Fi = H(F x (I i ), t i ). In addition to the above loss function E 1 , we use an additional constraint on the moderator output that encourages the variance of m, σ 2 m = Σ i (m i −m) 2 to be high, wherem is the mean of m within a batch. This encourages the moderator to assign images to both the models, instead of always assigning them to a single model.</p><p>In an ideal case, we expect the moderator to correctly classify MNIST digits from SVHN digits. This would enable each of the competitors to specialize on either MNIST or SVHN, but not both. In such a case, the accuracy of the model under CC would be better than training a single network on the MNIST+SVHN mixture.</p><p>Experimental Results For simplicity, we use a CNN with 2 convolutional layers followed by 2 fully-connected layers for both the digit classification networks (R, F ) as well as the moderator network M . Each of the convolutional layers use a kerel size of 5 and 40 feature maps. Each of the fully connected layers have 40 neurons.</p><p>We   the same dataset. We see that our performance is better on the mixture dataset as well as individual datasets (see <ref type="table" target="#tab_7">Table 7</ref>). As shown in <ref type="table" target="#tab_7">Table 7</ref>, the network R specializes on SVHN digits and network F specializes on MNIST digits. By using the networks (R, F, M ), we get the best results as M picks the specialized networks depending on the data sample.</p><p>We also examine the classification accuracy of the moderator on MNIST and SVHN digits. We observe that moderator can accurately classify the digits into either MNIST or SVHN without any labels (see <ref type="table" target="#tab_8">Table 8</ref>). The moderator learns to assign 100% of MNIST digits to F and 100% of SVHN digits to R. This experiment provides further evidence to support the notion that CC can be generalized to other problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.2 Theoretical Analysis</head><p>Competitive Collaboration is an alternating optimization procedure. In the competition phase, we minimize E 1 with respect to x; in the collaboration phase we minimize E 2 = E 1 + λL M with respect to y. One might rightfully worry about the convergence properties of such a procedure, where we optimize different objectives in the alternating steps.</p><p>It is important to note that-while E 1 and E 2 are different functions-they are in fact closely related. For example, they have the same minimizer with respect to the moderator output, namely assigning all the mass to the network with lower loss. Ideally, we would want to use E 1 as the objective function in both phases, but resort to using E 2 in the collaboration phase, since it has empirically proven to be more efficient in pushing the moderator towards this optimal choice.</p><p>Hence, while we are minimizing different objective func-tions in the competition and collaboration phases, they are closely related and have the same "goal". In the following, we formalize this mathematically by identifying general assumptions on how "similar" two functions have to be for such an alternating optimization procedure to converge. Roughly speaking, we need the gradients of the two objectives to form an acute angle and to be of similar scales. We will then discuss to what extent these assumptions are satisfied in the case of Competitive Collaboration. Proofs are outsourced to the end of this section for readability.</p><p>General Convergence Theorem Assume we have two functions f, g :</p><formula xml:id="formula_18">R n × R m → R<label>(18)</label></formula><p>and are performing alternating gradient descent updates of the form</p><formula xml:id="formula_19">x t+1 = x t − α∇ x f (x t , y t ),<label>(19)</label></formula><formula xml:id="formula_20">y t+1 = y t − β∇ y g(x t+1 , y t ).<label>(20)</label></formula><p>We consider the case of single alternating gradient descent for convenience in the analysis. With minor modifications, the following analysis also extends to the case of multiple gradient descent updates (or even exact minimization) in each of the alternating steps. The following Theorem formulates assumptions on f and g under which such an alternating optimization procedure converges to a first-order stationary point of f . </p><formula xml:id="formula_21">β ∇ y f (x, y), ∇ y g(x, y) ≥ G 2 (x)β 2 2 ∇ y g(x, y) 2 + B ∇ y f (x, y) 2<label>(21)</label></formula><p>then (x t , y t ) converges to a first-order stationary point of f .</p><p>Eq. <ref type="formula" target="#formula_1">(21)</ref> is a somewhat technical assumption that lowerbounds the inner product of the two gradients in terms of their norms and, thus, encodes that these gradients have to form an acute angle and be of similar scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convergence of Competitive Collaboration</head><p>We now discuss to what extent the assumptions for Theorem 1 are satisfied in the case of Competitive Collaboration. For the mathematical considerations to follow, we introduce a slightly more abstract notation for the objective functions of Competitive Collaboration. For a single data point, E 1 has the form</p><formula xml:id="formula_22">f (x, y) = M (y)L R (x) + (1 − M (y))L F (x),<label>(22)</label></formula><p>where M (y) ∈ [0, 1] is a function of y (the weights of the moderator) and L R (x), L F (x) &gt; 0 are functions of x (the weights of the two competing networks). The loss function</p><formula xml:id="formula_23">E 2 reads g(x, y) = f (x, y) + λ · − log(M (y) + ε) if L R (x) &lt; L F (x), − log(1 − M (y) + ε) if L R (x) ≥ L F (x).<label>(23)</label></formula><p>The following Proposition shows that f and g satisfy the conditions of Theorem 1 under certain assumptions.</p><p>Proposition 1. Let f and g be defined by Equations <ref type="formula" target="#formula_1">(22)</ref> and <ref type="formula" target="#formula_1">(23)</ref>, respectively. If M (y), L R (x) and L F (x) are Lipschitz smooth, then f and g fulfill the assumptions of Theorem 1.</p><p>The smoothness conditions on M (y), L R (x), L F (x) are standard as they are, for example, needed to guarantee convergence of gradient descent for optimizing any of these objective functions individually.</p><p>This Proposition shows that the objectives for individual data points satisfy Theorem 1. In practice, however, we are concerned with multiple data points and objectives of the form</p><formula xml:id="formula_24">f (x, y) = 1 n n i=1 f (i) (x, y),<label>(24)</label></formula><p>where</p><formula xml:id="formula_25">f (i) (x, y) =M (i) (y)L (i) R (x) + (1 − M (i) (y))L (i) F (x),<label>(25)</label></formula><p>and</p><formula xml:id="formula_26">g(x, y) = 1 n n i=1 g (i) (x, y),<label>(26)</label></formula><p>where</p><formula xml:id="formula_27">g (i) (x, y) = f (i) (x, y) + λ · − log(M (i) (y) + ε) if L (i) R (x) &lt; L (i) F (x), − log(1 − M (i) (y) + ε) if L (i) R (x) ≥ L (i) F (x).<label>(27)</label></formula><p>While we have just found a suitable lower bound on the inner product of ∇ y f (i) and ∇ y g (i) , unfortunately, the sum structure of ∇ y f and ∇ y g makes it really hard to say anything definitive about the value of their inner product. It is plausible to assume that ∇ y f and ∇ y g will be sufficiently close to guarantee convergence in practical settings. However, the theory developed in Theorem 1 does not directly apply.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.3 Proofs</head><p>Proof of Theorem 1. The update of x is a straight-forward gradient descent step on f . Using the Lipschitz bound on f , we get</p><formula xml:id="formula_28">f (x t+1 , y t ) ≤ f (x t , y t ) − α ∇ x f (x t , y t ), ∇ x f (x t , y t ) + G 1 α 2 2 ∇ x f (x t , y t ) 2 = f (x t , y t ) − α − G 1 α 2 2 ∇ x f (x t , y t ) 2 ≤ f (x t , y t ) − A ∇ x f (x t , y t ) 2<label>(28)</label></formula><p>with A &gt; 0 due to our assumption on α. For the update of y, we have</p><formula xml:id="formula_29">f (x t+1 , y t+1 ) ≤ f (x t+1 , y t ) − β ∇ y f (x t+1 , y t ), ∇ y g(x t+1 , y t ) + β 2 G 2 (x) 2 ∇ y g(x t+1 , y t ) 2 .<label>(29)</label></formula><p>Using the assumption on the inner product, this yields</p><formula xml:id="formula_30">f (x t+1 , y t+1 ) ≤ f (x t+1 , y t ) − B ∇ y f (x t+1 , y t ) 2 .<label>(30)</label></formula><p>Combining the two equations, we get</p><formula xml:id="formula_31">f (x t+1 , y t+1 ) ≤ f (x t , y t ) − A ∇ x f (x t , y t ) 2 − B ∇ y f (x t+1 , y t ) 2 ≤ f (x t , y t ) − C ∇ x f (x t , y t ) 2 + ∇ y f (x t+1 , y t ) 2 .<label>(31)</label></formula><p>with C = max(A, B). We define G t = ∇ x f (x t , y t ) 2 + ∇ y f (x t+1 , y t ) 2 and rewrite this as</p><formula xml:id="formula_32">G t ≤ f (x t , y t ) − f (x t+1 , y t+1 ) C<label>(32)</label></formula><p>Summing this equation for t = 0, . . . , T , we get</p><formula xml:id="formula_33">T t=0 G t ≤ f (x 0 , y 0 ) − f (x T +1 , y T +1 ) C .<label>(33)</label></formula><p>Since f is lower-bounded, this implies G t → 0, which in turn implies convergence to a first-order stationary point of f .</p><p>Proof of Proposition 1.</p><p>The gradient of f with respect to x is</p><formula xml:id="formula_34">∇ x f (x, y) = M (y)∇L R (x) + (1 − M (y))∇L F (x)<label>(34)</label></formula><p>Since M (y) is bounded, ∇ x f is Lipschitz continuous in x given that L R and L F are Lipschitz smooth.</p><p>For the assumptions on the y-gradients, we fix x and treat the two cases in the definition of g separately. We only consider the case L R (x) &lt; L F (x) here, the reverse case is completely analogous. Define</p><formula xml:id="formula_35">L(x) = L F (x) − L R (x) &gt; 0.</formula><p>The gradient of f with respect to y is Image Ground Truth Prediction <ref type="figure">Figure 5</ref>: Qualitative results on Make3D test set.</p><p>roads. In addition, it segments other moving objects, such as the bicyclist. We compare the qualitative results for single image depth prediction in <ref type="figure">Figure 8</ref>. We also contrast our results with basic models that were trained independently without a joint loss in <ref type="figure">Figure 9</ref>. We observe that our model produces better results, capturing moving objects such as cars and bikes, as well as surface edges of trees, pavements and buildings.</p><p>We compare the qualitative results for optical flow estimation in <ref type="figure" target="#fig_0">Figure 10</ref>. We show that our method performs better than UnFlow <ref type="bibr" target="#b23">[24]</ref>, Geonet <ref type="bibr" target="#b35">[36]</ref> and DF-Net <ref type="bibr" target="#b37">[38]</ref> . Our flow estimations are better at the boundaries of the cars and pavements. In contrast, competing methods produce blurry flow fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6. Additional Experiments</head><p>Depth evaluation on Make3D dataset. We also test on the Make3D dataset <ref type="bibr" target="#b28">[29]</ref> without training on it. We use our our model that is trained only on Cityscapes and KITTI. Our method outperforms previous work <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b6">7]</ref> as shown in <ref type="table" target="#tab_9">Table 9</ref>. We show qualitative results in <ref type="figure">Fig. 5</ref>.</p><p>Pose evaluation on Sintel. We test on Sintel's alley sequence <ref type="bibr" target="#b1">[2]</ref> without training on it and compare it with Zhou et al. <ref type="bibr" target="#b36">[37]</ref>. For this comparison, Zhou et al.'s model is taken from Pinard's implementation. We show quantitative evaluation using relative errors on pose in <ref type="table" target="#tab_10">Table 10</ref>.</p><p>Training using a shared encoder. We train the camera motion network, C and motion segmentation network, M using a common shared encoder but different decoders. Intuitively, it seems that camera motion network can benefit from knowing static regions in a scene, which are learned by the motion segmentation network. However, we observe a Zhou <ref type="bibr" target="#b36">[37]</ref> DF-Net <ref type="bibr" target="#b37">[38]</ref> Godard <ref type="bibr" target="#b6">[7]</ref> CC (ours) 0.383 0.331 0.361 0.320  <ref type="bibr" target="#b36">[37]</ref> 0.002 ± 0.001 0.027 ± 0.019 CC (ours) 0.002 ± 0.001 0.022 ± 0.015 Sequence 09 Sequence 10 Shared Encoder 0.017 ± 0.009 0.015 ± 0.009 Uncoupled Networks 0.012 ± 0.007 0.012 ± 0.008 <ref type="table">Table 11</ref>: Absolute Trajectory errors on KITTI Odometry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Depth Pose Flow Mask</p><p>Geonet <ref type="bibr" target="#b35">[36]</ref> 15ms 4ms 45ms -CC (ours) 13ms 2ms 34ms 3ms performance degradation on camera motion estimates <ref type="table">(Table  11</ref>). The degradation of results using a shared encoder are because feature encodings for one network might not be optimal for other networks. Our observation is consistent with Godard et al. <ref type="bibr" target="#b6">[7]</ref> (Supp. Mat. <ref type="table" target="#tab_3">Table 4</ref>), where sharing an encoder for depth and camera motion estimation improves depth but the perfomance on camera motion estimates are not as good.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7. Timing Analysis</head><p>We analyze inference time of our network and compare it with Geonet <ref type="bibr" target="#b35">[36]</ref> in <ref type="table" target="#tab_1">Table 12</ref>. We observe that our networks have a faster run time using the same sized 128 ×416 images on a single TitanX GPU. This is because our networks are simpler and smaller than ones used by Geonet.</p><p>For training, we measure the time taken for each iteration consisting of forward and backward pass using a batch size of 4. Training depth and camera motion networks (D, C) takes 0.96s per iteration. Traing the mask network, M takes 0.48s per iteration, and the flow network F takes 1.32s per iteration. All iterations have a batch size of 4. In total, it takes about 7 days for all the networks to train starting with random initialization on a single 16GB Tesla V100. . Convolutional layers are red (stride 2) and orange (stride 1) and upconvolution layers are green (stride 2). Other colors refer to special layers. Each layer is followed by ReLU, except prediction layers. In each block, the numbers indicate the number of channels of the input feature map, the number of channels of the output feature map, and the filter size. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>Ground Truth Zhou et al. <ref type="bibr" target="#b36">[37]</ref> Geonet <ref type="bibr" target="#b35">[36]</ref> DF-Net <ref type="bibr" target="#b37">[38]</ref> CC (ours) <ref type="figure">Figure 8</ref>: Qualitative results on single view depth prediction. Top row: we show image, interpolated ground truth depths, Zhou et al. <ref type="bibr" target="#b36">[37]</ref> results. Bottom row: we show results from Geonet <ref type="bibr" target="#b35">[36]</ref>, DF-Net <ref type="bibr" target="#b37">[38]</ref> and CC (ours) results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>Ground Truth CC (DispResNet) Basic (DispNet) Basic+ssim (DispNet) CC (DispNet) <ref type="figure">Figure 9</ref>: Ablation studies on single view depth prediction. Top row: we show image, interpolated ground truth depths, CC using DispResNet architecture. Bottom row: we show results using Basic, Basic+ssim and CC models using DispNet architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>Ground Truth UnFlow-CSS <ref type="bibr" target="#b23">[24]</ref> Geonet <ref type="bibr" target="#b35">[36]</ref> DF-Net <ref type="bibr" target="#b37">[38]</ref> CC (ours) <ref type="figure" target="#fig_0">Figure 10</ref>: Qualitative results on Optical Flow estimation. Top row: we show image 1, ground truth flow, and predictions from UnFlow-CSS <ref type="bibr" target="#b23">[24]</ref>. Bottom row: we show predictions from Geonet <ref type="bibr" target="#b35">[36]</ref>, DF-Net <ref type="bibr" target="#b37">[38]</ref> and CC (ours) model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Unsupervised Learning of Depth, Camera Motion, Optical Flow and Motion Segmentation. Left, top to bottom: sample image, soft masks representing motion segmentation, estimated depth map. Right, top to bottom: static scene optical flow, segmented flow in the moving regions and combined optical flow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The network R = (D, C) reasons about the scene by estimating optical flow over static regions using depth, D, and camera motion, C. The optical flow network F estimates flow over the whole image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Training cycle of Competitive Collaboration: The moderator M drives two competitors {R, F } (first phase, left). Later, the competitors collaborate to train the moderator to ensure fair competition in the next iteration (second phase, right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Collaboration Step Update χ by jointly training (D θ , C φ , F ψ , M χ ) with λ = (1.0, 0.5, 0.005, 0.3) ; EndLoop Algorithm 1: Network Training Algorithm Visual results. Top to bottom: Sample image, estimated depth, soft consensus masks, motion segmented optical flow and combined optical flow.consists of stacked residual convolutional layers. The decoder has stacked upconvolutional layers to produce masks (m −− , m − , m + , m ++ ) of the reference frames. The camera motion network C φ consists of stacked convolutions followed by adaptive average pooling of feature maps to get the camera motions (e −− , e − , e + , e ++ ). The networks D θ , F ψ , M χ output their results at 6 different spatial scales. The predictions at the finest scale are used. The highest scale is of the same resolution as the image, and each lower scale reduces the resolution by a factor of 2. We show the network architecture details in the Appendix A.4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Theorem 1 .</head><label>1</label><figDesc>Assume f is lower-bounded and x → ∇ x f (x, y) is Lipschitz continuous with constant G 1 for every y and y → ∇ y f (x, y) is Lipschitz continuous with constant G 2 (x). Assume α ≤ 2L −1 1 . If there is a constant B &gt; 0 such that</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Architecture of the DispNet (left), MaskNet (center-top), FlowNetC (right) and Camera Motion Network (centerbottom)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Network Predictions. Top row: we show image, predicted depth, consensus masks. Bottom row: we show static scene optical flow, segmented flow in the moving regions and full optical flow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Method Data AbsRel SqRel RMS RMSlog &lt;1.25 &lt;1.25 2 &lt;1.25 3</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Error</cell><cell></cell><cell></cell><cell>Accuracy, δ</cell></row><row><cell cols="3">Eigen et al. [5] coarse</cell><cell>k</cell><cell>0.214</cell><cell cols="2">1.605 6.563</cell><cell>0.292</cell><cell>0.673</cell><cell>0.884</cell><cell>0.957</cell></row><row><cell cols="3">Eigen et al. [5] fine</cell><cell>k</cell><cell>0.203</cell><cell cols="2">1.548 6.307</cell><cell>0.282</cell><cell>0.702</cell><cell>0.890</cell><cell>0.958</cell></row><row><cell cols="3">Liu et al. [20]</cell><cell>k</cell><cell>0.202</cell><cell cols="2">1.614 6.523</cell><cell>0.275</cell><cell>0.678</cell><cell>0.895</cell><cell>0.965</cell></row><row><cell cols="3">Zhou et al. [37]</cell><cell>cs+k</cell><cell>0.198</cell><cell cols="2">1.836 6.565</cell><cell>0.275</cell><cell>0.718</cell><cell>0.901</cell><cell>0.960</cell></row><row><cell cols="4">Mahjourian et al. [22] cs+k</cell><cell>0.159</cell><cell cols="2">1.231 5.912</cell><cell>0.243</cell><cell>0.784</cell><cell>0.923</cell><cell>0.970</cell></row><row><cell cols="3">Geonet-Resnet [36]</cell><cell>cs+k</cell><cell>0.153</cell><cell cols="2">1.328 5.737</cell><cell>0.232</cell><cell>0.802</cell><cell>0.934</cell><cell>0.972</cell></row><row><cell cols="2">DF-Net [38]</cell><cell></cell><cell>cs+k</cell><cell>0.146</cell><cell cols="2">1.182 5.215</cell><cell>0.213</cell><cell>0.818</cell><cell>0.943</cell><cell>0.978</cell></row><row><cell cols="2">CC (ours)</cell><cell></cell><cell>cs+k</cell><cell>0.139</cell><cell cols="2">1.032 5.199</cell><cell>0.213</cell><cell>0.827</cell><cell>0.943</cell><cell>0.977</cell></row><row><cell cols="3">Zhou et al.* [37]</cell><cell>k</cell><cell>0.183</cell><cell cols="2">1.595 6.709</cell><cell>0.270</cell><cell>0.734</cell><cell>0.902</cell><cell>0.959</cell></row><row><cell cols="3">Mahjourian et al. [22]</cell><cell>k</cell><cell>0.163</cell><cell cols="2">1.240 6.220</cell><cell>0.250</cell><cell>0.762</cell><cell>0.916</cell><cell>0.968</cell></row><row><cell cols="3">Geonet-VGG [36]</cell><cell>k</cell><cell>0.164</cell><cell cols="2">1.303 6.090</cell><cell>0.247</cell><cell>0.765</cell><cell>0.919</cell><cell>0.968</cell></row><row><cell cols="3">Geonet-Resnet [36]</cell><cell>k</cell><cell>0.155</cell><cell cols="2">1.296 5.857</cell><cell>0.233</cell><cell>0.793</cell><cell>0.931</cell><cell>0.973</cell></row><row><cell cols="3">Godard et el. [7]</cell><cell>k</cell><cell>0.154</cell><cell cols="2">1.218 5.699</cell><cell>0.231</cell><cell>0.798</cell><cell>0.932</cell><cell>0.973</cell></row><row><cell cols="2">DF-Net [38]</cell><cell></cell><cell>k</cell><cell>0.150</cell><cell cols="2">1.124 5.507</cell><cell>0.223</cell><cell>0.806</cell><cell>0.933</cell><cell>0.973</cell></row><row><cell cols="2">CC (ours)</cell><cell></cell><cell>k</cell><cell>0.140</cell><cell cols="2">1.070 5.326</cell><cell>0.217</cell><cell>0.826</cell><cell>0.941</cell><cell>0.975</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Error</cell><cell></cell><cell cols="2">Accuracy, δ</cell></row><row><cell>Method</cell><cell>Data</cell><cell>Net D</cell><cell></cell><cell>Net F</cell><cell cols="6">AbsRel SqRel RMS RMSlog &lt;1.25 &lt;1.25 2 &lt;1.25 3</cell></row><row><cell>Basic</cell><cell>k</cell><cell>DispNet</cell><cell></cell><cell>-</cell><cell>0.184</cell><cell cols="2">1.476 6.325</cell><cell>0.259</cell><cell>0.732</cell><cell>0.910</cell><cell>0.967</cell></row><row><cell>Basic + ssim</cell><cell>k</cell><cell>DispNet</cell><cell></cell><cell>-</cell><cell>0.168</cell><cell cols="2">1.396 6.176</cell><cell>0.244</cell><cell>0.767</cell><cell>0.922</cell><cell>0.971</cell></row><row><cell>CC + ssim</cell><cell>k</cell><cell>DispNet</cell><cell cols="2">FlowNetC</cell><cell>0.148</cell><cell cols="2">1.149 5.464</cell><cell>0.226</cell><cell>0.815</cell><cell>0.935</cell><cell>0.973</cell></row><row><cell>CC + ssim</cell><cell>k</cell><cell cols="3">DispResNet FlowNetC</cell><cell>0.144</cell><cell cols="2">1.284 5.716</cell><cell>0.226</cell><cell>0.822</cell><cell>0.938</cell><cell>0.973</cell></row><row><cell>CC + ssim</cell><cell>k</cell><cell cols="3">DispResNet PWC Net</cell><cell>0.140</cell><cell cols="2">1.070 5.326</cell><cell>0.217</cell><cell>0.826</cell><cell>0.941</cell><cell>0.975</cell></row><row><cell>CC + ssim</cell><cell cols="4">cs+k DispResNet PWC Net</cell><cell>0.139</cell><cell cols="2">1.032 5.199</cell><cell>0.213</cell><cell>0.827</cell><cell>0.943</cell><cell>0.977</cell></row></table><note>Table 1: Results on Depth Estimation. Supervised methods are shown in the first rows. Data refers to the training set: Cityscapes (cs) and KITTI (k). Zhou el al.* shows improved results from their github page.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ablation studies on Depth Estimation. Joint training using Competitive Collaboration and better architectures improve the results. The benefits of CC can be seen when depth improves by using a better network for flow (row 4 vs 5).</figDesc><table><row><cell>Method</cell><cell>Sequence 09 Sequence 10</cell></row><row><cell>ORB-SLAM (full)</cell><cell>0.014 ± 0.008 0.012 ± 0.011</cell></row><row><cell>ORB-SLAM (short)</cell><cell>0.064 ± 0.141 0.064 ± 0.130</cell></row><row><cell>Mean Odometry</cell><cell>0.032 ± 0.026 0.028 ± 0.023</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Results on Optical Flow.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">We also compare with</cell></row><row><cell cols="6">supervised methods (top 2 rows) that are trained on synthetic</cell></row><row><cell cols="6">data only; unsupervised methods specialized for optical flow</cell></row><row><cell cols="6">(middle 3 rows) and joint methods that solve more than one</cell></row><row><cell cols="6">task (bottom 4 rows). * refers to our Pytorch implementation</cell></row><row><cell cols="6">used in our framework which gives slightly lower accuracy.</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Average EPE</cell></row><row><cell>Method</cell><cell>Net D</cell><cell>Net F</cell><cell>SP</cell><cell cols="2">MP Total</cell></row><row><cell>R</cell><cell>DispNet</cell><cell>-</cell><cell>7.51</cell><cell cols="2">32.75 13.54</cell></row><row><cell>F</cell><cell>-</cell><cell cols="2">FlowNetC 15.32</cell><cell>6.20</cell><cell>14.68</cell></row><row><cell>CC</cell><cell>DispNet</cell><cell>FlowNetC</cell><cell>6.35</cell><cell>6.16</cell><cell>7.76</cell></row><row><cell>11.86</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CC</cell><cell cols="2">DispResNet PWC Net</cell><cell>5.67</cell><cell>5.04</cell><cell>6.21</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Ablation studies on Flow estimation. SP, MP refer to static scene and moving region pixels. EPE is computed over KITTI 2015 training set. R, F are trained independently without CC.</figDesc><table><row><cell>Motion Segmentation. We evaluate the estimated motion</cell></row><row><cell>segmentations using the KITTI 2015 training set [6] that pro-</cell></row><row><cell>vides ground truth segmentation for moving cars. Since our</cell></row><row><cell>approach does not distinguish between different semantic</cell></row><row><cell>classes while estimating segmentation, we evaluate segmen-</cell></row><row><cell>tations only on car pixels. Specifically, we only consider car</cell></row><row><cell>pixels and compute Intersection over Union (IoU) scores for</cell></row><row><cell>moving and static car pixels. In Table 6, we show the IoU</cell></row><row><cell>scores of the segmentation masks obtained using our tech-</cell></row><row><cell>nique under different conditions. We refer to the masks ob-</cell></row><row><cell>tained with the motion segmentation network I m−m+&gt;0.5</cell></row><row><cell>as 'MaskNet' and refer to the masks obtained with flow</cell></row><row><cell>consensus I ||ν(e+,d)−u+||&lt;λc as 'Consensus'. The final</cell></row><row><cell>motion segmentation masks m  *  obtained with the intersec-</cell></row><row><cell>tion of the above two estimates are referred to as 'Joint'</cell></row><row><cell>(Eq. 14). IoU results indicate substantial IoU improvements</cell></row><row><cell>with 'Joint' masks compared to both 'MaskNet' and 'Con-</cell></row><row><cell>sensus' masks, illustrating the complementary nature of dif-</cell></row><row><cell>ferent masks. Qualitative results are shown in Figure 4 and</cell></row><row><cell>in the Appendix A.5.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Motion Segmentation Results. Intersection Over Union (IoU) scores on KITTI2015 training dataset images computed over car pixels.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Percentage classification errors. M and S refer to MNIST and SVHN respectively.</figDesc><table><row><cell></cell><cell cols="2">MNIST SVHN</cell></row><row><cell>R</cell><cell>0%</cell><cell>100%</cell></row><row><cell>F</cell><cell>100%</cell><cell>0%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Assignments of moderator to each of the competitors.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Absolute Relative errors on Make3D test set.</figDesc><table><row><cell>alley 1</cell><cell>alley 2</cell></row><row><cell>Zhou et al.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Relative errors on Sintel alley sequences.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 12 :</head><label>12</label><figDesc>Average runtime on TitanX GPU with images of size 128 × 418.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that this is different from the forward and backward optical flow in the context of two-frame estimation.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Frederik Kunstner for verifying the proofs. We are grateful to Clément Pinard for his github repository. We use it as our initial code base. We thank Georgios Pavlakos for helping us with several revisions of the paper. We thank Joel Janai for preparing optical flow visualizations, and Clément Gorard for his Make3d evaluation code.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The network C predicts camera motion that consist of camera rotations sinα, sinβ, sinγ, and translations t x , t y , t z . Thus e = (sinα, sinβ, sinγ, t x , t y , t z ). Given camera motion and depth d, we transform the image coordinates (x, y) into world coordinates (X, Y, Z).</p><p>where (c x , c y , f ) constitute the camera intrinsics. We now transform the world coordinates given the camera rotation and translation.</p><p>denote 3D rotation and translation, and X = [X, Y, Z] T . Hence, in image coordinates</p><p>We can now apply the warping as,</p><p>The static flow transformer is defined as,</p><p>A.3. The flow warping function, w f</p><p>The flow warping function w f is given by</p><p>where, (u x , u y ) is the optical flow, and (x, y) is the spatial coordinate system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Network Architectures</head><p>We briefly describe the network architectures below. For details, please refer to <ref type="figure">Figure 6</ref>.</p><p>Depth Network D. Our depth network is similar to Disp-NetS <ref type="bibr" target="#b22">[23]</ref> and outputs depths at 6 different scales. Each convolution and upconvolution is followed by a ReLU except the prediction layers. The prediction layer at each scale has a non-linearity given by 1/(α sigmoid(x) + β). The architecture of DispResNet is obtained by replacing convolutional blocks in DispNet by residual blocks <ref type="bibr" target="#b9">[10]</ref>.</p><p>Camera Motion Network C. The camera motion network consists of 8 convolutional layers, each of stride 2 followed by a ReLU activation. This is followed by a convolutional layer of stride 1, whose feature maps are averaged together to get the camera motion.</p><p>Flow Network F . We use FlowNetC architecture [4] with 6 output scales of flow and is shown in <ref type="figure">Figure 6</ref>. All convolutional and upconvolutional layers are followed by a ReLU except prediction layers. The prediction layers have no activations. For PWC Net, we use the network architecture from Janai et al. <ref type="bibr" target="#b11">[12]</ref>.</p><p>Mask Network M . The mask network has a U-Net [4] architecture. The encoder is similar to the camera motion with 6 convolutional layers. The decoder has 6 upconvolutional layers. Each of these layers have ReLU activations. The prediction layers use a sigmoid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5. Qualitative Results</head><p>The qualitative results of the predictions are shown in <ref type="figure">Figure 7</ref>. We would like to point out that, our method is able to segment the moving car, and not the parked cars on the</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A database and evaluation methodology for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A naturalistic open source movie for optical flow evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The Cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Flownet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2366" to="2374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the KITTI vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Digging into self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brostow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01260</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural expectation maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Van Steenkiste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6694" to="6704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mask rcnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
	<note>Computer Vision (ICCV</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised learning of multi-frame optical flow with occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Janai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Güney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Slow flow: Exploiting high-speed cameras for accurate and diverse optical flow reference data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Janai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Güney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2017</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2017<address><addrLine>Piscataway, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Back to basics: Unsupervised learning of optical flow via brightness constancy and motion smoothness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Mixture models for optical flow computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Jepson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of Toronto</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Posenet: A convolutional network for real-time 6-dof camera relocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grimes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2938" to="2946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gradientbased learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images using deep convolutional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Unsupervised learning of depth and ego-motion from monocular video using 3d geometric constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05522</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">UnFlow: Unsupervised learning of optical flow with a bidirectional census loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07837</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop on deep learning and unsupervised feature learning</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2011</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<title level="m">The 2017 davis challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Optical flow estimation using a spatial pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Grabcut: Interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM transactions on graphics (TOG)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="309" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.02371</idno>
		<title level="m">PWC-Net: CNNs for optical flow using pyramid, warping, and cost volume</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Demon: Depth and motion network for learning monocular stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ummenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">SfM-Net: learning of structure and motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<idno>abs/1704.07804</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Temporal interpolation as an unsupervised pretraining task for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition (GCPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Optical flow in mostly rigid scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sevilla-Lara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Geonet: Unsupervised learning of dense depth, optical flow and camera pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Df-net: Unsupervised joint learning of depth and flow using cross-task consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
