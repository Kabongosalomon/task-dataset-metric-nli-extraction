<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Benchmark for Generic Product Detection: A Low Data Baseline for Dense Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srikrishna</forename><surname>Varadarajan</surname></persName>
							<email>srikrishna@paralleldots.com</email>
							<affiliation key="aff0">
								<orgName type="institution">ParallelDots, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonaal</forename><surname>Kant</surname></persName>
							<email>sonaal@paralleldots.com</email>
							<affiliation key="aff0">
								<orgName type="institution">ParallelDots, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muktabh</forename><forename type="middle">Mayank</forename><surname>Srivastava</surname></persName>
							<email>muktabh@paralleldots.com</email>
							<affiliation key="aff0">
								<orgName type="institution">ParallelDots, Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Benchmark for Generic Product Detection: A Low Data Baseline for Dense Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T13:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Dense Object Detection</term>
					<term>Grocery Products</term>
					<term>Retail Products</term>
					<term>Benchmark</term>
					<term>Generic SKU Detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Object detection in densely packed scenes is a new area where standard object detectors fail to train well . Dense object detectors like RetinaNet (Lin et al., 2017) trained on large and dense datasets show great performance. We train a standard object detector on a small, normally packed dataset with data augmentation techniques. This dataset is 265 times smaller than the standard dataset, in terms of number of annotations. This low data baseline achieves satisfactory results (mAP=0.56) at standard IoU of 0.5. We also create a varied benchmark for generic SKU product detection by providing full annotations for multiple public datasets. It can be accessed at this URL. We hope that this benchmark helps in building robust detectors that perform reliably across different settings in the wild.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The real-world applications of computer vision span multiple industries like banking, agriculture, governance, healthcare, automotive, retail, and manufacturing. A few prominent ones include self-driving cars, automated retail stores like Amazon Go, and automated surveillance. The use of object detectors is absolutely a critical part of such real-world products. The area of research in object detectors has been quite vibrant, with a considerable number of datasets spanning various domains. However, the subtopic of object detection in dense scenes is rarely explored. Dense object detection is quite relevant to multiple applications, for example, in surveillance and retail industries. Some of these applications are crowd counting, monitoring and auditing of retail shelves, insights into brand presence for sales, marketing teams, and so on.</p><p>Exemplar based object detection refers to the detection and classification of objects from scene images with the supervision of an exemplar image of the object. Most object detection datasets are quite large, with enough number of instances of every object category. Most of the object detection methods <ref type="bibr" target="#b9">(Ren et al., 2015)</ref> depend on balanced and large object detection datasets to perform well in every category. These guarantees cannot be made for realworld applications where the object categories vary widely both in variety and in availability. For exam-ple, in the retail domain, the gathering of data to train an end to end object detection model is highly timeconsuming as well as costly. This is because gathering enough data which covers all the variants of objects and has equal representation of each object is going to be much harder. For example, making sure that our dataset contains a specific rare Mercedes logo design requires us to search across multiple showrooms or market places. We cannot collect a balanced dataset for object detection due to the availability of various logos. A similar case can be made when we need to monitor retail shelves, which has thousands of SKUs having different availability and frequency.</p><p>Moreover, in a dynamic world, where new products, new marketing materials, new logos keep getting introduced, the importance of incorporating incremental learning in real-world applications becomes greatly necessary. Unfortunately, the methods of incremental learning for object detection lead to a vast and unacceptable drop in performance <ref type="bibr" target="#b10">(Shmelkov et al., 2017)</ref>. A lot of these applications also involve distinguishing between extremely fine-grained classes. E.g., retail shelf monitoring, logo monitoring, face recognition. Building an end-to-end detector that would do both the dense object detection and finegrained recognition is a very challenging task whose real-world performance is quite bad. Hence, to tackle this problem, we introduce to decouple detection and classification. We propose to use a general object detector that predicts bounding boxes of objects that is This brings us to the current work of generic object detection in densely packed scenes. Previous works  have shown that training dense object detectors like RetinaNet <ref type="bibr" target="#b7">(Lin et al., 2017)</ref> on large dense object detection datasets works well. In this work, we explore the effectiveness of standard object detectors, trained on very low data. Our training data is 265 times smaller than the other methods, in terms of number of annotations. We provide a low data baseline for dense object detection task. We train a standard detector, namely Faster-RCNN <ref type="bibr" target="#b9">(Ren et al., 2015)</ref>, on a small dataset of normal scenes. This achieves a satisfactory performance (mAP) at the standard IoU of 0.5.</p><p>We also create a varied benchmark for generic SKU product detection by annotating every SKU in multiple datasets. The motivation behind this is to create detectors that are robust across different settings. It is quite common for deep learning based detectors to transfer poorly to other datasets. In the industry, there is a high need for robust detectors that perform reliably in the wild. This benchmark consists of 6 datasets <ref type="table">(Table 1)</ref> used solely as test sets. Models trained on any dataset can be tested on this benchmark to measure the progress of robust generic SKU detection. The benchmark datasets, evaluation code, and the leaderboard are available at this URL 1 2 Related Work Focal loss for dense object detection was introduced by <ref type="bibr" target="#b7">(Lin et al., 2017)</ref>. A method to localize objects more precisely in dense scenes was proposed by . This achieves significant increase in performance (mAP) at higher IoUs. Identifying real-world products (in situ) by training on exemplar template product images (in vitro) was initially proposed by <ref type="bibr" target="#b8">(Merler et al., 2007)</ref>. They released a database of 120 SKUs for product classification. Six in vitro images were collected from the web for each product and used for training. The in situ images were provided from frames of videos captured in a grocery store. There have been a few retail product checkout datasets by <ref type="bibr" target="#b14">(Wei et al., 2019)</ref> and <ref type="bibr">(Follmann et al., 2018)</ref>. Both of them are densely packed product datasets arranged in the fashion of a checkout counter, with many overlapping regions between objects as well.</p><p>3 Benchmark Datasets  recently released a huge benchmark dataset for product detection in densely packed scenes. To increase diversity to the task of generic product detection, we release a benchmark of datasets. Details of the datasets are shown in <ref type="table">Table  1</ref>. Please note that all of these datasets are used as a test set on which we benchmark our models. We welcome the community to participate in this benchmark by submitting their results. <ref type="bibr" target="#b15">(Zhang et al., 2007)</ref> released a database of 3153 supermarket images. They also provided information regarding what product is present in each image. We annotate every object in the entire dataset to provide ground truth for the evaluation of general object detection. The average number of objects per image in this dataset is 37, while the average object area is roughly 0.052 megapixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">WebMarket</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Grocery products (GP)</head><p>A multi-label classification approach was proposed by <ref type="bibr" target="#b4">(George and Floerkemeier, 2014)</ref>   <ref type="table" target="#tab_1">Dataset   #Images  #Images with SKU count greater than  5  10  20  40  60  80  100  SKU110K-Test 2941  2941 2941 2939 2934 2926 2902 2822  WebMarket  3153  3139 3048 2526 1119 349  121  57  TobaccoShelves 354  354  349  308  130  18  3  2  Holoselecta  295  293  293  249  113  0  0  0  GP  680  604  407  101  9  1  0  0  CAPG-GP  234  209  160  82  33  4</ref> 0 0   <ref type="bibr" target="#b12">(Tonioni and di Stefano, 2017)</ref> and annotated with the desired object-level annotations. We provide individual bounding box annotation for every product for all 680 images in this dataset. The average number of objects per image in this dataset is 13, while the average object size is roughly 0.293 megapixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">CAPG-GP</head><p>A fine-grained grocery product dataset was released by <ref type="bibr" target="#b3">(Geng et al., 2018)</ref>. It consists of 234 test images taken from 2 stores. The authors annotated only the products belonging to certain categories. To create ground truth for generic object detection, we decided to annotate every product in the entire dataset. The average number of objects per image in this dataset is 20, while the average object area is roughly 0.377 megapixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Existing General Product Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Holoselecta</head><p>Most recently, <ref type="bibr" target="#b2">(Fuchs et al., 2019)</ref> released a dataset of 300 real-world images of Selecta vending machines containing 10,000 objects belonging to 90 categories of packaged products. The images in this dataset were quite varied in their sizes, as shown in <ref type="table" target="#tab_2">Table 3</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">TobaccoShelves</head><p>A retail product dataset was released by <ref type="bibr" target="#b13">(Varol and Salih, 2015)</ref> containing 345 images of tobacco shelves collected 40 stores with four cameras. The annotations of every product were also released by the authors. The average number of objects per image in this dataset is 37, while the average object area is roughly 1.1% of the entire image. The images in this dataset were also quite varied in size, ranging from 1.4 megapixels to 10.5 megapixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">SKU110K</head><p>Recently,  released a huge dataset for precise object detection in densely packed scenes. This dataset contains 11,762 images that were split into train, validation, and test sets. The test set consists of 2941 images with the average number of objects per image being 146. The average object area is roughly 0.27%, making it the lowest among all the datasets in this benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Denseness of the Datasets</head><p>The denseness of the datasets depends on two factors. The average number of objects and the relative sizes of the objects. SKU110K  is by far one of the most dense datasets for object detection. An analysis of the denseness of other datasets in the current benchmark is shown in <ref type="table" target="#tab_1">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Low Data Baseline Approach</head><p>We collected close to 300 images encompassing various shapes in which retail products occur  by querying images from the public domain (e.g., GoogleImages, OpenImages). The total number of annotations in our dataset is 4556. This is in contrast with SKU110K-Train, the data on which  was trained, where the total number of annotations is 1.2 million, as shown in <ref type="table" target="#tab_4">Table 4</ref>. This makes our training dataset 265 times smaller than the SKU110K-Train. We apply standard object detection augmentations from <ref type="bibr" target="#b0">(Buslaev et al., 2018)</ref>. We train a standard object detector, Faster-RCNN <ref type="bibr" target="#b9">(Ren et al., 2015)</ref>, on our training set described above as a low data baseline for this benchmark. We call this model LDB300 (Low Data Baseline). We measure the performance across different datasets, to test the robustness of the model in the wild and its effectiveness in generic product detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Implementation &amp; Results</head><p>We use standard post-processing steps like Non-Max Suppression after our detections. We use multiscale testing (2 scales) since lot of the datasets have high variance in object sizes. For the SKU110K dataset, only one scale is used. The inference settings of the compared Full Approach can be obtained from the code base released by the authors at URL. Multi-scale testing can be employed on the Full Approach as well, but this might be highly time consuming. For example, it is 10 times slower (FPS) than Faster-RCNN, as shown in .</p><p>We use the same evaluation metric as the recent work . This is the standard evaluation metric used by COCO <ref type="bibr" target="#b6">(Lin et al., 2014)</ref>. Average precision (AP) and Average Recall (AR 300 ) are reported at IoU=.50:.05:.95 (averaged by varying IoU between 0.5 and 0.95 with 0.05 intervals). 300 here represents the maximum number of objects in an image. AP and AR at specific IoUs (0.50 and 0.75) are also reported.</p><p>Our baseline results on the SKU110K dataset (Table 5) shows satisfactory performance of mAP=0.560 at 0.5 IoU, even with very low data (265 times in terms of number of annotations). For some of the datasets like GP, CAPG-GP, our baseline results are quite close to the state-of-the-art. This could be because of the huge variation in object sizes as well the non-dense nature of the scenes in these datasets, which can be seen from <ref type="table" target="#tab_1">Table 1 and Table 2</ref>. This method serves as a simple baseline while methods exploiting the shape of the objects and structure of densely packed scenes look promising.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>The varying results from <ref type="table" target="#tab_5">Table 5</ref> shows that evaluating on a varied benchmark is necessary to have a detector perform reliably in the wild. For example, the Full Approach  trained on the huge SKU110K dataset does not perform well on the GP <ref type="bibr" target="#b4">(George and Floerkemeier, 2014)</ref> dataset as is.</p><p>A qualitative output of our method on different datasets are shown in <ref type="figure" target="#fig_2">Figure 1, 2, 3, 4</ref>, 5. The performance on TobaccoShelves was a bit low <ref type="table" target="#tab_5">(Table  5)</ref>, which is seen qualitatively in <ref type="figure">Figure 5</ref>. One can see that, our method performs well when homoge-neous objects are present throughout the image. It also detects objects of different aspect ratios comfortably. Current limitations of this baseline model include precise detection of the objects, detecting objects with occlusion from shelves, as well as handling multi-scale objects ranging from 0.1% to 20% of the scene. Precise detection can be helped by EM-Merger module from . Better scaleinvariant object detectors <ref type="bibr" target="#b11">(Singh and Davis, 2017)</ref> can be tried as future work.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>accompanied by 680 annotated shop images from their GP dataset. The annotation provided by them covered a group of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Sample predictions of our method on GP<ref type="bibr" target="#b4">(George and Floerkemeier, 2014)</ref> dataset. Blue boxes denote groundtruth objects, while Red denotes predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Ouputs of our method on Holoselecta<ref type="bibr" target="#b15">(Zhang et al., 2007)</ref> datasetFigure 4: Ouputs of our method on Holoselecta (Fuchs et al., 2019) datasetFigure 5: Ouputs of our method on TobaccoShelves (Varol and Salih, 2015) dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Analysis on the denseness of the generic product datasets. # represents the count.</figDesc><table><row><cell>Type of Images</cell><cell cols="2">Avg Img Size #Images</cell></row><row><cell>Type 1 (HoloLens)</cell><cell>1.11</cell><cell>30</cell></row><row><cell>Type 2</cell><cell>8.13</cell><cell>8</cell></row><row><cell cols="2">Type 3 (OnePlus-6T) 16.03</cell><cell>208</cell></row><row><cell>Type 4</cell><cell>24</cell><cell>49</cell></row><row><cell>Total</cell><cell>15.62</cell><cell>295</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Details of Holoselecta dataset. # represents the count. Average size of the entire image is shown in megapixels same products in a bounding box rather than bound- ing boxes for individual boxes. A subset of 70 images was chosen by</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Statistics of trainset. # represents the count. Number of Annotations is denoted by #Anns</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Performance of our Faster-RCNN across different general product datasets. * denotes results obtained using the trained model given at URL as is.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/ParallelDots/generic-sku-detectionbenchmark</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Buslaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Parinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Khvedchenya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Iglovikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Kalinin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Böttger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Härtinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>König</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ulrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<idno>abs/1804.08292</idno>
		<title level="m">Albumentations: fast and flexible image augmentations. CoRR, abs/1809.06839. Follmann</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Mvtec D2S: densely segmented supermarket dataset. CoRR</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards identification of packaged products via computer vision: Convolutional neural networks for object detection and image classification in retail environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Grundmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fleisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Conference on the Internet of Things</title>
		<meeting>the 9th International Conference on the Internet of Things<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="1" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fine-grained grocery product recognition by one-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lai</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM International Conference on Multimedia, MM &apos;18</title>
		<meeting>the 26th ACM International Conference on Multimedia, MM &apos;18<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1706" to="1714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Recognizing products: A per-exemplar multi-label image classification approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Floerkemeier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="440" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Precise detection in densely packed scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eisenschtat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ratzon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<idno>abs/1904.00853</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Microsoft COCO: common objects in context. CoRR, abs/1405.0312</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recognizing groceries in situ using in vitro training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Merler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Galleguillos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2007 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1506.01497</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Incremental learning of object detectors without catastrophic forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shmelkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alahari</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
		<idno>abs/1708.06977</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">An analysis of scale invariance in object detection -SNIP. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<idno>abs/1711.08189</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Product recognition in store shelves as a sub-graph isomorphism problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tonioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Di Stefano</surname></persName>
		</author>
		<idno>abs/1707.08378</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Toward retail product recognition on grocery shelves</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salih</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">944309</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">RPC: A large-scale retail product checkout dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/1901.07249</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">I</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Where&apos;s the weet-bix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kweon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I.-S</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Yagi</title>
		<imprint>
			<publisher>Springer</publisher>
			<biblScope unit="volume">4843</biblScope>
			<biblScope unit="page" from="800" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Figure 1: Ouputs of our method spanning different type of objects on SKU110K</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goldman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>dataset</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
