<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection General Distribution ( ) (a) (b) ( ) Top 1 value</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
							<email>wangwenhai362@163.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Nanjing University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
							<email>xlhu@mail.tsinghua.edu.cn</email>
							<affiliation key="aff3">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="laboratory">are from PCA Lab, Key Lab of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, and Jiangsu Key Lab of Image and Video Understanding for Social Security</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection General Distribution ( ) (a) (b) ( ) Top 1 value</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>* Corresponding author.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Localization Quality Estimation (LQE) is crucial and popular in the recent advancement of dense object detectors since it can provide accurate ranking scores that benefit the Non-Maximum Suppression processing and improve detection performance. As a common practice, most existing methods predict LQE scores through vanilla convolutional features shared with object classification or bounding box regression. In this paper, we explore a completely novel and different perspective to perform LQE -based on the learned distributions of the four parameters of the bounding box. The bounding box distributions are inspired and introduced as "General Distribution" in GFLV1, which describes the uncertainty of the predicted bounding boxes well. Such a property makes the distribution statistics of a bounding box highly correlated to its real localization quality. Specifically, a bounding box distribution with a sharp peak usually corresponds to high localization quality, and vice versa. By leveraging the close correlation between distribution statistics and the real localization quality, we develop a considerably lightweight Distribution-Guided Quality Predictor (DGQP) for reliable LQE based on GFLV1, thus producing GFLV2. To our best knowledge, it is the first attempt in object detection to use a highly relevant, statistical representation to facilitate LQE. Extensive experiments demonstrate the effectiveness of our method. Notably, GFLV2 (ResNet-101) achieves 46.2 AP at 14.6 FPS, surpassing the previous state-of-the-art ATSS baseline (43.6 AP at 14.6 FPS) by absolute 2.6 AP on COCO test-dev, without sacrificing the efficiency both in training and inference. Codes are available at https://github.com/implus/GFocalV2.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>General Distribution</head><p>( )  <ref type="figure">Figure 2</ref>: Comparisons of input features for predicting localization quality between existing works (left) and ours (right). Existing works focus on different spatial locations of convolutional features, including (a): point <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18]</ref>, (b): region <ref type="bibr" target="#b12">[13]</ref>, (c): border <ref type="bibr" target="#b26">[27]</ref> dense points, (d): border <ref type="bibr" target="#b26">[27]</ref> middle points, (e): border <ref type="bibr" target="#b26">[27]</ref> extreme points, (f): regular sampling points <ref type="bibr" target="#b38">[39]</ref>, and (g): deformable sampling points <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. In contrast, we use the statistic of learned box distribution to produce reliable localization quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Dense object detector <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b26">27]</ref> which directly predicts pixel-level object categories and bounding boxes over feature maps, becomes increasingly popular due to its elegant and effective framework. One of the crucial techniques underlying this framework is Localization Quality Estimation (LQE). With the help of better LQE, high-quality bounding boxes tend to score higher than lowquality ones, greatly reducing the risk of mistaken suppression in Non-Maximum Suppression (NMS) processing.</p><p>Many previous works <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b26">27]</ref> have explored LQE. For example, the YOLO family <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref> first adopt Objectness to describe the localization quality, which is defined as the Intersection-over-Union (IoU) between the predicted and ground-truth box. After that, IoU is further explored and proved to be effective in IoU-Net <ref type="bibr" target="#b12">[13]</ref>, IoU-aware <ref type="bibr" target="#b35">[36]</ref>, PAA <ref type="bibr" target="#b13">[14]</ref>, GFLV1 <ref type="bibr" target="#b17">[18]</ref> and VFNet <ref type="bibr" target="#b38">[39]</ref>. Recently, FCOS <ref type="bibr" target="#b32">[33]</ref> and ATSS <ref type="bibr" target="#b39">[40]</ref> introduce Centerness, the distance degree to the object center, to suppress low-quality detection results. Generally, the aforementioned methods share a common characteristic that they are all based on vanilla convolutional features, e.g., features of points, borders or regions (see <ref type="figure">Fig. 2</ref> (a)-(g)), to estimate the localization quality.</p><p>Different from previous works, in this paper, we explore a brand new perspective to conduct LQE -by directly utilizing the statistics of bounding box distributions, instead of using the vanilla convolutional features (see <ref type="figure">Fig. 2</ref>). Here the bounding box distribution is introduced as "General Distribution" in GFLV1 <ref type="bibr" target="#b17">[18]</ref>, where it learns a discrete probability distribution of each predicted edge ( <ref type="figure" target="#fig_0">Fig. 1 (a)</ref>) for describing the uncertainty of bounding box regression. Interestingly, we observe that the statistic of the General Distribution has a strong correlation with its real localization quality, as illustrated in <ref type="figure" target="#fig_0">Fig. 1 (b)</ref>. More specifically in <ref type="figure" target="#fig_0">Fig. 1</ref> (c) and (d), the shape (flatness) of bounding box distribution can clearly reflect the localization quality of the predicted results: the sharper the distribution, the more accurate the predicted bounding box, and vice versa. Consequently, it can potentially be easier and very efficient to conduct better LQE by the guidance of the distribution information, as the input (distribution statistics of bounding boxes) and the output (LQE scores) are highly correlated.</p><p>Inspired by the strong correlation between the distribution statistics and LQE scores, we propose a very lightweight sub-network with only dozens of (e.g., 64) hidden units, on top of these distribution statistics to produce reliable LQE scores, significantly boosting the detection performance. Importantly, it brings negligible additional computation cost in practice and almost does not affect the training/inference speed of the basic object detectors. In this paper, we term this lightweight sub-network as Distribution-Guided Quality Predictor (DGQP), since it relies on the guidance of distribution statistics for quality predictions.</p><p>By introducing the lightweight DGQP that predicts reliable LQE scores via statistics of bounding box distributions, we develop a novel dense object detector based on the framework of GFLV1, thus termed GFLV2. To verify the effectiveness of GFLV2, we conduct extensive experiments on the challenging benchmark COCO <ref type="bibr" target="#b21">[22]</ref>. Notably, based on ResNet-101 <ref type="bibr" target="#b10">[11]</ref>, GFLV2 achieves impressive detection performance (46.2 AP), i.e., 2.6 AP gains over the stateof-the-art ATSS baseline (43.6 AP) on COCO test-dev, under the same training schedule and without sacrificing the efficiency both in training and inference.</p><p>In summary, our contributions are as follows:</p><p>• To our best knowledge, our work is the first to bridge the statistics of bounding box distributions and localization quality estimation in an end-to-end dense object detection framework.</p><p>• The proposed GFLV2 is considerably lightweight and cost-free in practice. It can also be easily plugged into most dense object detectors with a consistent gain of ∼2 AP, and without loss of training/inference speed.</p><p>• Our GFLV2 (Res2Net-101-DCN) achieves very competitive 53.3 AP (multi-scale testing) on COCO dataset among dense object detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Formats of LQE: Early popular object detectors <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b9">10]</ref> simply treat the classification confidence as the formulation of LQE score, but there is an obvious inconsistency between them, which inevitably degrades the detection performance. To alleviate this problem, AutoAssign <ref type="bibr" target="#b42">[43]</ref> and BorderDet <ref type="bibr" target="#b26">[27]</ref> employ additional localization features to rescore the classification confidence, but they still lack an explicit definition of LQE.</p><p>Recently, FCOS <ref type="bibr" target="#b32">[33]</ref> and ATSS <ref type="bibr" target="#b39">[40]</ref> introduce a novel format of LQE, termed Centerness, which depicts the distance degree to the center of the object. Although Centerness is effective, recent researches <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b38">39]</ref> show that it has certain limitations and may be suboptimal for LQE. SABL <ref type="bibr" target="#b34">[35]</ref> introduces boundary buckets for coarse localization, and utilizes the averaged bucketing confidence as a formulation of LQE.</p><p>After years of technical iterations <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b38">39]</ref>, IoU has been deeply studied and becomes increasingly popular as an excellent measurement of LQE. IoU is first known as the Objectness in YOLO <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>, where the network is supervised to produce estimated IoUs between predicted boxes and ground-truth ones, to reduce ranking basis during NMS. Following the similar paradigm, IoU-Net <ref type="bibr" target="#b12">[13]</ref>, Fitness NMS <ref type="bibr" target="#b33">[34]</ref>, MS R-CNN <ref type="bibr" target="#b11">[12]</ref>, IoUaware <ref type="bibr" target="#b35">[36]</ref>, PAA <ref type="bibr" target="#b13">[14]</ref> utilize a separate branch to perform LQE in the IoU form. Concurrently, GFLV1 <ref type="bibr" target="#b17">[18]</ref> and VFNet <ref type="bibr" target="#b38">[39]</ref> demonstrate a more effective format, by merging the classification score with IoU to reformulate a joint representation. Due to its great success <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b38">39]</ref>, we build our GFLV2 based on the Classification-IoU Joint Representation <ref type="bibr" target="#b17">[18]</ref>, and develop a novel approach for reliable LQE. Input Features for LQE: As shown in the left part of <ref type="figure">Fig. 2</ref>, previous works directly use convolutional features as input for LQE, which only differ in the way of spatial sampling. Most existing methods <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18]</ref> adopt the point features (see <ref type="figure">Fig. 2</ref> (a)) to produce LQE scores for high efficiency. IoU-Net <ref type="bibr" target="#b12">[13]</ref> predicts IoU based on the region features as shown in <ref type="figure">Fig. 2 (b)</ref>. BorderDet <ref type="bibr" target="#b26">[27]</ref> designs three types of border-sensitive features (see <ref type="figure">Fig. 2</ref> (c)-(e)) to facilitate LQE. Similar with BorderDet, a star-shaped sampling manner (see <ref type="figure">Fig. 2</ref> (f)) is designed in VFNet <ref type="bibr" target="#b38">[39]</ref>. Alternatively, HSD <ref type="bibr" target="#b1">[2]</ref> and RepPoints <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b3">4]</ref> focus on features with learned locations (see <ref type="figure">Fig. 2</ref> (g)) via the deformable convolution <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b45">46]</ref>.</p><p>The aforementioned methods mainly focus on extracting discriminating convolutional features with various spatial aspects for better LQE. Different from previous methods, our proposed GFLV2 is designed in an artful perspective: predicting LQE scores by its directly correlated variablesthe statistics of bounding box distributions (see the right part of <ref type="figure">Fig. 2</ref>). As later demonstrated in <ref type="table" target="#tab_3">Table 3</ref>, compared with convolutional features shown in <ref type="figure">Fig. 2</ref> (a)-(g), the statistics of bounding box distributions achieve an impressive efficiency and a high accuracy simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we first briefly review the Generalized Focal Loss (i.e., GFLV1 <ref type="bibr" target="#b17">[18]</ref>), and then derive the proposed GFLV2 based on the relevant concepts and formulations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Generalized Focal Loss V1</head><p>Classification-IoU Joint Representation: This representation is the key component in GFLV1, which is designed to reduce the inconsistency between localization quality estimation and object classification during training and inference. Concretely, given an object with category label c ∈ {1, 2, ..., m} (m indicates the total number of categories), GFLV1 utilizes the classification branch to produce the joint representation of Classification and IoU as J = [J 1 , J 2 , ..., J m ], which satisfies:</p><formula xml:id="formula_0">J i = IoU(b pred , b gt ), if i = c; 0, otherwise,<label>(1)</label></formula><p>where IoU(b pred , b gt ) denotes the IoU between the predict bounding box b pred and the ground truth b gt .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>General Distribution of Bounding Box Representation:</head><p>Modern detectors <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b32">33]</ref>  To be compatible with the convolutional networks, the continuous domain is converted into the discrete one, via discretizing the range [y 0 , y n ] into a list [y 0 , y 1 , ..., y i , y i+1 , ..., y n−1 , y n ] with even intervals ∆ (∆ = y i+1 − y i , ∀i ∈ {0, 1, ..., n − 1}). As a result, given the discrete distribution property n i=0 P (y i ) = 1, the estimated regression valueŷ can be presented as:</p><formula xml:id="formula_1">y = n i=0 P (y i )y i .<label>(2)</label></formula><p>Compared with the Dirac delta distribution, the General Distribution P (x) can faithfully reflect the prediction quality (see <ref type="figure" target="#fig_0">Fig. 1</ref> (c)-(d)), which is the cornerstone of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Generalized Focal Loss V2</head><p>Decomposed Classification-IoU Representation: Although the joint representation solves the inconsistency problem <ref type="bibr" target="#b17">[18]</ref> between object classification and quality estimation during training and testing, there are still some limitations in using only the classification branch to predict the joint representation. In this work, we decompose the joint representation explicitly by leveraging information from both classification (C) and regression (I) branches:</p><formula xml:id="formula_2">J = C × I, (3) where C = [C 1 , C 2 , ..., C m ], C i ∈ [0, 1]</formula><p>denotes the Classification Representation of total m categories, and I ∈ [0, 1] is a scalar that stands for the IoU Representation.</p><p>Although J is decomposed into two components, we use the final joint formulation (i.e., J) in both the training and testing phases, so it can still avoid the inconsistency problem as mentioned in GFLV1. Specifically, we first combine C from the classification branch and I from the proposed Distribution-Guided Quality Predictor (DGQP) in regression branch, into the unified form J. Then, J is supervised by Quality Focal loss (QFL) as proposed in <ref type="bibr" target="#b17">[18]</ref> during training, and used directly as NMS score in inference. Distribution-Guided Quality Predictor: DGQP is the key component of GFLV2. It delivers the statistics of the learned General Distribution P into a tiny sub-network (see red dotted frame in <ref type="figure" target="#fig_2">Fig. 3</ref>) to obtain the predicted IoU scalar I, which helps to generate high-quality Classification-IoU Joint Representation (Eq. <ref type="formula">(3)</ref>). Following GFLV1 <ref type="bibr" target="#b17">[18]</ref>, we adopt the relative offsets from the location to the four sides of a bounding box as the regression targets, which are represented by the General Distribution. For convenience, we mark the left, right, top and bottom sides as {l, r, t, b}, and define the discrete probabilities of the w side as P w = [P w (y 0 ), P w (y 1 ), ..., P w (y n )], where w ∈ {l, r, t, b}.</p><p>As illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, the flatness of the learned distribution is highly related to the quality of the final detected bounding box, and some relevant statistics can be used to reflect the flatness of the General Distribution. As a result, such statistical features have a very strong correlation with the localization quality, which will ease the training difficulty and improves the quality of estimation. Practically, we recommand to choose the Top-k values along with the mean value of each distribution vector P w , and concatenate them as the basic statistical feature F ∈ R 4(k+1) :</p><formula xml:id="formula_3">F = Concat Topkm(P w ) | w ∈ {l, r, t, b} ,<label>(4)</label></formula><p>where Topkm(·) denotes the joint operation of calculating Top-k values and their mean value. Concat(·) means the channel concatenation. Selecting Top-k values and their mean value as the input statistics have two benefits:</p><p>• Since the sum of P w is fixed (i.e., n i=0 P w (y i ) = 1), Top-k values along with their mean value can basically reflect the flatness of the distribution: the larger, the sharper; the smaller, the flatter;</p><p>• Top-k and mean values can make the statistical feature insensitive to its relative offsets over the distribution domain (see <ref type="figure" target="#fig_4">Fig. 4</ref>), resulting in a robust representation which is not affected by object scales.  Given the statistical feature F of General Distribution as input, we design a very tiny sub-network F(·) to predict the final IoU quality estimation. The sub-network has only two Fully-Connected (FC) layers, which are followed by ReLU <ref type="bibr" target="#b15">[16]</ref> and Sigmoid, respectively. Consequently, the IoU scalar I can be calculated as:</p><formula xml:id="formula_4">I = F(F) = σ(W 2 δ(W 1 F)),<label>(5)</label></formula><p>where δ and σ refer to the ReLU and Sigmoid, respectively. W 1 ∈ R p×4(k+1) and W 2 ∈ R 1×p . k denotes the Topk parameter and p is the channel dimension of the hidden layer (k = 4, p = 64 is a typical setting in our experiment). Complexity: The overall architecture of GFLV2 is illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>. It is worth noting that the DGQP module is very lightweight. First, it only brings thousands of ad-  <ref type="table">Table 1</ref>: Performances of different combinations of the input statistics by fixing k = 4 and p = 64. "Mean" denotes the mean value, "Var" denotes the variance number, and "Dim" is short for "Dimension" that means the total amount of the input channels.</p><p>ditional parameters, which are negligible compared to the number of parameters of the entire detection model. For example, for the model with ResNet-50 <ref type="bibr" target="#b10">[11]</ref> and FPN <ref type="bibr" target="#b19">[20]</ref>, the extra parameters of the DGQP module only account for ∼0.003%. Second, the computational overhead of the DGQP module is also very small due to its extremely light structure. As shown in <ref type="table" target="#tab_6">Table 5</ref> and 8, the use of the DGQP module hardly reduces the training and inference speed of the original detector in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head><p>Experimental Settings: We conduct experiments on COCO benchmark <ref type="bibr" target="#b21">[22]</ref>, where trainval35k with 115K images is used for training and minival with 5K images for validation in our ablation study. Besides, we obtain the main results on test-dev with 20K images from the evaluation server. All results are produced under mmdetection <ref type="bibr" target="#b2">[3]</ref> for fair comparisons, where the default hyperparameters are always adopted. Unless otherwise stated, we apply the standard 1x learning schedule (12 epochs) without multi-scale training for the ablation study, based on ResNet-50 <ref type="bibr" target="#b10">[11]</ref> backbone. The training/testing details follow the descriptions in previous works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b3">4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Ablation Study</head><p>Combination of Input Statistics: In addition to the pure Top-k values, there are some statistics that may reflect more characteristics of the distributions, such as the mean and variance of these Top-k numbers. Therefore, we conduct experiments to investigate the effect of their combinations as input, by fixing k = 4 and p = 64. From <ref type="table">Table 1</ref>, we observe that the Top-4 values with their mean number perform best. Therefore, we default to use such a combination as the standard statistical input in the following experiments. Structure of DGQP (i.e., k, p): We then examine the impact of different parameters of k, p in DGQP on the detection performance. Specifically, we report the effect of k and p by fixing one and varying another in <ref type="table" target="#tab_2">Table 2</ref>. It is observed that k = 4, p = 64 steadily achieves the optimal accuracy among various combinations.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistic of General Distribution</head><p>Decomposed Form Composed Form </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Type of Input Features:</head><p>To the best of our knowledge, the proposed DGQP is the first to use the statistics of learned distributions of bounding boxes for the generation of better LQE scores in the literature. Since the input (distribution statistics) and the output (LQE scores) are highly correlated, we speculate that it can be more effective or efficient than ordinary convolutional input proposed in existing methods. Therefore, we fix the hidden layer dimension of DGQP (i.e., p = 64) and compare our statistical input with most existing possible types of convolutional inputs, from point (a), region (b), border (c)-(e), regular points (f), and deformable points (g), respectively <ref type="figure">(Fig. 2)</ref>.   is the best formulation of Classification-IoU Joint Representation in the case of using distribution statistics. There are basically two formats: the Composed Form and the proposed Decomposed Form (Sec. 3.2), as illustrated in <ref type="figure" target="#fig_5">Fig. 5</ref>.</p><p>Here the "Decomposed" (the left part of <ref type="figure" target="#fig_5">Fig. 5</ref>) means that the final joint representation can be explicitly decomposed through multiplication by two components, i.e., J = C × I in Eq. (3). Whilst the "Composed" (the right part of <ref type="figure" target="#fig_5">Fig. 5</ref>) shows that J is directly obtained through FC layers where its input feature is enriched (d is the dimension of the appended feature) by the information of distribution statistics. From <ref type="table" target="#tab_5">Table 4</ref>, our proposed Decomposed Form is always superior than the Composed Forms with various d settings in both accuracy and running speed. Compatibility for Dense Detectors: Since GFLV2 is very lightweight and can be adapted to various types of dense detectors, we employ it to a series of recent popular detection methods. For those detectors that do not support the distributed representation of bounding boxes, we make the minimal and necessary modifications to enable it to generate distributions for each edge of a bounding box. Based on the results in <ref type="table" target="#tab_6">Table 5</ref>, GFLV2 can consistently improve ∼2 AP in popular dense detectors, without loss of inference speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparisons with State-of-the-arts</head><p>In this section, we compare GFLV2 with state-of-theart approaches on COCO test-dev in <ref type="table" target="#tab_10">Table 7</ref>. Following previous works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b32">33]</ref>  training strategy and 2x learning schedule (24 epochs) are adopted during training. For a fair comparison, the results of single-model single-scale testing for all methods are reported, including their corresponding inference speeds (FPS). We also report additional multi-scale testing results for GFLV2. The visualizaion of the accuracy-speed tradeoff is demonstrated in <ref type="figure">Fig. 6</ref>, and we observe that GFLV2 pushes the envelope of accuracy-speed boundary to a new level. Our best result with a single Res2Net-101-DCN model achieves considerably competitive 53.3 AP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Analysis</head><p>Although the proposed DGQP module has been shown to improve the performance of dense object detectors, we would also like to understand how its mechanism operates. DGQP Improves LQE: To assess whether DGQP is able to benefit the estimation of localization quality, we first obtain the predicted IoUs (given by four representative models with IoU as the quality estimation targets) and their corresponding real IoUs over all the positive samples on COCO minival. Then we calculate their Pearson Correlation Coefficient (PCC) in <ref type="table" target="#tab_8">Table 6</ref>. It demonstrates that DGQP in GFLV2 indeed improves the linear correlation between the estimated IoUs and the ground-truth ones by a considerable margin (+0.26) against GFLV1, which eventually leads to an absolute 0.9 AP gain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Backbone</head><p>Epoch MS train FPS AP AP 50 AP 75 AP S AP M AP L Reference multi-stage: Faster R-CNN w/ FPN <ref type="bibr" target="#b19">[20]</ref>    <ref type="bibr" target="#b43">[44]</ref> or their official repositories <ref type="bibr" target="#b26">[27]</ref>, while others are measured on the same machine with a single GeForce RTX 2080Ti GPU under the same mmdetection <ref type="bibr" target="#b2">[3]</ref> framework, using a batch size of 1 whenever possible. "n/a" means that both trained models and timing results from original papers are not available. R: ResNet <ref type="bibr" target="#b10">[11]</ref>. X: ResNeXt <ref type="bibr" target="#b36">[37]</ref>. HG: Hourglass <ref type="bibr" target="#b24">[25]</ref>. DCN: Deformable Convolutional Network <ref type="bibr" target="#b45">[46]</ref>. R2: Res2Net <ref type="bibr" target="#b7">[8]</ref>. . For many existing approaches <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b17">18]</ref>, they fail to produce the highest LQE scores for the best candidates. In contrast, our GFLV2 reliably assigns larger quality scores for those real high-quality ones, thus reducing the risk of mistaken suppression in NMS processing. White: ground-truth bounding boxes; Other colors: predicted bounding boxes. DGQP Eases the Learning Difficulty: <ref type="figure" target="#fig_7">Fig. 8</ref> provides the visualization of the training losses on LQE scores, where DGQP in GFLV2 successfully accelerates the training process and converges to lower losses. Visualization on Inputs/Outputs of DGQP: To study the behavior of DGQP, we plot its inputs and corresponding outputs in <ref type="figure" target="#fig_8">Fig. 9</ref>. For a better view, we select the mean Top-1 values to represent the input statistics. It is observed that the outputs are highly correlated with inputs as expected. Training/Inference Efficiency: We also compare the training and inference efficiency among recent state-of-the-art dense detectors in <ref type="table" target="#tab_12">Table 8</ref>. Note that PAA <ref type="bibr" target="#b13">[14]</ref>, Rep-PointsV2 <ref type="bibr" target="#b3">[4]</ref> and BorderDet <ref type="bibr" target="#b26">[27]</ref> bring an inevitable time overhead (52%, 65%, and 22% respectively) during training, and the latter two also sacrifice inference speed by 30% and 14%, respectively. In contrast, our proposed GFLV2 can achieve top performance (∼41 AP) while still maintaining the training and inference efficiency. Qualitative Results: In <ref type="figure" target="#fig_6">Fig. 7</ref>, we qualitatively demon-   strate the mechanism how GFLV2 makes use of its more reliable IoU quality estimations to maintain accurate predictions during NMS. Unfortunately for other detectors, high-quality candidates are wrongly suppressed due to their relatively lower localization confidences, which eventually leads to a performance degradation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose to learn reliable localization quality estimation, through the guidance of statistics of bounding box distributions. It is an entirely new and completely different perspective in the literature, which is also conceptually effective as the information of distribution is highly correlated to the real localization quality. Based on it we develop a dense object detector, namely GFLV2. Extensive experiments and analyses on COCO dataset further validate its effectiveness, compatibility and efficiency. We hope GFLV2 can serve as simple yet effective baseline for the community.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Motivation of utilizing the highly relevant statistics of learned bounding box distributions to guide the better generation of its estimated localization quality. (a): The illustration of General Distribution in GFLV1 [18] to represent bounding boxes, which models the probability distribution of the predicted edges. (b): The scatter diagram of the relation between Top-1 (mean of four sides) value of General Distribution of predicted boxes and their real localization quality (IoU between the prediction and ground-truth), calculated over all validation images on COCO [22] dataset, based on GFLV1 model. (c) and (d): Two specific examples from (b), where the sharp distribution corresponds to higher quality, whilst the flat one stands for lower quality usually. Green: predicted bounding boxes; White: ground-truth bounding boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>usually describe the bounding box regression by Dirac delta distribution: y = +∞ −∞ δ(x − y)x dx. Unlike them, GFLV1 introduces a flexible General Distribution P (x) to represent the bounding box, where each edge of the bounding box can be formulated as:ŷ = +∞ −∞ P (x)x dx = yn y0 P (x)x dx, under a predefined output range of [y 0 , y n ].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The illustration of the proposed Generalized Focal Loss V2 (GFLV2), where a novel and tiny Distribution-Guided Quality Predictor (DGQP) uses the statistics of learned bounding box distributions to facilitate generating reliable IoU quality estimations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Topkm(·) feature is robust to object scales.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Different ways to utilize the distribution statistics, including Decomposed Form (left) and Composed Form (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Visualization of predicted bounding boxes before and after NMS, along with their corresponding predicted LQE scores (only Top-4 scores are plotted for a better view)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Comparisons of losses on LQE between GFLV1 and GFLV2. DGQP helps to ease the learning difficulty with lower losses during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Visualization of the input Top-1 values (mean of four sides) from the learned distribution and the output IoU quality prediction given by DGQP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performances of various k, p in DGQP. k = 0 denotes the baseline version without the usage of DGQP (i.e., GFLV1).</figDesc><table><row><cell>Input Feature</cell><cell></cell><cell>AP</cell><cell cols="3">AP50 AP75 FPS</cell></row><row><cell cols="3">Baseline (ATSS [40] w/ QFL [18]) 39.9</cell><cell>58.5</cell><cell>43.0</cell><cell>19.4</cell></row><row><cell></cell><cell>(a)</cell><cell>40.2</cell><cell>58.6</cell><cell>43.7</cell><cell>19.3</cell></row><row><cell></cell><cell>(b)</cell><cell>40.5</cell><cell>59.0</cell><cell>44.0</cell><cell>14.0</cell></row><row><cell></cell><cell>(c)</cell><cell>40.5</cell><cell>58.7</cell><cell>44.1</cell><cell>16.2</cell></row><row><cell>Convolutional Features</cell><cell>(d)</cell><cell>40.6</cell><cell>59.0</cell><cell>44.0</cell><cell>18.3</cell></row><row><cell></cell><cell>(e)</cell><cell>40.6</cell><cell>58.9</cell><cell>44.1</cell><cell>17.8</cell></row><row><cell></cell><cell>(f)</cell><cell>40.7</cell><cell>59.0</cell><cell>44.1</cell><cell>17.9</cell></row><row><cell></cell><cell>(g)</cell><cell>40.8</cell><cell>58.9</cell><cell>44.6</cell><cell>18.4</cell></row><row><cell cols="2">Distribution Statistics (ours)</cell><cell>41.1</cell><cell>58.8</cell><cell>44.9</cell><cell>19.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparisons among different input features by fixing the hidden layer dimension of DGQP.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>Type</cell><cell></cell><cell>AP</cell><cell cols="3">AP50 AP75 FPS</cell></row><row><cell cols="2">Baseline (GFLV1 [18])</cell><cell>40.2</cell><cell>58.6</cell><cell>43.4</cell><cell>19.4</cell></row><row><cell></cell><cell>d = 16</cell><cell>40.5</cell><cell>58.5</cell><cell>43.7</cell><cell>19.2</cell></row><row><cell></cell><cell>d = 32</cell><cell>40.5</cell><cell>58.5</cell><cell>43.7</cell><cell>19.2</cell></row><row><cell>Composed Form</cell><cell>d = 64</cell><cell>40.7</cell><cell>58.5</cell><cell>44.3</cell><cell>19.1</cell></row><row><cell></cell><cell cols="2">d = 128 40.7</cell><cell>58.6</cell><cell>44.4</cell><cell>18.9</cell></row><row><cell></cell><cell cols="2">d = 256 40.7</cell><cell>58.3</cell><cell>44.2</cell><cell>18.5</cell></row><row><cell cols="2">Decomposed Form (ours)</cell><cell>41.1</cell><cell>58.8</cell><cell>44.9</cell><cell>19.4</cell></row></table><note>shows that our dis- tribution statistics perform best in overall AP, also fastest in inference, compared against various convolutional features. Usage of the Decomposed Form: Next, we examine what</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparisons between Decomposed Form (proposed) andComposed Form (with various dimension d settings).</figDesc><table><row><cell>Method</cell><cell>GFLV2 AP</cell><cell cols="3">AP50 AP75 FPS</cell></row><row><cell>RetinaNet [21]</cell><cell>36.5</cell><cell>55.5</cell><cell>38.7</cell><cell>19.0</cell></row><row><cell>RetinaNet [21]</cell><cell>38.6 (+2.1)</cell><cell>56.2</cell><cell>41.7</cell><cell>19.0</cell></row><row><cell>FoveaNet [15]</cell><cell>36.4</cell><cell>55.8</cell><cell>38.8</cell><cell>20.0</cell></row><row><cell>FoveaNet [15]</cell><cell>38.5 (+2.1)</cell><cell>56.8</cell><cell>41.6</cell><cell>20.0</cell></row><row><cell>FCOS [33]</cell><cell>38.5</cell><cell>56.9</cell><cell>41.4</cell><cell>19.4</cell></row><row><cell>FCOS [33]</cell><cell>40.6 (+2.1)</cell><cell>58.2</cell><cell>43.9</cell><cell>19.4</cell></row><row><cell>ATSS [40]</cell><cell>39.2</cell><cell>57.4</cell><cell>42.2</cell><cell>19.4</cell></row><row><cell>ATSS [40]</cell><cell>41.1 (+1.9)</cell><cell>58.8</cell><cell>44.9</cell><cell>19.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>Integrating GFLV2 into various popular dense object de- tectors. A consistent ∼2 AP gain is observed without loss of in- ference speed.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Pearson Correlation Coefficients (PCC) for representative dense object detectors. * denotes the application of Classification-IoU Joint Representation, instead of additional Centerness branch.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Comparisons between state-of-the-art detectors (single-model and single-scale results except the last row) on COCO test-dev. "MStrain" and "MStest" denote multi-scale training and testing, respectively. FPS values with</figDesc><table /><note>* are from</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Comparisons of training and inference efficiency based on ResNet-50 backbone. "Training Hours" is evaluated on 8 GeForce RTX 2080Ti GPUs under standard 1x schedule (12 epochs).</figDesc><table /><note>* denotes the application of Classification-IoU Joint Representation.</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hierarchical shot detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiale</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Reppoints v2: Verification meets regression for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08508</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Centripetalnet: Pursuing high-quality keypoint pairs for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoxuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengju</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Centernet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghua</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin-Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Res2net: A new multi-scale backbone architecture. TPAMI</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mask scoring r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaojin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Acquisition of localization confidence for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Probabilistic anchor assignment with iou prediction for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hee Seok</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Foveabox: Beyond anchor-based object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03797</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Generalized focal loss: Learning qualified and distributed bounding boxes for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scale-aware trident networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Grid r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Libra r-cnn: Towards balanced learning for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Borderdet: Border feature for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Revisiting the sibling head in object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanglu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Improving object localization with fitness nms and bounded iou loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lachlan</forename><surname>Tychsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Petersson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Side-aware boundary localization for more precise object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Iou-aware single-stage object detector for accurate localization. Image and Vision Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengkai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoping</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Reppoints: Point set representation for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Varifocalnet: An iou-aware dense object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feras</forename><surname>Dayoub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niko</forename><surname>Sünderhauf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.13367</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Freeanchor: Learning to match anchors for visual object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaosong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<title level="m">Objects as points</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengkai</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuhang</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.03496</idno>
		<title level="m">Autoassign: Differentiable label assignment for dense object detection</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Soft anchor-point object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Feature selective anchor-free module for single-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
