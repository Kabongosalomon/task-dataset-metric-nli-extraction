<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Weighted Tsetlin Machine: Compressed Representations with Weighted Clauses</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-01-14">14 Jan 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Phoulady</surname></persName>
							<email>1adrian.phoulady@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole-Christoffer</forename><surname>Granmo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Artificial Intelligence Research</orgName>
								<orgName type="institution">University of Agder</orgName>
								<address>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><forename type="middle">R</forename><surname>Gorji</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Artificial Intelligence Research</orgName>
								<orgName type="institution">University of Agder</orgName>
								<address>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hady</forename><forename type="middle">Ahmady</forename><surname>Phoulady</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">California State University</orgName>
								<address>
									<settlement>Sacramento</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The Weighted Tsetlin Machine: Compressed Representations with Weighted Clauses</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-01-14">14 Jan 2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Tsetlin Machines</term>
					<term>Pattern Recognition</term>
					<term>Proposi- tional Logic</term>
					<term>Learning Automata</term>
					<term>Frequent Pattern Mining</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Tsetlin Machine (TM) is an interpretable mechanism for pattern recognition that constructs conjunctive clauses from data. The clauses capture frequent patterns with high discriminating power, providing increasing expression power with each additional clause. However, the resulting accuracy gain comes at the cost of linear growth in computation time and memory usage. In this paper, we present the Weighted Tsetlin Machine (WTM), which reduces computation time and memory usage by weighting the clauses. Real-valued weighting allows one clause to replace multiple, and supports fine-tuning the impact of each clause. Our novel scheme simultaneously learns both the composition of the clauses and their weights. Furthermore, we increase training efficiency by replacing k Bernoulli trials of success probability p with a uniform sample of average size pk, the size drawn from a binomial distribution. In our empirical evaluation, the WTM achieved the same accuracy as the TM on MNIST, IMDb, and Connect-4, requiring only 1/4, 1/3, and 1/50 of the clauses, respectively. With the same number of clauses, the WTM outperformed the TM, obtaining peak test accuracies of respectively 98.63%, 90.37%, and 87.91%. Finally, our novel sampling scheme reduced sample generation time by a factor of 7.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Tsetlin Machine (TM) is a novel learning mechanism that was introduced in 2018 (Granmo 2018). Like artificial neural networks, the TM can map an exponential number of feature value combinations to an appropriate response. However, neural networks are based on multiplication, accumulation, and non-linear activation across multiple layers of neurons, whereas the TM is based on propositional logic formulas, formed by learning automata.</p><p>For n binary features, there can be 2 2 n propositional formulas. Equipped with an adequate number of conjunctive clauses, a TM can learn any of them in a relatively simple form. In addition to being computationally simple, the models built by the TM have the advantage of being interpretable by human experts, which may be of vital importance in sensitive applications such as medical decision making <ref type="bibr" target="#b6">Rudin 2019)</ref>.</p><p>Learning Automata. Learning automata have attracted considerable interest because they can learn the optimal action when operating in unknown stochastic environments <ref type="bibr" target="#b9">(Thathachar and Sastry 1987)</ref>. As such, they have been used for pattern classification over more than four decades. Early work includes stochastic learning automata based classifiers and games of learning automata (Barto and Anandan 1985; <ref type="bibr" target="#b9">Thathachar and Sastry 1987)</ref>. These approaches can learn the optimal classifier for specific forms of discriminant functions (e.g., linear classifiers) also when feedback is noisy. Along the same lines, Sastry and Thathachar have provided several algorithms based on cooperating systems of learning automata <ref type="bibr" target="#b7">(Sastry and Thathachar 1999)</ref>.</p><p>More recently, Zahiri proposed hyperplane-and rulebased classifiers, which performed comparably to other well-known methods in the literature <ref type="bibr" target="#b11">(Zahiri 2008;</ref><ref type="bibr" target="#b11">Zahiri 2012)</ref>. Recent research also includes a noise-tolerant learning algorithm built upon a team of continuous-action learning automata <ref type="bibr" target="#b8">(Sastry, Nagendra, and Manwani 2009)</ref>. Further, Goodwin, Yazidi, and Jonassen proposed a learning automata-guided random walk on a grid to construct robust discriminant functions <ref type="bibr" target="#b2">(Goodwin, Yazidi, and Jonassen 2016)</ref>. Finally, Motieghader et al. introduced a hybrid scheme that combines a genetic algorithm with learning automata to address the gene selection problem in cancer classification <ref type="bibr" target="#b3">(Motieghader et al. 2017</ref>).</p><p>In general, previous learning automata schemes have mainly addressed relatively small-scale pattern recognition tasks. Lately, however, several TM-based approaches have demonstrated promising scalability properties. This includes the natural language understanding approach by Berge et al., which uses the conjunctive clauses of the TM to capture textual patterns   <ref type="figure">Figure 1</ref>: A two-action Tsetlin automaton with 2N internal states weighted conjunctive clauses. Our intent is to significantly reduce memory usage and computation time by using the clause weights to compress the pattern representation of the TM. We start with giving a brief overview of the TM in Section 2. Then, in Section 3, we introduce the WTM and the concept of weighted clauses, that is, clauses that output continuous rather than binary values. Real-valued weighting allows one clause to replace multiple, and supports fine-tuning the impact of each clause. Our novel scheme simultaneously learns both the composition of each clause as well as their weights. Further, we simplify sample generation during learning by replacing Bernoulli process sampling with sampling from a binomial distribution followed by uniform sampling. We evaluate our approach empirically on MNIST, IMDb, and Connect-4 in Section 4, demonstrating that the WTM outperforms the TM in all the experiments. We conclude in Section 5, and present paths ahead for further research.</p><formula xml:id="formula_0">1 2 · · · N N +1 N +2 · · · 2N β 2 β 2 β 1 β 2 β 2 β 1 β 1 β 1 α 1 α 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Tsetlin Machine</head><p>TM learning is based on forming formulas in propositional logic to capture frequent patterns of high discriminating power. We here describe how the TM operates multiple teams of so-called Tsetlin automata <ref type="bibr" target="#b10">(Tsetlin 1961)</ref> to build discriminative conjunctive clauses, followed by majority voting to decide the final classification from the clause outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Tsetlin Automaton</head><p>A two-action Tsetlin automaton <ref type="bibr" target="#b10">(Tsetlin 1961;</ref><ref type="bibr" target="#b4">Narendra and Thathachar 2012</ref>) is a learning automaton with 2N states φ 1 , φ 2 , . . . , φ 2N and two actions α 1 , α 2 <ref type="figure">(Figure 1</ref>). It performs its actions sequentially in an environment. That is, in the states φ 1 , . . . , φ N , the Tsetlin automaton performs action α 1 , and in the states φ N +1 , . . . , φ 2N , it performs action α 2 . The environment responds with two types of input to the automaton: penalties β 1 and rewards β 2 . A state transition function governs learning. With the penalty input β 1 , the transition function changes the state φ i to φ i+1 for 1 ≤ i ≤ N and to φ i−1 for N + 1 ≤ i ≤ 2N . Conversely, the reward input β 2 makes the state φ i change to φ i−1 for 1 &lt; i ≤ N and to φ i+1 for N + 1 ≤ i &lt; 2N , while leaving it unchanged for i = 1, 2N . In effect, β 1 makes the Tsetlin automaton change state toward the centre, whereas β 2 moves the state away from the centre, towards the extreme ends 1 or 2N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Tsetlin Machine Structure and Inference</head><p>A binary classifier over o binary features can be regarded as a Boolean function -each of the 2 o possible combinations of the o binary inputs belongs to one of the classes 0 or 1.</p><p>A Boolean function can always be represented as a disjunction of conjunctive clauses, referred to as disjunctive normal form. For binary input features x = (x 1 , . . . , x o ), each conjunctive clause C j is represented by a subset L j of the literal set L = {x 1 , . . . ,</p><formula xml:id="formula_1">x o , ¬x 1 , . . . , ¬x o }. Knowing L j , we have C j (x) = l∈Lj l.</formula><p>(1)</p><p>The Boolean function f in disjunctive normal form with clauses C 1 , . . . , C m then becomes</p><formula xml:id="formula_2">f (x) = m j=1 C j (x) = m j=1 l∈Lj l.<label>(2)</label></formula><p>Similar to Karnaugh maps (Karnaugh 1953), conjunctive clauses form the core of function representation in the TM. In brief, a TM learns the composition of each conjunctive clause by specifying its literals using a team of 2o Tsetlin automata, one Tsetlin automaton per literal. Each Tsetlin automaton decides between two actions. Action α 1 means excluding the associated literal, whereas action α 2 means including it in the conjunctive clause. To exemplify, <ref type="figure">Figure 2</ref> shows the configuration of a team of eight Tsetlin automata that form a conjunctive clause from the three binary features x 1 , x 2 , and x 3 .</p><formula xml:id="formula_3">TA 1 TA 2 TA 3 TA 4 TA 5 TA 6 α 2 x 1 x 1 α 1 x 2 α 2 x 3 x 3 α 1 ¬x 1 α 2 ¬x 2 ¬x 2 α 1 ¬x 3 Figure 2: A team of six two-action Tsetlin automata forming the clause x 1 ∧ x 3 ∧ ¬x 2</formula><p>Instead of forming a disjunction of the conjunctive clauses, a TM sums up the clause outputs to produce an ensemble effect. While any propositional formula can be represented in disjunctive normal form, it turns out that the introduced ensemble effect helps dealing with noisy data (Granmo 2018). Further, the TM groups the clauses into positive ones C + 1 , C + 2 , . . . , C + cP and negative ones C − 1 , C − 2 , . . . , C − cN . In effect, for the input x = (x 1 , . . . , x o ), the TM computes the signed sum</p><formula xml:id="formula_4">s(x) = cP j=1 C + j (x) − cN j=1 C − j (x)<label>(3)</label></formula><p>to perform classification. If the signed sum is negative, the TM classifies the input to classŷ = 0, and otherwise, it classifies it to classŷ = 1. In other words, the classification is performed by applying the unit step function on s(x):</p><formula xml:id="formula_5">y = u s(x) = u cP j=1 C + j (x) − cN j=1 C − j (x) . (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Tsetlin Machine Learning</head><p>The TM builds upon reinforcement learning, with feedback given directly to the conjunctive clauses. Each clause, in turn, passes the feedback onward to its individual Tsetlin automata. There are two types of feedback: Feedback Type I and Feedback Type II. When a clause receives Type I feedback, its automata are modified so that the clause eventually evaluates to 1 for the current input. If it already evaluates to 1, Type I feedback will instead refine the clause by inserting more literals. Type II feedback, on the other hand, makes the affected clauses eventually evaluate to 0. In combination, these two types of feedback gradually modify which literals are present in each clause in a way that over time improves the classification accuracy.</p><p>Overall Feedback Loop A TM is initialized by randomly setting the states of the individual Tsetlin automata, thus producing clauses with randomly selected literals. From there, training data is fed to the TM, one example (x, y) at a time.</p><p>In other words, the learning can be performed on-line. For an input x of class y = 0, the goal is to make the signed sum s(x) of the clauses become negative (see <ref type="table" target="#tab_5">Equation 4</ref>). If this is not the case, we mitigate by giving some of the negative clauses Type I feedback, and some of the positive clauses Type II feedback. For input x of class y = 1, on the other hand, the learning goal is to make the signed sum s(x) become non-negative (again see Equation <ref type="formula">4</ref>). Accordingly, if the signed sum of the clauses is negative, we increase it by giving some of the positive clauses Type I feedback and some of the negative clauses Type II feedback.</p><p>To introduce an ensemble effect, feedback is given to a random selection of the clauses based on a hyperparameter T -the target of summation. In brief, the TM strives to make the signed sum s(x) reach −T for an input of class y = 0. Conversely, it seeks to make s(x) reach T for an input of class y = 1. To achieve this, we first clamp s(x) between −T and T : c(x) = clamp s(x), −T, T . Second, each clause receives feedback with probability p c (x) proportional to the difference between the clamped sum and the summation target:</p><formula xml:id="formula_6">p c (x) =      T + c(x) 2T , if y = 0, T − c(x) 2T , if y = 1.<label>(5)</label></formula><p>The above random clause selection has an additional crucial effect: it guides the clauses to distribute themselves across the significant frequent sub-patterns, as opposed to clustering on a few ones. This is achieved by making the expected frequency of feedback to each clause proportional to the difference between the clamped sum c(x) and the summation target ±T . That is, feedback activity calms down as s(x) approaches its target ±T , and comes to a complete standstill when the target is met or surpassed. In this manner, only a fraction of the available clauses are stimulated to recognize each frequent sub-pattern.</p><p>Type I Feedback We first cover the details of Type I feedback, which plays two roles. Firstly, Type I feedback attempts to modify clauses that evaluate to 0 for the current input x, so that they eventually start outputting 1 instead. This is achieved by making the clauses sparser, removing literals one by one. If a clause already outputs 1, however, Type I feedback refines the clause by including more literals, making it denser. <ref type="table">Table 1</ref> contains the rules by which Type I feedback operates. Recall that each Tsetlin automaton is associated with a specific literal, within a specific clause. As seen in the table, the value of the clause in combination with the value of the literal governs feedback to the associated Tsetlin automaton.</p><p>(1) If both the clause and the literal are of value 1, the Tsetlin automaton receives feedback that stimulates including the literal. The purpose is to refine the encompassing clause, so that it provides a more specific representation of the input x.</p><p>(2) All the other Tsetlin automata (those associated with clauses or literals of value 0), on the other hand, receive feedback that stimulates excluding their literals. This occurs randomly with probability p s . The goal here is to combat overfitting by making the clause sparser, and in the extreme, erase the pattern completely to make room for a new.</p><p>Note that the probability p s is a hyperparameter that controls the sparsity of the clauses produced. In brief, the sparsity of clauses increases with p s , simply because a higher p s produces more exclude actions relative to include actions. Also note that as learning proceeds, Type I feedback makes the states of the Tsetlin automata gradually move away from the centre states. The automata thus become increasingly confident in their decisions. This stabilizes the composition of the clauses, calming down exploration.</p><p>Type II Feedback The purpose of giving Type II feedback to a clause is to ensure that it eventually outputs 0 for the current input. <ref type="table" target="#tab_2">Table 2</ref> contains the rules for Type II feedback. As seen, it is only when a clause outputs 1 that its Tsetlin automata are given Type II feedback, and then only those automata whose literals are of value 0. These Tsetlin automata are quite simply penalized for their exclude actions. Accordingly, sooner or later, one of the excluded literals are included instead, turning the clause output to 0.</p><p>Contrary to Type I feedback, Type II feedback is not intended to make the Tsetlin automata more confident, or the clauses denser/sparser. Rather, Type II feedback disrupts  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Weighted Tsetlin Machine</head><p>We now introduce the Weighted Tsetlin Machine (WTM), which enhances the classic TM with weighted clauses as well as improving sampling efficiency. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Binomial Distribution Based Type I Feedback</head><p>Random number generation is relatively time consuming, yet an essential part of giving Type I feedback. That is, when randomizing feedback to the 2o Tsetlin automata of a clause, one samples from a Bernoulli process of length u = 2o with success probability p s . We here propose to increase training efficiency by leveraging that p s typically is small in TM learning, and that the number of successes S u of the Bernoulli process has a Binomial distribution:</p><formula xml:id="formula_7">P (S u = k) = u k p k s (1 − p s ) u−k .<label>(6)</label></formula><p>To single out the Tsetlin automata to receive feedback more efficiently, we first assume that there are no successes:</p><formula xml:id="formula_8">B[1 . . u] ← 0.</formula><p>We then obtain the number of successes q by sampling once from the corresponding binomial distribution. Finally, we update B[1 . . u] by randomly selecting q Tsetlin automata. As summarized in Algorithm 1, we thus produce the complete sample by generating about 2p s o + 1 random values on average, rather than 2o.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Weighted Clauses</head><p>Due to the ensemble effect of TM learning, several versions of similar clauses may occur multiple times in the final clas-1 The code for the Weighted Tsetlin Machine can be found at https://github.com/adrianphoulady/weighted-tsetlin-machine-cpp (C++), and https://github.com/cair/pyTsetlinMachine (Python/C).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BINOMIAL-UNIFORM</head><formula xml:id="formula_9">-SAMPLING(u, p) 1 B[1 . . u] ← 0 2 q ← BINOMIAL(u, p) 3 k ← 1 4 while k ≤ q 5 v ← UNIFORM(1, u) 6 if B[v] = 1 7 B[v] ← 1 8 k ← k + 1 9 return B</formula><p>Algorithm 1: Generating u Bernoulli process samples with success probability p from a binomial distribution sifier. The question we investigate here is whether the collection of clauses as a whole can be represented more compactly by weighting the clauses. Then, instead of repeating a clause w times, we can keep a single instance, assigning it the weight w. Further, by allowing w to be real-valued, we can introduce fractional clauses. Thus, if we want to halve the effect of a clause, we simply give it the weight w = 0.5. In this manner, a more compact representation can be obtained by using fewer clauses, reducing the number of model parameters, to facilitate both faster learning and faster classification.</p><p>In all brevity, we associate a positive real-valued weight w j with each clause j, multiplying the weights with the respective clause outputs:</p><formula xml:id="formula_10">s ′ (x) = cP j=1 w + j C + j (x) − cN j=1 w − j C − j (x).<label>(7)</label></formula><p>In this structure, to increase or reduce the impact of a clause, one merely needs to adjust its weight. Furthermore, the overall sum s ′ (x) is now real-valued, generalizing the TM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">WTM Recognition</head><p>Recognition in the WTM is conducted in the same way as for the TM. That is, we simply apply the step function to the weighted sum:</p><formula xml:id="formula_11">y = u s ′ (x) = u cP j=1 w + j C + j (x) − cN j=1 w − j C − j (x) .</formula><p>(8) The multiclass WTM operates similarly, too. In brief, with n classes, we introduce one WTM per class i = 1, . . . , n. The resulting weighted sums s ′ i (x) are then used for classification. That is, the largest weighted sum determines the predicted class:ŷ = argmax</p><formula xml:id="formula_12">i {s ′ i (x)}.<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">WTM Learning</head><p>The crucial question that remains is how the weights can be learned to provide a compact, yet accurate pattern representation. We here propose a simple scheme for learning the weights.</p><p>Initialization. First, we initialize all the weights to 1.0:</p><formula xml:id="formula_13">w + j ← 1.0,<label>(10)</label></formula><p>w − j ← 1.0.</p><p>(11) Thus, at the start of learning, the behaviour of the clauses are identical to those of the TM.</p><p>Weight Updating. Adjusting the Tsetlin automata's states in the WTM is performed in the same way as for the TM. However, we introduce a novel approach for updating the weights. This updating is governed by Type I and Type II feedback. Note that, below, we only update the weights of those clauses that evaluate to 1 for the current input x.</p><p>Learning Rate. Weight learning is controlled by a learning rate γ ∈ [0, ∞). For Type I feedback, each clause C + j /C − j outputting 1 has its weight w + j /w − j updated by multiplying it by 1 + γ:</p><formula xml:id="formula_14">w + j ← w + j · (1 + γ), if C + j (x) = 1,<label>(12)</label></formula><formula xml:id="formula_15">w − j ← w − j · (1 + γ), if C − j (x) = 1.</formula><p>(13) For Type II feedback, we instead update the weights by dividing by 1 + γ:</p><formula xml:id="formula_16">w + j ← w + j / (1 + γ), if C + j (x) = 1,<label>(14)</label></formula><formula xml:id="formula_17">w − j ← w − j / (1 + γ), if C − j (x) = 1.<label>(15)</label></formula><p>When a clause evaluates to 0, its weight remains unchanged:</p><formula xml:id="formula_18">w + j ← w + j , if C + j (x) = 0,<label>(16)</label></formula><formula xml:id="formula_19">w − j ← w − j , if C − j (x) = 0.</formula><p>(17) As seen, for Type I feedback, the weights are increased to strengthen the impact of the associated clauses, thus reinforcing true positive frequent patterns. For Type II feedback, on the other hand, the weights are decreased instead. This is to diminish the impact of the associated clauses, thus combating false positives. Observe that for γ = 0, the weights of the clauses are fixed at the initial value 1.0, making the resulting WTM equivalent to a TM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Empirical Evaluation</head><p>We here present empirical results on the performance of the WTM in comparison with the TM. To this end, we use datasets from three different areas of pattern recognition: MNIST handwritten digit recognition <ref type="bibr" target="#b2">(LeCun et al. 1998)</ref>, IMDb sentiment analysis <ref type="bibr" target="#b3">(Maas et al. 2011)</ref>, and Connect-4 game winner prediction <ref type="bibr" target="#b2">(Dua and Graff 2017)</ref>. We further investigate the behaviour of the novel WTM mechanisms in more detail, including learning speed and weight distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">MNIST Handwritten Digit Recognition</head><p>The MNIST dataset contains images of handwritten digits, 60,000 training and 10,000 test examples. Each example is a labelled 28 × 28-pixel grayscale image The pixels are represented by integers between 0 (black) and 255 (white), and each label is an integer from 0 to 9. We binarize the input pixels to get 768 bits of data. A value less than 77 = ⌈.3 · 255⌉ is converted to 0, and a value greater than or equal to 77 is converted to 1. Note that other binarization thresholds provided similar performance because the image pixels are mainly black and white.</p><p>Feedback Generation Speedup Employing 2,000 classclauses on MNIST produces 12.6 millions Type I feedback calls in the first epoch . In turn, each call samples n = 2 · 28 · 28 = 1568 values from a Bernoulli process with success probability p s = .1, to assign feedback to the individual Tsetlin automata. This is the most time consuming part of TM learning. Whereas recognizing the 60,000 images takes 23.5 seconds, feedback generation using standard Bernoulli process sampling takes 61.2 seconds. This is about 2/3 of the training time in the first epoch. However, replacing the Bernoulli process sampling with our binomial distribution based sampling approach (Algorithm 1), we are able to speed up the random feedback generation by a factor of 7, significantly improving the overall learning speed of the TM.</p><p>Weighted Clauses with WTM We now turn to evaluating the effect of weighted clauses. In our first experiment we used a 10-class TM with 2,000 clauses per class as baseline. The accuracy obtained by this configuration was parred by a WTM with merely 500 clauses. This reduced memory usage 4 times, while increasing execution time by a factor of 4. Together with the fast feedback generation procedure, the full WTM ran approximately 10 times faster than the TM, measured over 300 epochs of training.   Lastly, we investigated the behaviour of the WTM weight learning. In <ref type="table" target="#tab_5">Table 4</ref>, the maximum, minimum, and mean of the weights are reported per class. As seen, how the weights are distributed varies from class to class. The weights span from 0.09 to 21.01, and we observe that every class both have fractional and large weight clauses. As a representative example, <ref type="figure" target="#fig_0">Figure 3</ref> shows the distribution of positive weights after training epoch 300 for the digit '9' class. The distribution reveals that the weights are relatively diverse, including two distinct peaks, around 0.6 and 5.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Maximum</head><p>The benefits of weighed clauses thus becomes apparent. The clause with the largest weight has 21.01 0.09 = 233.44 times more impact than the clause with the smallest weight. To achieve this effect with a standard TM, one would need approximately 233 times more copies of the first clause, compared to the second clause. Hence, the power of the WTM scheme!  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">IMDb Sentiment Analysis</head><p>The IMDb sentiment analysis dataset consists of 50,000 highly polar movie reviews, organized into two categories: positive and negative reviews. The dataset is split evenly into 25,000 reviews for training and 25,000 reviews for testing. In this experiment, we use the unigrams and bigrams of each review as feature candidates, selecting the best 5,000 ones based on Chi-square feature selection. Each review is thus represented by a binary feature vector of size 5,000, marking the presence and absence of unigrams/bigrams.</p><p>We first compare a WTM with 3,200 class-clauses with a TM with 10,000 class-clauses. After 30 epochs of training, the WTM reached a peak testing accuracy of 89.54%, compared to 89.38% for the TM. With an equal number of clauses, 25,000 per class, the WTM obtained an accuracy of 90.25% ± 0.16%, significantly higher than what is achieved with the TM (  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Connect-4 Winner Prediction</head><p>Connect-4 is a two-player board game similar to Tic-Tac-Toe, played on a 6 × 7 grid. Each player tries to connect four of her pieces in a row, either horizontally, vertically, or diagonally. The Connect-4 dataset <ref type="bibr" target="#b2">(Dua and Graff 2017)</ref> contains all 67,557 legal positions of the board after eight moves, in which neither player has won yet and the next move is not forced. Each position is labelled as either a win, a loss, or a draw -the outcome of the game after optimal play from the respective position. We assign 10% of the data to testing, keeping 90% for training. We further represent the board state as a binary feature vector of size 6 × 7 × 2. The first 6 × 7 bits capture the placement of player one pieces, and the last 6 × 7 bits capture the placements of player two. Using a WTM with 200 clauses, we achieved a similar mean test accuracy to a TM with 10,000 clauses over the last 200 epochs of 1,000 epochs of training; that is, the WTM provided similar accuracy with 50 times fewer clauses! Table 7 contains the resulting test accuracy statistics of a WTM and a TM with the same number of 25,000 clauses per class, again, demonstrating superior performance of the WTM compared to the TM. Here, the TM spent on average 105 seconds per epoch, while the WTM used 76 seconds, which again is slightly faster despite the addition of weights.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Maximum Minimum Mean</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>The distribution of weights learnt from MNIST with a WTM possessing 4,000 clauses per class Comparison with Other Methods</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>). Also, TMbased convolution for image analysis was introduced by Granmo et al., employing the clauses as interpretable filters (Granmo et al. 2019). Further, Abeyrathna et al. designed a novel TM for regression (Abeyrathna et al. 2019a) that supports continuous input (Abeyrathna et al. 2019b). All of these schemes compare favourably with state-of-the-art pattern classification and regression techniques.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Type II Feedback the targeted clause by introducing new literals that poten- tially have high discrimination power. These literals are only picked up by Type I feedback if they take part in frequent sub-patterns. Otherwise, Type I feedback will throw them back into exclusion. In this manner, overfitting is combated, while discrimination power is increased.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>MNIST test accuracy statistics of the TM and the WTM with 4,000 clauses per class Furthermore, by running the TM and WTM with the same number of clauses (4,000 per class) we observed that the</figDesc><table><row><cell>Class</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell></row><row><cell>Maximum</cell><cell>7.1</cell><cell>8.49</cell><cell>16.88</cell><cell cols="3">19.02 12.15 16.71</cell><cell>8.58</cell><cell>18.27</cell><cell cols="2">19.02 21.01</cell></row><row><cell>Minimum</cell><cell>0.37</cell><cell>0.33</cell><cell>0.21</cell><cell>0.16</cell><cell>0.22</cell><cell>0.22</cell><cell>0.29</cell><cell>0.18</cell><cell>0.11</cell><cell>0.09</cell></row><row><cell>Mean</cell><cell>1.95</cell><cell>1.94</cell><cell>2.67</cell><cell>2.99</cell><cell>2.54</cell><cell>2.77</cell><cell>2.16</cell><cell>2.65</cell><cell>3.25</cell><cell>3.45</cell></row><row><cell>Ratio</cell><cell>19</cell><cell>26</cell><cell>82</cell><cell>122</cell><cell>54</cell><cell>74</cell><cell>29</cell><cell>103</cell><cell>177</cell><cell>231</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Clause weight statistics per class for a WTM with 4,000 clauses, trained on MNIST</figDesc><table><row><cell>WTM was significantly more accurate than the TM, with</cell></row><row><cell>slightly faster execution time (1.7 minutes versus 2 minutes</cell></row><row><cell>per epoch when both employ binomial distribution based</cell></row><row><cell>feedback). After 130 epochs, the WTM had a steady training</cell></row><row><cell>accuracy of 100%, whereas the TM peaked at 99.86%. The</cell></row><row><cell>WTM also had a higher testing accuracy, improving peak</cell></row><row><cell>accuracy from 98.27% to 98.63%. Table 3 contains test ac-</cell></row><row><cell>curacy statistics of the WTM, collected from the last 50 of</cell></row><row><cell>300 epochs of single-run training (multiple runs behave sim-</cell></row><row><cell>ilarly). As seen, the WTM learning is relatively stable, with</cell></row><row><cell>a small difference between minimum and maximum test ac-</cell></row><row><cell>curacy.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Table 5contrasts the WTM mean test accuracy on MNIST against vanilla versions of selected well-known classification methods, without applying performance enhancing techniques (such as boosting, ensembles, and convolution), and without data augmentation (such as deskewing, blurring, shifting, and resizing). 2 Note that the TM and WTM operate on the threshold-based binary encoding of the images (compression factor 8), rather than the original greyscale images. As seen, the WTM performs favourably compared to the other techniques, under comparable conditions.</figDesc><table><row><cell>Method Accuracy (%)</cell></row><row><cell>2-layer NN, 800 Hidden Units 98.6</cell></row><row><cell>Weighted Tsetlin Machine 98.5 ± 0.0</cell></row><row><cell>K-nearest Neighbors, L3 97.2</cell></row><row><cell>3-layer NN, 500+150 Hidden Units 97.1</cell></row><row><cell>40 PCA + Quadratic Classifier 96.7</cell></row><row><cell>1,000 RBF + Linear Classifier 96.4</cell></row><row><cell>Logistic Regression 91.5</cell></row><row><cell>Linear Classifier (1-layer NN) 88.0</cell></row><row><cell>Decision Tree 87.8</cell></row><row><cell>Multinomial Naive Bayes 83.2</cell></row><row><cell>Table 5: Comparison of vanilla methods on MNIST</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6</head><label>6</label><figDesc>). The average processing time per /yann.lecun.com/exdb/mnist/ and the original TM paper (Granmo 2018).</figDesc><table><row><cell>2 The</cell><cell>results</cell><cell>are</cell><cell>obtained</cell><cell>from</cell></row><row><cell>http:/</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>IMDb test accuracy statistics for a WTM and a TM with 25,000 clauses per class epoch was 16 minutes for the TM and 14 minutes for the WTM when both use binomial distribution based feedback.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Connect-4 test accuracy statistics for a WTM and a TM with 25,000 clauses per class</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Further Work</head><p>In this paper, we introduced the Weighted Tsetlin Machine (WTM), generalizing the TM by adding weighted clauses. Instead of repeating a clause multiple times to increase its impact, we assign a weight to each clause. The weights control the impact of the clauses, and are learnt using a novel variant of Type I and Type II feedback. In this manner, the WTM can obtain a more compact representation of patterns. In addition, since the weights can take fractional values, the clauses can more easily be fine-tuned to represent complex patterns. We further proposed a new sampling approach for Type I feedback generation, replacing sampling from a Bernoulli process with sampling from a binomial distribution followed by uniform sampling.</p><p>Our empirical results showed that the WTM performs comparably to the WTM when equipped with much fewer clauses (from 3 to 50 times less clauses). Furthermore, with the same amount of clauses, the WTM outperformed the TM accuracy-wise on MNIST, IMDb, In our further work, we intend to investigate whether the WTM approach also can improve the Regression Tsetlin Machine (Abeyrathna et al. 2019a) for logic regression <ref type="bibr" target="#b5">(Ruczinski, Kooperberg, and LeBlanc 2003)</ref>, being capable of more fine-grained tuning of clauses. Another improvement, also investigated in <ref type="bibr" target="#b0">(Abeyrathna et al. 2019b)</ref> for the regular TM, would be to introduce real-valued instead of binary inputs. This, for instance, would enable the WTM to operate directly on greyscale images, eliminating the need for binarization.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A scheme for continuous input to the tsetlin machine with applications to forecasting disease outbreaks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abeyrathna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems</title>
		<editor>Moura Oliveira, P.</editor>
		<editor>Novais, P.</editor>
		<editor>and Reis, L. P.</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1985" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="360" to="375" />
		</imprint>
	</monogr>
	<note>Pattern-recognizing stochastic learning automata</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Using the Tsetlin Machine to Learn Human-Interpretable Rules for High-Accuracy Text Categorization with Medical Applications</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="115134" to="115146" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Distributed learning automata for solving a classification task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazidi</forename><surname>Goodwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goodwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yazidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Jonassen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O.-C</forename><surname>Granmo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Glimsdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goodwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Omlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">T</forename><surname>Berge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O.-C ;</forename><surname>Granmo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09688</idno>
		<idno>arXiv:1804.01508</idno>
	</analytic>
	<monogr>
		<title level="m">The Tsetlin Machine -A Game Theoretic Bandit Driven Approach to Optimal Pattern Recognition with Propositional Logic</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1998" />
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>IEEE congress on evolutionary computation. Gradient-based learning applied to document recognition. Proceedings of the IEEE</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A hybrid gene selection algorithm for microarray cancer classification using genetic algorithm and learning automata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="246" to="254" />
		</imprint>
	</monogr>
	<note>Learning word vectors for sentiment analysis</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learning automata: an introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Narendra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Thathachar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Courier Corporation</publisher>
		</imprint>
	</monogr>
	<note>Narendra and Thathachar</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Logic regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kooperberg</forename><surname>Ruczinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Leblanc ; Ruczinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kooperberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leblanc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="475" to="511" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="206" to="215" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Rudin</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning automata algorithms for pattern classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thathachar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sadhana</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4-5</biblScope>
			<biblScope unit="page" from="261" to="292" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
	<note>Sastry and Thathachar</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A team of continuous-action learning automata for noise-tolerant learning of half-spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nagendra</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nagendra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Manwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="28" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Part B (Cybernetics)</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning optimal discriminant functions through a cooperative game of automata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Thathachar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="73" to="85" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
	<note>Man, and Cybernetics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On behaviour of finite automata in random medium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Tsetlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Avtom I Telemekhanika</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1354" />
			<date type="published" when="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Classification rule discovery using learning automata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Zahiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Zahiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Machine Learning and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="205" to="213" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Pattern Recognition Letters</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
