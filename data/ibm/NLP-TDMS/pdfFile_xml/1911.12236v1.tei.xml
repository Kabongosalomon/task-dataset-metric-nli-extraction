<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PointRGCN: Graph Convolution Networks for 3D Vehicles Detection Refinement</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesus</forename><surname>Zarzar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
								<address>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Giancola</surname></persName>
							<email>silvio.giancola@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
								<address>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
							<email>bernard.ghanem@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
								<address>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PointRGCN: Graph Convolution Networks for 3D Vehicles Detection Refinement</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In autonomous driving pipelines, perception modules provide a visual understanding of the surrounding road scene. Among the perception tasks, vehicle detection is of paramount importance for a safe driving as it identifies the position of other agents sharing the road. In our work, we propose PointRGCN: a graph-based 3D object detection pipeline based on graph convolutional networks (GCNs) which operates exclusively on 3D LiDAR point clouds. To perform more accurate 3D object detection, we leverage a graph representation that performs proposal feature and context aggregation. We integrate residual GCNs in a twostage 3D object detection pipeline, where 3D object proposals are refined using a novel graph representation. In particular, R-GCN is a residual GCN that classifies and regresses 3D proposals, and C-GCN is a contextual GCN that further refines proposals by sharing contextual information between multiple proposals. We integrate our refinement modules into a novel 3D detection pipeline, PointRGCN, and achieve state-of-the-art performance on the easy difficulty for the bird eye view detection task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The task of autonomous driving has received much deserved attention in recent years. Current progress in computer vision helps in providing reliable scene understanding from visual and geometrical data. In particular, autonomous agents must detect vehicles, pedestrians, cyclists, and other objects on the road to ensure a safe navigation. Pipelines for autonomous driving leverage 3D computer vision for tasks such as object detection <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10]</ref>, tracking <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> and trajectory prediction <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Current state-of-the-art methods in 3D vehicle detection prefer LiDAR point clouds over images. Performances of monocular <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b32">33]</ref> and stereobased <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b14">15]</ref> 3D object detection techniques are not yet comparable with LiDAR-based techniques <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b15">16]</ref>. Recent image-based methods for 3D vehicle detection even relies on LiDAR supervision <ref type="bibr" target="#b32">[33]</ref> to generate surrogate point clouds from images. Still, defining which point cloud rep-R-GCN C-GCN <ref type="figure">Figure 1</ref>: Proposed GCN modules. Insights on the Receptive field for both our proposed modules. The per-proposal R-GCN gathers meaningful features information between points on each proposal. The per-frame C-GCN gathers contextual information between proposals on each frame. Note that this non-contractual representation does not consider the dynamic graph on features for neighbors.</p><p>resentation fits the most with deep learning is not straightforward and remains an open problem <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b13">14]</ref>. Recent advances in graph convolution networks <ref type="bibr" target="#b13">[14]</ref> suggest that graph representations could provide better features for point cloud processing. Such a representation already outperforms the state-of-the-art in many other computer vision tasks <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34]</ref>. Thus, we investigate the use of a graph representation for LiDAR point cloud in the task of 3D vehicle detection. In our work, we propose two modules exploiting convolution operations on point cloud graph representations. We tackle vehicle detection in a common two-stage fashion by generating a set of high-recall proposals from a LI-DAR scene point clouds which we successively fine-tune to obtain refined detections. In particular, we introduce PointRGCN: a novel pipeline consisting of two refinement modules. R-GCN <ref type="figure">(Figure 1 (top)</ref>) provides a per-proposal feature aggregation based on a GCN that utilizes all points contained a proposal. C-GCN <ref type="figure">(Figure 1 (bottom)</ref>) gathers per-frame information from all proposals, looking for a contextual consistency in the road scene. We will release our code after the review process. Contributions. We summarize our contributions as fol-lows. (i) We are the first, to the best of our knowledge, to leverage graph representation and GCNs for the task of 3D vehicle detection. (ii) We propose R-GCN and C-GCN, two GCN-based modules for per-proposal and per-frame feature aggregation, designed to gather all features within and between proposals. (iii) We show competitive performances for our novel pipeline PointRGCN in the challenging KITTI <ref type="bibr" target="#b6">[7]</ref> 3D vehicle detection task and exhibit a`2% AP BEV boost on the easy subset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Our work relates to methods delving with 3D point cloud representations, graph convolutional networks, and 3D object detection architectures. We present previous works on these topics, and highlight the novelty of our pipeline with respect to each of these works.</p><p>3D Point Cloud Representation. Finding an efficient representation for sparse LiDAR point clouds is paramount for 3D computer vision tasks. Current works leverage projection, voxelization or per-point features. Image projections allow for efficient convolution operations on the image space, and are commonplace for 3D object classification and retrieval tasks <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b29">30]</ref>. Projecting provides coarse but structured inputs for further deep convolution network. Voxelization is a volumetric representation that discretizes 3D space into a structured grid. Voxelizing 3D space allows for 3D CNN processing <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b24">25]</ref>, but are not memory efficient due to the sparsity of point clouds. PointNet <ref type="bibr" target="#b21">[22]</ref> and PointNet++ <ref type="bibr" target="#b23">[24]</ref> solve the sparsity problem by learning a deep feature representation for each point, with all points contributing to a more complete representation. However, the context shared between points is limited to point relations in 3D space <ref type="bibr" target="#b23">[24]</ref>, and shapes still lack structure. In order to overcome these issues, we use GCNs as the backbone for both refinement modules in our network.</p><p>Graph Convolutional Network. To cope with current limitations of point cloud representation methods, Wang et al. <ref type="bibr" target="#b34">[35]</ref> propose a graph representation for point clouds and show improved performances in classification and segmentation tasks. Li et al. <ref type="bibr" target="#b13">[14]</ref> show that GCNs can run as deep as 112 layers by introducing residual skip connections, similar to ResNet. GCNs have an increased receptive field with respect to PointNet++ <ref type="bibr" target="#b23">[24]</ref> since the edge connections are dynamically assigned for each intermediate layer. With such features, GCNs have shown success in tasks such as segmentation <ref type="bibr" target="#b31">[32]</ref>, point cloud deformation <ref type="bibr" target="#b18">[19]</ref>, adversarial generation <ref type="bibr" target="#b27">[28]</ref> and point cloud registration <ref type="bibr" target="#b33">[34]</ref>. However, To the best of our knowledge, we introduce the first 3D vehicle detection module using a graph representation on LiDAR input.</p><p>Single-Stage 3D Object Detection. Single-stage detectors train a single network end-to-end to generate a small set of detection boxes with high precision. VoxelNet <ref type="bibr" target="#b41">[42]</ref> directly generates detections from a Region Proposal Network using a voxelized point cloud. SECOND <ref type="bibr" target="#b37">[38]</ref> improves upon VoxelNet by leveraging sparse convolution operations on the voxelized input. PointPillars <ref type="bibr" target="#b10">[11]</ref> uses a pillars representation based on SECOND voxelisation with infinite height. Voxel-FPN <ref type="bibr" target="#b30">[31]</ref> introduces a multi-scale voxel feature pyramid network based on VoxelNet <ref type="bibr" target="#b41">[42]</ref>. Qi et al. <ref type="bibr" target="#b19">[20]</ref> recently introduced a Deep Hough Voting scheme for indoor detection, adopting PointNet++ <ref type="bibr" target="#b23">[24]</ref> for the feature backbone. We preferred a two-stage detector since single-stage methods tend to attain lower performances, and GCNs are computationally expensive to compute on complete point clouds.</p><p>Two-Stage 3D Object Detection. On the other hand, twostage detection methods divide the detection pipeline into two sub-modules. One module is trained to generate a large number of high-recall proposals, and a second one to refine those proposals. Frustum PointNet <ref type="bibr" target="#b20">[21]</ref> uses 2D proposals from RGB images and localizes 3D bounding boxes using a PointNet <ref type="bibr" target="#b21">[22]</ref> representation on the 2D proposal's frustum. Liang <ref type="bibr" target="#b15">[16]</ref> propose a multi sensor multi view aggregation taking into account both LiDAR and RGB camera information. Alternatively, STD <ref type="bibr" target="#b39">[40]</ref> defines spherical anchors to obtain higher recall in the proposal stage. Chen et al. <ref type="bibr" target="#b5">[6]</ref> use a voxelization for their Region Proposal Network (RPN), and use PointNet features for proposal refinement. Also, the concurrent work of Lehner et al. <ref type="bibr" target="#b11">[12]</ref> proposes a coarse RPN on the top view projection which allows for a denser voxelization of proposals during refinement. PointRCNN <ref type="bibr" target="#b25">[26]</ref> creates object proposals by first performing point segmentation using a PointNet++ backbone, regressing a proposal for each foreground point, and finally refining the proposal using PointNet++. Part-A 2 Net <ref type="bibr" target="#b26">[27]</ref> improves upon PointRCNN by adding a part aware loss for intra-proposal point locations, and performing proposal refinement by using a Voxel Feature Encoding.</p><p>In our work, we build upon PointRCNN <ref type="bibr" target="#b25">[26]</ref> by using its proposal scheme, but leverage GCNs to refine proposals. Due to the computational and memory complexity of GCNs, we limit their usage to the refinement stage. In particular, we combine point features for each proposal with R-GCN, and aggregate proposal context information for each frame with C-CGN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section, we present our 3D object detection pipeline, PointRGCN, which takes advantage of recent advances in GCNs. Our model takes as an input a LIDAR scan of a scene and generates tight 3D bounding boxes for cars found in the LIDAR point cloud. Similar to common practice, we split object detection into two stages: generat-ing high recall proposals, and refining the set of proposals to get detections with high precision. The main focus of our network is on the second stage of object detection: refining proposals. Therefore, we take the region proposal network from an existing method, PointRCNN <ref type="bibr" target="#b25">[26]</ref>, to generate proposals. During refinement, we take advantage of geometric information contained in point clouds by leveraging a graph representation for points within proposals. Furthermore, we aggregate the contextual information across proposals in a second refinement step. In the following section, we present an introduction to GCNs and how they are applied in the context of object detection. We then present the details of our pipeline PointRGCN, including the two main components of our refinement network: R-GCN, and C-GCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Graph Embedding and GCNs.</head><p>Definitions. A graph G is defined as a tuple consisting of a set of nodes and a set of edges, pV, Eq, in which an edge e i,j P E, between nodes v i P V and v j P V takes a binary value signifying whether node v i is related to node v j or not. Connections between nodes are directed, i.e. an edge e i,j may differ from its reciprocal edge e j,i . Each node is represented by a c-dimensional feature vector, i.e. v i P R c . Graph Convolutional Networks. GCNs are an extension of CNNs which operate on graph representations instead of regular image grids. Given an input graph G, a graph convolution operation F aggregates features from k nodes in a neighborhood N pkq pvq of a given node v. A convolution operation F updates the value of the given node by aggregating features amongst its neighbor nodes, as presented in Equation <ref type="formula" target="#formula_0">(1)</ref>. This aggregates features from nearby nodes, mirroring the way convolutional filters aggregate nearby pixels. Note that unlike CNNs, GCNs do not apply different weights to different neighbors.</p><formula xml:id="formula_0">FpG l , W l q " UpdatepAggregatepG l , W a l q, W u l q G l`1 " FpG l , W l q`G l<label>(1)</label></formula><p>EdgeConv. Node features are aggregated around neighbors N pkq pv l q of nodes v l at layer l. In particular, the feature difference h u l´h v l q, for given neighbor features u l P N pkq pv l q is concatenated to the current node's features h v l , and processed with a shared multi-layered perceptron (MLP). This makes use of both a local geometry encoded by the feature difference as well as a global geometry obtained from the features of each node center h v l . Then, a max operation pools features between the neighbors and updates the current node's feature h v l`1 for next layer l`1.</p><p>Equation <ref type="formula" target="#formula_1">(2)</ref> illustrates the aggregation and update steps introduced by EdgeConv <ref type="bibr" target="#b34">[35]</ref>.</p><formula xml:id="formula_1">h v l`1 " maxpthpu l q|u l P N pkq pv l quq hpu l q " MLPpconcatph v l , h u l´h v l qq<label>(2)</label></formula><p>MRGCN. Rather than performing the MLP operation on all neighbors before the maxpool operation in Equation <ref type="formula" target="#formula_1">(2)</ref>, MRGCN <ref type="bibr" target="#b13">[14]</ref> performs the maxpool first, and applies the MLP only once. In particular, the difference features h u lh v l q for all neighbor features u l P N pkq pv l q are maxpooled into an intermediate representation h N pkq pv l q . Then, that representation is concatenated with the current node feature and processed once with an MLP. This reduces the memory budget since a single intermediate feature vector is stored for each neighbor, rather than k of them. Also, the MLP is performed only once, allowing for faster inference.</p><formula xml:id="formula_2">h v l`1 " MLP pconcat ph v l , h N pv l qqq h N pkq pv l q " maxpth u l´h v l |u l P N pkq pv l quq<label>(3)</label></formula><p>Dynamic Graphs. Dynamic GCNs recompute a new graph per layer using the feature space produced by each intermediate representation <ref type="bibr" target="#b33">[34]</ref>. In particular, edges e i,j are recomputed in each layer by taking k closest nodes using a Euclidean distance in the current layer's feature space h. By using dynamic graph updates, the receptive field becomes dynamic. This allows relations to be drawn between far apart nodes, given they have similarities, and thus provides additional contextual information. Dilated GCNs. Convolutional kernel dilations showed improvements in CNNs <ref type="bibr" target="#b40">[41]</ref>, and have recently been extended into graph convolutions as well. In the convolution domain, dilation increases a convolutional filter's receptive field without increasing the number of model parameters. Dilation is achieved by using a larger convolution kernel, but fixing every other kernel coefficient to 0. Similarly, dilation on GCNs is performed by skipping neighbors to increase the receptive field <ref type="bibr" target="#b12">[13]</ref>. In this manner, further neighbors can be reached without increasing the total number of neighbors aggregated in each graph convolution. A dilation of d implies that every d-th neighbor is used for aggregation, which effectively increases the receptive field to dˆk neighbors at a given layer. We use a linearly increasing dilation with respect to the number of layers, i.e. the l-th layer uses a dilation of l. Residual Skip Connections. In a similar manner, residual skip connections have been adopted from CNNs. We apply residual skip connections by adding the previous graph nodes features to the next layer, providing a better gradient flow and faster convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">PointRGCN Vehicle Detection Pipeline</head><p>Our pipeline is composed of 3 main modules: a region proposal network, a graph-based proposal feature extraction network, and a graph-based proposal context aggregation network. The input to our network is a scene point cloud, and the output is a set of refined boxes R <ref type="figure">(Figure 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Point Cloud</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposals</head><p>R-GCN C-GCN Detection <ref type="figure">Figure 2</ref>: PointRGCN: We propose a novel object detection pipeline that introduce Graph Convolution Networks (GCNs) in the refinement module. Proposals: We generate proposals by regressing a bounding box per each foreground vehicle points, similar than PointRCNN <ref type="bibr" target="#b25">[26]</ref>. R-GCN: We improve the per-proposal feature extraction used to classify and regress the 3D object proposals by introducing residual GCNs. C-GCN: We share the features between proposals to embed a contexual consistency between the objects to detect.</p><p>RPN: Proposal Generation. This module takes as input the scene point cloud, and generates a set of proposal bounding boxes R. Proposal boxes b i are defined by a 7dimensional vectors consisting of a center, dimensions, and a rotation along the vertical axis: b i " px, y, z, h, w, l, θq.</p><p>The main requirement for this module is to have a high recall for the set of proposals R in the input scene. In our work, we leverage proposals from <ref type="bibr" target="#b25">[26]</ref>. We do not alter on the proposal generation as the average recall of their method is relatively high already. This proposal framework first learns to segment foreground and background points, and extrapolates a proposal bounding box b i for each foreground point. Both proposals and segmentation features are used further down the pipeline during proposal refinement. Current GCN-based feature extraction methods are memory intensive hence not suitable for feature extraction from large point clouds such as an outdoor scene captured with LI-DAR. As such, we rely on PointNet++ <ref type="bibr" target="#b23">[24]</ref> to extract meaningful per-point features for the segmentation performed in this stage.</p><p>R-GCN: Proposal Feature Extraction. The aim of this network is to take a set of proposal boxes R with a high recall, and extract a set of features from each proposal. These features are later on used to regress better bounding boxes. To achieve this, proposal boxes are first expanded to include extra context from objects surrounding the proposal. LIDAR points lying within each of the expanded proposal boxes are cropped, and a total of P points are kept to form the set of points P i for any proposal i. The isolated proposal point clouds are transformed to a canonical reference frame where the proposal's center lies at the origin, and where the proposal axes are aligned with the canonical frame's axes. For each point, we project its canonical coordinates into a larger space to match the dimensionality of the point's RPN features. We then concatenate the projected canonical coordinates with the corresponding RPN features, and reduce the concatenated vector's dimensionality in order to obtain a per-point feature vector. Our R-GCN module processes these per-point feature vectors using N r layers of MRGCN <ref type="bibr" target="#b13">[14]</ref>. Each layer has a fixed number F r of filters, and residual skip connections are used between consecutive layers. We then create a local multi-layer feature vector for each point by concatenating the output features from every graph convolution layer. For each proposal, we learn a global point feature by projecting the local multi-layer feature into a C r -dimensional space, and performing a max pool across all points. The global point feature is then concatenated to every point in each proposal. Finally, a max pool is performed across all points for each proposal box. This generates an pN rˆFr`Cr q-dimensional output feature vector that captures both global and local information within proposals.</p><p>C-GCN: Context Aggregation. We would like to exploit the fact that most proposals will be related by physical (e.g. ground plane) and road (e.g. lanes) constraints. For this purpose, we leverage the information encapsulated in each proposal features to further refine all proposals. We gather the set of |R| proposal feature outputs from our R-GCN module into a graph representation of the current frame. In particular, each node on the graph represents a proposal with its R-GCN feature representation. We then process the proposal graph with N c layers of EdgeConv <ref type="bibr" target="#b34">[35]</ref>, each with F c filters, and residual skip connections between consecutive layers. We compute a global feature like for R-GCN and concatenate it to each proposal's local feature vector, producing an output of dimension pN cˆFc`Cc q with aggregated proposal information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Detection Prediction</head><p>In the last module, the feature vector generated for each proposal is used to regress a refined detection box along with a classification score for the given proposal. Two sets of fully connected layers are used for this purpose: one for classification, and one for regression. The purpose of classification is to predict whether a given proposal is good enough. Regression is performed in two different manners specified below: binned regression, and residual regression as is done in <ref type="bibr" target="#b25">[26]</ref>. Binned regression is performed for f P tx, z, θu, while residual regression is performed for box features f P ty, h, w, lu.</p><p>Classification. Proposal classification is performed by predicting a confidence score of whether a proposal is good enough to predict a refined box or not. It is assumed that a proposal which has a large IoU with a ground truth bounding box should lead to an even better refined detection. Therefore, a proposal is considered positive if its IoU with the closest ground truth bounding box is larger than a set threshold, and negative if its IoU is lower than a different threshold. Proposals in the gray area between the positive and negative thresholds are not used during training. This classification score is used to determine which detection should be kept during inference.</p><p>Binned Regression Binned regression is done by performing a classification among a set V f of different bins for each box feature f using binned regression. Alongside this bin classification, an intra-bin regression is also performed to allow for finer box prediction. Bin centers v i f for bin indices i P r0, |V f |q are calculated as shown in Equation <ref type="formula" target="#formula_3">(4)</ref>, for a given feature f P tx, z, θu. Here, S f is the search space which is being discretized into bins, δ f is the bin resolution, and v f is the proposal box feature. The bin center with the highest classification score,v f , is used to generate the refined box. Refined box featuresr f are calculated as shown in Equation <ref type="formula" target="#formula_3">(4)</ref>, where y breg f is the network regression output corresponding to the bin with the highest score. The bin size δ f is used to normalize regression outputs in order have a more stable training.</p><formula xml:id="formula_3">v i f " b f`p i`0.5qδ f´Sf r f "v f`δf y breg f<label>(4)</label></formula><p>Residual Regression. Residual regression can be thought of as a binned regression with a single bin except that bin classification is no longer necessary since there is a single bin. Thus, only one regression value y breg f is calculated by the network for each box feature f that is being regressed in this manner. Refined box features are obtained in the same manner as with binned regression, as shown in Equation (4). However, as bins are no longer being calculated,v f and δ f take new meanings with some subtle differences between the case when f P tyu, and the case when f P th, w, lu. When f P tyu,v f " b f , and δ f " 1. This means the single bin center is located at the proposal's y location, and there is no normalization of the calculated regression value. If f P th, w, lu, bothv f and δ f are taken to be the mean of the box feature among all training samples i.e.v f " δ f " t1.53, 1.63, 3.88u, for f P th, w, lu. Therefore, both the single bin's center and size are fixed to the mean of the corresponding box dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Losses</head><p>Our final loss is composed of two main components, proposal classification L cls and proposal regression L reg , which we minimize jointly. The proposal regression loss can be further decomposed into a binned classification loss, and a binned regression loss. The loss equations used for both classification and regression are shown in Equation <ref type="formula" target="#formula_4">(5)</ref>. A binary cross entropy loss is used for both proposal and bin classifications while a smooth L1 loss is used for the regression tasks, i.e. F cls is the binary cross entropy loss, and F reg is the smooth L1 loss. B reg is the subset of proposals with an IoU large enough to be used for regression.</p><formula xml:id="formula_4">L cls " 1 |B| ÿ b F cls p x cls b , Ď cls b q L reg " 1 |B| ÿ bPBreg ÿ f Ptx,z,θu F cls p y bcls b f , Ě bcls b f q 1 |B reg | ÿ bPBreg ÿ f F reg p y breg b f , Ě breg b f q<label>(5)</label></formula><p>Regression Targets We compute the targets for regression as follows. The ground truth bin for a given proposal b and box feature f can be computed as shown in Equation <ref type="formula">(6)</ref>, whereb f is the ground truth bounding box with the highest IoU for proposal b.</p><p>Ď bin Once the ground truth bin has been computed for a given feature, its regression target can be computed as shown in Equation <ref type="formula">(7)</ref>, wherev f is the ground truth bin center. In the cases where there is a single bin, the ground truth bin center is equal to the single bin center i.e.v f "v f . It is worth noting that regression is only performed for the single ground truth bin for each proposal. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we detail the experiments performed to validate our pipeline. We first present the experimental setup, including the dataset, network, and training details. Afterwards, we show our main results on KITTI testing set and compare our pipeline with other state-of-the-art in both 3D object detection and BEV object detection. Finally, we present ablation studies performed on different model components to validate our pipeline design choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental setup</head><p>Dataset. We perform our experiments on the KITTI dataset <ref type="bibr" target="#b6">[7]</ref> for 3D object detection, where detections are divided into 3 difficulty levels: easy, moderate, and hard, based on occlusion and distance with respect to the vehicle's camera. There are three major labeled object classes: cars, pedestrians, and cyclists. We report results only on the car category since it has the highest number of training samples. Note that the same pipeline is applicable to other categories without further modifications. The publicly available training set is divided into training and validation subset with 3712 and 3769 samples each as common practice, and all results reported are taken from the validation subset except for the final testing results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R-GCN.</head><p>We refine proposals by considering their canonical representation, centered on the bounding box reference system. Proposal bounding boxes are extended by 1 meter in every direction before being cropped, and 512 points are randomly sampled for each proposal to allow for batch training. We use N r " 5 residual MRGCN <ref type="bibr" target="#b13">[14]</ref> layers with F r " 64 filters each and a linearly increasing dilation. We look for k " 16 neighbors per node and use C r " 1024 for the global feature size. We use an aggregation of both canonical point coordinates and RPN features as input for each node. We report ablation studies on the type of convolution layer (i.e. EdgeConv <ref type="bibr" target="#b34">[35]</ref> vs MRGCN <ref type="bibr" target="#b13">[14]</ref>), network depth, the use of residual skip connections, the use of dilation and the importance of input RPN features in <ref type="table" target="#tab_2">Table 3</ref>.</p><p>C-GCN. We gather contextual information across proposals in order to detect more consistent and coherent bounding boxes. For each proposal, we maxpool the R-GCN point features to define each node features. We define the edges of the graph by dynamically looking for the closest k " 16 neighbors in the feature space. We use N c " 3 residual EdgeConv <ref type="bibr" target="#b34">[35]</ref> layers with F c " 64 filters without any dilation. We compute a global feature of size C c " 1024. In our complete PointRGCN pipeline, we use the features from R-GCN, but also experiment with the the RCNN module introduced in PointRCNN <ref type="bibr" target="#b25">[26]</ref>, to highlight the contribution of our sole additional C-GCN. We experiment with the type of layer (EdgeConv <ref type="bibr" target="#b34">[35]</ref> vs MRGCN <ref type="bibr" target="#b13">[14]</ref>), the depth of the network, the residual skip connections, dilation and additional RPN features aggregation. We report these ablation studies in <ref type="table" target="#tab_3">Table 4</ref>.</p><p>Bin Parameters. For location coordinates f P x, z, we set the search space S f " 1.5m, and the bin resolution δ f " 0.5m. This leads to a total of 6 bins for f P x, z. In the case of f " θ, we use a search space S f " 22.5 0 , and bin resolution δ f " 5 0 , for a total of 9 bins.</p><p>Training Details. We use the offline training scheme from <ref type="bibr" target="#b25">[26]</ref> where RPN proposals and features are pre-computed and saved. Data augmentation is performed by artificially adding vehicles to the scene along with random global rotations and translations. During training, 300 RPN proposals are saved for each frame, but we only consider 64 proposals for the losses. Proposals are considered positive if they have an IoU greater than 0.6, and negative if they have an IoU lower than 0.45. Only proposals with an IoU of at least 0.55 are considered for the regression loss. The network is trained for 50 epochs on KITTI train only, using Adam optimizer, with a batch size of 256 proposals (i.e. 4 frames), and a learning rate of 0.0002. We run all our training on v100 GPUs with 32GB of memory, and our validation and testing on GTX1080Ti GPUs with 12GB of memory. <ref type="table" target="#tab_0">Table 1</ref> reports the average precision at an IoU of 0.7 for 3D and BEV object detection on KITTI test. We obtain results that are comparable against other published performances. PointRGCN is on average 2 nd , after the recently proposed method STD <ref type="bibr" target="#b39">[40]</ref>. We consistently outperform our baseline PointRCNN <ref type="bibr" target="#b25">[26]</ref>, and achieve state-of-the-art on the easy difficulty on BEV detection task with an impressive 2.13% improvement. We believe the fine-grained information gathered by the graph representation helps in localizing vehicles in the BEV space, but struggles to improve further the height parameters (i.e. r y , r h ) with respect to PointRCNN <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Main Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>We compare our method with the state-of-the-art on KITTI validation and provide extensive ablation studies for both our R-GCN and C-GCN modules. In order to demonstrate the effectiveness of our R-GCN and C-GCN modules, we compare each of our modules against PointRCNN <ref type="bibr" target="#b25">[26]</ref> in <ref type="table" target="#tab_1">Table 2</ref>. An improvement in performance is obtained by using R-GCN over RCNN on KITTI validation in hard settings. By combining both modules we obtain a performance increase in the easy and moderate categories in KITTI validation. Additionally, our full pipeline generalize better than PointRCNN <ref type="bibr" target="#b25">[26]</ref> as shown from the overall increase in performance on KITTI test. This aligns with the better generalization capability of current GCN <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Further Ablation on R-GCN</head><p>We further investigate the contribution of R-GCN in Table 3, by ablating it using different depths, layer types, residual skip connections, dilations, and input features.</p><p>Depth. We test our R-GCN module using 1, 3, 5 and 10 layers. As shown in <ref type="table" target="#tab_2">Table 3</ref>, the R-GCN module is fairly  insensitive across different network depths. The best results are achieved from 3 to 5 layers.</p><p>Network Design. We show that dilation and residual connections are paramount. Without residual skip connections, the performance drops 5.36%. Li et al. <ref type="bibr" target="#b13">[14]</ref> showed that residual skip connection allows the gradient to flow better between layers, improving the smoothness of the GCN convergence. Without dilation, the performance drops 5.14% on the moderate validation subset. Dilation provides the necessary receptive field for our GCN to gather enough fea-ture information. On the other hand, MRGCN <ref type="bibr" target="#b13">[14]</ref> and EdgeConv <ref type="bibr" target="#b34">[35]</ref> perform fairly similarly, with the biggest difference being in memory and speed requirements. We have measured EdgeConv to be approximately 40ms slower than MRGCN when using a 5-layer network. Additionally, the 5-layer network using MRGCN only consumes a maximum of 1840MB of memory while EdgeConv consumes a maximumum of 2165MB of memory.</p><p>Input data. We exhibit a drop of 4.16% on the moderate subset by using only point coordinates as features for the  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Further Ablation on C-GCN</head><p>We further investigate the contribution of C-GCN in Table 4, by ablating it using different depths, layer types, residual skip connections, dilation, and input features.</p><p>Depth. We tested our C-GCN modules with up to 40 layers. Since the number of nodes is lower than for the R-GCN, adding layers increases the inference time without providing improved performances. We have thus chosen to use a total of 3 layers for the C-GCN network in our pipeline.</p><p>Network Design. We show on <ref type="table" target="#tab_3">Table 4</ref> that MRGCN <ref type="bibr" target="#b13">[14]</ref> performs slightly less than EdgeConv <ref type="bibr" target="#b34">[35]</ref>, as expected, since MRGCN is a simplification over EdgeConv. Like for our R-GCN, residual skip connection are necessary to make the GCN converge smoothly, accounting for 3.84% in the moderate difficulty. Using dilation slightly improves performances on the hard difficulty, but prefer not using dilation since dilation may overflow neighbors for deeper networks. Using a global feature derived from the RPN point features does not provide further improvements. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Qualitative Results</head><p>We show qualitative results of our detection pipeline in <ref type="figure" target="#fig_2">Figure 3</ref>. It can be observed that in some cases our method is able to detect vehicles which are not labeled in the dataset due to their partial visibility, as in <ref type="figure" target="#fig_2">Figure 3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Current advances in graph convolutional networks have led to better performances in varied 3D computer vision tasks. This has motivated us to leverage GCNs for the task of 3D vehicle detection, and demonstrate their effectiveness for vehicle detection. We introduced two novel GCN-based modules for point feature and context aggregation both within and between proposals. Making use of both modules, we presented a novel pipeline PointRGCN: a two-stage 3D vehicle detection pipeline that introduces graph representations for proposal boxes refinement. We showed comparable results with recent works, and report a new state-of-the-art on the easy subset in BEV settings for the KITTI dataset. Still, much work remains to be done in the field of GCNs, from which this work and others stemming from it will benefit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Supplementary Material.</head><p>We present in <ref type="figure">Figure 4</ref> the Precision-Recall curves from the KITTI leaderboard, established on the testing set. In particular, we compare the results of PointRCNN, our R-GCN module alone, and our PointRGCN pipeline (from left to right). We refer to the 3D@0.7, BEV@0.7, 2D@0.7 and AOS metrics (from top to bottom). Note how our model improves the 2D@0.7 and AOS metrics on the Easy diffi-culty setting, reaching a recall of 100%. This shows that we are able to find all easy vehicle samples in the 2D RGB frame with a precision of 60%. Such performances were previously unseen with PointRCNN.</p><p>To enable further comparison, we provide the ablation study of our method on BEV, 2D Bounding Boxes and AOS. We complement the ablation study on R-GCN from <ref type="table" target="#tab_2">Table 3</ref> in <ref type="table" target="#tab_4">Table 5</ref> and on C-GCN from <ref type="table" target="#tab_3">Table 4</ref> in <ref type="table" target="#tab_5">Table 6</ref>.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>targets are the one-hot encoded vector of ground truth bins Ď bin b f amongst all }V { } possible bins, i.e. Ě bcls b f " onehotp Ď bin b f q.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Qualitative Results: Top: Ground truth, PointRCNN [26] and PointRGCN [ours] detection results on BEV projections. Bottom: We project the detections from PointRGCN [ours] onto the RGB image plane for visualization. (a)showcases where our pipeline is able to detect unlabeled vehicles from the dataset, (b) showcases where our pipeline is able to avoid false positives compared with PointRCNN, and (c) shows failure cases for our pipeline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a). Additionally, we show an improvement over PointRCNN in cases such as Figure 3(b), where false positives are avoided due to our C-GCN. Failure cases can be observed in images Figure 3(c), where false positives occur due to large occlusion of vehicles in heavy traffic and due to vegetation clutter at large distances. Overall, we are able to provide tight bounding boxes with high precision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Main Results on KITTI testing set. We report metrics published in papers. The first 4 methods leverage LiDAR and RGB information, while the next 7 LiDAR only. For each columns, we highlight the first, second and third best published method using LiDAR only. Our method performs best on the easy difficulty for AP BEV .</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>3D @ 0.7 IoU</cell><cell></cell><cell>BEV @ 0.7 IoU</cell><cell>Time</cell></row><row><cell>Method</cell><cell>Modality</cell><cell cols="3">Easy Mode. Hard</cell><cell>Easy Mode. Hard</cell><cell>(ms)</cell></row><row><cell>MV3D [5]</cell><cell>L+I</cell><cell cols="4">66.77 52.73 51.31 85.82 77.00 68.94</cell><cell>240</cell></row><row><cell>AVOD [10]</cell><cell>L+I</cell><cell cols="4">73.59 65.78 58.38 86.80 85.44 77.73</cell><cell>100</cell></row><row><cell>AVOD-FPN [10]</cell><cell>L+I</cell><cell cols="4">81.94 71.88 66.38 88.53 83.79 77.90</cell><cell>100</cell></row><row><cell>F-PointNet [21]</cell><cell>L+I</cell><cell cols="4">81.20 70.39 62.19 88.70 84.00 75.33</cell><cell>170</cell></row><row><cell>UberATG-MMF [16]</cell><cell>L+I</cell><cell cols="4">86.81 76.75 68.41 89.49 87.47 79.10</cell><cell>80</cell></row><row><cell>VoxelNet [42]</cell><cell>L</cell><cell cols="4">77.49 65.11 57.73 89.35 79.26 77.39</cell><cell>220</cell></row><row><cell>PIXOR [39]</cell><cell>L</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>84.44 80.04 74.31</cell><cell>100</cell></row><row><cell>SECOND [38]</cell><cell>L</cell><cell cols="4">83.13 73.66 66.20 88.07 79.37 77.95</cell><cell>50</cell></row><row><cell>PointPillars [11]</cell><cell>L</cell><cell cols="4">79.05 74.99 68.30 88.35 86.10 79.83</cell><cell>16</cell></row><row><cell>PointRCNN [26]</cell><cell>L</cell><cell cols="4">85.94 75.76 68.32 89.47 85.68 79.10</cell><cell>100</cell></row><row><cell>Fast Point R-CNN [6]</cell><cell>L</cell><cell cols="4">84.28 75.73 67.39 88.03 86.10 78.17</cell><cell>65</cell></row><row><cell>STD [40]</cell><cell>L</cell><cell cols="4">86.61 77.63 76.06 89.66 87.76 86.89</cell><cell>80</cell></row><row><cell>R-GCN only (ours)</cell><cell>L</cell><cell cols="4">83.42 75.26 68.73 91.91 86.05 81.05</cell><cell>239</cell></row><row><cell>PointRGCN (ours)</cell><cell>L</cell><cell cols="4">85.97 75.73 70.60 91.63 87.49 80.73</cell><cell>262</cell></row><row><cell>(a)</cell><cell>(b)</cell><cell></cell><cell></cell><cell></cell><cell>(c)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ablation for PointRGCN. We evaluate our modules R-GCN and C-GCN singularly and compared with PointRGCN, on KITTI validation. Best results shown in bold. Time in ms.</figDesc><table><row><cell>Method</cell><cell>Easy Mode. Hard</cell><cell>Time</cell></row><row><cell cols="2">PointRCNN [26] 88.45 77.67 76.30</cell><cell>100</cell></row><row><cell>PointRGCN</cell><cell>88.37 78.54 77.60</cell><cell>262</cell></row><row><cell>R-GCN</cell><cell>88.08 78.45 77.81</cell><cell>239</cell></row><row><cell>[26] + C-GCN</cell><cell>87.56 77.58 75.94</cell><cell>147</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Ablation for R-GCN on KITTI validation set. We validate here the setup of our R-GCN network. Our setup and best results in bold. Time in ms.</figDesc><table><row><cell>R-GCN (3D@0.7)</cell><cell>Easy Mode. Hard</cell><cell>Time</cell></row><row><cell>PointRGCN</cell><cell>88.37 78.54 77.60</cell><cell>262</cell></row><row><cell>R-GCN alone</cell><cell>88.08 78.45 77.81</cell><cell>239</cell></row><row><cell>1 layers</cell><cell>87.29 78.09 77.34</cell><cell>135</cell></row><row><cell>3 layers</cell><cell>87.96 78.48 77.94</cell><cell>188</cell></row><row><cell>5 layers</cell><cell>88.08 78.45 77.81</cell><cell>239</cell></row><row><cell>10 layers</cell><cell>83.33 76.56 75.47</cell><cell>447</cell></row><row><cell>w/ EdgeConv [35]</cell><cell>87.75 78.33 77.68</cell><cell>282</cell></row><row><cell>w/o residual</cell><cell>82.05 73.12 73.04</cell><cell>237</cell></row><row><cell>w/o dilation</cell><cell>82.84 73.34 73.31</cell><cell>232</cell></row><row><cell>w/o RPN feat.</cell><cell>83.18 74.36 72.83</cell><cell>238</cell></row><row><cell cols="3">nodes of the graph. RPN features are an important source</cell></row><row><cell cols="3">of semantic information when processing proposal points</cell></row><row><cell cols="3">since they have been trained for semantic segmentation.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Ablation for C-GCN on KITTI validation set. We validate here the setup of our C-GCN network. Our setup and best results in bold. Time in ms.</figDesc><table><row><cell>C-GCN (3D@0.7)</cell><cell>Easy Mode. Hard</cell><cell>Time</cell></row><row><cell>PointRGCN</cell><cell>88.37 78.54 77.60</cell><cell>262</cell></row><row><cell>C-GCN alone</cell><cell>87.56 77.58 75.94</cell><cell>147</cell></row><row><cell>1 layers</cell><cell>86.11 77.30 75.12</cell><cell>145</cell></row><row><cell>3 layers</cell><cell>87.56 77.58 75.94</cell><cell>147</cell></row><row><cell>5 layers</cell><cell>86.45 76.95 75.88</cell><cell>151</cell></row><row><cell>10 layers</cell><cell>85.67 76.56 75.08</cell><cell>160</cell></row><row><cell>20 layers</cell><cell>82.69 75.45 72.96</cell><cell>173</cell></row><row><cell>30 layers</cell><cell>83.12 74.15 71.98</cell><cell>192</cell></row><row><cell>40 layers</cell><cell>81.44 72.22 67.48</cell><cell>206</cell></row><row><cell>w/ MRGCN [14]</cell><cell>85.02 76.09 73.51</cell><cell>152</cell></row><row><cell>w/o residual</cell><cell>82.84 73.74 73.17</cell><cell>150</cell></row><row><cell>w/ dilation</cell><cell>87.03 77.37 76.06</cell><cell>150</cell></row><row><cell>w/ RPN feat.</cell><cell>82.89 74.05 73.74</cell><cell>150</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Ablation for R-GCN on KITTI validation set. We validate here the setup of our R-GCN network. Our setup and best results in bold. Time in ms. 87.71 85.66 96.19 89.71 88.58 96.18 89.58 88.35 262 R-GCN alone 89.76 87.80 86.11 96.22 89.59 88.94 96.21 89.45 88.73 239 1 layers 89.40 87.59 85.86 90.36 89.38 88.65 90.35 89.23 88.43 135 3 layers 89.67 87.77 86.25 96.67 89.46 88.84 96.66 89.32 88.63 188 5 layers 89.76 87.80 86.11 96.22 89.59 88.94 96.21 89.45 88.73 239 10 layers 89.41 87.57 86.07 90.31 89.19 88.45 90.30 89.05 88.23 447 w/ EdgeConv [35] 89.41 87.69 85.97 96.29 89.61 88.91 96.28 89.44 88.67 282 w/o residual 87.04 84.53 78.03 89.09 87.52 87.11 89.08 87.33 86.82 237 w/o dilation 87.84 84.70 78.31 89.60 87.56 87.11 89.59 87.36 86.81 232 w/o RPN feat. 89.41 87.39 85.51 90.31 88.96 88.07 90.30 88.77 87.83 238</figDesc><table><row><cell></cell><cell>BEV @ 0.7 IoU</cell><cell>2D @ 0.7 IoU</cell><cell>AOS</cell><cell>Time</cell></row><row><cell>R-GCN</cell><cell>Easy Mode. Hard</cell><cell>Easy Mode. Hard</cell><cell>Easy Mode. Hard</cell><cell>(ms)</cell></row><row><cell>PointRGCN</cell><cell>89.68</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Ablation for C-GCN on KITTI validation set. We validate here the setup of our C-GCN network. Our setup and best results in bold. Time in ms. 87.71 85.66 96.19 89.71 88.58 96.18 89.58 88.35 262 C-GCN alone 89.57 86.86 84.90 96.16 89.36 87.67 96.15 89.21 87.42 147 1 layers 89.61 87.35 85.43 90.66 89.48 88.43 90.65 89.35 88.19 145 3 layers 89.57 86.86 84.90 96.16 89.36 87.67 96.15 89.21 87.42 147 5 layers 89.35 86.69 84.90 95.17 89.23 87.60 95.16 89.05 87.33 151 10 layers 89.30 86.33 85.21 90.57 89.25 87.38 90.56 89.12 87.16 160 20 layers 88.95 85.80 84.75 90.64 89.17 86.97 90.63 89.02 86.72 173 30 layers 87.57 85.11 83.95 89.90 88.29 86.21 89.89 88.11 85.91 192 40 layers 89.47 86.47 85.15 90.41 88.64 86.15 90.41 88.48 85.91 206 w/ MRGCN [14] 89.19 86.83 85.21 90.59 89.30 87.35 90.59 89.13 87.08 152 w/o residual 87.29 84.46 78.22 89.39 87.70 87.03 89.38 87.47 86.71 150 w/ dilation 89.32 86.92 85.21 95.72 89.47 87.97 95.71 89.32 87.73 150 w/ RPN feat. 87.81 84.96 78.73 89.75 87.96 87.50 89.75 87.72 87.15 150</figDesc><table><row><cell></cell><cell>BEV @ 0.7 IoU</cell><cell>2D @ 0.7 IoU</cell><cell>AOS</cell><cell>Time</cell></row><row><cell>C-GCN</cell><cell>Easy Mode. Hard</cell><cell>Easy Mode. Hard</cell><cell>Easy Mode. Hard</cell><cell>(ms)</cell></row><row><cell>PointRGCN</cell><cell>89.68</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Traphic: Trajectory prediction in dense and heterogeneous traffic using weighted interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uttaran</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniket</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinesh</forename><surname>Manocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Argoverse: 3d tracking and forecasting with rich maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Fang</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patsorn</forename><surname>Sangkloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jagjeet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slawomir</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Hartnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustav</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2147" to="2156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sanja Fidler, and Raquel Urtasun. 3d object proposals using stereo imagery for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustav</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1259" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1907" to="1915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast point r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Leveraging shape completion for 3d siamese tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Giancola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesus</forename><surname>Zarzar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Joint monocular 3d vehicle detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hou-Ning</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi-Zhi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV), October</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Joint 3d proposal generation and object detection from view aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melissa</forename><surname>Mozifian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungwook</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12697" to="12705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Lehner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Mitterecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hofmarcher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.04093</idno>
		<title level="m">Bernhard Nessler, and Sepp Hochreiter. Patch refinement-localized 3d object detection</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Deepgcns: Making gcns go as deep as cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Mller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guocheng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Itzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdulellah</forename><surname>Delgadillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Abualshour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghanem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Stereo r-cnn based 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">and Raquel Urtasun. Multi-task multi-sensor fusion for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7345" to="7353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">3d bounding box estimation using deep learning and geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsalan</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Kosecka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7074" to="7082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Graphx-convolution for point cloud deformation in 2d-to-3d conversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh-Duc</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonghwa</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woojae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghoon</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep hough voting for 3d object detection in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgbd data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="918" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view cnns for object classification on 3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyuan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5648" to="5656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Octnet: Learning deep 3d representations at high resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gernot</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Osman Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3577" to="3586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="770" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Part-aˆ2 net: 3d part-aware and aggregation neural network for object detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.03670</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sung Woo Park, and Junseok Kwon. 3d point cloud generative adversarial network based on tree structured graph convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong Wook</forename><surname>Shu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Evangelos Kalogerakis, and Erik Learned-Miller. Multi-view convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="945" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Tangent convolutions for dense prediction in 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Yi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3887" to="3896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Voxel-fpn: multiscale voxel feature aggregation in 3d object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayan</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.05286</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Graph attention convolution for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaolin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenman</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyansh</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8445" to="8453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.03304</idno>
		<title level="m">Deep closest point: Learning representations for point cloud registration</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sanjay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">146</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multi-level fusion based 3d object detection from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2345" to="2353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Second: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3337</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pixor: Realtime 3d object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7652" to="7660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Std: Sparse-to-dense 3d object detector for point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zetong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4490" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R-Gcn</forename><surname>Alone</surname></persName>
		</author>
		<title level="m">ours) PointRGCN (ours)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Precision Recall Curves the official KITTI test leaderboard for PointRCNN [26], our R-GCN alone and our PointRGCN</title>
	</analytic>
	<monogr>
		<title level="m">Top to bottom: 3D@0.7, BEV@0.7, 2D@0.7 and AOS</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
	<note>For PointRCNN [26], 3D@0.7 and BEV@0.7 are taken from their paper, but since they do not report 2D@0.7 and AOS we took the best entry on the KITTI leaderboard</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
