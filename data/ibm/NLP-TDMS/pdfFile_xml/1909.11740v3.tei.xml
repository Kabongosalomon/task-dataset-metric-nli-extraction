<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">UNITER: UNiversal Image-TExt Representation Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
							<email>yen-chun.chen@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="department">Microsoft Dynamics 365 AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
							<email>lindsey.li@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="department">Microsoft Dynamics 365 AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
							<email>licheng.yu@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="department">Microsoft Dynamics 365 AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
							<email>ahmed.elkholy@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="department">Microsoft Dynamics 365 AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
							<email>fiahmed@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="department">Microsoft Dynamics 365 AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
							<email>zhe.gan@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="department">Microsoft Dynamics 365 AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
							<email>yu.cheng@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="department">Microsoft Dynamics 365 AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Microsoft Dynamics 365 AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">UNITER: UNiversal Image-TExt Representation Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>including Visual Question Answering</term>
					<term>Image-Text Retrieval</term>
					<term>Referring Expression Comprehension</term>
					<term>Visual Commonsense Reasoning</term>
					<term>Visual Entailment</term>
					<term>and NLVR 2 1</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Joint image-text embedding is the bedrock for most Visionand-Language (V+L) tasks, where multimodality inputs are simultaneously processed for joint visual and textual understanding. In this paper, we introduce UNITER, a UNiversal Image-TExt Representation, learned through large-scale pre-training over four image-text datasets (COCO, Visual Genome, Conceptual Captions, and SBU Captions), which can power heterogeneous downstream V+L tasks with joint multimodal embeddings. We design four pre-training tasks: Masked Language Modeling (MLM), Masked Region Modeling (MRM, with three variants), Image-Text Matching (ITM), and Word-Region Alignment (WRA). Different from previous work that applies joint random masking to both modalities, we use conditional masking on pre-training tasks (i.e., masked language/region modeling is conditioned on full observation of image/text). In addition to ITM for global image-text alignment, we also propose WRA via the use of Optimal Transport (OT) to explicitly encourage finegrained alignment between words and image regions during pre-training. Comprehensive analysis shows that both conditional masking and OTbased WRA contribute to better pre-training. We also conduct a thorough ablation study to find an optimal combination of pre-training tasks. Extensive experiments show that UNITER achieves new state of the art across six V+L tasks (over nine datasets)</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Most Vision-and-Language (V+L) tasks rely on joint multimodel embeddings to bridge the semantic gap between visual and textual clues in images and text, although such representations are usually tailored for specific tasks. For example, MCB <ref type="bibr" target="#b9">[11]</ref>, BAN <ref type="bibr" target="#b17">[19]</ref> and DFAF <ref type="bibr" target="#b11">[13]</ref> proposed advanced multimodal fusion methods for Visual Question Answering (VQA) <ref type="bibr" target="#b2">[3]</ref>. <ref type="bibr">SCAN [23]</ref> and MAttNet <ref type="bibr" target="#b31">[55]</ref> Equal contribution. <ref type="bibr" target="#b0">1</ref> Code is available at https://github.com/ChenRocks/UNITER. studied learning latent alignment between words and image regions for Image-Text Retrieval <ref type="bibr" target="#b26">[50]</ref> and Referring Expression Comprehension <ref type="bibr" target="#b16">[18]</ref>. While each of these models has pushed the state of the art on respective benchmarks, their architectures are diverse and the learned representations are highly task-specific, preventing them from being generalizable to other tasks. This raises a milliondollar question: can we learn a universal image-text representation for all V+L tasks?</p><p>In this spirit, we introduce UNiversal Image-TExt Representation (UNITER), a large-scale pre-trained model for joint multimodal embedding. We adopt Transformer <ref type="bibr" target="#b25">[49]</ref> as the core of our model, to leverage its elegant self-attention mechanism designed for learning contextualized representations. Inspired by BERT <ref type="bibr" target="#b8">[9]</ref>, which has successfully applied Transformer to NLP tasks through large-scale language modeling, we pre-train UNITER through four pretraining tasks: (i) Masked Language Modeling (MLM) conditioned on image; (ii) Masked Region Modeling (MRM) conditioned on text; (iii) Image-Text Matching (ITM); and (iv) Word-Region Alignment (WRA). To further investigate the effectiveness of MRM, we propose three MRM variants: (i) Masked Region Classification (MRC); (ii) Masked Region Feature Regression (MRFR); and (iii) Masked Region Classification with KL-divergence (MRC-kl).</p><p>As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, UNITER first encodes image regions (visual features and bounding box features) and textual words (tokens and positions) into a common embedding space with Image Embedder and Text Embedder. Then, a Transformer module is applied to learn generalizable contextualized embeddings for each region and each word through well-designed pre-training tasks. Compared with previous work on multimodal pre-training <ref type="bibr" target="#b23">[47,</ref><ref type="bibr">29,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr">24,</ref><ref type="bibr">42,</ref><ref type="bibr" target="#b36">60,</ref><ref type="bibr">25]</ref>: (i) our masked language/region modeling is conditioned on full observation of image/text, rather than applying joint random masking to both modalities; (ii) we introduce a novel WRA pre-training task via the use of Optimal Transport (OT) <ref type="bibr">[37,</ref><ref type="bibr" target="#b6">7]</ref> to explicitly encourage fine-grained alignment between words and image regions. Intuitively, OT-based learning aims to optimize for distribution matching via minimizing the cost of transporting one distribution to another. In our context, we aim to minimize the cost of transporting the embeddings from image regions to words in a sentence (and vice versa), thus optimizing towards better cross-modal alignment. We show that both conditional masking and OTbased WRA can successfully ease the misalignment between images and text, leading to better joint embeddings for downstream tasks.</p><p>To demonstrate the generalizable power of UNITER, we evaluate on six V+L tasks across nine datasets, including: (i) VQA; (ii) Visual Commonsense Reasoning (VCR) <ref type="bibr" target="#b34">[58]</ref>; (iii) NLVR 2 <ref type="bibr" target="#b20">[44]</ref>; (iv) Visual Entailment <ref type="bibr" target="#b28">[52]</ref>; (v) Image-Text Retrieval (including zero-shot setting) <ref type="bibr">[23]</ref>; and (vi) Referring Expression Comprehension <ref type="bibr" target="#b32">[56]</ref>. Our UNITER model is trained on a large-scale V+L dataset composed of four subsets: (i) COCO [26]; (ii) Visual Genome (VG) <ref type="bibr" target="#b19">[21]</ref>; (iii) Conceptual Captions (CC) <ref type="bibr">[41]</ref>; and (iv) SBU Captions <ref type="bibr">[32]</ref>. Experiments show that UNITER achieves new state of the art with significant performance boost across all nine downstream datasets. Moreover, training on additional CC and SBU data (containing unseen images/text in downstream tasks) further boosts model performance over training on COCO and VG only.</p><p>Our contributions are summarized as follows: (i) We introduce UNITER, a powerful UNiversal Image-TExt Representation for V+L tasks. (ii) We present Conditional Masking for masked language/region modeling, and propose a novel Optimal-Transport-based Word-Region Alignment task for pre-training. (iii) We achieve new state of the art on a wide range of V+L benchmarks, outperforming existing multimodal pre-training methods by a large margin. We also present extensive experiments and analysis to provide useful insights on the effectiveness of each pre-training task/dataset for multimodal encoder training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Self-supervised learning utilizes original data as its own source of supervision, which has been applied to many Computer Vision tasks, such as image colorization <ref type="bibr" target="#b35">[59]</ref>, solving jigsaw puzzles <ref type="bibr">[31,</ref><ref type="bibr" target="#b24">48]</ref>, inpainting <ref type="bibr">[35]</ref>, rotation prediction <ref type="bibr" target="#b13">[15]</ref>, and relative location prediction <ref type="bibr">[10]</ref>. Recently, pre-trained language models, such as ELMo [36], BERT <ref type="bibr" target="#b8">[9]</ref>, GPT2 [39], XLNet <ref type="bibr" target="#b30">[54]</ref>, RoBERTa <ref type="bibr">[27]</ref> and ALBERT <ref type="bibr">[22]</ref>, have pushed great advances for NLP tasks. There are two keys to their success: effective pre-training tasks over large language corpus, and the use of Transformer <ref type="bibr" target="#b25">[49]</ref> for learning contextualized text representations.</p><p>More recently, there has been a surging interest in self-supervised learning for multimodal tasks, by pre-training on large-scale image/video and text pairs, then finetuning on downstream tasks. For example, VideoBERT <ref type="bibr" target="#b22">[46]</ref> and CBT <ref type="bibr" target="#b21">[45]</ref> applied BERT to learn a joint distribution over video frame features and linguistic tokens from video-text pairs. ViLBERT <ref type="bibr">[29]</ref> and LXMERT <ref type="bibr" target="#b23">[47]</ref> introduced the two-stream architecture, where two Transformers are applied to images and text independently, which is fused by a third Transformer in a later stage. On the other hand, B2T2 <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr">VisualBERT [25]</ref>, Unicoder-VL [24] and VL-BERT <ref type="bibr">[42]</ref> proposed the single-stream architecture, where a single Transformer is applied to both images and text. VLP <ref type="bibr" target="#b36">[60]</ref> applied pre-trained models to both image captioning and VQA. More recently, multi-task learning [30] and adversarial training <ref type="bibr" target="#b10">[12]</ref> were used to further boost the performance. VALUE <ref type="bibr" target="#b5">[6]</ref> developed a set of probing tasks to understand pre-trained models. Our Contributions The key differences between our UNITER model and the other methods are two-fold: (i) UNITER uses conditional masking on MLM and MRM, i.e., masking only one modality while keeping the other untainted; and (ii) a novel Word-Region Alignment pre-training task via the use of Optimal Transport, while in previous work such alignment is only implicitly enforced by task-specific losses. In addition, we examine the best combination of pre-training tasks through a thorough ablation study, and achieve new state of the art on multiple V+L datasets, often outperforming prior work by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">UNiversal Image-TExt Representation</head><p>In this section, we first introduce the model architecture of UNITER (Section 3.1), then describe the designed pre-training tasks and V+L datasets used for pre-training (Section 3.2 and 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Overview</head><p>The model architecture of UNITER is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. Given a pair of image and sentence, UNITER takes the visual regions of the image and textual tokens of the sentence as inputs. We design an Image Embedder and a Text Embedder to extract their respective embeddings. These embeddings are then fed into a multi-layer Transformer to learn a cross-modality contextualized embedding across visual regions and textual tokens. Note that the self-attention mechanism in Transformer is order-less, thus it is necessary to explicitly encode the positions of tokens and the locations of regions as additional inputs.</p><p>Specifically, in Image Embedder, we first use Faster R-CNN 2 to extract the visual features (pooled ROI features) for each region. We also encode the location features for each region via a 7-dimensional vector. <ref type="bibr" target="#b2">3</ref> Both visual and location features are then fed through a fully-connected (FC) layer, to be projected into the same embedding space. The final visual embedding for each region is obtained by summing up the two FC outputs and then passing through a layer normalization (LN) layer. For Text Embedder, we follow BERT <ref type="bibr" target="#b8">[9]</ref> and tokenize the input sentence into WordPieces <ref type="bibr" target="#b27">[51]</ref>. The final representation for each sub-word token 4 is obtained via summing up its word embedding and position embedding, followed by another LN layer. <ref type="bibr" target="#b4">5</ref> We introduce four main tasks to pre-train our model: Masked Language Modeling conditioned on image regions (MLM), Masked Region Modeling conditioned on input text (with three variants) (MRM), Image-Text Matching (ITM), and Word-Region Alignment (WRA). As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, our MRM and MLM are in analogy to BERT, where we randomly mask some words or regions from the input and learn to recover the words or regions as the output of Transformer. Specifically, word masking is realized by replacing the token with a special token [MASK], and region masking is implemented by replacing the visual feature vector with all zeros. Note that each time we only mask one modality while keeping the other modality intact, instead of randomly masking both modalities as used in other pre-training methods. This prevents potential misalignment when a masked region happens to be described by a masked word (detailed in Section 4.2).</p><p>We also learn an instance-level alignment between the whole image and the sentence via ITM. During training, we sample both positive and negative imagesentence pairs and learn their matching scores. Furthermore, in order to provide a more fine-grained alignment between word tokens and image regions, we propose WRA via the use of Optimal Transport, which effectively calculates the minimum cost of transporting the contextualized image embeddings to word embeddings (and vice versa). The inferred transport plan thus serves as a propeller for better cross-modal alignment. Empirically, we show that both conditional masking and WRA contributes to performance improvement (in Section 4.2). To pre-train UNITER with these tasks, we randomly sample one task for each mini-batch, and train on only one objective per SGD update.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pre-training Tasks</head><p>Masked Language Modeling (MLM) We denote the image regions as v = {v 1 , ..., v K }, the input words as w = {w 1 , ..., w T }, and the mask indices as m ∈ N M . <ref type="bibr" target="#b5">6</ref> In MLM, we randomly mask out the input words with probability of 15%, and replace the masked ones w m with special token [MASK]. <ref type="bibr" target="#b6">7</ref> The goal is to predict these masked words based on the observation of their surrounding words w \m and all image regions v, by minimizing the negative log-likelihood: <ref type="bibr" target="#b3">4</ref> We use word/sub-word and token interchangeably throughout the rest of the paper. <ref type="bibr" target="#b4">5</ref> We also use a special modality embedding to help the model distinguish between textual and visual input, which is similar to the 'segment embedding' in BERT. This embedding is also summed before the LN layer in each embedder. For simplicity, this modality embedding is omitted in <ref type="figure" target="#fig_0">Figure 1</ref>. 6 N is the natural numbers, M is the number of masked tokens, and m is the set of masked indices. <ref type="bibr" target="#b6">7</ref> Following BERT, we decompose this 15% into 10% random words, 10% unchanged, and 80% [MASK].</p><formula xml:id="formula_0">L MLM (θ) = −E (w,v)∼D log P θ (w m |w \m , v) ,<label>(1)</label></formula><p>where θ is the trainable parameters. Each pair (w, v) is sampled from the whole training set D. Image-Text Matching (ITM) In ITM, an additional special token [CLS] is fed into our model, which indicates the fused representation of both modalities. The inputs to ITM are a sentence and a set of image regions, and the output is a binary label y ∈ {0, 1}, indicating if the sampled pair is a match. We extract the representation of [CLS] token as the joint representation of the input imagetext pair, then feed it into an FC layer and a sigmoid function to predict a score between 0 and 1. We denote the output score as s θ (w, v). The ITM supervision is over the [CLS] token. <ref type="bibr" target="#b7">8</ref> During training, we sample a positive or negative pair (w, v) from the dataset D at each step. The negative pair is created by replacing the image or text in a paired sample with a randomly-selected one from other samples. We apply the binary cross-entropy loss for optimization:</p><formula xml:id="formula_1">L ITM (θ) = −E (w,v)∼D [y log s θ (w, v) + (1 − y) log(1 − s θ (w, v))]) .<label>(2)</label></formula><p>Word-Region Alignment (WRA) We use Optimal Transport (OT) for WRA, where a transport plan T ∈ R T ×K is learned to optimize the alignment between w and v. OT possesses several idiosyncratic characteristics that make it a good choice for WRA: (i) Self-normalization: all the elements of T sum to 1 [37]. (ii) Sparsity: when solved exactly, OT yields a sparse solution T containing (2r − 1) non-zero elements at most, where r = max(K, T ), leading to a more interpretable and robust alignment <ref type="bibr">[37]</ref>. (iii) Efficiency: compared with conventional linear programming solvers, our solution can be readily obtained using iterative procedures that only require matrix-vector products <ref type="bibr" target="#b29">[53]</ref>, hence readily applicable to large-scale model pre-training. Specifically, (w, v) can be considered as two discrete distributions µ, ν, formulated as µ = T i=1 a i δ wi and ν = K j=1 b j δ vj , with δ wi as the Dirac function centered on w i . The weight vectors a = {a i } T i=1 ∈ ∆ T and b = {b j } K j=1 ∈ ∆ K belong to the T -and K-dimensional simplex, respectively (i.e., T i=1 a i = K j=1 b j = 1), as both µ and ν are probability distributions. The OT distance between µ and ν (thus also the alignment loss for the (w, v) pair) is defined as:</p><formula xml:id="formula_2">L WRA (θ) = D ot (µ, ν) = min T∈Π(a,b) T i=1 K j=1 T ij · c(w i , v j ) ,<label>(3)</label></formula><p>where Π(a, b) = {T ∈ R T ×K + |T1 m = a, T 1 n = b}, 1 n denotes an n-dimensional all-one vector, and c(w i , v j ) is the cost function evaluating the distance between</p><formula xml:id="formula_3">w i and v j . In experiments, the cosine distance c(w i , v j ) = 1 − w i vj ||wi||2||vj ||2 is used.</formula><p>The matrix T is denoted as the transport plan, interpreting the alignment between two modalities. Unfortunately, the exact minimization over T is computational intractable, and we consider the IPOT algorithm <ref type="bibr" target="#b29">[53]</ref> to approximate the OT distance (details are provided in the supplementary file). After solving T, the OT distance serves as the WRA loss that can be used to update the parameters θ. Masked Region Modeling (MRM) Similar to MLM, we also sample image regions and mask their visual features with a probability of 15%. The model is trained to reconstruct the masked regions v m given the remaining regions v \m and all the words w. The visual features of the masked region are replaced by zeros. Unlike textual tokens that are represented as discrete labels, visual features are high-dimensional and continuous, thus cannot be supervised via class likelihood. Instead, we propose three variants for MRM, which share the same objective base:</p><formula xml:id="formula_4">L MRM (θ) = E (w,v)∼D f θ (v m |v \m , w) .<label>(4)</label></formula><p>1) Masked Region Feature Regression (MRFR) MRFR learns to regress the Transformer output of each masked region v (i) m to its visual features. Specifically, we apply an FC layer to convert its Transformer output into a vector</p><formula xml:id="formula_5">h θ (v (i) m ) of same dimension as the input ROI pooled feature r(v (i) m ). Then we ap- ply L2 regression between the two: f θ (v m |v \m , w) = M i=1 h θ (v (i) m ) − r(v (i) m ) 2 2 . 2)</formula><p>Masked Region Classification (MRC) MRC learns to predict the object semantic class for each masked region. We first feed the Transformer output of the masked region v (i) m into an FC layer to predict the scores of K object classes, which further goes through a softmax function to be transformed into a normalized distribution g θ (v (i) m ) ∈ R K . Note that there is no groundtruth label, as the object categories are not provided. Thus, we use the object detection output from Faster R-CNN, and take the detected object category (with the highest confidence score) as the label of the masked region, which will be converted into a one-hot vector c(v</p><formula xml:id="formula_6">(i) m ) ∈ R K . The final objective minimizes the cross-entropy (CE) loss: f θ (v m |v \m , w) = M i=1 CE(c(v (i) m ), g θ (v (i) m )).</formula><p>3) Masked Region Classification with KL-Divergence (MRC-kl) MRC takes the most likely object class from the object detection model as the hard label (w.p. 0 or 1), assuming the detected object class is the groundtruth label for the region. However, this may not be true, as no ground-truth label is available. Thus, in MRC-kl, we avoid this assumption by using soft label as supervision signal, which is the raw output from the detector (i.e., a distribution of object classesc(v (i) m )). MRC-kl aims to distill such knowledge into UNITER as <ref type="bibr" target="#b14">[16]</ref>, by minimizing the KL divergence between two distributions:</p><formula xml:id="formula_7">f θ (v m |v \m , w) = M i=1 D KL (c(v (i) m )||g θ (v (i) m )).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Pre-training Datasets</head><p>We construct our pre-training dataset based on four existing V+L datasets: COCO [26], Visual Genome (VG) <ref type="bibr">[</ref>  makes the model framework more scalable, as additional image-sentence pairs are easy to harvest for further pre-training.</p><formula xml:id="formula_8">(106K) 5.06M (101K) 3.0M (3.0M) 990K (990K) val 25K (5K) 106K (2.1K) 14K (14K) 10K (10K)</formula><p>To study the effects of different datasets on pre-training, we divide the four datasets into two categories. The first one consists of image captioning data from COCO and dense captioning data from VG. We call it "In-domain" data, as most V+L tasks are built on top of these two datasets. To obtain a "fair" data split, we merge the raw training and validation splits from COCO, and exclude all validation and test images that appear in downstream tasks. We also exclude all co-occurring Flickr30K [38] images via URL matching, as both COCO and Flickr30K images were crawled from Flickr and may have overlaps. <ref type="bibr" target="#b8">9</ref> The same rule was applied to Visual Genome as well. In this way, we obtain 5.6M imagetext pairs for training and 131K image-text pairs for our internal validation, which is half the size of the dataset used in LXMERT <ref type="bibr" target="#b23">[47]</ref>, due to the filtering of overlapping images and the use of image-text pairs only. We also use additional Out-of-domain data from Conceptual Captions [41] and SBU Captions [32] for model training. <ref type="bibr">10</ref> The statistics on the cleaned splits are provided in <ref type="table" target="#tab_1">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate UNITER on six V+L tasks 11 by transferring the pre-trained model to each target task and finetuning through end-to-end training. We report experimental results on two model sizes: UNITER-base with 12 layers and UNITERlarge with 24 layers. <ref type="bibr" target="#b10">12</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Downstream Tasks</head><p>In VQA, VCR and NLVR 2 tasks, given an input image (or a pair of images) and a natural language question (or description), the model predicts an answer <ref type="bibr" target="#b8">9</ref> A total of 222 images were eliminated through this process. <ref type="bibr">10</ref> We apply the same URL matching method, excluding 109 images from training. <ref type="bibr" target="#b9">11</ref>   (or judges the correctness of the description) based on the visual content in the image. For Visual Entailment, we evaluate on the SNLI-VE dataset. The goal is to predict whether a given image semantically entails an input sentence. Classification accuracy over three classes ("Entailment", "Neutral" and "Contradiction") is used to measure model performance. For Image-Text Retrieval, we consider two datasets (COCO and Flickr30K) and evaluate the model in two settings: Image Retrieval (IR) and Text Retrieval (TR). Referring Expression (RE) Comprehension requires the model to select the target from a set of image region proposals given the query description. Models are evaluated on both ground-truth objects and detected proposals 13 (MAttNet <ref type="bibr" target="#b31">[55]</ref>). For VQA, VCR, NLVR 2 , Visual Entailment and Image-Text Retrieval, we extract the joint embedding of the input image-text pairs via a multi-layer perceptron (MLP) from the representation of the [CLS] token. For RE Comprehension, we use the MLP to compute the region-wise alignment scores. These MLP layers are learned during the finetuning stage. Specifically, we formulate VQA, VCR, NLVR 2 , Visual Entailment and RE Comprehension as classification problems and minimize the cross-entropy over the ground-truth answers/responses. For Image-Text Retrieval, we formulate it as a ranking problem. During finetuning, we sample three pairs of image and text, one positive pair from the dataset and two negative pairs by randomly replacing its sentence/image with others. We compute the similarity scores (based on the joint embedding) for both positive and negative pairs, and maximize the margin between them through triplet loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation on Pre-training Tasks</head><p>We analyze the effectiveness of different pre-training settings through ablation studies over VQA, NLVR 2 , Flickr30K and RefCOCO+ as representative V+L benchmarks. In addition to standard metrics 14 for each benchmark , we also use Meta-Sum (sum of all the scores across all the benchmarks) as a global metric.</p><p>Firstly, we establish two baselines: Line 1 (L1) in <ref type="table" target="#tab_3">Table 2</ref> indicates no pretraining is involved, and L2 shows the results from MLM initialized with pretrained weights from <ref type="bibr" target="#b8">[9]</ref>. Although MLM trained on text only did not absorb any image information during pre-training, we see a gain of approximately +30 on Meta-Sum over L1. Hence, we use the pre-trained weights in L2 to initialize our model for the following experiments.</p><p>Secondly, we validate the effectiveness of each pre-training task through a thorough ablation study. Comparing L2 and L3, MRFR (L3) achieves better results than MLM (L2) only on NLVR 2 . On the other hand, when pre-trained on ITM (L4) or MLM (L5) only, we observe a significant improvement across all the tasks over L1 and L2 baselines. When combining different pre-training tasks, MLM + ITM (L6) improves over single ITM (L4) or MLM (L5). When MLM, ITM and MRM are jointly trained (L7-L10), we observe consistent performance gain across all the benchmarks. Among the three variants of MRM (L7-L9), we observe that MRC-kl (L9) achieves the best performance (397.09) when combined with MLM + ITM, while MRC (L7) the worst (393.97). When combining MRC-kl and MRFR together with MLM and ITM (L10), we find that they are complimentary to each other, which leads to the second highest Meta-Sum score. The highest Meta-Sum Score is achieved by MLM + ITM + MRC-kl + MRFR + WRA (L11). We observe significant performance improvements from adding WRA, especially on VQA and RefCOCO+. It indicates the fine-grained alignment between words and regions learned through WRA during pre-training benefits the downstream tasks involving region-level recognition or reasoning. We use this optimal pre-training setting for the further experiments.</p><p>Additionally, we validate the contributions of conditional masking through a comparison study. When we perform random masking on both modalities simultaneously during pre-training, i.e., w/o conditional masking (L12), we observe a decrease in Meta-Sum score (396.51) compared to that with conditional masking (399.97). This indicates that the conditional masking strategy enables the model to learn better joint image-text representations effectively.</p><p>Lastly, we study the effects of pre-training datasets. Our experiments so far have been focused on In-domain data. In this study, we pre-train our model on Out-of-domain data (Conceptual Captions + SBU Captions). A performance drop (396.91 in L13) from the model trained on In-domain data (COCO + Visual Genome) (400.93 in L11) shows that although Out-of-domain data contain more images, the model still benefits more from being exposed to similar downstream images during pre-training. We further pre-train our model on both In-domain and Out-of-domain data. With doubled data size, the model continues to improve (405.24 in L14). <ref type="table" target="#tab_6">Table 3</ref> presents the results of UNITER on all downstream tasks. Both our base and large models are pre-trained on In-domain+Out-of-domain datasets, with the optimal pre-training setting: MLM+ITM+MRC-kl+MRFR+WRA. The implementation details of each task are provided in the supplementary file. We compare with both task-specific models and other pre-trained models on each downstream task. SOTA task-specific models include: MCAN <ref type="bibr" target="#b33">[57]</ref> for VQA, MaxEnt <ref type="bibr" target="#b20">[44]</ref>  Results show that our UNITER-large model achieves new state of the art across all the benchmarks. UNITER-base model also outperforms the others by a large margin across all tasks except VQA. Specifically, our UNITER-base model outperforms SOTA by approximately +2.8% for VCR on Q→AR, +2.5% for NLVR 2 , +7% for SNLI-VE, +4% on R@1 for Image-Text Retrieval (+15% for zero-shot setting), and +2% for RE Comprehension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results on Downstream Tasks</head><p>Note that LXMERT pre-trains with downstream VQA (+VG+GQA) data, which may help adapt the model to VQA task. However, when evaluated on unseen tasks such as NLVR 2 , UNITER-base achieves 3% gain over LXMERT.</p><p>In addition, among all the models pre-trained on image-text pairs only, our UNITER-base outperforms the others by &gt;1.5% on VQA.</p><p>It is also worth mentioning that both VilBERT and LXMERT observed two-stream model outperforms single-stream model, while our results show empirically that with our pre-training setting, single-stream model can achieve new state-of-the-art results, with much fewer parameters (UNITER-base: 86M, LXMERT: 183M, VilBERT: 221M). <ref type="bibr" target="#b14">16</ref> For VCR, we propose a two-stage pre-training approach: (i) pre-train on standard pre-training datasets; and then (ii) pre-train on downstream VCR dataset. Interestingly, while VLBERT and B2T2 observed that pre-training is not very helpful on VCR, we find that the second-stage pre-training can significantly boost model performance, while the first-stage pre-training still helps but with limited effects (results shown in <ref type="table" target="#tab_7">Table 4</ref>). This indicates that the proposed twostage approach is highly effective in our pre-trained model over new data that are unseen in pre-training datasets.    <ref type="table">Table 5</ref>: Experiments on three modified settings for NLVR 2 . All models use pre-trained UNITER-base timal performance, as the interactions between paired images are not learned during the pre-training stage. Thus, we experimented with three modified settings on NLVR 2 : (i) Triplet: joint embedding of images pairs and query captions; (ii) Pair : individual embedding of each image and each query caption; and (iii) Pair-biattn: a bidirectional attention is added to the Pair model to learn the interactions between the paired images.</p><p>Comparison results are presented in <ref type="table">Table 5</ref>. The Pair setting achieves better performance than the Triplet setting even without cross-attention between the image pairs. We hypothesize that it is due to the fact that our UNITER is pretrained with image-text pairs. Thus, it is difficult to finetune a pair-based pretrained model on triplet input. The bidirectional attention mechanism in the Pair-biattn setting, however, compensates the lack of cross-attention between images, hence yielding the best performance with a large margin. This show that with minimal surgery on the top layer of UNITER, our pre-trained model can adapt to new tasks that are very different from pre-training tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Visualization</head><p>Similar to <ref type="bibr" target="#b18">[20]</ref>, we observe several patterns in the attention maps of the UNITER model, as shown in <ref type="figure">Fig. 2</ref>. Note that different from <ref type="bibr" target="#b18">[20]</ref>, our attention mechanism operates in both inter-and intra-modalitiy manners. For completeness, we briefly discuss each pattern here:</p><p>-Vertical : attention to special tokens [CLS] or [SEP]; -Diagonal : attention to the token/region itself or preceding/following tokens/regions; -Vertical + Diagonal : mixture of vertical and diagonal; -Block : intra-modality attention, i.e., textual self-attention and visual selfattention; -Heterogeneous: diverse attentions that cannot be categorized and is highly dependent on actual input; -Reversed Block : inter-modality attention, i.e., text-to-image and image-totext attention. Note that Reversed Block <ref type="figure">(Fig. 2f)</ref> shows cross-modality alignment between tokens and regions. In <ref type="figure">Fig. 3</ref>, we visualize several examples of text-to-image attention to demonstrate the local cross-modality alignment between regions and tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we present UNITER, a large-scale pre-trained model providing UNiversal Image-TExt Representations for Vision-and-Language tasks. Four main pre-training tasks are proposed and evaluated through extensive ablation studies. Trained with both in-domain and out-of-domain datasets, UNITER outperforms state-of-the-art models over multiple V+L tasks by a significant margin. Future work includes studying early interaction between raw image pixels and sentence tokens, as well as developing more effective pre-training tasks.</p><p>22. Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., Soricut, R.: Albert: A lite bert for self-supervised learning of language representations. In: ICLR (2020) 3 23. Lee, K.H., Chen, X., Hua, G., Hu, H., He, X.: Stacked cross attention for image-text matching. In: ECCV <ref type="formula" target="#formula_0">(2018)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>This supplementary material has eight sections. Section A.1 describes the details of our dataset collection. Section A.2 describes our implementation details for each downstream task. Section A.3 provides detailed quantitative comparison between conditional masking and joint random masking. Section A.5 provides more results on VCR and NLVR 2 . Section A.6 provides a direct comparison to VLBERT and ViLBERT. Section A.7 provides some background on optimal transport (OT) and the IPOT algorithm that is used to calculate the OT distance. Section A.8 provides additional visualization example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Dataset Collection</head><p>As introduced, our full dataset is composed of four existing V+L datasets: COCO, Visual Genome, Conceptual Captions, and SBU Captions. The dataset collection is not simply combining them, as we need to make sure none of the downstream evaluation images are seen during pre-training. Among them, COCO is the most tricky one to clean, as several downstream tasks are built based on it. <ref type="figure">Figure 4</ref> lists the splits from VQA, Image-Text Retrieval, COCO Captioning, RefCOCO/RefCOCO+/RefCOCOg, and the bottom-up top-down (BUTD) detection <ref type="bibr" target="#b1">[2]</ref>, all from COCO images. As observed, the validation and test splits of different tasks are scattered across the raw COCO splits. Therefore, we exclude all those evaluation images that appeared in the downstream tasks. In addition, we also exclude all cooccurring Flickr30K images via URL matching, making sure the zero-shot imagetext retrieval evaluation on Flickr is fair. The remaining images become the COCO subset within our full dataset, as shown in <ref type="figure">Figure 4</ref> bottom row. We apply the same rules to Visual Genome, Conceptual Captions, and SBU Captions.  Visual Question Answering (VQA) We follow <ref type="bibr" target="#b33">[57]</ref> to take 3129 most frequent answers as answer candidates, and assign a soft target score to each candidate based on its relevancy to the 10 human responses. To finetune on VQA dataset, we use a binary cross-entropy loss to train a multi-label classifier using batch size of 10240 input units over maximum 5K steps. We use AdamW optimizer [28] with a learning rate of 3e − 4 and weight decay of 0.01. At inference time, the max-probable answer is selected as the predicted answer. For results on test-dev and test-std splits, both training and validation sets are used for training, and additional question-answer pairs from Visual Genome are used for data augmentation as in <ref type="bibr" target="#b33">[57]</ref>.</p><p>Visual Commonsense Reasoning (VCR) VCR can be decomposed into two multiple-choice sub-tasks: question-answering task (Q → A) and answerjustification task (QA → R). In the holistic setting (Q → AR), a model needs to first choose an answer from the answer choices, then select a supporting rationale from rationale choices if the chosen answer is correct. We train our model in two settings simultaneously. When testing in the holistic setting, we first apply the model to predict an answer, then obtain the rationale from the same model based on the given question and the predicted answer. To finetune on VCR dataset, we concatenate the question (the qeustion and the ground truth answer) and each answer (rationale) choice from the four possible answer (rationale) candidates. The 'modality embedding' is extended to help distinguish question, answer and rationale. Cross-entropy loss is used to train a classifier over two classes (''right'' or ''wrong'') for each question-answer pair (questionanswer-rationale triplet) with a batch size of 4096 input units over maximum 5K steps. We use AdamW optimizer with a learning rate of 1e − 4 and weight decay of 0.01.</p><p>Since the images and text in VCR dataset are very different from our pretraining dataset, we further pre-train our model on VCR, using MLM, MRFR and MRC-kl as the pre-training tasks. ITM is discarded because the text in VCR does not explicitly describe the image. The results of both pre-trainings on VCR are reported in <ref type="table" target="#tab_7">Table 4</ref> (in the main paper) and discussed in the main text. In conclusion, for downstream tasks that contain new data which is very different from the pre-training datasets, second-stage pre-training helps further boost the performance.</p><p>In our implementation, the second-stage pre-training is implemented with a batch size of 4096 intput units, a learning rate of 3e − 4 and a weight decay of 0.01 over maximum 60K steps. After second-stage pre-traing, we finetune our model with a learning rate of 6e − 5 over maximum 8K steps.</p><p>Natural Language for Visual Reasoning for Real (NLVR 2 ) NLVR2 is a new challenging task for visual reasoning. The goal is to determine whether a natural language statement is true about the given image pair. Here we discuss the three architecture variants of NLVR 2 finetuning in detail. Since UNITER only handles one image and one text input at pre-training, the 'modality embedding' is extended to help distinguish the additional image presented in the NLVR 2 task. For the Triplet setup, we concatenate the image regions and then feed into the UNITER model. An MLP transform is applied on the [CLS] output for binary classification. For the Pair setup, we treat one input example as two text-image pairs by repeating the text. The two [CLS] outputs from UNITER are then depth concatenated as the joint embedding for the example. Another MLP further transform this embedding for the final classification. For the Pairbiattn setup, the input format is the same as the Pair setup. As for the joint representation, instead of rely on only two [CLS] outputs, we apply a multi-head attention layer <ref type="bibr" target="#b25">[49]</ref> on one sequence of joint image-text embeddings to attend to the other sequence of embeddings, and vice versa. After this 'bidirectional' attention interactions, a simple attentional pooling is applied on each output sequences and then a final concat+MLP layer transforms the cross-attended joint representation for true/false classification.</p><p>We finetune UNITER on NLVR 2 for 8K steps with a batch size of 10K input units. AdamW optimizer is used with learning rate of 1e − 4 and weight decay of 0.01.</p><p>Image-Text Retrieval Two datasets are considered for this task: COCO and Flickr30K. COCO consists of 123K images, each accompanied with five humanwritten captions. We follow <ref type="bibr" target="#b15">[17]</ref> to split the data into 82K/5K/5K training/ validation/test images. Additional 30K images from MSCOCO validation set are also included to improve training as in <ref type="bibr">[23]</ref>. Flickr30K dataset contains 31K images collected from the Flickr website, with five textual descriptions per image. We follow <ref type="bibr" target="#b15">[17]</ref> to split the data into 30K/1K/1K training/validation/test splits. During finetuning, we sample two negative image-text pairs per positive sample from image and text sides, respectively. For COCO, we use batch size of 60 examples, learning rate of 2e − 5 and finetune our model for 20K steps. For Flickr30K, we finetune our model with a batch size of 120 examples and a learning rate of 5e − 5 over maximum 16K steps.</p><p>To obtain the final results in <ref type="table" target="#tab_6">Table 3</ref> in the main text, we further sample hard negatives to facilitate the finetuning. For every N steps, we randomly sample 128 negative images per text input and obtain a sparse scoring matrix for the whole training set. For each image, we choose the top 20 ranked negative sentences as hard negative samples. Similarly, we get 20 hard negative images for each sentence according to their scores. The hard negatives are sent to the model as additional negative samples. In the end, we have two randomly sampled negatives and two hard negative samples per positive sample. N is set to 4000 for COCO and 2500 for Flickr30K.</p><p>Visual Entailment (SNLI-VE) Visual Entailment is a task derived from Flickr30K images and Stanford Natural Language Inference (SNLI) dataset, where the goal is to determine the logical relationship between a natural language statement and an image. Similar to BERT for Natural Language Inference (NLI), we treat SNLI-VE as a three-way classification problem and apply an MLP Transform on [CLS] output. The UNITER model is finetuned using cross-entropy loss. The batch size is set to 10K input units and we use AdamW with learning rate of 8e − 5 to train for 3K steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Referring Expression Comprehension</head><p>We use three referring expression datasets: RefCOCO, RefCOCO+, and RefCOCOg for the evaluation, all collected on COCO images. To finetune UNITER on this task, we add a MLP layer on top of the region outputs from Transformer, to compute the alignment score between the query phrase/sentence and each region. Since only one object is paired with the query phrase/sentence, we apply cross-entropy loss on the normalized alignment scores. The finetuning is efficient -we train the model with a batch size of 64 examples and a learning rate of 5e − 5 for only 5 epochs, and achieve state-of-the-art performance.</p><p>Note all works including ours use off-the-shelf object detectors trained on COCO (and Visual Genome) to extract the visual features. While this does not affect other downstream tasks, it raises an issue for RE comprehension, as the val/test images of RefCOCO, RefCOCO+, and RefCOCOg are a subset of COCO's training split. Strictly, our object detector is not allowed to train with these val/test images. However, just for a "fair" comparison with concurrent works, we ignore this issue and use the same features <ref type="bibr" target="#b1">[2]</ref> as the others. We also update the results of MAttNet using this "contaminated" features, whose accuracy is 1.5% higher than the original one. As aforementioned, the interaction between sentence and image could start from tokens and pixels instead of the extracted features. We leave this study and RE comprehension with strictly correct features to future work. a man with his &lt;MASK&gt; and cat sitting on the sofa We further discuss the advantage of our proposed conditional masking over joint random masking used in <ref type="bibr" target="#b23">[47,</ref><ref type="bibr">29]</ref>. Intuitively, our conditional masking learns better latent alignment of entities (regions and words) across two modalities. <ref type="figure" target="#fig_3">Fig. 5</ref> shows an example image with "man with his dog and cat sitting on a sofa". With conditional masking, when the region of dog is masked, our model should be able to infer that the region is dog, based on the context of both surrounding regions and the full sentence ( <ref type="figure" target="#fig_3">Fig. 5(a)</ref>), and vice versa. However, for the joint masking implementation, it could happen when both the region of dog and the word dog are masked ( <ref type="figure" target="#fig_3">Fig. 5(b)</ref>). In such case, the model has to make the prediction blindly, which might lead to mis-alignment. To verify this intuition, we show the validation curves during pre-training of MLM and MRC-kl in <ref type="figure" target="#fig_5">Fig. 6</ref>. Each sub-figure shows a comparison between applying conditional masking and joint random masking during the pre-training of UNITER. The MLM accuracy measures how well UNITER can reconstruct the masked words, and MRC-kl accuracy 21 measures how well UNITER can classify the masked regions. In both cases, as shown in <ref type="figure" target="#fig_5">Fig. 6</ref>, our conditional masking converges faster and achieves higher final accuracy than joint random masking. In addition, <ref type="table" target="#tab_3">Table 2</ref> (row 10 &amp; 11) in the main paper shows our conditional masking also performs better on fine-tuned downstream tasks.   <ref type="table" target="#tab_3">Table 2</ref> in the main paper, we include results from UNITER-base when pre-trained with MRC only on in-domain data. <ref type="table" target="#tab_11">Table 7</ref> shows that MRC-only pre-training leads to a similar downstream performance to MRFR-only prer-training, which is a weak baseline compared with all other pre-training settings with in-domain data (line 4 -12 in <ref type="table" target="#tab_3">Table 2</ref>). <ref type="table" target="#tab_3">Table 2</ref> of the main paper, we show that adding WRA significantly improves model performance on VQA and RefCOCO+, while achieves comparable results on Flickr and NLVR 2 . By design, WRA encourages local alignment between each image region and each word in a sentence. Therefore, WRA mostly benefits downstream tasks relying on region-level recognition and reasoning such as VQA, while Flickr and NLVR 2 focus more on global rather than local alignments. We add additional ablation results for WRA of UNITERlarge when pre-trained with both In-domain and Out-of-domain data in <ref type="table" target="#tab_12">Table 8</ref>. We observe large performance gains in zero-shot setup for image/text retrieval and consistent gains across all other tasks.     Following the VCR setup in <ref type="table" target="#tab_7">Table 4</ref> of the main paper, we further construct an ensemble model using 10 UNITER-large.  . We pre-train UNITER on Conceptual Captions only using our proposed conditional masking and the best pre-training tasks. <ref type="table" target="#tab_1">Table 11</ref> shows that UNITER still consistently outperforms the other models by a visible margin on VQA and RefCOCO+.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Significance of WRA In</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 Review of Optimal Transport and the IPOT Algorithm</head><p>Optimal Transport We first provide a brief review of optimal transport, which defines distances between probability measures on a domain X (the sequence space in our setting). The optimal transport distance for two probability measures µ and ν is defined as [37]:</p><formula xml:id="formula_9">D c (µ, ν) = inf γ∈Π(µ,ν) E (x,y)∼γ [c(x, y)] ,<label>(5)</label></formula><p>where Π(µ, ν) denotes the set of all joint distributions γ(x, y) with marginals µ(x) and ν(y); c(x, y) : X × X → R is the cost function for moving x to y, e.g., the Euclidean or cosine distance. Intuitively, the optimal transport distance is the minimum cost that γ induces in order to transport from µ to ν. When c(x, y) is a metric on X, D c (µ, ν) induces a proper metric on the space of probability distributions supported on X, commonly known as the Wasserstein distance. One of the most popular choices is the 2−Wasserstein distance W 2 2 (µ, ν) where the squared Euclidean distance c(x, y) = x − y 2 is used as cost.</p><p>The IPOT algorithm Unfortunately, the exact minimization over T is in general computational intractable <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">14,</ref><ref type="bibr">40]</ref>. To overcome such intractability, we consider an efficient iterative approach to approximate the OT distance. We propose to use the recently introduced Inexact Proximal point method for Optimal Transport (IPOT) algorithm to compute the OT matrix T * , thus also the OT distance <ref type="bibr" target="#b29">[53]</ref>. Specifically, IPOT iteratively solves the following optimization problem using the proximal point method <ref type="bibr" target="#b4">[5]</ref>:</p><formula xml:id="formula_10">T (t+1) = arg min T∈Π(a,b) T, C + β · B(T, T (t) ) ,<label>(6)</label></formula><p>where the proximity metric term B(T, T (t) ) penalizes solutions that are too distant from the latest approximation, and 1 β is understood as the generalized</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Overview of the proposed UNITER model (best viewed in color), consisting of an Image Embedder, a Text Embedder and a multi-layer Transformer, learned through four pre-training tasks</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :Fig. 3 :</head><label>23</label><figDesc>Visualization of the attention maps learned by the UNITER-base model Text-to-image attention visualization example</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 , 3 ,</head><label>13</label><figDesc>11, 21  24. Li, G., Duan, N., Fang, Y., Jiang, D., Zhou, M.: Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training. In: AAAI (2020) 2, 4, 11 25. Li, L.H., Yatskar, M., Yin, D., Hsieh, C.J., Chang, K.W.: Visualbert: A simple and performant baseline for vision and language. arXiv preprint arXiv:1908.03557 (2019) 2, 4, 11, 24 26. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P.,Zitnick, C.L.: Microsoft coco: Common objects in context. In: ECCV (2014) 3, 7 27. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., Stoyanov, V.: Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 (2019) 3 28. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. In: ICLR (2019) 19 29. Lu, J., Batra, D., Parikh, D., Lee, S.: Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In: NeurIPS (2019) 2, 3, 11, 22, 24, 25 30. Lu, J., Goswami, V., Rohrbach, M., Parikh, D., Lee, S.: 12-in-1: Multi-task vision and language representation learning. In: CVPR (2020) 4 31. Noroozi, M., Favaro, P.: Unsupervised learning of visual representations by solving jigsaw puzzles. In: ECCV (2016) 3 32. Ordonez, V., Kulkarni, G., Berg, T.L.: Im2text: Describing images using 1 million captioned photographs. In: NeurIPS (2011) 3, 7, 8 33. Ott, M., Edunov, S., Grangier, D., Auli, M.: Scaling neural machine translation. WMT (2018) 19 34. Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z., Desmaison, A., Antiga, L., Lerer, A.: Automatic differentiation in pytorch (2017) 19 35. Pathak, D., Krahenbuhl, P., Donahue, J., Darrell, T., Efros, A.A.: Context encoders: Feature learning by inpainting. In: CVPR (2016) 3 36. Peters, M.E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., Zettlemoyer, L.: Deep contextualized word representations. In: NAACL (2018) 3 37. Peyré, G., Cuturi, M., et al.: Computational optimal transport. Foundations and Trends R in Machine Learning 11(5-6), 355-607 (2019) 3, 6, 25 38. Plummer, B.A., Wang, L., Cervantes, C.M., Caicedo, J.C., Hockenmaier, J., Lazebnik, S.: Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In: ICCV (2015) 8 39. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I.: Language models are unsupervised multitask learners (2019) 3 40. Salimans, T., Zhang, H., Radford, A., Metaxas, D.: Improving GANs using optimal transport. In: ICLR (2018) 25 41. Sharma, P., Ding, N., Goodman, S., Soricut, R.: Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In: ACL (2018) 3, 7, 8, 25 42. Su, W., Zhu, X., Cao, Y., Li, B., Lu, L., Wei, F., Dai, J.: Vl-bert: Pre-training of generic visual-linguistic representations. In: ICLR (2020) 2, 4, 11, 24, 25 43. Suhr, A., Artzi, Y.: Nlvr2 visual bias analysis. arXiv preprint arXiv:1909.10411 (2019) 24</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Example showing difference between conditional masking and joint random masking A.3 Conditional Masking vs. Joint Random Masking</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) Validation accuracy of MLM on COCO and VG datasets (b) Validation accuracy of MRC-kl on COCO and VG datasets</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Comparison of MLM and MRC-kl validation accuracy using joint masking and our proposed conditional masking A.4 More Ablation Studies on Pre-training Settings MRC-only Pre-training In addition to ablations shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>21], Conceptual Captions (CC) [41], and SBU Captions [32]. Only image and sentence pairs are used for pre-training, which</figDesc><table><row><cell>In-domain</cell><cell>Out-of-domain</cell></row></table><note>Split COCO Captions VG Dense Captions Conceptual Captions SBU Captions train 533K</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics on the datasets used for pre-training. Each cell shows #image-text pairs (#images)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>VQA, VCR, NLVR 2 , Visual Entailment, Image-Text Retrieval, and Referring Expression Comprehension. Details about the tasks are listed in the supplementary. 12 UNITER-base: L=12, H=768, A=12, Total Parameters=86M. UNITER-large: L=24, H=1024, A=16, Total Parameters=303M (L: number of stacked Transformer blocks; H: hidden activation dimension; A: number of attention heads). 882 and 3645 V100 GPU hours were used for pre-training UNITER-base and UNITER-large. ITM + MRC-kl + MRFR + WRA 405.24 72.70 85.77 94.28 77.18 75.31</figDesc><table><row><cell cols="3">Pre-training Data Pre-training Tasks</cell><cell cols="2">Meta-Sum VQA</cell><cell>IR (Flickr)</cell><cell>TR (Flickr)</cell><cell>NLVR 2 Ref-COCO+</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">test-dev val</cell><cell>val</cell><cell>dev</cell><cell>val d</cell></row><row><cell>None</cell><cell cols="2">1 None</cell><cell>314.34</cell><cell cols="2">67.03 61.74 65.55 51.02</cell><cell>68.73</cell></row><row><cell>Wikipedia + BookCorpus</cell><cell cols="2">2 MLM (text only)</cell><cell>346.24</cell><cell cols="2">69.39 73.92 83.27 50.86</cell><cell>68.80</cell></row><row><cell></cell><cell cols="2">3 MRFR</cell><cell>344.66</cell><cell cols="2">69.02 72.10 82.91 52.16</cell><cell>68.47</cell></row><row><cell></cell><cell cols="2">4 ITM</cell><cell>385.29</cell><cell cols="2">70.04 78.93 89.91 74.08</cell><cell>72.33</cell></row><row><cell></cell><cell cols="2">5 MLM</cell><cell>386.10</cell><cell cols="2">71.29 77.88 89.25 74.79</cell><cell>72.89</cell></row><row><cell>In-domain (COCO+VG)</cell><cell cols="2">6 MLM + ITM 7 MLM + ITM + MRC 8 MLM + ITM + MRFR</cell><cell>393.04 393.97 396.24</cell><cell cols="2">71.55 81.64 91.12 75.98 71.46 81.39 91.45 76.18 71.73 81.76 92.31 76.21</cell><cell>72.75 73.49 74.23</cell></row><row><cell></cell><cell cols="2">9 MLM + ITM + MRC-kl</cell><cell>397.09</cell><cell cols="2">71.63 82.10 92.57 76.28</cell><cell>74.51</cell></row><row><cell></cell><cell cols="2">10 MLM + ITM + MRC-kl + MRFR</cell><cell>399.97</cell><cell cols="2">71.92 83.73 92.87 76.93</cell><cell>74.52</cell></row><row><cell></cell><cell cols="2">11 MLM + ITM + MRC-kl + MRFR + WRA</cell><cell>400.93</cell><cell cols="2">72.47 83.72 93.03 76.91</cell><cell>74.80</cell></row><row><cell></cell><cell>12</cell><cell>MLM + ITM + MRC-kl + MRFR (w/o cond. mask)</cell><cell>396.51</cell><cell cols="2">71.68 82.31 92.08 76.15</cell><cell>74.29</cell></row><row><cell>Out-of-domain (SBU+CC)</cell><cell cols="2">13 MLM + ITM + MRC-kl + MRFR + WRA</cell><cell>396.91</cell><cell cols="2">71.56 84.34 92.57 75.66</cell><cell>72.78</cell></row><row><cell>In-domain + Out-of-domain</cell><cell cols="2">14 MLM +</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Evaluation on pre-training tasks and datasets using VQA, Image-Text Retrieval on Flickr30K, NLVR 2 , and RefCOCO+ as benchmarks. All results are obtained from UNITER-base. Averages of R@1, R@5 and R@10 on Flickr30K for Image Retrieval (IR) and Text Retrieval (TR) are reported. Dark and light grey colors highlight the top and second best results across all the tasks trained with In-domain data</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Results on downstream V+L tasks from UNITER model, compared with task-specific state-of-the-art (SOTA) and previous pre-trained models. ZS: Zero-Shot, IR: Image Retrieval and TR: Text Retrieval Different from other tasks, NLVR 2 takes two images as input. Thus, directly finetuning UNITER pre-trained with image-sentence pairs might not lead to op-</figDesc><table><row><cell cols="4">Stage I Stage II Q→A QA→ R Q → AR</cell></row><row><cell>N</cell><cell>N</cell><cell>72.44 73.71</cell><cell>53.52</cell></row><row><cell>N</cell><cell>Y</cell><cell>73.52 75.34</cell><cell>55.6</cell></row><row><cell>Y</cell><cell>N</cell><cell>72.83 75.25</cell><cell>54.94</cell></row><row><cell>Y</cell><cell>Y</cell><cell cols="2">74.56 77.03 57.76</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Experiments on two-stage pre-</figDesc><table><row><cell>Setting</cell><cell>dev test-P</cell></row><row><cell>Triplet</cell><cell>73.03 73.89</cell></row><row><cell>Pair</cell><cell>75.85 75.80</cell></row><row><cell cols="2">Pair-biattn 77.18 77.85</cell></row><row><cell>training for VCR. Results are from UNITER-</cell><cell></cell></row><row><cell>base on VCR val split. Stage I and Stage</cell><cell></cell></row><row><cell>II denote first-stage and second-stage pre-</cell><cell></cell></row><row><cell>training</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Different data splits from downstream tasks based on COCO images. Our UNITER pre-training avoids seeing any downstream evaluation images</figDesc><table><row><cell>MS COCO (raw) VQA Img-Txt Retrieval RefCOCO(+/g) BUTD UNITER Img Captioning 1 VQA 2 VCR 3 NLVR 2 4 Visual Entailment val test 5 Image-Text Retrieval 6 RE Comprehension Fig. 4: Task</cell><cell>train train train train train train train Datasets VQA VCR NLVR 2 SNLI-VE COCO Flickr30K Flickr30K Image Src. COCO Movie Clips Web Crawled 214K 107K Accuracy val test test val train test val train test val train val train test test train / val #Images #Text Metric 204K 1.1M VQA-score 110K 290K Accuracy Flickr30K 31K 507K Accuracy COCO 92K 460K Recall@1,5,10 32K 160K RefCOCO COCO 20K 142K Accuracy RefCOCO+ 20K 142K RefCOCOg 26K 95K</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Statistics on the datasets of downstream tasks</figDesc><table /><note>A.2 Implementation Details Our models are implemented based on PyTorch 17 [34]. To speed up training, we use Nvidia Apex 18 for mixed precision training. All pre-training experiments are run on Nvidia V100 GPUs (16GB VRAM; PCIe connection). Finetuning experiments are implemented on the same hardware or Titan RTX GPUs (24GB VRAM). To further speed up training, we implement dynamic sequence length to reduce padding and batch examples by number of input units (text tokens + image regions). For large pre-training experiments, we use Horovod 19 + NCCL 20 for multi-node communications (on TCP connections through ethernet) with up to 4 nodes of 4x V100 server. Gradient accumulation [33] is also applied to reduce multi-GPU communication overheads.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Pre-training DataPre-training Tasks Meta-Sum VQA IR</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>(Flickr)</cell><cell>TR (Flickr)</cell><cell>NLVR 2 Ref-COCO+</cell></row><row><cell></cell><cell></cell><cell></cell><cell>test-dev val</cell><cell>val</cell><cell>dev</cell><cell>val d</cell></row><row><cell>In-domain (COCO+VG)</cell><cell>MRC</cell><cell>350.97</cell><cell cols="3">66.23 77.17 84.57 52.31</cell><cell>70.69</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Additional ablation results of MRC-only pre-training for UNITER-base with in-domain data.</figDesc><table><row><cell cols="3">WRA pre-train VQA NLVR 2 SNLI-VE</cell><cell>ZS IR (flickr)</cell><cell>ZS TR (flickr)</cell><cell>Ref-COCO</cell><cell>Ref-COCO+</cell><cell>Ref-COCOg</cell></row><row><cell></cell><cell>test-std test</cell><cell>test</cell><cell>val</cell><cell cols="3">val testB d testB</cell><cell>test</cell></row><row><cell>N</cell><cell>73.40 79.50</cell><cell>78.98</cell><cell cols="4">65.82 77.50 74.17 78.89</cell><cell>87.73</cell></row><row><cell>Y</cell><cell cols="6">74.02 79.98 79.38 68.74 83.60 74.98 79.75 88.47</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>A direct ablation on WRA pre-training task using UNITER-large, all pretrained on both In-domain + Out-of-domain data, with MLM + ITM + MRC-kl + MRFR (+ WRA). For simplicity, only R@1 is reported for ZS IR and ZS TR.</figDesc><table><row><cell>Model</cell><cell cols="3">Q→A QA→ R Q → AR</cell></row><row><cell>VLBERT-large (single)</cell><cell>75.8</cell><cell>78.4</cell><cell>59.7</cell></row><row><cell>ViLBERT (10 ensemble)</cell><cell>76.4</cell><cell>78.0</cell><cell>59.8</cell></row><row><cell>UNITER-large (single)</cell><cell>77.3</cell><cell>80.8</cell><cell>62.8</cell></row><row><cell cols="3">UNITER-large (10 ensemble) 79.8 83.4</cell><cell>66.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>VCR results from VLBERT [42], ViLBERT [29], and UNITER</figDesc><table><row><cell>Model</cell><cell cols="4">Balanced Unbalanced Overall Consistency</cell></row><row><cell>VisualBERT</cell><cell>67.3</cell><cell>68.2</cell><cell>67.3</cell><cell>26.9</cell></row><row><cell>LXMERT</cell><cell>76.6</cell><cell>76.5</cell><cell>76.2</cell><cell>42.1</cell></row><row><cell cols="2">UNITER-large 80.0</cell><cell>81.2</cell><cell>80.4</cell><cell>50.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 10 :</head><label>10</label><figDesc>NLVR 2 results on test-U split from VisualBERT [25], LXMERT [47], and UNITER A.5 More Results on VCR and NLVR2</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 9</head><label>9</label><figDesc>shows the comparison between VLBERT, ViLBERT and UNITER on VCR. The Q → AR accuracy of our ensemble model outperforms ViLBERT [29] ensemble by a large margin of 7.0%. Note even single UNITER-large already outperforms ViLBERT ensemble and VLBERT-large by 3.0%. Besides, we also compare our UNITER-large with LXMERT [47] and Visu-alBERT [25] on an additional testing split of NLVR 2 in Table 10. Our results consistently outperform the previous SOTA on all metrics 22 by a large margin of ∼4.0%. 72.34 78.52 62.61 VLBERT-base 71.16 71.60 77.72 60.99 UNITER-base 71.22 72.49 79.36 63.65</figDesc><table><row><cell>Model</cell><cell>VQA RefCOCO+ (det)</cell></row><row><cell></cell><cell>test-dev val testA testB</cell></row><row><cell>ViLBERT</cell><cell>70.55</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 11 :</head><label>11</label><figDesc>A direct comparison between ViLBERT [29], VLBERT [42], and our UNITER, all trained on Conceptual Captions [41] only A.6 Direct Comparison to VLBERT and ViLBERT To further demonstrate our idea, we conduct a direct comparison to ViLBERT [29] and VLBERT [42], trained on Conceptual Captions [41]</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Our Faster R-CNN was pre-trained on Visual Genome object+attribute data [2]. 3 [x1, y1, x2, y2, w, h, w * h] (normalized top/left/bottom/right coordinates, width, height, and area.)</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">Performing this during pre-training also alleviates the mismatch problem between pre-training and downstream finetuning tasks, since most of the downstream tasks take the representation of the [CLS] token as the joint representation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13">The evaluation splits of RE comprehension using detected proposals are denoted as val d , test d , etc.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14">Details about the metrics are listed in the supplementary.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15">MAttNet results are updated using the same features as the others. More details are provided in the supplementary file.<ref type="bibr" target="#b14">16</ref> The word embedding layer contains excessive rare words, thus excluded from the parameter counts.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17">https://pytorch.org/ 18 https://github.com/NVIDIA/apex 19 https://github.com/horovod/horovod 20 https://github.com/NVIDIA/nccl</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="21">When validating on MRC-kl accuracy, we simply pick the most confident category from the predicted probability and measure its correctness.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="22">The balanced and unbalanced evaluations were introduced in [43].</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>for k = 1, . . . K do // K = 1 in practice 7:</p><p>T (t+1) = diag(δ)Qdiag(σ) 10: end for 11: Return T, C stepsize. This renders a tractable iterative scheme towards the exact OT solution.</p><p>In this work, we employ the generalized KL Bregman divergence B(T,</p><p>ij as the proximity metric. Algorithm 1 describes the implementation details for IPOT.</p><p>Note that the Sinkhorn algorithm <ref type="bibr" target="#b7">[8]</ref> can also be used to compute the OT matrix. Specifically, the Sinkhorn algorithm tries to solve the entropy regularized optimization problem:</p><p>is the entropy regularization term and &gt; 0 is the regularization strength. However, in our experiments, we empirically found that the numerical stability and performance of the Sinkhorn algorithm is quite sensitive to the choice of the hyper-parameter , thus only IPOT is considered in our model training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.8 Additional Visualization</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fusion of detected objects in text for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Reitter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">VQA: Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<title level="m">Convex optimization</title>
		<imprint>
			<publisher>Cambridge university press</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Behind the scene: Revealing the secrets of pre-trained vision-and-language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.07310</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Graph optimal transport for cross-domain alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sinkhorn distances: Lightspeed computation of optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="page">26</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">; C</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL (2019) 2, 3, 4, 10 10. Doersch</title>
		<imprint>
			<publisher>ICCV</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>Unsupervised visual representation learning by context prediction</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Large-scale adversarial training for vision-and-language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06195</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Dynamic fusion with intra-and inter-modality attention flow for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning generative models with sinkhorn divergences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Genevay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Peyré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AISTATS</publisher>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Referitgame: Referring to objects in photographs of natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bilinear attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Revealing the dark secrets of bert</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kovaleva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rumshisky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A corpus for reasoning about natural language grounded in photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Suhr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ACL</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05743</idno>
		<title level="m">Contrastive bidirectional transformer for temporal representation learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Lxmert: Learning cross-modality encoder representations from transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02940</idno>
		<title level="m">Selfie: Self-supervised pretraining for image embedding</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning deep structure-preserving image-text embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<title level="m">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadav</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.06706</idno>
		<title level="m">Visual entailment: A novel task for finegrained image understanding</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A fast proximal point method for Wasserstein distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04307</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Mattnet: Modular attention network for referring expression comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Modeling context in referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Poirson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Deep modular co-attention networks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">From recognition to cognition: Visual commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unified visionlanguage pre-training for image captioning and vqa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
