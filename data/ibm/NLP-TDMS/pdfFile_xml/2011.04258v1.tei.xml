<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improved Soccer Action Spotting using both Audio and Video Streams</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastien</forename><surname>Vanderplaetse</surname></persName>
							<email>bastien.vanderplaetse@umons.ac.be</email>
							<affiliation key="aff0">
								<orgName type="institution">ISIA Lab University of Mons</orgName>
								<address>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St√©phane</forename><surname>Dupont</surname></persName>
							<email>stephane.dupont@umons.ac.be</email>
							<affiliation key="aff1">
								<orgName type="institution">ISIA Lab University of Mons</orgName>
								<address>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improved Soccer Action Spotting using both Audio and Video Streams</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a study on multi-modal (audio and video) action spotting and classification in soccer videos. Action spotting and classification are the tasks that consist in finding the temporal anchors of events in a video and determine which event they are. This is an important application of general activity understanding. Here, we propose an experimental study on combining audio and video information at different stages of deep neural network architectures. We used the SoccerNet benchmark dataset, which contains annotated events for 500 soccer game videos from the Big Five European leagues. Through this work, we evaluated several ways to integrate audio stream into video-only-based architectures. We observed an average absolute improvement of the mean Average Precision (mAP) metric of 7.43% for the action classification task and of 4.19% for the action spotting task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The annual revenue of the global sports market was estimated to $90.9 billions in 2017 <ref type="bibr" target="#b15">[15]</ref>. From this large amount, $28.7 billion came from the European soccer market <ref type="bibr" target="#b16">[16]</ref>, more than half ($15.6 billion) of which was generated by the Big Five European soccer leagues (EPL, Ligue 1, Bundesliga, Serie A and La Liga) <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b18">18]</ref>. The main interest of sports broadcast is entertainment, but sports videos are also used by professionals for strategy analysis, player scouting or statistics generation. These statistics are traditionaly gathered by professional analysts watching a lot of videos and identifying the events occuring within a game. For football, this annotation task takes over 8 hours to provide up to 2000 annotations per game, according to Matteo Campodonico, CEO of Wyscout, a company specialized in soccer analytics <ref type="bibr" target="#b74">[73]</ref>.</p><p>To assist sports annotators in this task, several automated computer vision methods can be devised to address many of the challenges in sports video understanding: field and lines localization <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b37">37]</ref>, ball position <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b64">64]</ref> and camera motion <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b76">75]</ref> tracking, detection of players <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b70">70]</ref>, their moves <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b65">65]</ref>, and pose <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b81">80]</ref> and the team they are playing for <ref type="bibr" target="#b35">[35]</ref>. Detecting key actions in soccer videos remains a difficult challenge since these events within the videos are sparse, making machine learning on massive datasets difficult to achieve. Some work has nevertheless achieved significant results in that direction <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b28">28]</ref>.</p><p>In this paper, we focus on action spotting and classification in soccer videos. This task has been defined as finding the anchors of human-induced soccer events in a video <ref type="bibr" target="#b28">[28]</ref> as well as naming the action categories. Several issues arise when dealing with this task. Important actions often have no clear start and end frames in the video, they are temporally discontinuous (i.e. adjacent frames may have different annotations), and they are rather rare. To improve action spotting performance, we propose to use both audio and video input streams while previous work did only use video. Different audio-visual neural network architectures are compared. Our intuition leads us to believe that some categories of actions trigger particular reactions on the part of the public present in the stadium. For example, when a goal is scored, fans shout out. Similarly, a red card can cause discontent. Audio signals should hence provide useful information in such key cases, for instance to distinguish real scored goals from goal attempts. This is what we will show in the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions. (i)</head><p>We carried out an initial analysis about the possibilities of adding audio as an input in a soccer action spotting and classification context. (ii) Our best approach improved the performance of action classification on SoccerNet <ref type="bibr" target="#b28">[28]</ref> by 7.43% absolute with the addition of audio, compared to the video-only baseline. (iii) We also increased the performance of the action spotting on the same dataset by 4.19% absolute.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Sports Analytics and Related Applications. Computer vision methods have been developed to help understand sport broadcasts, carry out analytics within a game <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b66">66]</ref>, or even assist in broadcast production. Interesting use cases innclude the automatic summarization of games <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b69">69]</ref>, the identification of salient game actions <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b77">76]</ref> or the reporting of commentaries of live game video streams <ref type="bibr" target="#b79">[78]</ref>.</p><p>Early work used camera shot segmentation and classification to summarize games <ref type="bibr" target="#b21">[21]</ref> or focused on identifying video production patterns in order to detect salient actions of the game <ref type="bibr" target="#b55">[55]</ref>. Later, Bayesian networks have been used to detect goals, penalties, corner kicks and cards events <ref type="bibr" target="#b34">[34]</ref> or to summarize games <ref type="bibr" target="#b63">[63]</ref>.</p><p>More recently, deep learning approaches have been applied. Long Short-Term Memory (LSTM) networks <ref type="bibr" target="#b32">[32]</ref> enabled to temporally traverse soccer videos to identify the salient actions by temporally aggregating particular features <ref type="bibr" target="#b68">[68]</ref>. These features can be local descriptors, extracted by a Bag-of-Words (BOW) approach, or global descriptors, extracted by using Convolutional Neural Networks (CNN). Besides features, semantic information, such as player localization <ref type="bibr" target="#b40">[40]</ref>, as well as pixel information <ref type="bibr" target="#b8">[9]</ref>, are also used to train attention models to extract relevant frame features. Besides, a loss function for action spotting was proposed to tackle the issue of unclear action temporal location, by better handling the temporal context around the actions during training <ref type="bibr" target="#b10">[10]</ref>.</p><p>Some of the most recent works propose to identify kicks and goals in soccer games by using automatic multicamera-based systems <ref type="bibr" target="#b82">[81]</ref>. Another work uses logical rules to define complex events in soccer videos in order to perform visual reasoning on these events <ref type="bibr" target="#b39">[39]</ref>. These complex events can be visualized as a succession of different visual observations during the game. For example, a "corner kick" occurs when a player of the defending team hits the ball, which passes over the goal line. This complex event is the succession of simple visual observations: the ball is seen near a flag, a player comes near the position of the ball, this player kicks the ball, and the goal post becomes visible in the scene. The logical rules used to define these complex events are Event Calculus (EC) <ref type="bibr" target="#b60">[60]</ref>, i.e. a logic framework for representing and reasoning about events and their effects. These EC allow to describe a scene with atomic descriptions through First Order Logic (FOL).</p><p>Activity Recognition. Activity recognition is the general problem of detecting and then classifying video segments according to a predefined set of activity or action classes in order to understand the videos. Most methods use temporal segments <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b75">74</ref>] that need to be pruned and classified <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b62">62]</ref>.</p><p>A common way to detect activities is to aggregate and pool these temporal segments, which allows to search for a consensus <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b67">67]</ref>. Naive methods use average or maximum pooling, which require no learning. More complex ones aim to find a structure in a feature set by clustering and pooling these features while improving discrimination. These work use learnable pooling like BOW <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b36">36]</ref>, Fisher Vector <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b51">51]</ref> or VLAD <ref type="bibr" target="#b2">[3]</ref>. Some works improve these techniques by respectively extending them by the incorporation of the following Deep Neural Network (DNN) architectures: NetFV <ref type="bibr" target="#b61">[61]</ref>, SoftDBOW <ref type="bibr" target="#b54">[54]</ref> or NetVLAD <ref type="bibr" target="#b29">[29]</ref>.</p><p>Instead of pooling features, some works try to identify which features might be the more useful given the video context. Some of these approaches represent and harness information in both temporal and/or spatial neighborhoods <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b42">42]</ref>, while other ones focus on attention models <ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b73">72]</ref> as a way to better leverage the surrounding information by learning adaptive confidence scores. For instance, the evidence of objects and scenes within a video can be exploited by a semantic encoder for improving activity detection <ref type="bibr" target="#b31">[31]</ref>. Moreover, coupling recognition with context-gating allows the learnable pooling methods to produce state-of-the-art recognition performance on very large benchmarks <ref type="bibr" target="#b47">[47]</ref>.</p><p>Advanced methods for temporal integration use particular neural network architectures, such as Convolution Neural Network (CNN) <ref type="bibr" target="#b58">[58]</ref> or Recurrent Neural Network (RNN) <ref type="bibr" target="#b52">[52]</ref>. More particularly, LSTM architectures are often chosen for motion-aware sequence learning tasks, which is beneficial for activity recognition <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4]</ref>. Attention models are also harnessed to better integrate spatio-temporal information. Within this category of approaches, recent work uses a 2-models-based attention mechanism <ref type="bibr" target="#b53">[53]</ref>. The first one consists of a spatial-level attention model, which determines the important regions in a frame, and the second one concerns the temporal-level attention, which is used to harness the discriminative frames in a video. Another work proposes a convolutional LSTM network supporting multiple convolutional kernels and layers coupled with an attention-based mechanism <ref type="bibr" target="#b1">[2]</ref>.</p><p>Multimodal approaches. Using several different and complementary input modalities can improve model performance in both action classification and action spotting tasks, since this leverages more information about the video. Earlier work uses textual sources <ref type="bibr" target="#b50">[50]</ref>, such as the game logs manually encoded by operators.</p><p>Recently, research in multimodal models use, in addition to the RGB video streams, information about the motion within the video sequences: the optical flow can be used <ref type="bibr" target="#b78">[77,</ref><ref type="bibr" target="#b80">79]</ref> or even player pose sequences <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b72">71]</ref>. For golf and tennis tournaments, a multimodal architecture using the reactions (such as high fives or fist pumps) and expressions of the players (aggressive, smiling, etc.), spectators (crowd cheering) and commentator (tone and word analysis), and even game analytics, was proposed <ref type="bibr" target="#b46">[46]</ref>. Some work use the audio stream of the video but in a different manner than ours. The audio stream was used to make audio-visual classification of sport types <ref type="bibr" target="#b25">[25]</ref>. Also, acoustic information was used to detect tennis events and track time boundaries of each tennis point in a match <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>Our main objective is to set-up multimodal architectures and analyze the benefit of the audio stream on the performance of a model within the soccer action spotting and classification tasks. Action spotting is the task that consists in finding the right temporal anchors of events in a video. The more a candidate spot is close to the target event, the more the spotting is considered as good. Reaching perfect spotting is hence particularly complex. Tolerance intervals are hence typically used.Regarding classification, we will use a typology of different soccer actions classes and evaluate how well our systems distinguish those classes.</p><p>We use the SoccerNet dataset <ref type="bibr" target="#b28">[28]</ref>. It uses a typology of 3 soccer event categories: goals, substitutions and cards (both yellow and red cards).</p><p>This section starts by explaining how the video and the audio streams are represented with feature vectors to be used as input of the different models. Next, it presents the baseline approach proposed in <ref type="bibr" target="#b28">[28]</ref>. This approach consists of training models for soccer action classification but to include a background class too, so that both classification and action spotting tasks can be addressed.Since the baseline <ref type="bibr" target="#b28">[28]</ref> uses only video stream, we finish this section by exposing how we use the audio stream too, with different variants for multimodal fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Video and Audio Representations</head><p>We want to work with both video and audio streams. The volume of data available for training may however be insufficient for training fully end-to-end machine learning architectures. Hence, we will here reuse existing visual and auditory feature extraction architectures, pre-trained on relevant visual and audio reference datasets. As explained in more details later, we used a ResNet <ref type="bibr" target="#b30">[30]</ref> trained on the ImageNet <ref type="bibr" target="#b19">[19]</ref> data for the visual stream and, for the audio stream, a VGG <ref type="bibr" target="#b59">[59]</ref> trained on spectrogram representations of the AudioSet <ref type="bibr" target="#b27">[27]</ref> data. Fine-tuning of these models might be considered in the future, but at this stage, we keep their parameters fixed during our training process. In practice, we hence extracted visual and auditory features before running our experiments.</p><p>Video streams. For the video streams, we used the features extracted by <ref type="bibr" target="#b28">[28]</ref> using ResNet-152 <ref type="bibr" target="#b30">[30]</ref>, a deep convolutional neural network, pretrained on 1000 categories ImageNet <ref type="bibr" target="#b19">[19]</ref> dataset. Particularly, they used the output of the fc1000 layer, which is a 1,000-way fully-connected layer with a softmax function in the end. This layer outputs a 2,048-dimensional feature vector representation for each frame of the video. To extract these features, each video was unified at 25 frames per second (fps) and trimmed at the start of the game, since the reference time for the event annotations is the game start. Each frame was resized and cropped to a 224 √ó 224 resolution. A TensorFlow <ref type="bibr" target="#b0">[1]</ref> implementation was used to extract the features of the videos every 0.5 second, hence a 2 frames per second sampling rate. Then, Principal Component Analysis (PCA) was applied on the extracted features to reduce their dimension to 512. This still retain 93.9% of their variance 1 Audio streams. For the audio streams, we used VGG <ref type="bibr" target="#b59">[59]</ref>, a deep convolutional network architecture. Particularly, we used VGGish, a VGG architecture pretrained on AudioSet <ref type="bibr" target="#b27">[27]</ref>. AudioSet is a benchmark dataset containing 632 audio event categories and 1,789,621 labeled 10seconds audio segments from YouTube videos. To extract audio features, we used a TensorFlow implementation of a pretrained slim version of VGGish 2 . We extracted the output of the last convolutional layer (conv4/conv4 2) to which we applied a global average pooling to get 512-dimensional feature vector. Since this model uses a chunk of the audio stream spectrogram as input and we want to have the same frame rate as the video features, we trimmed the audio streams at the game start and divided them into chunks of 0.5 second.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">SoccerNet baseline approach</head><p>The baseline approach proposed in <ref type="bibr" target="#b28">[28]</ref> is divided into two parts: (i) video chunk classification and (ii) action spotting. In order to compare the performance with and without the audio stream, we followed the same approach and used the best performing models as baselines for our approach.</p><p>Video chunk classification. For the classification task, shallow pooling neural networks are used. Each video is chunked into windows of duration T seconds. Since the features are sampled at 2 frames per second, the input matrix to our systems is, for each chunk to be classified, an aggregation of W = 2T feature vectors. Therefore, the dimension of the input is W √ó 512. Although quite rare, some chunks may have multiple labels when several actions are temporally close-by. Our deep learning architectures will hence use a sigmoid activation function at their last layer. For all the classes, a multi binary cross-entropy loss is minimized. The Adam optimizer is used. The learning rate follows a step decay and early stopping is applied, based on the validation set performance. The final evaluation metric used is the mAP accross the classes defined on SoccerNet <ref type="bibr" target="#b28">[28]</ref>.</p><p>One of the main challenges in designing the neural network architectures for this task was related to the temporal pooling method to be used. Indeed, the selected feature extraction approaches use fixed image based models, while we want to use chunks consisting of several video frames as input to provide the system with longer term information that should be beneficial (or even necessary) to achieve useful performance.</p><p>Seven pooling techniques have been tested by <ref type="bibr" target="#b28">[28]</ref>: (i) mean pooling and (ii) max pooling along the aggregation axis outputting 512-long features, (iii) a custom CNN with kernel dimension of 512 √ó 20 traversing the temporal dimension. At last, the approaches and implementations proposed by <ref type="bibr" target="#b47">[47]</ref> such as (iv) SoftDBOW, (v) NetFV, (vi) NetVLAD and (vii) NetRVLAD have also been compared. These last pooling methods use clustering-based aggregation techniques to harness context-gating, which is a learnable non-linear unit aiming to model the interdependencies among the network activations. To predict the labels for the input window, a fully connected layer was then stacked after the pooling layer of each model. Dropout is also used during training in order to improve generalization. We use a keep probability of 60%.</p><p>Action spotting. For the spotting task, <ref type="bibr" target="#b28">[28]</ref> reused their best performing model from the classification task. This model is applied on each testing video. In this case, instead of splitting video into consecutive chunks, a sliding window of size W is used through the videos, with a stride of 1 second. Therefore, for each position of the sliding window, a W √ó 512 matrix can be obtained by the content currently covered by this window. This matrix is used as input for the model, which computes a probability vector for classifying the video chunk. Then, we get for each game a series of predictions consisting of probabilities to belong to each class (including the background no-action class).</p><p>To obtain the final spotting candidates from the predictions series, three methods were used in <ref type="bibr" target="#b28">[28]</ref>: (i) a watershed method using the center time within computed segment proposals; (ii) the time index of the maximum value of the watershed segment as the candidate; and (iii) the local maxima along the video and applying non-maximumsuppression (NMS) within the window. Here, a tolerance Œ¥ is added to the mAP as evaluation metric. Therefore, a candidate spot is defined as positive if it lands within a tolerance of Œ¥ seconds around the true temporal anchor of an event. Another metric is the Average-mAP, which is the area under the mAP curve with Œ¥ ranging from 5 to 60 seconds.</p><p>In this paper, we use NetVLAD and NetRVLAD as pooling layers, since they are the best approaches in the chosen baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Audio Input and Multimodal Fusion</head><p>We need to define architectures of audio-visual models in order to study the influence of the audio stream on performance. We decided to use the baseline architecture described in Section 3.2 for both visual and audio streams, the only difference being the feature extraction front-end (cfr. 3.1).</p><p>Next, we need to determine where the two models need to merge their pipeline in order to get better results. Both visual and audio processing pipelines can be applied until their last layers, with their outputs then being combined with a late fusion mechanism. Earlier fusion points will also be investigated. The general appearance of our multi-modal pipeline is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. We hence train our models with different merge points, illustrated by green circles and arrows on the figure. At the merge points, the audio stream and video stream representation vectors are concatenated, and the concatenated vectors are then processed by a single pipeline consisting of the remaining part of the baseline processing pipeline downstream the fusion point. We distinguish 5 merge points and 7 methods.</p><p>The first two methods are the only ones to be applied after having trained both models. The first method multiplies the probabilities estimated for each class, while the second one averages the logits, followed by the sigmoid activation function, applied on the resulting logits vectors. The third method applies the same process then method one, but the two parallel models are trained with a loss computed from the output of the common sigmoid function, instead of using pre-trained models. Methods 4 and 5 have their merge point respectively before the fully-connected layer, and before the dropout layer of the model. Merging before and after the dropout can lead to different results. Indeed, if we merge after the dropout, we will keep for each training sample the same proportion of activations from both flows, while merging before the dropout does not ensure that the information from both streams is kept fairly. Methods 6 and 7 both merge the representation vectors before the pooling layer. The difference between these methods lies in the size of the pooling layer output (1,024 for method 5 and 512 for method 6).</p><p>Every method, except the two first ones, requires a training since there are learnable parameters in both the pooling process and in the fully-connected layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">SoccerNet dataset</head><p>For our experiments, we use the same dataset as our reference baseline: SoccerNet <ref type="bibr" target="#b28">[28]</ref>. This dataset contains videos for 500 soccer games from the Big Five European leagues (EPL, La Liga, Ligue 1, Bundesliga and Serie A): 300 as training set, 100 as validation set and 100 as testing set. There are 6,637 events referenced for these games, split into 3 classes: "goals", the instant the ball crosses the goal line to enter the net; "cards", the instant a yellow or a red card is shown by the referee; and "substitutions", the instant a new player enters in the field to replace another one. Each one of these events is annotated by the exact second it occurs in the game. For the classification task, a fourth class was added: "background", which corresponds to the absence of the three events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Video chunk classification</head><p>We train models with our 7 fusion methods. In the baseline, the best model uses NetVLAD as pooling layer, with a number of clusters of k = 512. However, such a large number of clusters incurs a larger computational load, which increases linearly with the value of k. Therefore, we first compare our merging methods with a smaller number of clusters: k = 64. According to <ref type="bibr" target="#b28">[28]</ref>, the best pooling method with a number of cluster k = 64 is NetRVLAD. Therefore, we try our merging methods on models having a 64-clusters NetRVLAD as pooling layer. Our results are presented on <ref type="table" target="#tab_0">Table 1</ref>. The video baseline result is obtained by executing the code provided by Giancola et a., and the audio baseline uses the same code, but using the audio stream as input. We also compared the performance for chunks of size 60 seconds or else 30 seconds.</p><p>We observe that a chunk of 60 seconds provides better results. This can be explained by the fact that, with 30seconds video chunks, the "background" class represent 93% of the training data samples, whereas for a 60-seconds window, it represents 87%. Since there are more samples in the "background" class, the 30-seconds models tend to classify more samples with this label, which reduces performance on other classes.</p><p>Regarding multimodal fusion, we can see that using only the audio stream provides inferior results than the video-only model. On the other hand, all our methods to combine video and audio streams improve over the performance of mono-modal systems. The best performance is obtained by the fourth merging method, which correspond to the merge point localized before the last fully connected classification layer.</p><p>In <ref type="table">Table 2</ref>, we compare the mAP for each class for the video baseline, the audio baseline and our best fusion method. We can observe that including audio improves the performance on each category, especially for the "goals" event, where the relative reduction of the error rate exceeds 50%. Moreover, if the audio baseline generally performs worse than the video baseline, this is not the case for the "goals" class, where audio alone yields better results than video alone. This corroborates our intuitions exposed in Section 1. Indeed, a scored goal, which clearly leads to a strong emotional reaction from the public as well as the commentators, is easier to detect through the audio stream than the video stream, where it could for instance be confused with shots on target. However, the audio stream does not seem to provide sufficient information to efficiently detect cards, leading to a poor result for this category. Finally, audio carries information about the substitutions. This can likely be explained by the fact that the public can applaud or boo the player that comes in or comes out of the field, depending on his status or the quality of his play during the game.</p><p>Another interesting observation concerns the difference between the confusion matrices of 60-seconds and 30seconds models. <ref type="table" target="#tab_1">Table 3</ref> presents these confusion matrices for the model trained with the merge point before the fully connected layer (fourth merging method). If we focus only on the samples classified in one of the three events of interest ("cards", "substitutions" and "goals"), we can see that the proportion of errors is lower in the 30-seconds version (2.68% instead of 4.83%). This observation can be explained by the fact that a smaller video chunk size reduces the probability to have multiple different events in the same window. Therefore, it becomes easier to determine the dif- ferences between the three classes of interest. However, as explained earlier, the overall mAP score is worse due to the higher proportion of "background" samples. After finding the best merging method, we used the best baseline model from <ref type="bibr" target="#b28">[28]</ref>, i.e. with a 512-clusters NetVLAD as pooling layer, and we trained it three times with different input configurations: (i) only video stream, (ii) only audio stream, and (iii) both video and audio stream, by using our best merging method. For each one of these configurations, we used 60-seconds and 20-seconds video chunks. <ref type="table">Table 4</ref> presents the mAP score for each of these models. As previously, we observe that using only audio stream performs worse than video stream alone but the combination of the two streams provides significantly improved performance. Moreover, the use of 60-seconds chunks for training performs way better than the use of 20-seconds chunks, except for the combination of audio and video, where the difference is non-significant.</p><p>The model using NetVLAD as pooling layer and both video and audio streams is the one registering the best results for the classification task with a mAP score of 75.2%. In average, adding the audio stream as input to the models increases the mAP by 7.43% in absolute terms compared to the video-only-based models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Action spotting</head><p>Following the methodology proposed by <ref type="bibr" target="#b28">[28]</ref>, the action spotting task, as described in Section 3.2, uses the best trained models from the classification task and the spotting results are obtained using three method variants: segment center, segment maximum and NMS. For each method, we compute the Average-mAP, which is the area under the mAP curve as a function of a tolerance Œ¥ in the precise time instant of the detected event, ranging from 5 to 60 seconds. In order to make comparisons, we applied this spotting process to the 6 trained models using NetVLAD as pooling layer, and to 3 of the models using NetRVLAD: (i) videoonly, (ii) audio-only, and (iii) both video and audio with the merge point before the fully connected layer. <ref type="table" target="#tab_2">Table 5</ref> presents the Average-mAP for each.</p><p>Similarly to the classification task, using only audio is not as good as using video alone, but the combination improves the performance. What differs from classification is that smaller video chunks leads to better results, regardless of the method used. Our intuition is that shorter chunks enable to distinguish and detect actions that are temporally closer to each other. However, the high difference in the Average-mAP scores between models trained on 20seconds windows and the ones trained on 60-seconds windows is particularly important. This can be explained by the fact that models trained with 60-seconds video chunks will have decreasing performances when tolerance Œ¥ becomes lower than 60 seconds since the models was not trained for this. However, if models trained on 20-seconds video chunks performs well with Œ¥ = 20 seconds, it will still be efficient for higher values of Œ¥. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates this suggestions by showing the mAP as a function of tolerance Œ¥ for both NetVLAD-based models using audio and video streams. We can observe that both models tend to their best performance when Œ¥ is higher or equals to the corresponding window size.</p><p>The best model is the one trained on 20-seconds video chunks and using NetVLAD as pooling layer and both video and audio streams, yielding an Average-mAP of 56%. In average, adding the audio stream as input to the models increases the Average-mAP of 4.19% absolute compared to the video-only models. <ref type="figure">Figure 3</ref> presents the evolution of performance on both the training set and the validation set during the training process, for our best model. The green line represents the evolution of the mAP classification score on the training set and the blue line is the evolution of the mAP score on the validation set. The horizontal dotted blue line represent the best mAP score reached on the validation set. The vertical black lines indicate the epochs at which a step decay was applied on the learning rate.  We can observe that the mAP score on the training set quickly reaches a very high value, while performance on <ref type="figure">Figure 3</ref>. Evolution of the learning curves, i.e. training and validation mAP score curves, through the epochs for our best model. the validation set always remains much lower. A generalization gap of about 35% is visible, between the performance on the training set and on the validation set. Even our best performing model significantly overfits, possibly as a consequence of the still too small size of the training set. Indeed, despite being one of the best benchmark for the soccer action spotting and classification challenges, SoccerNet contains annotations for 500 games, which represents only 3,965 annotated events available for training. This represents one of the current limitations of our study, and strategies to either increase the training set size, reduce over-fitting, or increase the generalization capabilities of our models should represent an important avenue for future research. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Additional observations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Future Work</head><p>In future works, we suggest to pursue the exploration of additional types of input streams, like optical flow, or even language streams such as transcriptions of commentators speech.</p><p>Furthermore, exploring more elaborate fusion mechanisms could be interesting. In order to improve our current models, one could also harness others feature extraction models than ResNet and VGG.</p><p>Another aspect that can be analyzed more in depth is trying to get a better understanding of the information carried by the audio stream. The audio in SoccerNet videos contain a mix between the commentators' voice and the sound coming from the stadium , including the field and the public. Therefore, we still do not know which of those different information source have the most impact on performance.</p><p>To address the issue related to the size of SoccerNet dataset, increasing the number of training samples is a possible solution. This could rely in part on data augmentation strategies, as well as annotating additional soccer game videos, or making use of unsupervised learning techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we studied the influence of the audio stream on soccer action classification and action spotting tasks, with performance evaluations on the SoccerNet baseline. For both tasks, using only the audio stream provides worse results than using only the video stream, except on the "goals" class, where audio significantly exceeds video performance. Furthermore, combining both streams yields to better results on every category of actions. Combining audio and video streams improves, in average, the performance of action classification on SoccerNet by 7.43% absolute, and the performance of action spotting by 4.19%. We also showed that using smaller video chunk sizes performs worse on classification, but improves the results for the action spotting task.</p><p>A more in-depth study of the audio stream could lead to a better understanding of what actually provides information that the visual processing model fails to identify. In particular, separating the voices of commentators from the sound ambiance coming from the stadium could definitely help in this study.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Example of a short caption, which should be centered.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>mAP score as a function of tolerance Œ¥ for NetVLADbased models, trained with 20-seconds and 60-seconds video chunks, by using both audio and video streams.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Classification metric (mAP) for different merging methods and different video chunk sizes using NetRVLAD, with k = 64 clusters, as pooling layer.ModelsT = 60 sec. T = 30 sec.</figDesc><table><row><cell cols="2">Video baseline [28]</cell><cell>66.0</cell><cell></cell><cell>58.7</cell></row><row><cell cols="2">Audio baseline</cell><cell>50.6</cell><cell></cell><cell>43.7</cell></row><row><cell cols="2">Merging method 1</cell><cell>68.4</cell><cell></cell><cell>63.7</cell></row><row><cell cols="2">Merging method 2</cell><cell>72.6</cell><cell></cell><cell>67.3</cell></row><row><cell cols="2">Merging method 3</cell><cell>73.4</cell><cell></cell><cell>69.3</cell></row><row><cell cols="2">Merging method 4</cell><cell>73.7</cell><cell></cell><cell>68.8</cell></row><row><cell cols="2">Merging method 5</cell><cell>72.8</cell><cell></cell><cell>68.7</cell></row><row><cell cols="2">Merging method 6</cell><cell>64.1</cell><cell></cell><cell>59.6</cell></row><row><cell cols="2">Merging method 7</cell><cell>64.2</cell><cell></cell><cell>58.1</cell></row><row><cell cols="5">Table 2. Comparison of the classification metric (mAP) on each</cell></row><row><cell>label.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Labels</cell><cell cols="2">Video baseline [28]</cell><cell>Audio baseline</cell><cell>Merge method 4</cell></row><row><cell>"background"</cell><cell>97.6</cell><cell></cell><cell>96.7</cell><cell>98.0</cell></row><row><cell>"cards"</cell><cell>60.5</cell><cell></cell><cell>19.2</cell><cell>63.9</cell></row><row><cell>"substitutions"</cell><cell>69.8</cell><cell></cell><cell>55.1</cell><cell>72.6</cell></row><row><cell>"goals"</cell><cell>67.7</cell><cell></cell><cell>77.3</cell><cell>84.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Confusion matrix for the model using the fourth merging method, with 60-seconds video chunks and 30-seconds video chunks.</figDesc><table><row><cell></cell><cell cols="2">60-seconds video chunks</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Predicted labels</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">background cards subs goals</cell></row><row><cell></cell><cell></cell><cell>background</cell><cell>7673</cell><cell>95</cell><cell>80</cell><cell>42</cell></row><row><cell></cell><cell>Groundtruth</cell><cell>cards</cell><cell>178</cell><cell>243</cell><cell>13</cell><cell>3</cell></row><row><cell></cell><cell>labels</cell><cell>subs</cell><cell>175</cell><cell>9</cell><cell>310</cell><cell>11</cell></row><row><cell></cell><cell></cell><cell>goals</cell><cell>91</cell><cell>1</cell><cell>2</cell><cell>215</cell></row><row><cell></cell><cell cols="2">30-seconds video chunks</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Predicted labels</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">background cards subs goals</cell></row><row><cell></cell><cell></cell><cell>background</cell><cell>16768</cell><cell>97</cell><cell>116</cell><cell>43</cell></row><row><cell></cell><cell>Groundtruth</cell><cell>cards</cell><cell>224</cell><cell>212</cell><cell>5</cell><cell>0</cell></row><row><cell></cell><cell>labels</cell><cell>subs</cell><cell>217</cell><cell>11</cell><cell>305</cell><cell>4</cell></row><row><cell></cell><cell></cell><cell>goals</cell><cell>114</cell><cell>0</cell><cell>0</cell><cell>209</cell></row><row><cell cols="3">Table 4. Classification metric (mAP) for models using NetVLAD,</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">with k = 512 clusters, as pooling layer.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Models</cell><cell cols="2">T = 60 sec. T = 20 sec.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Video-based NetVLAD baseline</cell><cell>67.5</cell><cell>56.6</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Audio-based NetVLAD baseline</cell><cell>46.8</cell><cell>35.9</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Audio + Video NetVLAD</cell><cell>75.2</cell><cell>75.0</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 .</head><label>5</label><figDesc>Average-mAP for action spotting. Video Models Seg. max Seg. center NMS Seg. max Seg. center NMS Seg. Max Seg. center NMS</figDesc><table><row><cell cols="8">Video-only Audio + NetRVLAD Audio-only 30.8% 41.9% 30.2% 21.8% 30.3% 22.1% 34.0% 47.6%</cell><cell>33.4%</cell></row><row><cell>60-sec. chunks NetVLAD</cell><cell>29.6%</cell><cell>43.4%</cell><cell>29.0% 19.9%</cell><cell>27.1%</cell><cell>19.5%</cell><cell>32.3%</cell><cell>48.7%</cell><cell>31.8%</cell></row><row><cell>20-sec. chunks NetVLAD</cell><cell>49.2%</cell><cell>50.2%</cell><cell>49.4% 30.0%</cell><cell>31.0%</cell><cell>30.0%</cell><cell>54.0%</cell><cell>56.0%</cell><cell>53.6%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Although the reference publication does not mention it, we assume the PCA transformation matrix is estimated on the SoccerNet training data. 2 https://github.com/DTaoo/VGGish</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tensorflow: Largescale machine learning on heterogeneous</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart√≠n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>J√≥zefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Levenberg ; Pete Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Zheng</surname></persName>
		</author>
		<editor>Ilya Sutskever, Kunal Talwar, Paul A. Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda B. Vi√©gas, Oriol Vinyals,</editor>
		<imprint>
			<date type="published" when="2016" />
			<pubPlace>Dan Man√©, Rajat Monga, Sherry Moore, Derek Gordon Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner</pubPlace>
		</imprint>
	</monogr>
	<note>distributed systems. CoRR, abs/1603.04467</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep multi-kernel convolutional lstm networks and an attention-based mechanism for videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Agethen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Winston</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="819" to="829" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">All about vlad</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1578" to="1585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Action classification in soccer videos with long short-term memory recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moez</forename><surname>Baccouche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Mamalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atilla</forename><surname>Baskurt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICANN</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Detection of tennis events from acoustic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">K</forename><surname>Baughman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Reiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nancy</forename><forename type="middle">M</forename><surname>Greco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Hammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqiang</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-person 3d pose estimation and tracking in sports</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewis</forename><surname>Bridgeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Volino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Yves</forename><surname>Guillemaut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Hilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sst: Single-stream temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyamal</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6373" to="6382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Temporal hockey action recognition via pose and optical flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Neher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanav</forename><surname>Vats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Clausi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">S</forename><surname>Zelek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A bottom-up approach based on semantics for the interpretation of the main camera stream in soccer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Cioppa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Deli√®ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Van Droogenbroeck</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1846" to="184609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A context-aware loss function for action spotting in soccer videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Cioppa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Deli√®ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Giancola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Van Droogenbroeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rikke</forename><surname>Gade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
		<idno>abs/1912.01326</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Arthus: Adaptive real-time human segmentation in sports through online distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Cioppa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Deli√©ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Istasse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><forename type="middle">De</forename><surname>Vleeschouwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Van Droogenbroeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Developing analytical tools to impact u.va. football performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Corscadden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Eastman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reece</forename><surname>Echelberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connor</forename><surname>Hagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clark</forename><surname>Kipp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Magnusson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Valeiras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Systems and Information Engineering Design Symposium (SIEDS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="249" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Temporal context network for activity localization in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan Qiu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5727" to="5736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Fisher kernels for image descriptors: a theoretical overview and experimental results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr√°s</forename><forename type="middle">A</forename><surname>B√°lint Zolt√°n Dar√≥czy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lajos</forename><surname>Bencz√∫r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>R√≥nyai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Global sports market -total revenue from</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deloitte</surname></persName>
		</author>
		<ptr target="https://www.statista.com/statistics/370560/worldwide-sports-market-revenue/" />
	</analytic>
	<monogr>
		<title level="m">Statista -The Statistics Portal, 2020. Retrieved</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>in billion u.s. dollars</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Market size of the european football market from 2006/07 to 2015/16 (in billion euros)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deloitte</surname></persName>
		</author>
		<ptr target="https://www.statista.com/statistics/261223/european-soccer-market-total-revenue/" />
	</analytic>
	<monogr>
		<title level="m">Statista -The Statistics Portal, 2020. Retrieved March 9</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Revenue of the biggest (big five*) european soccer leagues from 1996/97 to 2017/18 (in million euros)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deloitte</surname></persName>
		</author>
		<ptr target="https://www.statista.com/statistics/261218/big-five-european-soccer-leagues-revenue/" />
	</analytic>
	<monogr>
		<title level="m">Statista -The Statistics Portal, 2020. Retrieved</title>
		<imprint>
			<date type="published" when="2020-03-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Revenue of the top european soccer leagues (big five*) from 2006/07 to 2017/18 (in billion euros)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deloitte</surname></persName>
		</author>
		<ptr target="https://www.statista.com/statistics/261225/top-european-soccer-leagues-big-five-revenue/" />
	</analytic>
	<monogr>
		<title level="m">Statista -The Statistics Portal, 2020. Retrieved</title>
		<imprint>
			<date type="published" when="2020-03-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Imagenet: a large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009-06" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A review of visionbased systems for soccer video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D&amp;apos;</forename><surname>Tiziana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Orazio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="2911" to="2926" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automatic soccer video analysis and summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmet</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Murat Tekalp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajiv</forename><surname>Mehrotra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Society</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="796" to="807" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>IEEE transactions on image</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Robust camera calibration for sport videos using court models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Farin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Krabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">N</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>De With</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Effelsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IS&amp;T/SPIE Electronic Imaging</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">P</forename><surname>Wildes</surname></persName>
		</author>
		<idno>abs/1611.02155</idno>
		<title level="m">Spatiotemporal residual networks for video action recognition. ArXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">What will happen next? forecasting player moves in sports videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panna</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3362" to="3371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Audio-visual classification of sports types</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rikke</forename><surname>Gade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Abou-Zleikha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mads</forename><surname>Graesb√∏ll Christensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshop (ICCVW)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="768" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Turn tap: Temporal unit regression network for temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakant</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3648" to="3656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Audio set: An ontology and humanlabeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP 2017</title>
		<meeting>IEEE ICASSP 2017<address><addrLine>New Orleans, LA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Soccernet: A scalable dataset for action spotting in soccer videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Giancola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohieddine</forename><surname>Amine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tarek</forename><surname>Dghaily</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Actionvlad: Learning spatio-temporal aggregation for action classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3165" to="3174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Scc: Semantic context cascade for efficient action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayner</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Barrios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3175" to="3184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J√ºrgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sports field localization via deep structured models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namdar</forename><surname>Homayounfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4012" to="4020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semantic analysis of soccer video using dynamic bayesian network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Lin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huang-Chia</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Yuan</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="749" to="760" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Associative embedding for team discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Istasse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><forename type="middle">De</forename><surname>Vleeschouwer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Aggregating local descriptors into a compact image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv√©</forename><surname>J√©gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P√©rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3304" to="3311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Optimizing through learned errors for accurate sports field registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan Camilo Gamboa</forename><surname>Higuera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baptiste</forename><surname>Angles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrsan</forename><surname>Javan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang Moo</forename><surname>Yi</surname></persName>
		</author>
		<idno>abs/1909.08034</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Ball tracking in sports: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Paresh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Kamble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bhurchandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="page" from="1" to="51" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Visual reasoning on complex events in soccer videos using answer set programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdullah</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loris</forename><surname>Bozzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luciano</forename><surname>Serafini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Lazzerini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Soccer event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdullah</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Lazzerini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaetano</forename><surname>Calabrese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luciano</forename><surname>Serafini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP 2018</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Footandball: Integrated player and ball detector. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacek</forename><surname>Komorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Kurzejamski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Sarwas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multi-scale based context-aware net for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="337" to="348" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Pan-tilt-zoom slam for sports videos. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jikai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A survey on player tracking in soccer videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Manafifard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Ebadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid Abrishami</forename><surname>Moghaddam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">159</biblScope>
			<biblScope unit="page" from="19" to="46" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Action recognition with spatial-temporal discriminative filter banks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brais</forename><surname>Mart√≠nez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Modolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Tighe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5481" to="5490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Automatic curation of sports highlights using multimodal excitement features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Merler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Khoi-Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhiraj</forename><surname>Mac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc-Bao</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Kent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><forename type="middle">N</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">R</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rog√©rio</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidt Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1147" to="1160" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Learnable pooling with context gating for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<idno>abs/1706.06905</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Meta fisher vector for event recognition in generative encoded visual streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Stap: Spatial-temporal attention-aware pooling for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tam</forename><forename type="middle">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="77" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Multimodal feature extraction and fusion for semantic mining of soccer video: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Payam</forename><surname>Oskouie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Alipour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir-Masoud</forename><surname>Eftekhari-Moghadam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="173" to="210" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Foreground fisher vector: Encoding class-relevant foreground to improve image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongsheng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinggang</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="4716" to="4729" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Temporal attention-gated model for robust sequence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadas</forename><surname>Baltrusaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Tax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="820" to="829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Twostream collaborative learning with spatial-temporal attention for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunzhen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="773" to="786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Lost in quantization: Improving particular object retrieval in large scale image databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ond≈ôej</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Football video segmentation based on video production strategy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reede</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joemon</forename><forename type="middle">M</forename><surname>Jose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECIR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A deep architecture for multimodal summarization of soccer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melissa</forename><surname>Sanabria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr√©d√©ric</forename><surname>Sherly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Precioso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Menguy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Proceedings of the 2nd International Workshop on Multimedia Content Analysis in Sports, MMSports &apos;19</title>
		<meeting>of the 2nd International Workshop on Multimedia Content Analysis in Sports, MMSports &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="16" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Generation of ball possession statistics in soccer using minimum-cost flow network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saikat</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amlan</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipti Prasad</forename><surname>Mukherjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1049" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>arXiv 1409.1556</idno>
		<imprint>
			<date type="published" when="2014-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Probabilistic event calculus for event recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasios</forename><surname>Skarlatidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Paliouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Artikis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Vouros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computational Logic</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Deep fishernet for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2244" to="2250" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Video sequence classification using spatiotemporal features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Event detection and summarization in soccer videos using bayesian network and copula</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Tavassolipour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmood</forename><surname>Karimian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohreh</forename><surname>Kasaei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="291" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Soccer: Who has the ball? generating visual analytics and player statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajkumar</forename><surname>Theagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Pala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bir</forename><surname>Bhanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1830" to="18308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">A video-based tracking system for football player analysis using efficient convolution operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen Hong Thinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu</forename><forename type="middle">Thi</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phuong</forename><surname>Dzung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luu Manh</forename><surname>Vu Quang Dzung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Advanced Technologies for Communications (ATC)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="149" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Computer vision for sports: Current applications and research topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rikke</forename><surname>Gade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Hilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">159</biblScope>
			<biblScope unit="page" from="3" to="18" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Video classification with channel-separated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5551" to="5560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Football action recognition using hierarchical lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takamasa</forename><surname>Tsunoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuhiro</forename><surname>Komori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masakazu</forename><surname>Matsugu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="155" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Flexible automatic football filming and summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Turchini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Seidenari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><surname>Galteri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Ferracani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Becchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Proceedings of the 2nd International Workshop on Multimedia Content Analysis in Sports, MMSports &apos;19</title>
		<meeting>of the 2nd International Workshop on Multimedia Content Analysis in Sports, MMSports &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="108" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Estimating the number of soccer players using simulation-based occlusion handling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Noor Ul Huda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kasper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rikke</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">B</forename><surname>Gade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moeslund</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
				<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1905" to="190509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Two-stream action recognition in ice hockey using player pose sequences and optical flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanav</forename><surname>Vats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Neher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Clausi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">S</forename><surname>Zelek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th Conference on Computer and Robot Vision (CRV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="181" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Fast and accurate action detection in videos with motion-centric attention model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinzhuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="117" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wyscout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wyscout</surname></persName>
		</author>
		<ptr target="https://wyscout.com/" />
		<title level="m">WyScout, 2020. Retrieved March 9</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Exploring frame segmentation networks for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Dou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis. Commun. Image Represent</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="296" to="302" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Robust moving camera calibration for synthesizing free viewpoint soccer video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Nonaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Sankoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sei</forename><surname>Naito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1185" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">A novel framework for fine grained action recognition in soccer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Yaparla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allaparthi</forename><surname>Sriteja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Sai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garimella Rama</forename><surname>Munnangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Murthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IWANN</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Twostream convolutional network for improving activity recognition using convolutional long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikai</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="67772" to="67780" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Fine-grained video captioning for sports narrative</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanyu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6006" to="6015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Correlation net: Spatiotemporal multimodal deep learning for action recognition. Signal Process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Novanto</forename><surname>Yudistira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takio</forename><surname>Kurita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Commun</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page">115731</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Refining joint locations for human pose tracking in sports videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Zecha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Einfalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Lienhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">An automatic multi-camera-based event extraction system for real soccer videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yumeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Analysis and Applications</title>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
