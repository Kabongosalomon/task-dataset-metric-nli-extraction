<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Training Region-based Object Detectors with Online Hard Example Mining</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
							<email>abhinavg@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Training Region-based Object Detectors with Online Hard Example Mining</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The field of object detection has made significant advances riding on the wave of region-based ConvNets, but their training procedure still includes many heuristics and hyperparameters that are costly to tune. We present a simple yet surprisingly effective online hard example mining (OHEM) algorithm for training region-based ConvNet detectors. Our motivation is the same as it has always beendetection datasets contain an overwhelming number of easy examples and a small number of hard examples. Automatic selection of these hard examples can make training more effective and efficient. OHEM is a simple and intuitive algorithm that eliminates several heuristics and hyperparameters in common use. But more importantly, it yields consistent and significant boosts in detection performance on benchmarks like PASCAL VOC 2007 and 2012. Its effectiveness increases as datasets become larger and more difficult, as demonstrated by the results on the MS COCO dataset. Moreover, combined with complementary advances in the field, OHEM leads to state-of-the-art results of 78.9% and 76.3% mAP on PASCAL VOC 2007 and 2012 respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image classification and object detection are two fundamental computer vision tasks. Object detectors are often trained through a reduction that converts object detection into an image classification problem. This reduction introduces a new challenge that is not found in natural image classification tasks: the training set is distinguished by a large imbalance between the number of annotated objects and the number of background examples (image regions not belonging to any object class of interest). In the case of sliding-window object detectors, such as the deformable parts model (DPM) <ref type="bibr" target="#b11">[12]</ref>, this imbalance may be as extreme as 100,000 background examples to every one object. The recent trend towards object-proposal-based detectors <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b31">32]</ref> mitigates this issue to an extent, but the imbalance ratio may still be high (e.g., 70:1). This challenge opens space for learning techniques that cope with imbal-ance and yield faster training, higher accuracy, or both.</p><p>Unsurprisingly, this is not a new challenge and a standard solution, originally called bootstrapping (and now often called hard negative mining), has existed for at least 20 years. Bootstrapping was introduced in the work of Sung and Poggio <ref type="bibr" target="#b29">[30]</ref> in the mid-1990's (if not earlier) for training face detection models. Their key idea was to gradually grow, or bootstrap, the set of background examples by selecting those examples for which the detector triggers a false alarm. This strategy leads to an iterative training algorithm that alternates between updating the detection model given the current set of examples, and then using the updated model to find new false positives to add to the bootstrapped training set. The process typically commences with a training set consisting of all object examples and a small, random set of background examples.</p><p>Bootstrapping has seen widespread use in the intervening decades of object detection research. <ref type="bibr">Dalal</ref> and Triggs <ref type="bibr" target="#b6">[7]</ref> used it when training SVMs for pedestrian detection. Felzenszwalb et al. <ref type="bibr" target="#b11">[12]</ref> later proved that a form of bootstrapping for SVMs converges to the global optimal solution defined on the entire dataset. Their algorithm is often referred to as hard negative mining and is frequently used when training SVMs for object detection <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32]</ref>. Bootstrapping was also successfully applied to a variety of other learning models, including shallow neural networks <ref type="bibr" target="#b24">[25]</ref> and boosted decision trees <ref type="bibr" target="#b8">[9]</ref>. Even modern detection methods based on deep convolutional neural networks (ConvNets) <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>, such as R-CNN <ref type="bibr" target="#b14">[15]</ref> and SPPnet <ref type="bibr" target="#b15">[16]</ref>, still employ SVMs trained with hard negative mining.</p><p>It may seem odd then that the current state-of-the-art object detectors, embodied by Fast R-CNN <ref type="bibr" target="#b13">[14]</ref> and its descendants <ref type="bibr" target="#b23">[24]</ref>, do not use bootstrapping. The underlying reason is a technical difficulty brought on by the shift towards purely online learning algorithms, particularly in the context of deep ConvNets trained with stochastic gradient descent (SGD) on millions of examples. Bootstrapping, and its variants in the literature, rely on the aforementioned alternation template: (a) for some period of time a fixed model is used to find new examples to add to the active training set; (b) then, for some period of time the model is trained on the fixed active training set. Training deep ConvNet detectors with SGD typically requires hundreds of thousands of SGD steps and freezing the model for even a few iterations at a time would dramatically slow progress. What is needed, instead, is a purely online form of hard example selection.</p><p>In this paper, we propose a novel bootstrapping technique called online hard example mining 1 (OHEM) for training state-of-the-art detection models based on deep ConvNets. The algorithm is a simple modification to SGD in which training examples are sampled according to a non-uniform, non-stationary distribution that depends on the current loss of each example under consideration. The method takes advantage of detection-specific problem structure in which each SGD mini-batch consists of only one or two images, but thousands of candidate examples. The candidate examples are subsampled according to a distribution that favors diverse, high loss instances. Gradient computation (backpropagation) is still efficient because it only uses a small subset of all candidates. We apply OHEM to the standard Fast R-CNN detection method and show three benefits compared to the baseline training algorithm:</p><p>• It removes the need for several heuristics and hyperparameters commonly used in region-based ConvNets.</p><p>• It yields a consistent and significant boosts in mean average precision.</p><p>• Its effectiveness increases as the training set becomes larger and more difficult, as demonstrated by results on the MS COCO dataset.</p><p>Moreover, the gains from OHEM are complementary to recent improvements in object detection, such as multiscale testing <ref type="bibr" target="#b15">[16]</ref> and iterative bounding-box regression <ref type="bibr" target="#b12">[13]</ref>. Combined with these tricks, OHEM gives state-ofthe-art results of 78.9% and 76.3% mAP on PASCAL VOC 2007 and 2012, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Object detection is one of the oldest and most fundamental problems in computer vision. The idea of dataset bootstrapping <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b29">30]</ref>, typically called hard negative mining in recent work <ref type="bibr" target="#b11">[12]</ref>, appears in the training of most successful object detectors <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29]</ref>. Many of these approaches use SVMs as the detection scoring function, even after training a deep convolutional neural network (ConvNet) <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> for feature extraction. One notable exception is the Fast R-CNN detector <ref type="bibr" target="#b13">[14]</ref> and its descendants, such as Faster R-CNN <ref type="bibr" target="#b23">[24]</ref>. Since these models do not use SVMs, and are trained purely online with SGD, existing hard example mining techniques cannot be immediately applied. This work addresses that problem by introducing an online hard example mining algorithm that improves optimization and detection accuracy. We briefly review hard example mining, modern ConvNet-based object detection, and relationships to concurrent works using hard example selection for training deep networks.</p><p>Hard example mining. There are two hard example mining algorithms in common use. The first is used when optimizing SVMs. In this case, the training algorithm maintains a working set of examples and alternates between training an SVM to convergence on the working set, and updating the working set by removing some examples and adding others according to a specific rule <ref type="bibr" target="#b11">[12]</ref>. The rule removes examples that are "easy" in the sense that they are correctly classified beyond the current model's margin. Conversely, the rule adds new examples that are hard in the sense that they violate the current model's margin. Applying this rule leads to the global SVM solution. Importantly, the working set is usually a small subset of the entire training set.</p><p>The second method is used for non-SVMs and has been applied to a variety of models including shallow neural networks <ref type="bibr" target="#b24">[25]</ref> and boosted decision trees <ref type="bibr" target="#b8">[9]</ref>. This algorithm usually starts with a dataset of positive examples and a random set of negative examples. The machine learning model is then trained to convergence on that dataset and subsequently applied to a larger dataset to harvest false positives. The false positives are then added to the training set and then the model is trained again. This process is usually iterated only once and does not have any convergence proofs.</p><p>ConvNet-based object detection. In the last three years significant gains have been made in object detection. These improvements were made possible by the successful application of deep ConvNets <ref type="bibr" target="#b18">[19]</ref> to ImageNet classification <ref type="bibr" target="#b7">[8]</ref>. The R-CNN <ref type="bibr" target="#b14">[15]</ref> and OverFeat <ref type="bibr" target="#b25">[26]</ref> detectors lead this wave with impressive results on PASCAL VOC <ref type="bibr" target="#b10">[11]</ref> and Ima-geNet detection. OverFeat is based on the sliding-window detection method, which is perhaps the most intuitive and oldest search method for detection. R-CNN, in contrast, uses region proposals <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34]</ref>, a method that was made popular by the selective search algorithm <ref type="bibr" target="#b31">[32]</ref>. Since R-CNN, there has been rapid progress in regionbased ConvNets, including SPPnet <ref type="bibr" target="#b15">[16]</ref>, MR-CNN <ref type="bibr" target="#b12">[13]</ref>, and Fast R-CNN <ref type="bibr" target="#b13">[14]</ref>, which our work builds on.</p><p>Hard example selection in deep learning. There is recent work <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b32">33]</ref> concurrent to our own that selects hard examples for training deep networks. Similar to our approach, all these methods base their selection on the current loss for each datapoint. <ref type="bibr" target="#b26">[27]</ref> independently selects hard positive and negative example from a larger set of random examples based on their loss to learn image descriptors.  <ref type="figure">Figure 1</ref>: Architecture of the Fast R-CNN approach (see Section 3 for details).</p><p>Given a positive pair of patches, <ref type="bibr" target="#b32">[33]</ref> finds hard negative patches from a large set using triplet loss. Akin to our approach, <ref type="bibr" target="#b21">[22]</ref> investigates online selection of hard examples for mini-batch SGD methods. Their selection is also based on loss, but the focus is on ConvNets for image classification. Complementary to <ref type="bibr" target="#b21">[22]</ref>, we focus on online hard example selection strategy for region-based object detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Overview of Fast R-CNN</head><p>We first summarize the Fast R-CNN <ref type="bibr" target="#b13">[14]</ref> (FRCN) framework. FRCN takes as input an image and a set of object proposal regions of interest (RoIs). The FRCN network itself can be divided into two sequential parts: a convolutional (conv) network with several convolution and max-pooling layers <ref type="figure">(Figure 1</ref>, "Convolutional Network"); and an RoI network with an RoI-pooling layer, several fully-connected (fc) layers and two loss layers ( <ref type="figure">Figure 1</ref>, "RoI Network").</p><p>During inference, the conv network is applied to the given image to produce a conv feature map, size of which depends on the input image dimensions. Then, for each object proposal, the RoI-pooling layer projects the proposal onto the conv feature map and extracts a fixed-length feature vector. Each feature vector is fed into the fc layers, which finally give two outputs: (1) a softmax probability distribution over the object classes and background; and (2) regressed coordinates for bounding-box relocalization.</p><p>There are several reasons for choosing FRCN as our base object detector, apart from it being a fast end-to-end system. Firstly, the basic two network setup (conv and RoI) is also used by other recent detectors like SPPnet and MR-CNN; therefore, our proposed algorithm is more broadly applicable. Secondly, though the basic setup is similar, FRCN also allows for training the entire conv network, as opposed to both SPPnet and MR-CNN which keep the conv network fixed. And finally, both SPPnet and MR-CNN require features from the RoI network to be cached for training a separate SVM classifier (using hard negative mining). FRCN uses the RoI network itself to train the desired classifiers. In fact, <ref type="bibr" target="#b13">[14]</ref> shows that in the unified system using the SVM classifiers at later stages was unnecessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Training</head><p>Like most deep networks, FRCN is trained using stochastic gradient descent (SGD). The loss per example RoI is the sum of a classification log loss that encourages predicting the correct object (or background) label and a localization loss that encourages predicting an accurate bounding box (see <ref type="bibr" target="#b13">[14]</ref> for details).</p><p>To share conv network computation between RoIs, SGD mini-batches are created hierarchically. For each minibatch, N images are first sampled from the dataset, and then B/N RoIs are sampled from each image. Setting N = 2 and B = 128 works well in practice <ref type="bibr" target="#b13">[14]</ref>. The RoI sampling procedure uses several heuristics, which we describe briefly below. One contribution of this paper is to eliminate some of these heuristics and their hyperparameters.</p><p>Foreground RoIs. For an example RoI to be labeled as foreground (fg), its intersection over union (IoU) overlap with a ground-truth bounding box should be at least 0.5. This is a fairly standard design choice, in part inspired by the evaluation protocol of the PASCAL VOC object detection benchmark. The same criterion is used in the SVM hard mining procedures of R-CNN, SPPnet, and MR-CNN. We use the same setting.</p><p>Background RoIs. A region is labeled background (bg) if its maximum IoU with ground truth is in the interval [bg lo, 0.5). A lower threshold of bg lo = 0.1 is used by both FRCN and SPPnet, and is hypothesized in <ref type="bibr" target="#b13">[14]</ref> to crudely approximate hard negative mining; the assumption is that regions with some overlap with the ground truth are more likely to be the confusing or hard ones. We show in Section 5.4 that although this heuristic helps convergence and detection accuracy, it is suboptimal because it ignores some infrequent, but important, difficult background regions. Our method removes the bg lo threshold.</p><p>Balancing fg-bg RoIs: To handle the data imbalance described in Section 1, <ref type="bibr" target="#b13">[14]</ref> designed heuristics to rebalance the foreground-to-background ratio in each mini-batch to a target of 1 : 3 by undersampling the background patches at random, thus ensuring that 25% of a mini-batch is fg RoIs. We found that this is an important design decision for the training FRCN. Removing this ratio (i.e. randomly sampling RoIs), or increasing it, decreases accuracy by ∼3 points mAP. With our proposed method, we can remove this ratio hyperparameter with no ill effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Our approach</head><p>We propose a simple yet effective online hard example mining algorithm for training Fast R-CNN (or any Fast R-CNN style object detector). We argue that the current way of creating mini-batches for SGD (Section 3.1) is inefficient and suboptimal, and we demonstrate that our approach leads to better training (lower training loss) and higher testing performance (mAP). Our main observation is that these alternating steps can be combined with how FRCN is trained using online SGD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Online hard example mining</head><p>The key is that although each SGD iteration samples only a small number of images, each image contains thousands of example RoIs from which we can select the hard examples rather than a heuristically sampled subset. This strategy fits the alternation template to SGD by "freezing" the model for only one mini-batch. Thus the model is updated exactly as frequently as with the baseline SGD approach and therefore learning is not delayed.</p><p>More specifically, the online hard example mining algorithm (OHEM) proceeds as follows. For an input image at SGD iteration t, we first compute a conv feature map using the conv network. Then the RoI network uses this feature map and the all the input RoIs (R), instead of a sampled mini-batch <ref type="bibr" target="#b13">[14]</ref>, to do a forward pass. Recall that this step only involves RoI pooling, a few fc layers, and loss computation for each RoI. The loss represents how well the current network performs on each RoI. Hard examples are selected by sorting the input RoIs by loss and taking the B/N examples for which the current network performs worst. Most of the forward computation is shared between RoIs via the conv feature map, so the extra computation needed to for-ward all RoIs is relatively small. Moreover, because only a small number of RoIs are selected for updating the model, the backward pass is no more expensive than before.</p><p>However, there is a small caveat: co-located RoIs with high overlap are likely to have correlated losses. Moreover, these overlapping RoIs can project onto the same region in the conv feature map, because of resolution disparity, thus leading to loss double counting. To deal with these redundant and correlated regions, we use standard non-maximum suppression (NMS) to perform deduplication (the implementation from <ref type="bibr" target="#b13">[14]</ref>). Given a list of RoIs and their losses, NMS works by iteratively selecting the RoI with the highest loss, and then removing all lower loss RoIs that have high overlap with the selected region. We use a relaxed IoU threshold of 0.7 to suppress only highly overlapping RoIs.</p><p>We note that the procedure described above does not need a fg-bg ratio for data balancing. If any class were neglected, its loss would increase until it has a high probability of being sampled. There can be images where the fg RoIs are easy (e.g. canonical view of a car), so the network is free to use only bg regions in a mini-batch; and viceversa when bg is trivial (e.g. sky, grass etc.), the mini-batch can be entirely fg regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation details</head><p>There are many ways to implement OHEM in the FRCN detector, each with different trade-offs. An obvious way is to modify the loss layers to do the hard example selection. The loss layer can compute loss for all RoIs, sort them based on this loss to select hard RoIs, and finally set the loss of all non-hard RoIs to 0. Though straightforward, this implementation is inefficient as the RoI network still allocates memory and performs backward pass for all RoIs, even though most RoIs have 0 loss and hence no gradient updates (a limitation of current deep learning toolboxes).</p><p>To overcome this, we propose the architecture presented in <ref type="figure" target="#fig_1">Figure 2</ref>. Our implementation maintains two copies of the RoI network, one of which is readonly. This implies that the readonly RoI network <ref type="figure" target="#fig_1">(Figure 2(a)</ref>) allocates memory only for forward pass of all RoIs as opposed to the standard RoI network, which allocates memory for both forward and backward passes. For an SGD iteration, given the conv feature map, the readonly RoI network performs a forward pass and computes loss for all input RoIs (R) <ref type="figure" target="#fig_1">(Figure 2</ref>, green arrows). Then the hard RoI sampling module uses the procedure described in Section 4.1 to select hard examples (R hard-sel ), which are input to the regular RoI network <ref type="figure" target="#fig_1">(Figure 2(b)</ref>, red arrows)). This network computes forward and backward passes only for R hard-sel , accumulates the gradients and passes them to the conv network. In practice, we use all RoIs from all N images as R, therefore the effective batch size for the readonly RoI network is |R| and for the regular RoI network is the standard B from Section 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Selective-Search</head><p>RoIs <ref type="formula">(</ref>  We implement both options described above using the Caffe <ref type="bibr" target="#b16">[17]</ref> framework (see <ref type="bibr" target="#b13">[14]</ref>). Our implementation uses gradient accumulation with N forward-backward passes of single image mini-batches. Following FRCN <ref type="bibr" target="#b13">[14]</ref>, we use N = 2 (which results in |R| ≈ 4000) and B = 128. Under these settings, the proposed architecture ( <ref type="figure" target="#fig_1">Figure 2</ref>) has similar memory footprint as the first option, but is &gt; 2× faster. Unless specified otherwise, the architecture and settings described above will be used throughout this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Analyzing online hard example mining</head><p>This section compares FRCN training with online hard example mining (OHEM) to the baseline heuristic sampling approach. We also compare FRCN with OHEM to a less efficient approach that uses all available example RoIs in each mini-batch, not just the B hardest examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental setup</head><p>We conduct experiments with two standard ConvNet architectures: VGG CNN M 1024 (VGGM, for short) from <ref type="bibr" target="#b4">[5]</ref>, which is a wider version of AlexNet <ref type="bibr" target="#b18">[19]</ref>, and VGG16 from <ref type="bibr" target="#b27">[28]</ref>. All experiments in this section are performed on the PASCAL VOC07 dataset. Training is done on the trainval set and testing on the test set. Unless specified otherwise, we will use the default settings from FRCN <ref type="bibr" target="#b13">[14]</ref>. We train all methods with SGD for 80k minibatch iterations, with an initial learning rate of 0.001 and we decay the learning rate by 0.1 every 30k iterations. The baseline numbers reported in <ref type="table" target="#tab_2">Table 1</ref> (row 1-2) were reproduced using our training schedule and are slightly higher than the ones reported in <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">OHEM vs. heuristic sampling</head><p>Standard FRCN, reported in <ref type="table" target="#tab_2">Table 1</ref> (rows 1 − 2), uses bg lo = 0.1 as a heuristic for hard mining (Section 3.1). To test the importance of this heuristic, we ran FRCN with bg lo = 0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Robust gradient estimates</head><p>One concern over using only N = 2 images per batch is that it may cause unstable gradients and slow convergence because RoIs from an image may be highly correlated <ref type="bibr" target="#b30">[31]</ref>. FRCN <ref type="bibr" target="#b13">[14]</ref> reports that this was not a practical issue for their training. But this detail might raise concerns over our training procedure because we use examples with high loss from the same image and as a result they may be more highly correlated. To address this concern, we experiment with N = 1 in order to increase correlation in an effort to break our method. As seen in <ref type="table" target="#tab_2">Table 1</ref> (rows 5 − 6, 11), performance of the original FRCN drops by ∼1 point with N = 1, but when using our training procedure, mAP remains approximately the same. This shows that OHEM is robust in case one needs fewer images per batch in order to reduce GPU memory usage. The easy examples will have low loss, and won't contribute much to the gradient; training will automatically focus on the hard examples. To compare this option, we ran standard FRCN training with a large mini-batch size of B = 2048, using bg lo = 0, N ∈ {1, 2} and with other hyperparameters fixed. Because this experiment uses a large mini-batch, it's important to tune the learning rate to adjust for this change. We found optimal results by increasing it to 0.003 for VGG16 and 0.004 for VGGM. The outcomes are reported in <ref type="table" target="#tab_2">Table 1</ref> (rows 7 − 10). Using these settings, mAP of both VGG16 and VGGM increased by ∼1 point compared to B = 128, but the improvement from our approach is still &gt; 1 points over using all RoIs. Moreover, because we compute gradients with a smaller mini-batch size training is faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Better optimization</head><p>Finally, we analyze the training loss for the various FRCN training methods discussed above. It's important to measure training loss in a way that does not depend on the sampling procedure and thus results in a valid comparison between methods. To achieve this goal, we take model snapshots from each method every 20k steps of optimization and run them over the entire VOC07 trainval set to compute the average loss over all RoIs. This measures the training set loss in a way that does not depend on the example sampling scheme. <ref type="figure">Figure 3</ref> shows the average loss per RoI for VGG16 with the various hyperparameter settings discussed above and presented in <ref type="table" target="#tab_2">Table 1</ref>. We see that bg lo = 0 results in the highest training loss, while using the heuristic bg lo = 0.1 results in a much lower training loss. Increasing the minibatch size to B = 2048 and increasing the learning rate  <ref type="figure">Figure 3</ref>: Training loss is computed for various training procedures using VGG16 networks discussed in Section 5. We report mean loss per RoI. These results indicate that using hard mining for training leads to lower training loss than any of the other heuristics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Computational cost</head><p>OHEM adds reasonable computational and memory overhead, as reported in <ref type="table" target="#tab_4">Table 2</ref>. OHEM costs 0.09s per training iteration for VGGM network (0.43s for VGG16) and requires 1G more memory (2.3G for VGG16). Given that FRCN <ref type="bibr" target="#b13">[14]</ref> is a fast detector to train, the increase in training time is likely acceptable to most users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">PASCAL VOC and MS COCO results</head><p>In this section, we evaluate our method on VOC 2012 <ref type="bibr" target="#b10">[11]</ref> as well as the more challenging MS COCO <ref type="bibr" target="#b20">[21]</ref> dataset. We demonstrate consistent and significant improvement in FRCN performance when using the proposed OHEM approach. Per-class results are also presented on VOC 2007 for comparison with prior work.</p><p>Experimental setup. We use VGG16 for all experiments. When training on VOC07 trainval, we use the SGD parameters as in Section 5 and when using extra data (07+12 and 07++12, see <ref type="table" target="#tab_5">Table 3</ref> and 4), we use 200k mini-batch iterations, with an initial learning rate of 0.001 and decay step size of 40k. When training on MS COCO <ref type="bibr" target="#b20">[21]</ref>, we use 240k  mini-batch iterations, with an initial learning rate of 0.001 and decay step size of 160k, owing to a larger epoch size. <ref type="table" target="#tab_5">Table 3</ref> shows that on VOC07, OHEM improves the mAP of FRCN from 67.2% to 69.9% (and 70.0% to 74.6% with extra data). On VOC12, OHEM leads to an improvement of 4.1 points in mAP (from 65.7% to 69.8%). With extra data, we achieve an mAP of 71.9% as compared to 68.4% mAP of FRCN, an improvement of 3.5 points. Interestingly the improvements are not uniform across categories. Bottle, chair, and tvmonitor show larger improvements that are consistent across the different PAS-CAL splits. Why these classes benefit the most is an interesting and open question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">VOC 2007 and 2012 results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">MS COCO results</head><p>To test the benefit of using OHEM on a larger and more challenging dataset, we conduct experiments on MS COCO <ref type="bibr" target="#b20">[21]</ref> and report numbers from test-dev 2015 evaluation server ( IoU ≥ 0.5, OHEM gives a 6.6 points boost in AP 50 . It is also interesting to note that OHEM helps improve the AP of medium sized objects by 4.9 points on the strict COCO AP evaluation metric, which indicates that the proposed hard example mining approach is helpful when dealing with smaller sized objects. Note that FRCN with and without OHEM were trained on MS COCO train set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Adding bells and whistles</head><p>We've demonstrated consistent gains in detection accuracy by applying OHEM to FRCN training. In this section, we show that these improvements are orthogonal to recent bells and whistles that enhance object detection accuracy. OHEM with the following two additions yields state-of-theart results on VOC and competitive results on MS COCO.</p><p>Multi-scale (M). We adopt the multi-scale strategy from SPPnet <ref type="bibr" target="#b15">[16]</ref> (and used by both FRCN <ref type="bibr" target="#b13">[14]</ref> and MR-CNN <ref type="bibr" target="#b12">[13]</ref>). Scale is defined as the size of the shortest side (s) of an image. During training, one scale is chosen at random, whereas at test time inference is run on all scales. For VGG16 networks, we use s ∈ {480, 576, 688, 864, 900} for training, and s ∈ {480, 576, 688, 864, 1000} during testing, with the max dimension capped at 1000. The scales and caps were chosen because of GPU memory constraints. is obtained using NMS on R F with an IoU threshold of 0.3 and weighted voting is performed on each box r i in R NMS F using boxes in R F with an IoU of ≥0.5 with r i (see <ref type="bibr" target="#b12">[13]</ref> for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">VOC 2007 and 2012 results</head><p>We report the results on VOC benchmarks in <ref type="table" target="#tab_5">Table 3</ref> and 4. On VOC07, FRCN with the above mentioned additions achieves 72.4% mAP and OHEM improves it to 75.1%, which is currently the highest reported score under this setting (07 data). When using extra data (07+12), OHEM achieves 78.9% mAP, surpassing the current stateof-the-art MR-CNN (78.2% mAP). We note that MR-CNN uses selective search and edge boxes during training, whereas we only use selective search boxes. Our multiscale implementation is also different, using fewer scales than MR-CNN. On VOC12 <ref type="table" target="#tab_6">(Table 4)</ref>, we consistently perform better than MR-CNN. When using extra data, we achieve state-of-the-art mAP of 76.3% (vs. 73.9% mAP of MR-CNN).</p><p>Ablation analysis. We now study in detail the impact of these two additions and whether OHEM is complementary to them, and report the analysis in <ref type="table" target="#tab_9">Table 6</ref>. Baseline FRCN mAP improves from 67.2% to 68.6% when using multiscale during both training and testing (we refer to this as M). However, note that there is only a marginal benefit of using it at training time. Iterative bbox regression (B) further improves the FRCN mAP to 72.4%. But more importantly, using OHEM improves it to 75.1% mAP, which is state-ofthe-art for methods trained on VOC07 data (see <ref type="table" target="#tab_5">Table 3</ref>). In fact, using OHEM consistently results in higher mAP for all variants of these two additions (see <ref type="table" target="#tab_9">Table 6</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">MS COCO results</head><p>MS COCO <ref type="bibr" target="#b20">[21]</ref> test-dev 2015 evaluation server results are reported in <ref type="table" target="#tab_7">Table 5</ref>. Using multi-scale improves the performance of our method to 24.4% AP on the standard COCO metric and to 44.4% AP 50 on the VOC metric. This again shows the complementary nature of using multi-scale and OHEM. Finally, we train our method using the entire MS COCO trainval set, which further improves performance to 25.5% AP (and 45.9% AP 50 ). In the 2015 MS COCO Detection Challenge, a variant of this approach finished 4 th place overall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>We presented an online hard example mining (OHEM) algorithm, a simple and effective method to train regionbased ConvNet detectors. OHEM eliminates several heuristics and hyperparameters in common use by automatically selecting hard examples, thus simplifying training. We conducted extensive experimental analysis to demonstrate the effectiveness of the proposed algorithm, which leads to better training convergence and consistent improvements in detection accuracy on standard benchmarks. We also reported state-of-the-art results on PASCAL VOC 2007 and 2012 when using OHEM with other orthogonal additions. Though we used Fast R-CNN throughout this paper, OHEM can be used for training any region-based ConvNet detector.</p><p>Our experimental analysis was based on the overall detection accuracy, however it will be an interesting future direction to study the impact of various training methodologies on individual category performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Recall the alternating steps that define a hard example mining algorithm: (a) for some period of time a fixed model is used to find new examples to add to the active training set; (b) then, for some period of time the model is trained on the fixed active training set. In the context of SVM-based object detectors, such as the SVMs trained in R-CNN or SPPnet, step (a) inspects a variable number of images (often 10's or 100's) until the active training set reaches a threshold size, and then in step (b) the SVM is trained to convergence on the active training set. This process repeats until the active training set contains all support vectors. Applying an analogous strategy to FRCN ConvNet training slows learning because no model updates are made while selecting examples from the 10's or 100's of images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Architecture of the proposed training algorithm. Given an image, and selective search RoIs, the conv network computes a conv feature map. In (a), the readonly RoI network runs a forward pass on the feature map and all RoIs (shown in green arrows). Then the Hard RoI module uses these RoI losses to select B examples. In (b), these hard examples are used by the RoI network to compute forward and backward passes (shown in red arrows).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>, bg lo=1) FRCN (N =1, bg lo=0, B=2048, LR=4e-3) Our Approach (N =1, bg lo=0)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 (</head><label>1</label><figDesc>rows 3 − 4) shows that for VGGM, mAP drops by 2.4 points, whereas for VGG16 it remains roughly the same. Now compare this to training FRCN with OHEM (rows 11 − 13). OHEM improves mAP by 2.4 points compared to FRCN with the bg lo = 0.1 heuristic for VGGM, and 4.8 points without the heuristic. This result demonstrates the sub-optimality of these heuristics and the effectiveness of our hard mining approach.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Impact of hyperparameters on FRCN training. Online hard example mining is based on the hypothesis that it is important to consider all RoIs in an image and then select hard examples for training. But what if we train with all the RoIs, not just the hard ones?</figDesc><table><row><cell></cell><cell>Experiment</cell><cell cols="2">Model N</cell><cell>LR</cell><cell>B</cell><cell>bg lo 07 mAP</cell></row><row><cell>1 2</cell><cell>Fast R-CNN [14]</cell><cell>VGGM VGG16</cell><cell cols="3">2 0.001 128</cell><cell>0.1</cell><cell>59.6 67.2</cell></row><row><cell>3 4</cell><cell>Removing hard mining heuristic (Section 5.2)</cell><cell>VGGM VGG16</cell><cell cols="3">2 0.001 128</cell><cell>0</cell><cell>57.2 67.5</cell></row><row><cell>5 6</cell><cell>Fewer images per batch (Section 5.3)</cell><cell cols="4">VGG16 1 0.001 128</cell><cell>0.1 0</cell><cell>66.3 66.3</cell></row><row><cell>7 8</cell><cell>Bigger batch, High LR</cell><cell>VGGM</cell><cell cols="3">1 0.004 2048 2</cell><cell>0</cell><cell>57.7 60.4</cell></row><row><cell>9 10</cell><cell>(Section 5.4)</cell><cell>VGG16</cell><cell cols="3">1 0.003 2048 2</cell><cell>0</cell><cell>67.5 68.7</cell></row><row><cell>11</cell><cell></cell><cell cols="4">VGG16 1 0.001 128</cell><cell>0</cell><cell>69.7</cell></row><row><cell>12 13</cell><cell>Our Approach</cell><cell>VGGM VGG16</cell><cell cols="3">2 0.001 128</cell><cell>0</cell><cell>62.0 69.9</cell></row><row><cell cols="7">5.4. Why just hard examples, when you can use all?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Computational statistics of training FRCN<ref type="bibr" target="#b13">[14]</ref> and FRCN with OHEM (using an Nvidia Titan X GPU). : uses gradient accumulation over two forward/backward passes lowers the training loss below the bg lo = 0.1 heuristic. Our proposed online hard example mining method achieves the lowest training loss of all methods, validating our claims that OHEM leads to better training for FRCN.</figDesc><table><row><cell></cell><cell cols="2">VGGM</cell><cell></cell><cell>VGG16</cell><cell></cell></row><row><cell></cell><cell cols="5">FRCN Ours FRCN FRCN* Ours*</cell></row><row><cell>time (sec/iter)</cell><cell>0.13</cell><cell>0.22</cell><cell>0.60</cell><cell>0.57</cell><cell>1.00</cell></row><row><cell>max. memory (G)</cell><cell>2.6</cell><cell>3.6</cell><cell>11.2</cell><cell>6.4</cell><cell>8.7</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>VOC 2007 test detection average precision (%). All methods use VGG16. Training set key: 07: VOC07 trainval, 07+12: union of 07 and VOC12 trainval. All methods use bounding-box regression. Legend: M: using multi-scale for training and testing, B: multi-stage bbox regression. FRCN refers to FRCN [14] with our training schedule. method M B train set mAP aero bike bird boat bottle bus car cat chair cow table dog horse mbike persn plant sheep sofa train tv FRCN [14] 07 66.9 74.5 78.3 69.2 53.2 36.6 77.3 78.2 82.0 40.7 72.7 67.9 79.6 79.2 73.0 69.0 30.1 65.4 70.2 75.8 65.8</figDesc><table><row><cell>FRCN</cell><cell>07</cell><cell>67.2 74.6 76.8 67.6 52.9 37.8 78.7 78.8 81.6 42.2 73.6 67.0 79.4 79.6 74.1 68.3 33.4 65.9 68.7 75.4 68.1</cell></row><row><cell>Ours</cell><cell>07</cell><cell>69.9 71.2 78.3 69.2 57.9 46.5 81.8 79.1 83.2 47.9 76.2 68.9 83.2 80.8 75.8 72.7 39.9 67.5 66.2 75.6 75.9</cell></row><row><cell>FRCN</cell><cell>07</cell><cell>72.4 77.8 81.3 71.4 60.4 48.3 85.0 84.6 86.2 49.4 80.7 68.1 84.1 86.7 80.2 75.3 38.7 71.9 71.5 77.9 67.8</cell></row><row><cell>MR-CNN [13]</cell><cell>07</cell><cell>74.9 78.7 81.8 76.7 66.6 61.8 81.7 85.3 82.7 57.0 81.9 73.2 84.6 86.0 80.5 74.9 44.9 71.7 69.7 78.7 79.9</cell></row><row><cell>Ours</cell><cell>07</cell><cell>75.1 77.7 81.9 76.0 64.9 55.8 86.3 86.0 86.8 53.2 82.9 70.3 85.0 86.3 78.7 78.0 46.8 76.1 72.7 80.9 75.5</cell></row><row><cell>FRCN [14]</cell><cell cols="2">07+12 70.0 77.0 78.1 69.3 59.4 38.3 81.6 78.6 86.7 42.8 78.8 68.9 84.7 82.0 76.6 69.9 31.8 70.1 74.8 80.4 70.4</cell></row><row><cell>Ours</cell><cell cols="2">07+12 74.6 77.7 81.2 74.1 64.2 50.2 86.2 83.8 88.1 55.2 80.9 73.8 85.1 82.6 77.8 74.9 43.7 76.1 74.2 82.3 79.6</cell></row><row><cell>MR-CNN [13]</cell><cell cols="2">07+12 78.2 80.3 84.1 78.5 70.8 68.5 88.0 85.9 87.8 60.3 85.2 73.7 87.2 86.5 85.0 76.4 48.5 76.3 75.5 85.0 81.0</cell></row><row><cell>Ours</cell><cell cols="2">07+12 78.9 80.6 85.7 79.8 69.9 60.8 88.3 87.9 89.6 59.7 85.1 76.5 87.1 87.3 82.4 78.8 53.7 80.5 78.7 84.5 80.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>VOC 2012 test detection average precision (%). All methods use VGG16. Training set key: 12: VOC12 trainval, 07++12: union of VOC07 trainval, VOC07 test, and VOC12 trainval. Legend: M: using multi-scale for training and testing, B: iterative bbox regression. http://host.robots.ox.ac.uk:8080/anonymous/XNDVK7.html, 2 http://host.robots.ox.ac.uk:8080/anonymous/H49PTT.html, 3 http://host.robots.ox.ac.uk:8080/anonymous/LSANTB.html, 4 http://host.robots.ox.ac.uk:8080/anonymous/R7EAMX.html</figDesc><table><row><cell>method</cell><cell cols="2">M B train set mAP aero bike bird boat bottle bus car cat chair cow table dog horse mbike persn plant sheep sofa train tv</cell></row><row><cell>FRCN [14]</cell><cell>12</cell><cell>65.7 80.3 74.7 66.9 46.9 37.7 73.9 68.6 87.7 41.7 71.1 51.1 86.0 77.8 79.8 69.8 32.1 65.5 63.8 76.4 61.7</cell></row><row><cell>Ours 1</cell><cell>12</cell><cell>69.8 81.5 78.9 69.6 52.3 46.5 77.4 72.1 88.2 48.8 73.8 58.3 86.9 79.7 81.4 75.0 43.0 69.5 64.8 78.5 68.9</cell></row><row><cell>MR-CNN [13]</cell><cell>12</cell><cell>70.7 85.0 79.6 71.5 55.3 57.7 76.0 73.9 84.6 50.5 74.3 61.7 85.5 79.9 81.7 76.4 41.0 69.0 61.2 77.7 72.1</cell></row><row><cell>Ours 2</cell><cell>12</cell><cell>72.9 85.8 82.3 74.1 55.8 55.1 79.5 77.7 90.4 52.1 75.5 58.4 88.6 82.4 83.1 78.3 47.0 77.2 65.1 79.3 70.4</cell></row><row><cell>FRCN [14]</cell><cell cols="2">07++12 68.4 82.3 78.4 70.8 52.3 38.7 77.8 71.6 89.3 44.2 73.0 55.0 87.5 80.5 80.8 72.0 35.1 68.3 65.7 80.4 64.2</cell></row><row><cell>Ours 3</cell><cell cols="2">07++12 71.9 83.0 81.3 72.5 55.6 49.0 78.9 74.7 89.5 52.3 75.0 61.0 87.9 80.9 82.4 76.3 47.1 72.5 67.3 80.6 71.2</cell></row><row><cell>MR-CNN [13]</cell><cell cols="2">07++12 73.9 85.5 82.9 76.6 57.8 62.7 79.4 77.2 86.6 55.0 79.1 62.2 87.0 83.4 84.7 78.9 45.3 73.4 65.8 80.3 74.0</cell></row><row><cell>Ours 4</cell><cell cols="2">07++12 76.3 86.3 85.0 77.0 60.9 59.3 81.9 81.1 91.9 55.8 80.6 63.0 90.8 85.1 85.3 80.7 54.9 78.3 70.8 82.8 74.9</cell></row></table><note>1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 )</head><label>5</label><figDesc>. On the standard COCO evaluation metric, FRCN<ref type="bibr" target="#b13">[14]</ref> scores 19.7% AP, and OHEM improves it to 22.6% AP.<ref type="bibr" target="#b1">2</ref> Using the VOC overlap metric of 2 COCO AP averages over classes, recall, and IoU levels. See http: //mscoco.org/dataset/#detections-eval for details.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>MS COCO 2015 test−dev detection average precision (%). All methods use VGG16. Legend: M: using multi-scale for training and testing. AP@IoU area FRCN † Ours Ours [+M] Ours* [+M] The network evaluates each proposal RoI to get scores and relocalized boxes R 1 . High-scoring R 1 boxes are the rescored and relocalized, yielding boxes R 2 . Union of R 1 and R 2 is used as the final set R F for postprocessing, where R NMS F</figDesc><table><row><cell>[0.50 : 0.95]</cell><cell cols="2">all 19.7</cell><cell>22.6</cell><cell>24.4</cell><cell>25.5</cell></row><row><cell>0.50</cell><cell cols="2">all 35.9</cell><cell>42.5</cell><cell>44.4</cell><cell>45.9</cell></row><row><cell>0.75</cell><cell cols="2">all 19.9</cell><cell>22.2</cell><cell>24.8</cell><cell>26.1</cell></row><row><cell cols="2">[0.50 : 0.95] small</cell><cell>3.5</cell><cell>5.0</cell><cell>7.1</cell><cell>7.4</cell></row><row><cell>[0.50 : 0.95]</cell><cell cols="2">med. 18.8</cell><cell>23.7</cell><cell>26.4</cell><cell>27.7</cell></row><row><cell>[0.50 : 0.95]</cell><cell cols="2">large 34.6</cell><cell>37.9</cell><cell>38.5</cell><cell>40.3</cell></row><row><cell cols="6">† from the leaderboard, *trained on trainval set</cell></row><row><cell cols="6">Iterative bounding-box regression (B). We adopt the</cell></row><row><cell cols="6">iterative localization and bounding-box (bbox) voting</cell></row><row><cell cols="2">scheme from [13].</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Impact of multi-scale and iterative bbox reg.</figDesc><table><row><cell cols="2">Multi-scale (M)</cell><cell>Iterative bbox</cell><cell cols="2">VOC07 mAP</cell></row><row><cell>Train</cell><cell>Test</cell><cell>reg. (B)</cell><cell>FRCN</cell><cell>Ours</cell></row><row><cell></cell><cell></cell><cell></cell><cell>67.2</cell><cell>69.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell>68.4</cell><cell>71.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>70.8</cell><cell>72.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell>71.9</cell><cell>74.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>67.7</cell><cell>70.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell>68.6</cell><cell>71.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell>71.2</cell><cell>72.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell>72.4</cell><cell>75.1</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We use the term hard example mining, rather than hard negative mining, because our method is applied in a multi-class setting to all classes, not just a "negative" class.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment. This project started as an intern project at Microsoft Research and continued at CMU. We thank Larry Zitnick, Ishan Misra and Sean Bell for many helpful discussions. AS was supported by the Microsoft Research PhD Fellowship. This work was also partially supported by ONR MURI N000141612007. We thank NVIDIA for donating GPUs.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">What is an object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Measuring the objectness of image windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Constrained parametric min-cuts for automatic object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BING: Binarized normed gradients for objectness estimation at 300fps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Integral channel features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Category independent object proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The pascal visual object classes (voc) challenge. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Object detection via a multi-region &amp; semantic segmentation-aware cnn model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Geodesic object proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Microsoft COCO: common objects in context. CoRR, abs/1405.0312</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Online batch selection for faster training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06343</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ensemble of exemplar-svms for object detection and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Neural networkbased face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno>abs/1312.6229</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ferraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6537</idno>
		<title level="m">Fracking deep convolutional image descriptors</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised discovery of mid-level discriminative patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning and Example Selection for Object and Pattern Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In MIT A.I. Memo</title>
		<imprint>
			<biblScope unit="volume">1521</biblScope>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Takáč</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bijral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Richtárik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1303.2314</idno>
		<title level="m">Mini-batch primal and dual methods for svms</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Smeulders. Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
