<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Personalized Federated Learning using Hypernetworks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviv</forename><surname>Shamsian</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviv</forename><surname>Navon</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Fetaya</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
						</author>
						<title level="a" type="main">Personalized Federated Learning using Hypernetworks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Personalized federated learning is tasked with training machine learning models for multiple clients, each with its own data distribution. The goal is to train personalized models in a collaborative way while accounting for data disparities across clients and reducing communication costs.</p><p>We propose a novel approach to this problem using hypernetworks, termed pFedHN for personalized Federated HyperNetworks. In this approach, a central hypernetwork model is trained to generate a set of models, one model for each client. This architecture provides effective parameter sharing across clients, while maintaining the capacity to generate unique and diverse personal models. Furthermore, since hypernetwork parameters are never transmitted, this approach decouples the communication cost from the trainable model size. We test pFedHN empirically in several personalized federated learning challenges and find that it outperforms previous methods. Finally, since hypernetworks share information across clients we show that pFedHN can generalize better to new clients whose distributions differ from any client observed during training.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Federated learning (FL) is the task of learning a model over multiple disjoint local datasets <ref type="bibr" target="#b37">(McMahan et al., 2017a;</ref><ref type="bibr" target="#b55">Yang et al., 2019)</ref>. It is particularly useful when local data cannot be shared due to privacy, storage, or communication constraints. This is the case, for instance, in IoT applications that create large amounts of data at edge devices, or with medical data that cannot be shared due to privacy <ref type="bibr" target="#b54">(Wu et al., 2020)</ref>. In federated learning, all clients collectively train a shared model without sharing data and while trying to minimize communication. Unfortunately, learning a single global model may fail when the data distribution varies across clients. For example, user data may come from different devices or geographical locales and is potentially heterogeneous. In the extreme, each client may be required to solve a different task. To handle such heterogeneity across clients, Personalized Federated Learning (PFL) <ref type="bibr" target="#b48">(Smith et al., 2017)</ref> allows each client to use a personalized model instead of a shared global model. The key challenge in PFL is to benefit from joint training while allowing each client to keep its own unique model and at the same time limit the communication cost. While several approaches were recently proposed for this challenge, these problems are far from being resolved.</p><p>In this work, we describe a new approach that aims to resolve these concerns. Our approach, which we name, pFedHN for personalized Federated HyperNetwork addresses this by using hypernetworks <ref type="bibr" target="#b13">(Ha et al., 2017)</ref>, a model that for each input produces parameters for a neural network. Using a single joint hypernetwork to generate all separate models allows us to perform smart parameter sharing. Each client has a unique embedding vector, which is passed as input to the hypernetwork to produce its personalized model weights. As the vast majority of parameters belong to the hypernetwork, most parameters are shared across clients. Despite that, by using a hypernetwork, we can achieve great flexibility and diversity between the models of each client. Intuitively, as the hypernetwork maps between the embedding space and the personal networks' parameter space, its image can be viewed as a low-dimensional manifold in that space. Thus, we can think of the hypernetwork as the coordinate map of this manifold. Each unique client's model is restricted to lay on this manifold and is parametrized by the embedding vector.</p><p>Another benefit of using hypernetworks is that the trained parameter vector of the hypernetwork, which is generally much larger than the parameter vectors of the clients that it produces, is never transmitted. Each client only needs to receive its own network parameters to make predictions and compute gradients. Furthermore, the hypernetwork only needs to receive the gradient or update direction to optimize its own parameters. As a result, we can train a large hypernetwork with the same communication costs as in previous models. Compared to previous parameter sharing schemes, e.g., <ref type="bibr" target="#b8">Dinh et al. (2020)</ref>; <ref type="bibr">McMahan et al. arXiv:2103.04628v1 [cs.</ref>LG] 8 Mar 2021 <ref type="bibr">(2017a)</ref>, hypernetworks open new options that were not directly possible before. Consider the case where each client uses a cell phone or a wearable device, each one with different computational resources. The hypernetwork can produce several networks per input, each with a different computational capacity, allowing each client to select its appropriate network. This paper makes the following contributions: (1) A new approach for personalized federated learning based on hypernetworks.</p><p>(2) This approach generalizes better (a) to novel clients that differ from the ones seen during training; and (b) to clients with different computational resources, allowing clients to have different model sizes. (3) A new set of state-of-the-art results for the standard benchmarks in the field CIFAR10, CIFAR100, and Omniglot.</p><p>The paper is organized as follows. Section 3 describes our model in detail. Section 4 establishes some theoretical results to provide insight into our model. Section 5 shows experimentally that pFedHN achieves stateof-the-art results on several datasets and learning setups. We make our source code publicly available at: https: //github.com/AvivSham/pFedHN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Federated Learning</head><p>Federated learning (FL) <ref type="bibr" target="#b37">(McMahan et al., 2017a;</ref><ref type="bibr" target="#b19">Kairouz et al., 2019;</ref><ref type="bibr" target="#b40">Mothukuri et al., 2021;</ref><ref type="bibr" target="#b27">Li et al., 2019;</ref> is a learning setup in machine learning in which multiple clients collaborate to solve a learning task while maintaining privacy and communication efficiency. Recently, numerous methods have been introduced for solving the various FL challenges. <ref type="bibr" target="#b9">Duchi et al. (2014)</ref>; <ref type="bibr" target="#b38">McMahan et al. (2017b)</ref>; <ref type="bibr" target="#b0">Agarwal et al. (2018)</ref>; <ref type="bibr" target="#b61">Zhu et al. (2019)</ref> proposed new methods for preserving privacy, and <ref type="bibr" target="#b45">Reisizadeh et al. (2020)</ref>; <ref type="bibr" target="#b6">Dai et al. (2019)</ref>; <ref type="bibr" target="#b3">Basu et al. (2020)</ref>; <ref type="bibr" target="#b30">Li et al. (2020b)</ref>; Stich (2018) focused on reducing communication cost. While some methods assume a homogeneous setup, in which all clients share a common data distribution <ref type="bibr" target="#b52">(Wang &amp; Joshi, 2018;</ref><ref type="bibr" target="#b32">Lin et al., 2018)</ref>, others tackle the more challenging heterogeneous setup in which each client is equipped with its own data distribution <ref type="bibr" target="#b59">(Zhou &amp; Cong, 2017;</ref><ref type="bibr" target="#b15">Hanzely &amp; Richtárik, 2020;</ref><ref type="bibr" target="#b58">Zhao et al., 2018;</ref><ref type="bibr" target="#b47">Sahu et al., 2018;</ref><ref type="bibr" target="#b20">Karimireddy et al., 2019;</ref><ref type="bibr" target="#b14">Haddadpour &amp; Mahdavi, 2019;</ref><ref type="bibr" target="#b16">Hsu et al., 2019)</ref>.</p><p>Perhaps the most known and commonly used FL algorithm is FedAvg <ref type="bibr" target="#b37">(McMahan et al., 2017a)</ref>. It learns a global model by aggregating local models trained on IID data. However, the above methods learn a shared global model for all clients instead of personalized per-client solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Personalized Federated Learning</head><p>The federated learning setup presents numerous challenges including data heterogeneity (differences in data distribution), device heterogeneity (in terms of computation capabilities, network connection, etc.), and communication efficiency . Especially data heterogeneity makes it hard to learn a single shared global model that applies to all clients. To overcome these issues, Personalized Federated Learning (PFL) aims to personalize the global model for each client in the federation <ref type="bibr" target="#b24">(Kulkarni et al., 2020)</ref>. Many papers proposed a decentralized version of the model agnostic meta-learning (MAML) problem <ref type="bibr" target="#b10">(Fallah et al., 2020a;</ref><ref type="bibr" target="#b29">Li et al., 2017;</ref><ref type="bibr" target="#b5">Behl et al., 2019;</ref><ref type="bibr" target="#b60">Zhou et al., 2019;</ref><ref type="bibr" target="#b11">Fallah et al., 2020b)</ref>. Since MAML approach relies on the Hessian matrix, which is computationally costly, several works attempted to approximate the Hessian <ref type="bibr" target="#b12">(Finn et al., 2017;</ref><ref type="bibr" target="#b44">Nichol et al., 2018)</ref>. Another approach to PFL is model mixing where the clients learn a mixture of the global and local models <ref type="bibr" target="#b7">(Deng et al., 2020;</ref><ref type="bibr" target="#b1">Arivazhagan et al., 2019)</ref>. <ref type="bibr" target="#b15">Hanzely &amp; Richtárik (2020)</ref> introduced a new neural network architecture that is divided into base and personalized layers. The central model trains the base layers by FedAvg and the personalized layers (also called top layers) are trained locally. <ref type="bibr">Liang et al. (2020) presented</ref> LG-FedAvg a mixing model where each client obtains local feature extractor and global output layers. This is an opposite approach to the conventional mixing model that enables lower communication costs as the global model requires fewer parameters. Other approaches to train the global and local models under different regularization . <ref type="bibr" target="#b8">Dinh et al. (2020)</ref> introduced pFedMe, a method that uses Moreau envelops as the client regularized loss. This regularization helps to decouple the personalized and global model optimizations. Alternatively, clustering methods for federated learning assume that the local data of each client is partitioned by nature <ref type="bibr" target="#b36">(Mansour et al., 2020)</ref>. Their goal is to group together similar clients and train a centralized model per group. In case of heterogeneous setup, some clients are "closer" than others in terms of data distribution. Based on this assumption and inspired by FedAvg, <ref type="bibr" target="#b57">Zhang et al. (2020)</ref> proposed pFedFOMO, an aggregation method where each client only federates with a subset of relevant clients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Hypernetworks</head><p>Hypernetworks (HNs) <ref type="bibr" target="#b21">(Klein et al., 2015;</ref><ref type="bibr" target="#b46">Riegler et al., 2015;</ref><ref type="bibr" target="#b13">Ha et al., 2017)</ref> are deep neural networks that output the weights of another target network, that performs the learning task. The idea is that the output weights vary depending on the input to the hypernetwork.</p><p>HNs are widely used in various machine learning domains, including language modeling <ref type="bibr" target="#b50">(Suarez, 2017)</ref>, computer vi- sion <ref type="bibr" target="#b13">(Ha et al., 2017;</ref><ref type="bibr" target="#b22">Klocek et al., 2019)</ref>, continual learning <ref type="bibr" target="#b51">(von Oswald et al., 2019)</ref>, hyperparameter optimization <ref type="bibr" target="#b33">(Lorraine &amp; Duvenaud, 2018;</ref><ref type="bibr" target="#b35">MacKay et al., 2019;</ref><ref type="bibr" target="#b2">Bae &amp; Grosse, 2020)</ref>, multi-objective optimization <ref type="bibr" target="#b43">(Navon et al., 2021)</ref>, and decoding block codes <ref type="bibr" target="#b42">(Nachmani &amp; Wolf, 2019)</ref>.</p><p>HNs are naturally suitable for learning a diverse set of personalized models, as HNs dynamically generate target networks conditioned on the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we first formalize the personalized federated learning (PFL) problem, then we present our personalized Federated HyperNetworks (pFedHN) approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation</head><p>Personalized federated learning (PFL) aims to collaboratively train personalized models for a set of n clients, each with its own personal private data. Unlike conventional FL, each client i is equipped with its own data distribution P i on X × Y. Assume each client has access to</p><formula xml:id="formula_0">m i IID samples from P i , S i = {(x (i) j , y (i) j )} mi i=1 . Let i : Y × Y → R +</formula><p>denote the loss function corresponds to client i, and L i the average loss over the personal training data L i (θ i ) = 1 mi j i (x j , y j ; θ i ). Here θ i denotes the personal model of client i. The PFL goal is to optimize</p><formula xml:id="formula_1">Θ * = arg min Θ 1 n n i=1 E x,y∼Pi [ i (x j , y j ; θ i )]</formula><p>(1) and the training objective is given by</p><formula xml:id="formula_2">arg min Θ 1 n n i=1 L i (θ i ) = arg min Θ 1 n n i=1 1 m i mi j=1 i (x j , y j ; θ i )<label>(2)</label></formula><p>where Θ denotes the collection of all personal model param-</p><formula xml:id="formula_3">eters {θ i } n i=1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Federated Hypernetworks</head><p>In this section, we describe our proposed personalized Federated Hypernetworks (pFedHN), a novel method for solving the PFL problem (eq. 2) using hypernetworks. Hypernetworks are deep neural networks that output the weights of another network, conditioning on its input. Intuitively, HNs simultaneously learn a family of target networks. Let h(·; ϕ) denote the hypernetwork parametrized by ϕ and f (·; θ) the target network parametrized by θ. The hypernetwork is located at the server and acts on a client descriptor v i (see <ref type="figure" target="#fig_0">Figure 1</ref>). The descriptor can be a trainable embedding vector for the client or fixed, provided that a good client representation is known a-priori. Given v i the HN outputs the weights for the i th client θ i = θ i (ϕ) := h(v i ; ϕ). Hence, the HN h learns a family of personalized models {h(v i ; ϕ) | i ∈ [n]}. pFedHN provides a natural way for sharing information across clients while maintaining the flexibility of personalized models, by sharing the parameters ϕ.</p><p>We adjust the PFL objective (eq. 2) according to the above setup to obtain arg min ϕ,v1,...,vn</p><formula xml:id="formula_4">1 n n i=1 L i (h(v i ; ϕ)).<label>(3)</label></formula><p>One crucial and attractive property of pFedHN is that it decouples the size of h and the communication cost. The amount of data transferred is determined by the size of the target network during the forward and backward communications, and does not depend on the size of h. Consequently, the hypernetwork can be arbitrarily large without impairing communication efficiency. Indeed, using the chain rule we Algorithm 1 Personalized Federated Hypernetwork input: R -number of rounds, K -number of local rounds, α -learning rate, η -client learning rate</p><formula xml:id="formula_5">for r = 1, ..., R do sample client i ∈ [n] set θi = h(vi; ϕ) andθi = θi for k = 1, ..., K do sample mini-batch B ⊂ Sĩ θi =θi − η∇θ i Li(B) ∆θi =θi − θi ϕ = ϕ − α∇ϕθ T i ∆θi vi = vi − α∇v i ϕ T ∇ϕθ T i ∆θi return: ϕ have ∇ ϕ L i = (∇ ϕ θ i ) T ∇ θi L i</formula><p>so the client only needs to communicate ∇ θi L i back to the hypernetwork, which has the same size as the personal network parameters θ i .</p><p>In our work, we used a more general update rule ∆ϕ = (∇ ϕ θ i ) T ∆θ i where ∆θ i is the change in the local model parameters after several local update steps. As the main limitation is the communication cost, we found it beneficial to perform several local update steps, on the client side per communication round. This aligns with prior work that highlighted the benefits of local optimization steps in terms of both convergence speed (hence communication cost) and final accuracy <ref type="bibr" target="#b37">(McMahan et al., 2017a;</ref><ref type="bibr" target="#b18">Huo et al., 2020)</ref>. Given the current personalized parameters θ i , we perform several local optimization steps on the personal data to obtainθ i . We then return the personal model update direction ∆θ i :=θ i − θ i therefore, the update for ϕ is given by (∇ ϕ θ i ) T (θ i − θ i ). This update rule is inspired by <ref type="bibr" target="#b56">Zhang et al. (2019)</ref>. Intuitively, suppose we have access to the optimal solution of the personal problem θ * i = arg min θi L i , then our update rule becomes the gradient of an approximation to the surrogate lossL i (v i , ϕ) = 1 2 ||θ * i −h(v i ; ϕ)|| 2 2 by replacing θ * i withθ i . In Appendix B ( <ref type="figure">Figure 5</ref>), we compare the results for a different number of local update steps and show considerable improvement over using the gradient, i.e., using a single step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Personal Classifier</head><p>In some cases, it is undesirable to learn the entire network end-to-end with a single hypernetwork. As an illustrative example, consider a case where clients differ only by the label ordering in their output vectors. In this case, having to learn the right label ordering per client adds another unnecessary difficulty if they were to learn the classification layer as well using the hypernetwork.</p><p>As another example, consider the case where each client solves an entirely separate task, similar to multitask learning, where the number of classes may differ between clients. It makes little sense to have the hypernetwork produce each unique task classification layer.</p><p>In these cases, it would be preferable for the hypernetwork to produce the feature extraction part of the target network, which contains most of the trainable parameters, while learning a local output layer for each client. Formally, let ω i denote the personal classifier parameters of client i. We modify the optimization problem (eq. 3) to obtain, arg min ϕ,v1,...,vn,ω1,...,ωn</p><formula xml:id="formula_6">1 n n i=1 L i (θ i , ω i ),<label>(4)</label></formula><p>where we define the feature extractor θ i = h(v i ; ϕ), as before. The parameters ϕ, v 1 , ..., v n are updated according to Alg. 1, while the personal parameters ω i are updated locally using</p><formula xml:id="formula_7">ω i = ω i − α∇ ωi L i .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Analysis</head><p>In this section, we theoretically analyze pFedHN. First, we provide an insight regarding the solution for the pFedHN (Eq. 3), using a simple linear version of our hypernetwork. Next, we describe the generalization bounds of our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">A Linear Model</head><p>Consider a linear version of the hypernetwork, where both the target model and the hypernetwork are linear models, θ i = W v i with ϕ := W ∈ R d×k and v i ∈ R k is the i th clients embedding. Let V denote the k × n matrix whose columns are the clients embedding vectors v i . We note that even for convex loss functions</p><formula xml:id="formula_8">L i (θ i ) the objective L(W, V ) = i L i (W v i ) might not be convex in (W, V )</formula><p>but block multi-convex. In one setting, however, we get a nice analytical solution.</p><p>Proposition 1. Let {X i , y i } be the data for client i and let the loss for linear regressor</p><formula xml:id="formula_9">θ i be L i (θ i ) = X i θ i − y i 2 . Furthermore assume for all i, X T i X i = I d . Define the empirical risk minimization (ERM) solution for client i as θ i = arg min θ∈R d X i θ −y i 2 . The optimal W, V minimiz- ing i X i W v i − y i</formula><p>2 are given by PCA on {θ 1 , ...,θ n }, where W is the top k principle components and v i is the coefficients forθ i in these components.</p><p>We provide the proof in Section A of the Appendix. The linear version of our pFedHN performs dimensionality reduction by PCA, but unlike classical dimensionality reduction which is unaware of the learning task, pFedHN uses multiple clients for reducing the dimensionality while preserving the optimal model as best as possible. This allows us to get solutions between the two extremes: A single shared model up to scaling (k = 1) and each client training locally (k ≥ n). We note that optimal reconstruction of the local models (k ≥ n) is generally suboptimal in terms of generalization performance, as no information is shared across clients.</p><p>This dimensionality reduction can also be viewed as a denoising process. Assume a linear regression with Gaussian noise model, i.e., for all clients p(y|x) = N (x T θ * i , σ 2 ) and that each client solves a maximum likelihood objective. From the central limit theorem for maximum likelihood estimators <ref type="bibr" target="#b53">(White, 1982)</ref> </p><formula xml:id="formula_10">we get that 1 √ n i (θ i − θ * i ) d −→ N (0, I d ) whereθ i is the maximum likelihood solution. This means that approximatelyθ i = θ * i + with ∼ N (0, σ i I),</formula><p>i.e., our local solutionsθ i are a noisy version of the optimal model θ * i with isotropic Gaussian noise. We can now view the linear hypernetworks as performing denoising onθ i , by PCA. PCA is a classic approach to denoising <ref type="bibr" target="#b41">(Muresan &amp; Parks, 2003)</ref> and is well suited for reducing isotropic noise when the energy of the original points is concentrated on a small dimensional subspace. Intuitively we think of our standard hypernetwork as a nonlinear extension of this approach, which has a similar effect by forcing the models to lay on a low-dimensional manifold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Generalization</head><p>We now investigate how pFedHN generalizes using the approach of <ref type="bibr" target="#b4">Baxter (2000)</ref>. The common approach for multitask learning with neural networks is to have a common feature extractor shared by all tasks and a per-task head operating on these features. This case was analyzed by <ref type="bibr" target="#b4">Baxter (2000)</ref>. Conversely, here the per-task parameters are the inputs to the hypernetwork. Next, we provide the generalization guarantee under this setting and discuss its implications.</p><formula xml:id="formula_11">Let D i = (x (i) j , y (i) j ) m j=1</formula><p>be the training set for the i th client, generated by a distribution P i . We denote byL D (ϕ, V ) the empirical loss of the hypernetwork</p><formula xml:id="formula_12">L D (ϕ, V ) = 1 n n i=1 1 m m j=1 i x (i) j , y (i) j ; h(ϕ, v i ) and by L(ϕ, V ) the expected loss L(ϕ, V ) = 1 n n i=1 E Pi [ i (x, y; h(ϕ, v i ))]</formula><p>. We assume weights of the hypernetwork and the embeddings are bounded in a ball of radius R, in which the following three Lipschitz conditions hold:</p><formula xml:id="formula_13">1. | i (x, y, θ 1 ) − i (x, y, θ 2 )| ≤ L θ 1 − θ 2 2. h(ϕ, v) − h(ϕ , v) ≤ L h ϕ − ϕ 3. h(ϕ, v) − h(ϕ, v ) ≤ L V v − v .</formula><p>Theorem 1. Let the hypernetwork parameter space be of dimension N and the embedding space be of dimension k. Under previously stated assumptions, there exists</p><formula xml:id="formula_14">M = O k 2 log RL(L h +L V ) δ + N n 2 log RL(L h +L V )</formula><p>δ such that if the number of samples per client m is greater than M , we have with probability at least</p><formula xml:id="formula_15">1 − δ for all ϕ, V that |L(ϕ, V ) −L D (ϕ, V )| ≤ .</formula><p>Theorem 1 provides insights on the parameter-sharing effect of pFedHN. The first term for the number of required samples M depends on the dimension of the embedding vectors; as each client corresponds to its unique embedding vector (i.e. not being shared between clients), this part is independent of the number of clients n. However, the second term depends on the size of the hypernetwork N , is reduced by a factor n, as the hypernetwork's weights are shared.</p><p>Additionally, the generalization is affected by the Lipschitz constant of the hypernetwork, L h (along with other Lipschitz constants), as it can affect the effective space we can reach with our embedding. In essence, this characterizes the price that we pay, in terms of generalization, for the hypernetworks flexibility. It might also open new directions to improve performance. However, our initial investigation into bounding the Lipschitz constant by adding spectral normalization <ref type="bibr" target="#b39">(Miyato et al., 2018)</ref> did not show any significant improvement, see Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We evaluate pFedHN in several learning setups using three common image classification datasets: CIFAR10, CI-FAR100, and Omniglot <ref type="bibr" target="#b23">(Krizhevsky &amp; Hinton, 2009;</ref><ref type="bibr" target="#b25">Lake et al., 2015)</ref> Unless stated otherwise, we report the Federated Accuracy, defined as 1</p><formula xml:id="formula_16">n i 1 mi j Acc f i x (i) j , y (i) j</formula><p>, averaged over three seeds. The experiments show that pFedHN outperforms classical FL approaches and leading PFL models.  Training Strategies: In all experiments, our target network shares the same architecture as the baseline models. Our hypernetwork is a simple fully-connected neural network, with three hidden layers and multiple linear heads per target weight tensor. We limit the training process to at-most 5000 server-client communication steps for most methods. One exception is LG-FedAvg which utilizes a pretrained FedAvg model, hence it is trained with additional 1000 communication steps. The Local baseline is trained for 2000 optimization steps on each client. For pFedHN, we set the number of local steps to K = 50, and the embedding dimension to 1 + n/4 , where n is the number of clients. We provide an extensive ablation study on design choices in Appendix C. We tune the hyperparameters of all methods using a pre-allocated held-out validation set. Full experimental details are provided in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Heterogeneous Data</head><p>We evaluate the different approaches on a challenging heterogeneous setup. We adopt the learning setup and the evaluation protocol described in <ref type="bibr" target="#b8">Dinh et al. (2020)</ref> for generating heterogeneous clients in terms of classes and size of local training data. First, we sample two/ten classes for each client for CIFAR10/CIFAR100; Next, for each client i and selected class c, we sample α i,c ∼ U (.4, .6), and assign it with αi,c j αj,c of the samples for this class. We repeat the above using 10, 50 and 100 clients. This procedure produces clients with different number of samples and classes. For the target network, we use a LeNet-based (Le-Cun et al., 1998) network with two convolution and two fully connected layers.</p><p>We also evaluate all methods using the Omniglot dataset <ref type="bibr" target="#b25">(Lake et al., 2015)</ref>. Omniglot contains 1623 different grayscale handwritten characters (with 20 samples each), from 50 different alphabets. Each alphabet obtains a varying number of characters. In this setup, we use 50 clients and assign an alphabet to each client. Therefore, clients receives different numbers of samples and the distribution of labels is disjoint across clients. We use a LeNet-based model with four convolution and two fully connected layers.</p><p>The results are presented in <ref type="table" target="#tab_0">Table 1</ref>. The two simple baselines, local and FedAvg, that do not use personalized federated learning perform quite poorly on most tasks 2 , showing the importance of personalized federated learning. pFedHN achieves large improvements of 2%-10% over all competing approaches. Furthermore, on the Omniglot dataset, where each client is allocated with a completely different learning task (different alphabet), we show significant improvement using pFedHN-PC. We present additional results on the MNIST dataset in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Computational Budget</head><p>We discussed above how the challenges of heterogeneous data can be handled using pFedHN. Another major challenge presented by personalized FL is that the communication, storage, and computational resources of clients may differ significantly. These capacities may even change in time due to varying network and power conditions. In such a setup, the server should adjust to the communication and computational policies of each client. Unfortunately, previous works do not address this resource heterogeneity. pFedHN can naturally adapt to this challenging learning setup by producing target networks of different sizes.</p><p>In this section, we evaluate the capacity of pFedHN to handle clients that differ in their computational and communication resource budget. We use the same split described in Section 5.1 with a total of 75 clients divided into the three equal-sized groups named S (small), M (medium), and L (large). The Models of clients within each group share the same architecture. The three architectures (of the three groups) have a different number of parameters.</p><p>We train a single pFedHN to output target client models of different sizes. Importantly, this allows pFedHN to share parameters between all clients even if those have different local model sizes. Baselines: For quantitative comparisons, and since the existing baseline methods cannot easily extend to this setup, we train three independent per-group models. Each group is trained for 5000 server-client communication steps. See details in Appendix C for further details.</p><p>The results are presented in <ref type="table" target="#tab_1">Table 2</ref>, showing that pFedHN achieves 4% − 8% improvement over all competing methods. The results demonstrate the flexibility of our approach, which is capable of adjusting to different client settings while maintaining high accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Generalization to Novel Clients</head><p>Next, we study an important learning setup where new clients join, and a new model has to be trained for their data.</p><p>In the general case of sharing models across clients, this would require retraining (or finetuning) the shared model. While PFL methods like pFedME <ref type="bibr" target="#b8">(Dinh et al., 2020)</ref> and Per-FedAvg <ref type="bibr" target="#b10">(Fallah et al., 2020a)</ref> can adapt to this setting by finetuning the global model locally, pFedHN architecture offers a significant benefit. Since the shared model learns a meta-model over the distribution of clients, it can in principle generalize to new clients without retraining. With pFedHN, once the shared model ϕ has been trained on a set of clients, extending to a new set of novel clients requires little effort. We freeze the hypernetwork weights ϕ and optimize an embedding vector v new . Since only a small number of parameters are being optimized, training is less prone to overfitting compared to other approaches. The success of this process depends on the capacity of the hypernetwork to learn the distribution over clients and generalize to clients that have different data distributions.</p><p>To evaluate pFedHN in this setting, we use the CIFAR10 dataset, with a total of 100 clients, of which 90 are used for training and 10 are held out novel clients. To allocate data samples, for each client i we first draw a sample from a Dirichlet distribution with parameter α = (α, ..., α) ∈ R 10 , p i ∼ Dir(α). Next we normalize the p i 's so that i p i,j = 1 for all j, to obtain the vectorp i . We now allocate samples according to thep i 's. For the training clients, we choose α = .1, whereas for the novel clients we vary α ∈ {.1, .25, .5, 1}. To estimate the "distance" between a novel client and the training clients, we use the total variation (TV) distance between the novel client and its nearest neighbor in the training set. The TV is computed over the empirical distributionsp. <ref type="figure" target="#fig_2">Figure 2</ref> presents the accuracy generalization gap as a function of the total variation distance. pFedHN achieves the best generalization performance for all levels of TV (corresponds to the different values for α). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Heterogeneity of personalized classifiers</head><p>We further investigate the flexibility of pFedHN-PC in terms of personalizing different networks for different clients. Potentially, since clients have their own personalized classifier ω i , the feature extractor component θ i generated by the HN may in principle become strongly similar across clients, making the HN redundant. The empirical results show that this is not the case because pFedHN-PC out-performs Fed-Per <ref type="bibr" target="#b1">(Arivazhagan et al., 2019)</ref>. However this raises a fundamental question about the interplay between local and shared personalized components. We provide additional insight to this topic by answering the question: do the feature extraction layers generated by the pFedHN-PC's hypernetwork significantly differ from each other? To investigate the level of personalization in θ i achieved by pFedHN-PC, we first train it on CIFAR10 dataset split among ten clients, with two classes assigned to each client. Next, for each client, we replace its feature extractor θ i with that of another client θ j while keeping its personal classifier ω i unaltered. <ref type="figure" target="#fig_3">Figure 3</ref> depicts the normalized accuracy in this mix-and-match experiment. Rows correspond to a client, and columns correspond to the feature extractor of another client. <ref type="figure" target="#fig_3">Figure 3</ref> are of interest. First, pFedHN-PC produces personalized feature extractors for each client since the accuracy achieved when crossing classifiers and feature extractors varies significantly. Second, some pairs of clients can be crossed without hurting the accuracy. Specifically, we had two clients learning to discriminate horse vs. dog. Interestingly, the client with ship and airplane classes performs quite well when presented with truck and bird, presumably because both of their feature extractors learned to detect sky.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Several effects in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Learned Client Representation</head><p>In our experiments, we learn to represent each client using a trainable embedding vector v i . These embedding vectors therefore learn a continuous semantic representation over the set of clients. The smooth nature of this representation gives the HN the power to share information across clients. We now wish to study the structure of that embedding space.</p><p>To examine how the learned embedding vectors reflect a meaningful representation over the client space, we utilize the hierarchy in CIFAR100 for generating clients with similar data distribution of semantically similar labels. Concretely, we split the CIFAR100 into 100 clients, where each client is assigned with data from one out of the twenty coarse classes uniformly (i.e., each coarse class is assigned to five clients).</p><p>In <ref type="figure" target="#fig_4">Figure 4</ref> we project the learned embedding vectors into R 2 using the t-SNE algorithm <ref type="bibr" target="#b34">(Maaten &amp; Hinton, 2008)</ref>. A clear structure is presented, in which clients from the same group (in terms of coarse labels) are clustered together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we present a novel approach for personalized federated learning. Our method trains a central hypernetwork to output a unique personal model for each client. We show through extensive experiments significant improvement in accuracy on all datasets and learning setups.</p><p>Sharing across clients through a central hypernetwork has several benefits compared to previous architectures. First, since it learns a unified model over the distribution of clients, the model generalizes better to novel clients, without the need to retrain the central model. Second, it naturally extends to handle clients with different compute power, by generating client models of different sizes. Finally, this architecture decouples the problem of training complexity from communication complexity, since local models that are transmitted to clients can be significantly more compact than the central model.</p><p>We expect that the current framework can be further extended in several important ways. First, the architecture opens questions about the best way of allocating learning capacity to a central model vs distributed components that are trained locally. Second, the question of generalization to clients with new distribution awaits further analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material for Personalized Federated Learning by Hypernetworks</head><p>A. Proof of Results</p><p>Proof for Proposition 1. Letθ i denote the optimal solution at client i,</p><formula xml:id="formula_17">thenθ i = (X T i X i ) −1 X T i y i = X T i y i . Denote θ i = W v i , we have arg min θi X i θ i − y i 2 2 = arg min θi (X i θ i − y i ) T (X i θ i − y i ) = arg min θi θ T i X T i X i θ i − 2θ T i X T i y + y T i y i = arg min θi θ T i θ i − 2 θ i ,θ i + y T i y i = arg min θi θ T i θ i − 2 θ i ,θ i + θ i 2 2 = arg min θi θ i −θ i 2 2</formula><p>Thus, our optimization problem becomes arg min W,V i W v i −θ i 2 2 . WLOG, we can optimize W over the set of all matrices with orthonormal columns, i.e. W T W = I. Since for each solution (W, V ) we can obtain the same loss for (W R, R −1 V ), and select a R that performs Gram-Schmidt on the columns of W . In case of fixed W the optimal solution for v i is given by v * i = (W T W ) −1 W Tθ i = W Tθ i . Hence, our optimization problem becomes, arg min</p><formula xml:id="formula_18">W ;W T W =I i W W Tθ i −θ i 2 2 ,</formula><p>which is equivalent to PCA on {θ i } i .</p><p>Proof for Theorem 1. We note a log(1/ ) factor missing in the statement of Theorem 1 in the paper, the correct statement should use</p><formula xml:id="formula_19">M = O k 2 log RL(L h +L V ) δ + N n 2 log RL(L h +L V ) δ .</formula><p>Using Theorem 4 from <ref type="bibr" target="#b4">(Baxter, 2000)</ref> and the notation used in that paper, we get that M = O 1 n 2 log</p><formula xml:id="formula_20">C( ,H n l ) δ where C( , H n l )</formula><p>is the covering number for H n l . In our case each element of H n l is parametrized by ϕ, v 1 , ..., v n and the distance is given by</p><formula xml:id="formula_21">d((ϕ, v1, ..., vn), (ϕ , v 1 , ..., v n )) = (5) E x i ,y i ∼P i 1 n (h(ϕ, vi)(xi), yi) − (h(ϕ , v i )(xi), yi)</formula><p>From the triangle inequality and our Lipshitz assumptions we get</p><formula xml:id="formula_22">d((ϕ, v 1 , ..., v n ), (ϕ , v 1 , ..., v n )) ≤ (6) 1 n E xi,yi∼Pi [| (h(ϕ, v i )(x i ), y i ) − (h(ϕ , v i )(x i ), y i )|] ≤ L h(ϕ, v i ) − h(ϕ , v i ) ≤ L h(ϕ, v i ) − h(ϕ, v i ) + L h(ϕ, v i ) − h(ϕ , v i ) ≤ L · L h ϕ − ϕ + L · L V v − v</formula><p>Now if we select a covering of the parameter space such that each ϕ has a point ϕ that is 2L(L h +L V ) away and each embedding v i has an embedding v i at the same distance we get an -covering in the d((ϕ, v 1 , ..., v n ), (ϕ , v 1 , ..., v n )) metric.</p><p>From here we see that log(C( , H n l )) = O (n · k + N ) log RL(L V +L h ) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Details</head><p>For all experiments presented in the main text, we use a fully-connected hypernetwork with 3 hidden layers of 100 hidden units each. For all relevant baselines, we aggregate over 5 clients at each round. We set K = 3 ,i.e., 60 local steps, for the pFedMe algorithm, as it was reported to work well in the original paper <ref type="bibr" target="#b8">(Dinh et al., 2020)</ref>.</p><p>Heterogeneous Data (Section 5.1). For the CIFAR experiments, we pre-allocate 10, 000 training examples for validation. For the Omniglot dataset, we use a 70%/15%/15% split for train/validation/test sets. The validation sets are used for hyperparameter tuning and early stopping. We search over learning-rate {1e − 1, 5e − 2, 1e − 2, 5e − 3}, and personal learning-rate {5e − 2, 1e − 2, 5e − 3, 1e − 3} for PFL methods using 50 clients. For the CIFAR datasets, the selected hyperparameters are used across all number of clients (i.e. 10, 50, 100).</p><p>Computational Budget (Section 5.2) We use the same hyperparameters selected in Section 5.1. To align with previous works <ref type="bibr" target="#b8">(Dinh et al., 2020;</ref><ref type="bibr" target="#b31">Liang et al., 2020;</ref><ref type="bibr" target="#b10">Fallah et al., 2020a)</ref>, we use a LeNet-based (target) network with two convolution layers, where the second layer has twice the number of filters in comparison to the first. Following these layers are two fully connected layers that output logits vector. In this learning setup, we use three different sized target networks with different numbers of filters for the first convolution layer. Specifically, for S/M/L sized networks, the first convolution layer consists of 8/16/32 filters, respectively. pFedHN's HN produces weights vector with size equal to the sum of the weights of the three sized networks combined. Then it sends the relevant weights according to the target network size of the client.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. MNIST</head><p>We provide additional experiment over MNIST dataset. We follow the same data partition procedure as in the CI-FAR10/CIFAR100 heterogeneity experiment, described in Section 5.1.</p><p>For this experiment we use a single hidden layer fullyconnected (FC) hypernetwork. The main network (or target network in the case of pFedHN) is a single hidden layer FC NN.</p><p>All FL/PFL methods achieve high classification accuracy on this dataset, which makes it difficult to attain meaningful comparisons. The results are presented in <ref type="table" target="#tab_2">Table 3</ref>. pFedHN achieves similar results to pFedMe. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Exploring Design Choices</head><p>In this section we return to the experimental setup of Section 5.1, and evaluate pFedHN using CIFAR10 dataset with 50 clients. First, we examine the effect of the local optimization steps. Next, we vary the capacity of the HN and observe the change in classification accuracy. Finally we vary the dimension of the client representation (embedding).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2.1. EFFECT OF LOCAL OPTIMIZATION</head><p>First, we examine the effect of performing local optimization step and transmitting ∆θ back to the hypernetwork. <ref type="figure">Figure 5</ref> shows the test accuracy throughout the training process. It compares training using the standard chain rule (steps = 1) with the case of training locally for k steps, k ∈ {25, 50, 100, 200}. Using our proposed update rule, i.e., making multiple local update steps, yields large improvements in both convergence speed and final accuracy, compared to using the standard chain rule (i.e., k = 1). The results show that pFedHN is relatively robust to the choice <ref type="figure">Figure 5</ref>. Effect of the number of local optimization steps on the test accuracy for the CIFAR10 dataset.</p><p>of local local optimization steps. As stated in the main text we set k = 50 for all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2.2. CLIENT EMBEDDING DIMENSION</head><p>Next, we investigate the effect of embedding vector dimension on pFedHN performance. Specifically, we run an ablation study on set of different embedding dimensions {5, 15, 25, 35}. The results are presented in <ref type="figure">Figure 6</ref> (a). We show pFedHN robustness to the dimension of the client embedding vector; hence we fix the embedding dimension through all experiments to 1+n/4 , where n is the number of client.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2.3. HYPERNETWORK CAPACITY</head><p>Here we inspect the effect of the HN's capacity on the local networks performance. We conducted an experiment in which we change the depth of the HN by stacking fully connected layers.</p><p>We evaluate pFedHN on CIFAR10 dataset using k ∈ {1, 2, 3, 4, 5} hidden layers. <ref type="figure">Figure 6</ref> (b) presents the final test accuracy. pFedHN achieves optimal performance with k = 3 and k = 4 hidden layers, with accuracies 88.38 and 88.42 respectively. We use a three hidden layers HN for all experiments in the main text. We show in Theorem 1 that the generalization is affected by the hypernetworks Lipschitz constant L h . This theoretical result suggests that we can benefit from bounding this (a) (b) <ref type="figure">Figure 6</ref>. Test results on CIFAR10 showing the effect of (a) the dimension of the the client embedding vector, and; (b) the number of hypernetwork's hidden layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Spectral Normalization</head><p>constant. Here we empirically test this by applying spectral normalization <ref type="bibr" target="#b39">(Miyato et al., 2018)</ref> for all layers of the HN. The results are presented in <ref type="table" target="#tab_3">Table 4</ref>. We do not observe any significant improvement compared to the results without spectral normalization (presented in <ref type="table" target="#tab_0">Table 1</ref> of the main text). Here we provide additional results on the generalization performance for novel clients, studied in Section 5.3 of the main text. <ref type="figure" target="#fig_5">Figure 7</ref> shows the accuracy of individual clients as a function of the total variation distance. Each point represents a different client, where the total variation distance is calculated w.r.t to the nearest training set client. As expected, the results show (on average) that the test accuracy decreases with the increase in the total variation distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4. Generalization to Novel Clients</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5. Fixed Client Representation</head><p>We wish to compare the performance of pFedHN when trained with a fixed vs trainable client embedding vectors. We use CIFAR10 with the data split described in Section 5.3 of the main text and 50 clients. We use a client embedding dimension of 10. We set the fixed embedding vector for client i to the vector of class proportions,p i , described in Section 5.3. pFedHN achieves similar performance with both the trainable and fixed client embedding, 84.12 ± 0.42 and 83.92 ± 0.36 respectively.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The Federated hypernetwork framework. (a) An HN is located on the server and communicate personal model for each clients. In turn, the clients send back the update direction ∆θi; (b) The HN acts on the client embedding vi to produce model weights θi. The client performs several local optimization steps to obtainθi, and sends back the update direction ∆θi =θi − θi.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Compared Methods: We evaluate and compare the following approaches: (1) pFedHN, Our proposed Federated HyperNetworks (2) pFedHN-PC, pFedHN with a personalized classifier per client ; (3) Local, Local training on each client, with no collaboration between clients; (4) Fe-dAvg (McMahan et al., 2017a), one of the first and perhaps the most widely used FL algorithm; (5) Per-FedAvg (Fallah et al., 2020a) a meta-learning based PFL algorithm. (6) pFedMe<ref type="bibr" target="#b8">(Dinh et al., 2020)</ref>, a PFL approach which adds a Moreau-envelopes loss term;(7)LG-FedAvg<ref type="bibr" target="#b31">(Liang et al., 2020)</ref> PFL method with local feature extractor and global output layers; (8) FedPer<ref type="bibr" target="#b1">(Arivazhagan et al., 2019)</ref> a PFL approach that learns per-client personal classifier on top of a shared feature extractor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Generalization to novel clients. The accuracy generalization gap between training and novel clients, defined as acc novel − acctrain, where acc denotes the average accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Model personalization. Rows correspond to clients, each with their trained in a binary classification task and keeping their personalized classifier ωi. Columns correspond to the feature extractor θj of another client. The diagonal corresponds to the stanard training with θi and ωi. For better visualization, values denote accuracy normalized per row: norm-acci,j = (acci,j − min acc i, )/(max acc i, − min acc i, ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>t-SNE visualization of the learned client representation v for the CIFAR100 dataset. Clients are tasked with classifying classes that belong to the same coarse class. Clients marked with the same color correspond to the same coarse-class, see text for details. pFedHN clustered together clients from the same group.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Accuracy for novel clients on the CIFAR10 test set. Each point represents a different client. Total variation is computed w.r.t the nearest training set client.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Heterogeneous data. Test accuracy over 10, 50, 100 clients on the CIFAR10, CIFAR100, and Omniglot datasets.</figDesc><table><row><cell></cell><cell></cell><cell>CIFAR10</cell><cell></cell><cell></cell><cell>CIFAR100</cell><cell></cell><cell>Omniglot</cell></row><row><cell># clients</cell><cell>10</cell><cell>50</cell><cell>100</cell><cell>10</cell><cell>50</cell><cell>100</cell><cell>50</cell></row><row><cell>Local</cell><cell>86.46 ± 4.02</cell><cell>68.11 ± 7.39</cell><cell>59.32 ± 5.59</cell><cell>58.98 ± 1.38</cell><cell>19.98 ± 1.41</cell><cell>15.12 ± 0.58</cell><cell>65.97 ± 0.86</cell></row><row><cell>FedAvg</cell><cell>51.42 ± 2.41</cell><cell>47.79 ± 4.48</cell><cell>44.12 ± 3.10</cell><cell>15.96 ± 0.55</cell><cell>15.71 ± 0.35</cell><cell>14.59 ± 0.40</cell><cell>41.61 ± 3.59</cell></row><row><cell>Per-FedAvg</cell><cell>76.65 ± 4.84</cell><cell>83.03 ± 0.25</cell><cell>80.19 ± 1.99</cell><cell>50.14 ± 1.06</cell><cell>45.89 ± 0.76</cell><cell>48.28 ± 0.70</cell><cell>76.46 ± 0.17</cell></row><row><cell>FedPer</cell><cell>87.27 ± 1.39</cell><cell>83.39 ± 0.47</cell><cell>80.99 ± 0.71</cell><cell>55.76 ± 0.34</cell><cell>48.32 ± 1.46</cell><cell>42.08 ± 0.18</cell><cell>69.92 ± 3.12</cell></row><row><cell>pFedMe</cell><cell>87.69 ± 1.93</cell><cell>86.09 ± 0.32</cell><cell>85.23 ± 0.58</cell><cell>51.97 ± 1.29</cell><cell>49.09 ± 1.10</cell><cell>45.57 ± 1.02</cell><cell>69.98 ± 0.28</cell></row><row><cell>LG-FedAvg</cell><cell>89.11 ± 2.66</cell><cell>85.19 ± 0.58</cell><cell>81.49 ± 1.56</cell><cell>53.69 ± 1.42</cell><cell>53.16 ± 2.18</cell><cell>49.99 ± 3.13</cell><cell>72.99 ± 5.00</cell></row><row><cell>pFedHN (ours)</cell><cell>90.83 ± 1.56</cell><cell>88.38 ± 0.29</cell><cell>87.97 ± 0.70</cell><cell>65.74 ± 1.80</cell><cell>59.48 ± 0.67</cell><cell>53.24 ± 0.31</cell><cell>72.03 ± 1.08</cell></row><row><cell cols="4">pFedHN-PC (ours) 92.47 ± 1.63 90.08 ± 0.63 88.09 ± 0.86</cell><cell cols="2">68.15 ± 1.49 60.17 ± 1.63</cell><cell>52.4 ± 0.74</cell><cell>81.89 ± 0.15</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Computational budget. Test accuracy for CIFAR10/100 with 75 clients and varying computational capacities.</figDesc><table><row><cell></cell><cell></cell><cell>CIFAR10</cell><cell></cell><cell></cell><cell>CIFAR100</cell><cell></cell></row><row><cell>Local model size</cell><cell>S</cell><cell>M</cell><cell>L</cell><cell>S</cell><cell>M</cell><cell>L</cell></row><row><cell>FedAvg</cell><cell>36.91 ± 2.26</cell><cell>42.09 ± 2.37</cell><cell>47.51 ± 1.97</cell><cell>9.79 ± 0.19</cell><cell>11.76 ± 1.14</cell><cell>17.86 ± 2.42</cell></row><row><cell>Per-FedAvg</cell><cell>70.72 ± 2.57</cell><cell>72.33 ± 2.57</cell><cell>75.13 ± 0.61</cell><cell>41.49 ± 0.91</cell><cell>43.22 ± 0.16</cell><cell>44.03 ± 1.77</cell></row><row><cell>pFedMe</cell><cell>81.21 ± 1.23</cell><cell>84.08 ± 1.63</cell><cell>83.15 ± 2.45</cell><cell>39.91 ± 0.81</cell><cell>41.99 ± 0.55</cell><cell>44.93 ± 1.63</cell></row><row><cell>LG-FedAvg</cell><cell>73.93 ± 3.65</cell><cell>53.13 ± 3.49</cell><cell>54.72 ± 2.50</cell><cell>33.48 ± 4.83</cell><cell>29.15 ± 1.51</cell><cell>23.01 ± 1.41</cell></row><row><cell>pFedHN (ours)</cell><cell cols="3">85.38 ± 1.21 86.92 ± 1.35 87.20 ± 0.76</cell><cell cols="3">48.04 ± 0.89 48.66 ± 1.21 50.66 ± 2.70</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparison on the MNIST dataset.</figDesc><table><row><cell></cell><cell></cell><cell>MNIST</cell><cell></cell></row><row><cell></cell><cell>10</cell><cell>50</cell><cell>100</cell></row><row><cell>FedAvg</cell><cell>96.22 ± 0.65</cell><cell>97.12 ± 0.07</cell><cell>96.99 ± 0.19</cell></row><row><cell>Per-FedAvg</cell><cell>97.72 ± 0.05</cell><cell>98.57 ± 0.07</cell><cell>98.75 ± 0.26</cell></row><row><cell>pFedMe</cell><cell>99.40 ± 0.04</cell><cell>99.30 ± 0.13</cell><cell>99.12 ± 0.06</cell></row><row><cell>pFedHN (ours)</cell><cell>99.53 ± 0.16</cell><cell>99.28 ± 0.11</cell><cell>99.16 ± 0.19</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>pFedHN with spectral-normalization. 90.94 ± 2.18 87.02 ± 0.22 85.3 ± 1.81</figDesc><table><row><cell></cell><cell>CIFAR10</cell><cell></cell></row><row><cell>10</cell><cell>50</cell><cell>100</cell></row><row><cell>pFedHN (ours)</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that the Fisher matrix is the identity from our assumption that X T i Xi = I d .</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">In the 10-client split, each client sees on average 10% of the train set. It is sufficient for training a model locally.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This study was funded by a grant to GC from the Israel Science Foundation (ISF 737/2018), and by an equipment grant to GC and Bar-Ilan University from the Israel Science Foundation (ISF 2332/18). AS and AN were funded by a grant from the Israeli Innovation Authority, through the AVATAR consortium.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">cpsgd: Communication-efficient and differentially-private distributed sgd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">X X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcmahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7564" to="7575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Federated learning with personalization layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Arivazhagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choudhary</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.00818</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Delta-stn: Efficient bilevel optimization for neural networks using structured response jacobians. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Qsparselocal-sgd: Distributed sgd with quantization, sparsification, and local computations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Data</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Karakus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Diggavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal on Selected Areas in Information Theory</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="217" to="226" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A model of inductive bias learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baxter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="149" to="198" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Behl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Baydin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torr</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1905.07435</idno>
		<title level="m">Adaptive model-agnostic meta-learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Hyper-sphere quantization: Communicationefficient sgd for federated learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1911.04655</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Kamani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahdavi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.13461</idno>
		<title level="m">Adaptive personalized federated learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Personalized federated learning with moreau envelopes. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">T</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">H</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Privacy aware learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="57" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Personalized federated learning: A meta-learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mokhtari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ozdaglar</surname></persName>
		</author>
		<idno>abs/2002.07948</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On the convergence theory of gradient-based model-agnostic metalearning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mokhtari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ozdaglar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1082" to="1092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Model-agnostic metalearning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03400</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hypernetworks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arxiv</surname></persName>
		</author>
		<idno>abs/1609.09106</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">On the convergence of local descent methods in federated learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Haddadpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahdavi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.14425</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Federated learning of a mixture of global and local models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hanzely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Richtárik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05516</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Measuring the effects of non-identical data distribution for federated visual classification. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Personalized cross-silo federated learning on non-iid data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.02090</idno>
		<title level="m">Faster ondevice training using new federated momentum algorithm</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kairouz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Avent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Bhagoji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bonawitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cormode</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cummings</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.04977</idno>
		<title level="m">Advances and open problems in federated learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Karimireddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">U</forename><surname>Stich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scaffold</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.06378</idno>
		<title level="m">Stochastic controlled averaging for on-device federated learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A dynamic convolutional layer for short rangeweather prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afek</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4840" to="4848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hypernetwork functional image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Klocek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Maziarka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wołczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tabor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andśmieja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="496" to="510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Survey of personalization techniques for federated learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth World Conference on Smart Trends in Systems, Security and Sustainability (WorldS4)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="794" to="797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Federated learning systems: Vision, hype and reality for data privacy and protection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<idno>abs/1907.09693</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Federated learning: Challenges, methods, and future directions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Sahu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Talwalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="50" to="60" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meta-Sgd</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09835</idno>
		<title level="m">Learning to learn quickly for few-shot learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Acceleration for compressed gradient descent in distributed and federated optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Richtárik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.11364</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ziyin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">B</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Auerbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Brent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.01523</idno>
		<title level="m">Think locally, act globally: Federated learning with local and global representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Don&apos;t use large mini-batches, use local sgd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">U</forename><surname>Stich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.07217</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Stochastic hyperparameter optimization through hypernetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lorraine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<idno>abs/1802.09419</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Self-tuning networks: Bilevel optimization of hyperparameters using structured best-response functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mackay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vicol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lorraine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
		<idno>abs/1903.03088</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Three approaches for personalization with applications to federated learning. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Suresh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Communication-efficient learning of deep networks from decentralized data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hampson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Arcas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Learning differentially private recurrent language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.06963</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshida</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A survey on security and privacy of federated learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mothukuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Parizi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pouriyeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dehghantanha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Generation Computer Systems</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="619" to="640" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Adaptive principal components and image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Muresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">W</forename><surname>Parks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 2003 International Conference on Image Processing</title>
		<meeting>2003 International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Hyper-graph-network decoders for block codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nachmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">; H M</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Buc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garnett</forename></persName>
		</author>
		<ptr target="https://proceedings" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>R.</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="2326" to="2336" />
		</imprint>
	</monogr>
	<note>Wallach,</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning the pareto front with hypernetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Navon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shamsian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=NjF772F4ZZR" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">On first-order meta-learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Fedpaq: A communication-efficient federated learning method with periodic averaging and quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reisizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mokhtari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jadbabaie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pedarsani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2021" to="2031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Conditioned regression models for non-blind single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rüther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="522" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">On the convergence of federated optimization in heterogeneous networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Sahu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sanjabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Talwalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.06127</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Federated multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sanjabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4424" to="4434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Local sgd converges fast and communicates little</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">U</forename><surname>Stich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09767</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Character-level language modeling with recurrent highway hypernetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Suarez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3269" to="3278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Von Oswald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Henning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sacramento</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Grewe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00695</idno>
		<title level="m">Continual learning with hypernetworks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Cooperative sgd: A unified framework for the design and analysis of communication-efficient sgd algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Joshi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.07576</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Maximum likelihood estimation of misspecified models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Personalized federated learning for intelligent iot applications: A cloud-edge based framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Open Journal of the Computer Society</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="35" to="44" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<title level="m">Federated machine learning: Concept and applications. arXiv: Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Lookahead optimizer: k steps forward, 1 step back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="9597" to="9608" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Personalized federated learning with first order model optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.08565</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Suda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Civin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00582</idno>
		<title level="m">Federated learning with non-iid data</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">On the convergence properties of a k-step averaging stochastic gradient descent algorithm for nonconvex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.01012</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Efficient meta learning via minibatch proximal update</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1534" to="1544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Federated heavy hitters discovery with differential privacy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kairouz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.08534</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
