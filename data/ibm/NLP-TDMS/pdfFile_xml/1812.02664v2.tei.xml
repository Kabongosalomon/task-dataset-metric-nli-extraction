<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Recursive Visual Attention in Visual Dialog</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulei</forename><surname>Niu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Beijing Key Laboratory of Big Data Management and Analysis Methods School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<postCode>100872</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<postCode>639798</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
							<email>hanwangzhang@ntu.edu.sg</email>
							<affiliation key="aff1">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<postCode>639798</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manli</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Beijing Key Laboratory of Big Data Management and Analysis Methods School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<postCode>100872</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhong</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Beijing Key Laboratory of Big Data Management and Analysis Methods School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<postCode>100872</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Beijing Key Laboratory of Big Data Management and Analysis Methods School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<postCode>100872</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
							<email>jrwen@ruc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Beijing Key Laboratory of Big Data Management and Analysis Methods School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<postCode>100872</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Recursive Visual Attention in Visual Dialog</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Visual dialog is a challenging vision-language task, which requires the agent to answer multi-round questions about an image. It typically needs to address two major problems: (1) How to answer visually-grounded questions, which is the core challenge in visual question answering (VQA); (2) How to infer the co-reference between questions and the dialog history. An example of visual co-reference is: pronouns (e.g., "they") in the question (e.g., "Are they on or off?") are linked with nouns (e.g., "lamps") appearing in the dialog history (e.g., "How many lamps are there?") and the object grounded in the image. In this work, to resolve the visual co-reference for visual dialog, we propose a novel attention mechanism called Recursive Visual Attention (RvA). Specifically, our dialog agent browses the dialog history until the agent has sufficient confidence in the visual co-reference resolution, and refines the visual attention recursively. The quantitative and qualitative experimental results on the large-scale VisDial v0.9 and v1.0 datasets demonstrate that the proposed RvA not only outperforms the state-of-the-art methods, but also achieves reasonable recursion and interpretable attention maps without additional annotations. The code is available at https://github.com/yuleiniu/rva.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Vision and language understanding has become an attractive and challenging interdisciplinary field in computer vision and natural language processing. Thanks to the rapid development of deep neural networks and the high quality of large-scale real-world datasets, researchers have achieved inspiring progress in a range of vision-language tasks, including visual relation detection <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b39">39]</ref>, image captioning <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b2">3]</ref>, referring expression grounding <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b40">40]</ref>, and visual question answering (VQA) <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b32">32]</ref>. However, <ref type="figure">Figure 1</ref>. Illustration of the intuition of Recursive Visual Attention in visual dialog. When our dialog agent meets an ambiguous question (e.g., "Are they on or off?"), it will recursively review the dialog history (see the first column) and refine the visual attention (see the third column), until it can resolve the visual co-reference (e.g., How many lamps are there?). The attention maps tagged with green check mark represent reasonable recursive visual attention, while those tagged with red cross mark in the dashed box represent false question-guided visual attention. comprehension and reasoning in vision and natural language are still far from being resolved, especially when the AI agent interacts with human in a continuous communication, such as vision-and-language navigation <ref type="bibr" target="#b3">[4]</ref> and visual dialog <ref type="bibr" target="#b8">[9]</ref>.</p><p>Visual dialog is one of the prototype tasks introduced in recent years <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. It can be viewed as the generalization of VQA, which requires the agent to answer the question about an image <ref type="bibr" target="#b5">[6]</ref> or video <ref type="bibr" target="#b33">[33]</ref> after comprehending and reasoning out of visual and textual contents. Different from one-round VQA, visual dialog is a multi-round conversation about an image. Therefore, one of the key challenges in visual dialog is visual co-reference resolution, since 98% of dialogs and 38% of questions in the large-scale VisDial dataset have at least one pronoun (e.g., "it", "they", "this", "he", "she") <ref type="bibr" target="#b8">[9]</ref>. For example, as illustrated in <ref type="figure">Figure 1</ref>, questions "Are they on or off?" and "What color is it?" contain pronouns that need to be resolved before answering. Recently, researchers have attempted to resolve the visual co-reference using attention memory <ref type="bibr" target="#b29">[29]</ref> at a sentence level, or applying the neural module networks <ref type="bibr" target="#b17">[18]</ref> at a word level. Specifically, an attention memory <ref type="bibr" target="#b29">[29]</ref> is established to store the image attention map at each round, while a reference pool <ref type="bibr" target="#b17">[18]</ref> is utilized to keep all the entities recognized from the dialog history. They both apply a soft attention over all the stored visual attentions for refinement. However, humans rarely remember all their previous visual attentions, and only review the topic-related dialog history when they are confused with the ambiguous question.</p><p>We expect our dialog agent to selectively review the dialog history like us humans during the conversation. For example, as illustrated in <ref type="figure">Figure 1</ref>, "Are they on or off?" is an ambiguous question and the dialog agent needs to resolve "they" before watching the image. The agent then recursively browses the dialog history and computes visual attention until it meets the unambiguous description "How many lamps are there?". One may argue that a natural language parser can achieve this goal by detecting whether there exists a pronoun in the question. However, not all pronouns are needed to be resolved, e.g., "Is it sunny?". Some abbreviate sentences without context are also ambiguous, e.g., "What color?". It is thus impractical to exhaust all cases using a natural language parser.</p><p>In this work, we formulate visual co-reference resolution in visual dialog as Recursive Visual Attention (RvA). As shown in <ref type="figure">Figure 1</ref>, the agent first infers whether it can ground the visual content based on the current question. If not, the agent will recursively review the topic-related dialog history and refine the visual attention. The recursion termination is that the agent feels "confident" in visual grounding, or it has backtracked to the beginning of dialog history. Thanks to the Gumbel-Max trick <ref type="bibr" target="#b11">[12]</ref> and its continuous softmax relaxation <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b23">23]</ref>, our agent can be end-to-end trained when making discrete decisions. In addition, we design two types of language features for different purposes. The reference-aware language feature helps with visual grounding and inference of reviewing dialog history, while the answering-aware language feature controls which attributes of the image feature should be activated for question answering.</p><p>Our main contributions are concluded as follows. First, we propose a novel Recursive Visual Attention (RvA) strategy for the visual co-reference resolution in visual dialog. Second, we carry out extensive experiments on VisDial v0.9 and v1.0 <ref type="bibr" target="#b8">[9]</ref>, and achieve state-of-the-art performances compared to other methods. Third, the qualitative results indicate that our dialog agent obtains reliable visual and language attention during the reasonable and history-aware recursive process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Visual Dialog. Visual dialog is a current vision and language task, which requires the agent to understand the dialog history, ground visual object, and answer the question. Recently, two popular dialog datasets were crowd-sourced on Amazon Mechanical Turk (AMT) <ref type="bibr" target="#b6">[7]</ref>. De Vries et al. <ref type="bibr" target="#b9">[10]</ref> collected GuessWhat dataset from a cooperative two-player game. Given the whole picture and its caption, one player asks questions to locate the selected object, while the other player replies in yes/no/NA. However, the questions are constrained to closed-ended questions. In comparison, Das et al. <ref type="bibr" target="#b8">[9]</ref> collected VisDial dataset by a different two-person chat style. During the live chat, the "questioner" asks questions to imagine the visual content in the picture based on the caption and chat history, while the "answerer" watches the picture and answer in a free-form way. We apply the second setting in this paper. Visual Co-reference Resolution. The task of visual co-reference resolution is to link expressions, typically pronoun and noun phrases referring to the same entity, and ground the referent in the visual content. Co-reference resolution has been used to improve visual comprehension in many tasks, such as visual grounding <ref type="bibr" target="#b13">[14]</ref>, action recognition <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b28">28]</ref>, and scene understanding <ref type="bibr" target="#b16">[17]</ref>. Recently Lu et al. <ref type="bibr" target="#b22">[22]</ref> proposed a history-conditioned attention mechanism to implicitly resolve the visual co-reference. Seo et al. <ref type="bibr" target="#b29">[29]</ref> used attention memory to store previous image attentions at a sentence level. Furthermore, neural module networks <ref type="bibr" target="#b4">[5]</ref> were applied to recognize entities in all the history at a word level <ref type="bibr" target="#b17">[18]</ref>. Different from recent works that proposed a soft attention mechanism over all the memorized attention maps <ref type="bibr" target="#b29">[29]</ref> or all the grounded entities <ref type="bibr" target="#b17">[18]</ref>, our proposed recursion predicts discrete attention over topic-related history, which is more intuitive and explainable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>In this section, we formally introduce the visual dialog task and our proposed Recursive Visual Attention (RvA) approach. The task of visual dialog <ref type="bibr" target="#b8">[9]</ref> is defined as follows. The dialog agent is expected to answer the question q T at round T by ranking a list of 100 candidate answers A T = {a <ref type="bibr" target="#b0">(1)</ref> T , · · · , a (100) T } in a discriminative manner, or producing a sentence in a generative manner. The extra information for visual dialog consists of the image I and the dialog history  </p><formula xml:id="formula_0">H = { c h0 , (q 1 , a 1 ) h1 , · · · , (q T −1 , a T −1 ) h T −1 }, where</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Recursive Visual Attention</head><formula xml:id="formula_1">Algorithm 1 Recursive Visual Attention 1: function RVA(V, Q, H, t) 2: cond, λ ← INFER(Q, t) 3:</formula><p>if cond then <ref type="bibr">4:</ref> return ATT(V, Q, t) </p><formula xml:id="formula_2">t p ← PAIR(Q, H, t) 7: return (1−λ) · RVA(V, Q, H, t p ) 8: +λ · ATT(V, Q, t) 9: end if 10: end function</formula><p>First of all, the overall structure of the proposed Recursive Visual Attention (RvA) method is shown in Algorithm 1. Here Q = {q 0 , q 1 , · · · , q T } represents the question feature set where the caption feature c is added into the question set as q 0 , H = {h 0 , h 1 , · · · , h T −1 } represents the history feature set, and V = {v 1 , · · · , v K } represents the region feature set. Given any question q t , our dialog agent first infers whether it understands the question q t for visual grounding. If not, our agent will pair the current question q t with its most related history h tp , and backtrack to the paired round t p . This process will be kept executing until the agent can understand the current traced question, or the dialog agent has backtracked to the beginning of the dialog. As a result, our dialog agent recursively modifies the visual attention by adding the question-guided attended visual attention at round t and the recursive visual attention at paired round t p , weighted by a learnable non-negative weight λ. For the question q T , the output visual attention is formulated by α T = RVA(V, Q, H, T ). The attended visual feature is further calculated by a weighted sum over all the region featuresv T = i α i v i .</p><p>In addition, we give a high-level view of Recursive Visual Attention (RvA) in <ref type="figure" target="#fig_1">Figure 2</ref>. Intuitively, all the switches on both the trunk and branches are initially open (i.e., turned off). Our RvA is recursively called from present to past, closing (i.e., turning on) the switch on the trunk until the recursion terminates. The switch of the question-guided visual feature v tp on the branch is closed if the history h tp is paired with the current traced question q t . When the recursion termination condition is met, we unroll the process from past to present and finally obtain the recursive visual feature.</p><p>We further design three modules to achieve the recursive visual attention algorithm, i.e., INFER , PAIR, and ATT (i.e., attend). In overview, INFER module asserts the recursion termination condition and computes visual feature fusion weight, PAIR module returns the paired round, and ATT module calculates question-guided visual attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Neural Modules</head><formula xml:id="formula_3">Algorithm 2 INFER Module 1: function INFER(Q, t) 2: z I t ← f I q (q t ) 3: o I t ← GS Sampler(W I z I t ) 4: α I t ← softmax(W I z I t ) 5: cond 1 ← t ? = 0 6: cond 2 ← o I t,0 ? = 1 7: cond ← cond 1 or cond 2 recursion termination 8: λ ← α I t,0</formula><p>attention fusion weight <ref type="bibr">9:</ref> return cond, λ 10: end function INFER Module. INFER module is designed to 1) determine whether to review the dialog history, 2) provide a weight to fuse the recursive visual attention and the question-guided visual attention. Specifically, INFER module takes the question feature q t as input. The outputs include 1) a Boolean cond to decide whether to terminate the recursion, and 2) a weight λ ∈ (0, 1) for visual attention fusion.</p><p>The recursion will be terminated if at least one of the following conditions is satisfied (see lines 5-7 in Algorithm 2). First, the review backtracks to the very starting point: caption. Second, the question q t is predicted to be unambiguous. In order to estimate the ambiguity of the question, we use a non-linear transformation <ref type="bibr" target="#b34">[34]</ref> f I q (·), followed by a Gumbel Sampling operation GS Sampler for differentiable discrete decision:</p><formula xml:id="formula_4">z I t = f I q (q t );<label>(1)</label></formula><formula xml:id="formula_5">o I t = GS Sampler(W I z I t )<label>(2)</label></formula><p>where W I denotes the learnable parameters. GS Sampler (see Section 3.3.2) outputs a 2-dim one-hot vector o I <ref type="figure">Figure 3</ref>. Word cloud visualization of word attention in RvA. For questions that our dialog agent thinks to be unambiguous (left), the word attentions are spread out a variety of nouns (e.g., "clouds", "drinks"). For questions that confuse the agent (right), the word attention significantly focuses on pronouns (e.g., "it", "they").</p><formula xml:id="formula_6">Algorithm 3 PAIR Module 1: function PAIR(Q, H, t) 2: e q t ← f P q (q t ) 3: for i ← 0, · · · , t−1 do 4: e h i ← f P h (h i ) 5: z P t,i ← MLP([e q t , e h i ]) 6: ∆ t,i ← t−i 7: end for 8: o P t ← GS Sampler(W P [z P t , ∆ t ]) 9: t p ← i o P t,i · i 10:</formula><p>return t p 11: end function PAIR Module. We observe that an ambiguous question often follows the latest topic. A simple idea is to directly pair the question with its latest history, i.e., set t p as t−1 in INFER module. However, the questioner sometimes traces back to an earlier topic, which means that the question has no relationship with its latest history. Therefore, we design a PAIR module to estimate which history is most related with the question q t . Algorithm 3 shows the structure of PAIR module. Specifically, PAIR module takes the question feature q t and the history feature H = {h 0 , · · · , h t−1 } as input, and predicts which history is most related to q t . The PAIR module is formulated as:</p><formula xml:id="formula_7">z P t,i = MLP([f P q (q t ), f P h (h i )])<label>(3)</label></formula><formula xml:id="formula_8">o P t = GS Sampler(W P [z P t , ∆ t ])<label>(4)</label></formula><formula xml:id="formula_9">t p = t−1 i=0 o P t,i · i<label>(5)</label></formula><p>where [·] is the concatenation operation. The PAIR module considers 1) the matching score between the question q t and the history h i , which is denoted as z P t,i ; 2) the "sequential distance" between q t and h i in the dialog, which is measured by ∆ t,i = t−i. Finally, GS Sampler outputs a t-dim one-hot vector o P t for discrete decision (i.e., pairing the question with a single history). The question q t will be paired with the k-th history h k if o P t,k = 1, i.e., the k-th history h k matches the question q t better than others.</p><formula xml:id="formula_10">Algorithm 4 ATT Module 1: function ATT(V, Q, t) 2: e q t ← f A q (q t ) 3: for i ← 1, · · · , K do 4: e v i ← f A v (v i ) 5: z A t,i ← L2Norm(e q t • e v i ) 6: end for 7: α A t ← softmax(W A Z A t ) 8:</formula><p>return α A t 9: end function ATT Module. ATT module takes visual features of regions V = {v 1 , · · · , v K } and the question feature q t as input, and outputs question-guided visual attention. As shown in Algorithm 4, the question-guided visual attention is formulated as:</p><formula xml:id="formula_11">z A t,i = L2Norm(f A q (q t ) • f A v (v i )) (6) α A t = softmax(W A Z A t )<label>(7)</label></formula><p>where f A q (·) and f A v (·) represents non-linear transformations to embed visual and language features into the same space, and • denotes Hadamard (element-wise) product for multi-modal feature fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training</head><p>As mentioned in Section 3.2, our Recursive Visual Attention takes visual and language representations as input, and applies Gumbel sampling for differentiable discrete decision. The details are given as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Feature Representation</head><p>Language Feature. Let W q t = {w q t,1 , · · · , w q t,m } be the word embeddings of the question q t . The word embeddings are passed through the bidirectional LSTM (bi-LSTM):</p><formula xml:id="formula_12">− → h q t,i = LSTM q f (w q t,i , − → h q t,i−1 ) (8) ← − h q t,i = LSTM q b (w q t,i , ← − h q t,i+1 )<label>(9)</label></formula><formula xml:id="formula_13">h q t,i = [ − → h q t,i , ← − h q t,i ]<label>(10)</label></formula><p>where − → h q t,i and ← − h q t,i represent forward and backward hidden state of the i-th word respectively, LSTM q f and LSTM q b represent the forward and backward LSTMs. We use the concatenation of last hidden states</p><formula xml:id="formula_14">e q t = [ − → h q t,m , ← − h q t,1</formula><p>] as the encoding of the whole question q t . Similarly, we can encode the history h i as e h i using the same bi-LSTM with different parameters. In PAIR module, we denote e q t as q t and e h i as h i to calculate the matching score between the question q t and the history h i . Note that the words contribute differently to the question representation for various purposes.</p><p>An example is illustrated in <ref type="figure" target="#fig_3">Figure 4</ref>. On one hand, the words "tablet" and "it" should be emphasized for recursion termination estimation and visual grounding. On the other hand, the phrase "what color" and the word "big" should be highlighted to activate specific attributes of the visual representation for question answering. Therefore, we encode each question using self-attention mechanisms <ref type="bibr" target="#b35">[35]</ref> into two forms: reference-aware question feature q ref t and answering-aware question feature q ans t . Different from prior attention mechanism that uses linear transformation followed by hyperbolic tangent (tanh) activation, we formulate the self-attention mechanism as:</p><formula xml:id="formula_15">z q, * t,i = L2Norm(f q, * q (h q t,i )) (11) α q, * t = softmax(W q, * Z q, * t )<label>(12)</label></formula><formula xml:id="formula_16">q * t = m i=1 α q, * t,i w q i<label>(13)</label></formula><p>where f q, * q (·) is a non-linear transformation function, W q, * is the learnable parameters, and * ∈ {ref, ans}. The as q t for recursion termination estimation and visual grounding. Visual Feature. Spatial image features with attention mechanism have been widely used in many vision and language tasks, such as image captioning and visual question answering. Recently, a bottom-up attention mechanism <ref type="bibr" target="#b2">[3]</ref> is proposed based on the Faster R-CNN framework. The ResNet model is utilized as backbone and trained on Visual Genome <ref type="bibr" target="#b18">[19]</ref> dataset to predict attributes and classes. In this paper, we apply the bottom-up attention mechanism and select top-K region proposals from each image, where K is simply fixed as 36.</p><p>After obtaining the visual featurev T using Recursive Visual Attention, we further refine the visual feature using the answering-aware question feature q ans T . The motivation is that only question-related attributes of visual content are useful for answering questions (e.g., "What color is the tablet?", "Does it look big?" in <ref type="figure" target="#fig_3">Figure 4</ref>). Motivated by the gating operations within LSTMs and GRUs, we further refine visual feature as:</p><formula xml:id="formula_17">v T =v T • f v q (q ans T )<label>(14)</label></formula><p>where the output of non-linear transformation f v q (·) works as a "visual feature filter" to deactivate the information unrelated to answering questions in the visual representationv t . Joint Embedding. Considering that the dialog history reflects prior knowledge of visual content, we obtain the "fact" embedding by attending to all the history as</p><formula xml:id="formula_18">z h T,i = L2Norm(f h q (e q T ) • f h h (e h i ))<label>(15)</label></formula><formula xml:id="formula_19">α h T = softmax(W h Z h T )<label>(16)</label></formula><formula xml:id="formula_20">h f T = T −1 i=0 α h T,i e h i<label>(17)</label></formula><p>where f h h and f h q are non-linear transformation functions. The "fact" embedding h f T is calculated by a weighted sum over all the history encodings.</p><p>Since we have obtained the filtered visual featureṽ T , the answering-aware question feature q ans T , and the fact embedding h f T for question q T , we concatenate these features and use a linear transform followed by a tangent activation to obtain the final joint embedding:</p><formula xml:id="formula_21">e J T = tanh(W J [ṽ T , q ans T , h f T ])<label>(18)</label></formula><p>where [·] denotes the concatenation operation. The joint embedding is further fed into the answering decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Gumbel Sampling</head><p>Our dialog agent needs to make a discrete decision in some cases, e.g., estimating whether to review the history and which history should be paired. In addition, we hope that the gradients can be back propagated through discrete decision making for end-to-end training. In order to achieve these goals, we utilize the Gumbel-Max trick <ref type="bibr" target="#b11">[12]</ref> with its continuous softmax relaxation <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b23">23]</ref>. Specifically, the samples z can be drawn from a categorical distribution with π = {π 1 , · · · , π c } as: z = one hot arg max k∈{1,...,c} (log(π k ) + g k )</p><p>where g = −log(−log(u)) with u ∼ unif[0, 1].</p><p>The softmax relaxation of Gumbel-Max trick is to replace non-differentiable arg max operation with the continuous softmax function:</p><formula xml:id="formula_23">z = softmax ((log(π) + g)/τ )<label>(20)</label></formula><p>where the temperature of the softmax function τ is empirically set as 1 in our work. During the training stage, we obtain an one-hot vector z as the discrete sample from Eq. 19 for forward propagation, and compute gradients w.r.t. π in Eq. 20 for back propagation. At the test stage, we greedily draw the sample with the largest probability without Gumbel samples g.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Our proposed model is evaluated on two real-world datasets: VisDial v0.9 and v1.0 <ref type="bibr" target="#b8">[9]</ref>. In this section, we first introduce the datasets, evaluation metrics, and implementation details. We then compare our method with the state-of-the-art models and provide qualitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Setup</head><p>The VisDial v0.9 <ref type="bibr" target="#b8">[9]</ref> dataset was collected based on MS-COCO <ref type="bibr" target="#b19">[20]</ref> images and captions. In a two-player chat game, one player attempts to learn about an unseen image and asks questions based on the previous dialog, while the other player watches the image and replies with free-form answers. The whole chat lasts for 10 rounds for each image. As a result, the VisDial v0.9 dataset contains 83k dialogs on MS-COCO training images and 40k dialogs on validation images. Recently, the VisDial v1.0 <ref type="bibr" target="#b8">[9]</ref> dataset was released, including additional 10k dialogs on Flickr images. The collection of dialogs on Flickr images is similar to that on MS-COCO images. Overall, the new train split consists of 123k dialogs on MS-COCO images, which is the combination of train and validation splits from VisDial v0.9. The validation and test splits have 2k and 8k dialogs on Flickr images, respectively. Different from val split in VisDial v0.9 where each image is associated with a 10-round dialog, the dialogs in VisDial v1.0 test split have a random length within 10 rounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Metrics</head><p>As in <ref type="bibr" target="#b8">[9]</ref>, we evaluated the responses at each round in VisDial v0.9 and the last round in VisDial v1.0 in a retrieval setting. Specifically, at test stage, each question is linked with a list of 100 candidate answers. The model is expected to rank over the candidates and return a ranked list for further evaluation. The metrics for retrieval performance evaluation are: 1) mean rank of human response (Mean); 2) recall@k (R@k), which is the existence of the human response in the top-k responses; 3) mean reciprocal rank (MRR) of the human response in the returned ranked list.</p><p>As for VisDial v1.0, we also used the newly introduced normalized discounted cumulative gain (NDCG), which penalizes the lower rank of answers with high relevance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Implementation Details</head><p>Language Model. We pre-processed the text data as follows. As in <ref type="bibr" target="#b8">[9]</ref>, we first lowercased all questions and answers, converted digits to words, and removed contractions, before tokenizing using the Python NLTK toolkit <ref type="bibr" target="#b0">[1]</ref>. The captions, questions, and answers were then padded or truncated to 40, 20 and 20, respectively. We kept words to those that occur at least 5 times in the training split, resulting in a vocabulary of 9,795 words for VisDial v0.9 and 11,336 words for VisDial v1.0. Our word embeddings are 300-dim vectors, initialized with pre-trained GloVe <ref type="bibr" target="#b26">[26]</ref> embeddings and shared across captions, questions and answers. The dimension of hidden states in all LSTMs is set to 512 in this work. Training Details. We minimized the standard crossentropy loss for the discriminative training, and the maximum likelihood estimation (MLE) loss for generative training. We used Adam <ref type="bibr" target="#b15">[16]</ref> with the learning rate of 1 × 10 −3 , multiplied by 0.5 after every epoch, decreasing to 5 × 10 −5 . We also applied Dropout [31] with a ratio of 0.5 before each fully-connected layer. Other settings are default in PyTorch <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparing Methods</head><p>We compared our proposed Recursive Visual Attention (RvA) model with the state-of-the-art methods in both discriminative and generative settings. Based on the design of encoders, these methods can be grouped into: Fusion-based Models. Early methods simply fuse image, question, and history features at different stages. These early methods include LF <ref type="bibr" target="#b8">[9]</ref> and HRE <ref type="bibr" target="#b8">[9]</ref>. Attention-based Models. Furthermore, some methods establish attention mechanisms over image, question, and history. The attention-based methods include HREA <ref type="bibr" target="#b8">[9]</ref>, MN <ref type="bibr" target="#b8">[9]</ref>, HCIAE <ref type="bibr" target="#b22">[22]</ref>, and CoAtt <ref type="bibr" target="#b37">[37]</ref>. VCoR-based models. Recent works have focused on explicit visual co-reference resolution (VCoR) in visual dialog. We compared our method with VCoR-based models including AMEM <ref type="bibr" target="#b29">[29]</ref> and CorefNMN <ref type="bibr" target="#b17">[18]</ref>. Ablative Models. In addition, we evaluate the individual contribution of following features and components in our method: 1) RPN: we replaced the region proposal network with VGG-16 <ref type="bibr" target="#b30">[30]</ref> model, and used the spatial grids of pool5 feature map as regions. 2) Bi-LSTM: we replaced bidirectional LSTM with the vanilla LSTM. 3) Rv: we only considered the termination condition of RvA, and replaced the recursive attention with question-guided attention. 4) FL: we withdrew the "visual feature filter" f v q (·) in Eq. 14, which controls the activation of visual attributes.   <ref type="table">Table 1</ref> reports the retrieval performances of our model and the comparing methods under the discriminative setting on VisDial v0.9. Overall, our RvA model outperforms the state-of-the-art methods across all the metrics. Specifically, our RvA model achieves approximately 2 points improvement on R@k, and 2% increase on MRR. In addition, the performance of our model drops significantly without recursive attention (i.e., Rv) or region proposal network (i.e., RPN), which demonstrates their substantial contributions to visual dialog. The similar conclusions can also be drawn on VisDial v1.0 in <ref type="table" target="#tab_2">Table 2</ref>   <ref type="table">Table 3</ref> shows a more comprehensive ablation (the component-wise and the feature-wise). It can be seen that by using the proposed recursive attention, any ablative method can be improved regardless of the usage of visual and language representations. Furthermore, our dialog agent could occupy the third place based on the VisDial v1.0 leaderboard 1 , while the team DL-61 <ref type="bibr" target="#b12">[13]</ref> has achieved the best NDCG record 0.5788 in a two-stage fashion. We also evaluated the retrieval performance of our model under generative setting on VisDial v0.9. As shown in <ref type="table" target="#tab_3">Table 4</ref>, our approach obtains an approximately 2 points higher R@k compared to the visual co-reference solution model CorefNMN <ref type="bibr" target="#b17">[18]</ref>. In addition, our RvA model outperforms nearly all state-of-the-art methods except CoAtt <ref type="bibr" target="#b37">[37]</ref>, which is trained using reinforcement learning. <ref type="bibr">RPN</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Quantitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Qualitative Results</head><p>The qualitative results shown in <ref type="figure" target="#fig_5">Figure 5</ref> and 6 demonstrate the following advantages of our RvA model: Reasonable Recursions.</p><p>Our RvA model achieves reasonable recursions represented by the recursive trees. These recursions can also be regarded as topic-aware dialog clips. Thanks to the reference-aware language feature, our RvA model is able to handle unambiguous sentences with pronouns (e.g., "Is it sunny outside?") and ambiguous sentences without pronouns (e.g., "How many are there?"). Note that it is hard to exhaust all these special cases using a natural language parser. Reliable Visual Attention. Our dialog agent successfully focuses on the correct region using recursive visual attention. In contrast, the question-guided visual attention sometimes fails due to the ambiguous question. On the validation set of VisDial v1.0, we observed that: 1) 56% of question-guided visual attention and 89% of recursive attention are reasonable for ambiguous questions; 2) 62% of dialogs require at least one accurate co-reference resolution. Since the recursive visual attention relies heavily on historical visual attention, our dialog agent needs to establish a robust visual attention mechanism. If it were otherwise, the agent would distrust historical visual attention and tend to learn more bias from generic language information, which would hurt the visual dialog system. History-aware Skipping Pairing. One may argue that PAIR module can be replaced with referring all the  <ref type="figure">Figure 6</ref>. An qualitative example of the history-aware recursion using PAIR module to resolve "it" in Q3. (a) represents the recursion obtained by our model using the ground-truth history. Our dialog agent skips the unrelated second history, and pairs the ambiguous question Q3 with the first history. (b) represents the recursion obtained by our model with the fake history, where the second answer "No" is replaced with "Yes". In this case, our dialog agent pairs the third question with its last history.</p><p>ambiguous questions to their last history (i.e., setting t p as t − 1 in INFER module) for simplicity. However, our PAIR module is able to skip the irrelevant dialog history and produce history-aware recursions. As illustrated in <ref type="figure">Figure 6</ref> (a), the dialog agent concludes from the dialog history that "there is no cord" in the image. Therefore, the agent skips the second history when pairing the ambiguous question "Is it black?". If we replace the second answer "no" with "yes" to make a fake history (see <ref type="figure">Figure 6</ref> (b)), the third question will be directly paired with its last history. The visual attention and predicted answer are also influenced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we formulated the visual co-reference resolution in visual dialog as Recursive Visual Attention (RvA), which consists of three simple neural modules that determine the recursion at run-time. Our dialog agent recursively reviews topic-related history to refine visual attention, and can be end-to-end trained when making discrete decisions of module assembling. Experimental results on the large-scale real-world datasets VisDial v0.9 and v1.0 demonstrate that our proposed model not only achieves state-of-the-art performance, but also obtains explainable recursion and attention maps. Moving forward, we are going to incorporate in-depth language parsing modules into RvA for more accurate recursive decisions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>c is the image caption and (q, a) is any question-answer pair.Next, we first provide an overall structure of RvA in Section 3.1, followed by Section 3.2 introducing the INFER, PAIR and ATT modules of RvA. The training details of RvA are given in Section 3.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>A high-level view of Recursive Visual Attention. The right-to-left direction (dashed blue) represents recursive call, and the left-to-right direction (dashed red) represents visual attention return. The cond variable controls the switch on the trunk, while tp controls the switch on the branch (see Algorithm 1).vt represents the attended feature ATT(V, Q, t).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>A qualitative example of question attentions. The reference-aware (ref ) question attention mainly emphasizes nouns (i.e., "tablet") and pronouns (i.e., "it") for recursion termination estimation and visual grounding. The answering-aware (ans) question attention highlights property words (i.e., "what color", "big") to record question type and activate specific attributes of visual representation for question answering. Darker color indicates higher weight.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>attended question features q ref t and q ans t are calculated by a weighted sum overall the words. In INFER and ATT modules, we denote q ref t</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative results of our RvA model on VisDial dataset. The number in the bracket is the rank of ground-truth (GT) answer in the returned sorted list. Our model successfully obtains interpretable reference-aware question attention (represented by the highlight color of words, darker color indicates higher weight), reliable recursive image attention (represented by the attention map below the question), and reasonable recursions (represented by the recursion tree). The root nodes in the recursion tree represent the questions to be answered, the red nodes denote the questions terminating recursions, and the leaf nodes represent question-guided visual attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Retrieval performance of discriminative models on the test-standard split of VisDial v1.0. † indicates that the model uses ResNet-152 features.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Bi-LSTM Rv MRR R@1 R@5 R@10 Mean 0.6377 49.67 80.86 89.14 4.35 0.6418 50.17 81.17 89.37 4.29 0.6396 49.83 81.16 89.34 4.30 0.6436 50.40 81.36 89.59 4.22 0.6534 51.78 82.28 90.21 Retrieval performance of generative models on the validation set of VisDial v0.9. ‡ indicates that the model is trained using reinforcement learning.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>4.09</cell></row><row><cell></cell><cell cols="2">0.6626 52.69 82.97 90.71</cell><cell>3.95</cell></row><row><cell></cell><cell cols="2">0.6551 51.81 82.35 90.24</cell><cell>4.07</cell></row><row><cell></cell><cell cols="2">0.6634 52.71 82.97 90.73</cell><cell>3.93</cell></row><row><cell cols="4">Table 3. Ablations of discriminative models on the validation set</cell></row><row><cell cols="4">of VisDial v0.9. RPN, Bi-LSTM and Rv indicate the usage of</cell></row><row><cell cols="4">region proposal network, bidirectional LSTM, and recursive image</cell></row><row><cell cols="2">attention, respectively.</cell><cell></cell></row><row><cell>Model</cell><cell>MRR</cell><cell cols="2">R@1 R@5 R@10 Mean</cell></row><row><cell>LF [9]</cell><cell cols="3">0.5199 41.83 61.78 67.59 17.07</cell></row><row><cell>HRE [9]</cell><cell cols="3">0.5237 42.29 62.18 67.92 17.07</cell></row><row><cell>HREA [9]</cell><cell cols="3">0.5242 42.28 62.33 68.17 16.79</cell></row><row><cell>MN [9]</cell><cell cols="3">0.5259 42.29 62.85 68.88 17.06</cell></row><row><cell cols="4">CorefNMN [18] 0.535 43.66 63.54 69.93 15.69</cell></row><row><cell>HCIAE [22]</cell><cell cols="3">0.5386 44.06 63.55 69.24 16.01</cell></row><row><cell>CoAtt [37]</cell><cell cols="3">0.5411 44.32 63.82 69.75 16.47</cell></row><row><cell>CoAtt  ‡ [37]</cell><cell cols="3">0.5578 46.10 65.69 71.74 14.43</cell></row><row><cell>RvA w/o RPN</cell><cell cols="3">0.5417 43.75 64.21 71.85 11.18</cell></row><row><cell>RvA</cell><cell cols="3">0.5543 45.37 65.27 72.97 10.71</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t for discrete decision, where the binary element o I t,0 is encoded as the Boolean output to determine whether q t is ambiguous. As illustrated inFigure 3, our dialog agent successfully learns the relation between words and recursion termination without additional annotations.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://evalai.cloudcv.org/web/challenges/ challenge-page/103/leaderboard/298</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was partially supported by National Natural Science Foundation of China (61573363 and 61832017), the Fundamental Research Funds for the Central Universities and the Research Funds of Renmin University of China (15XNLQ01), and NTU-Alibaba JRI.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="http://www.nltk.org/.6" />
		<title level="m">NLTK</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pytorch</surname></persName>
		</author>
		<ptr target="https://pytorch.org/.6" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niko</forename><surname>Sünderhauf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3674" to="3683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Amazon&apos;s Mechanical Turk: A new source of inexpensive, yet high-quality, data?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Buhrmester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tracy</forename><surname>Kwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">D</forename><surname>Gosling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perspectives on Psychological Science</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="5" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sca-cnn: Spatial and channel-wise attention in convolutional networks for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5659" to="5667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Visual dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khushi</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avi</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deshraj</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="326" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Guesswhat?! visual object discovery through multi-modal dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Harm De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarath</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5503" to="5512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daylen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01847</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Statistical theory of extreme values and some practical applications: a series of lectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emil</forename><forename type="middle">Julius</forename><surname>Gumbel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">US Government Printing Office</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Image-question-answer synergistic network for visual dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09774</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Finding &quot;it&quot;: Weakly-supervised reference-aware visual grounding in instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De-An</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyamal</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucio</forename><surname>Dery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Animesh</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5948" to="5957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">What are you talking about? text-to-image coreference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3558" to="3565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visual coreference resolution in visual dialog using neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="153" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Visual relationship detection with language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="852" to="869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Best of both worlds: Transferring knowledge from discriminative learning to a generative visual dialog model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anitha</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="314" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Chris J Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.00712</idno>
		<title level="m">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Generation and comprehension of unambiguous object descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oana</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Modeling context between objects for referring expression understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Varun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nagaraja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vlad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="792" to="807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Linking people in videos with &quot;their&quot; names using coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="95" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Generating descriptions with grounded and co-referenced people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4979" to="4989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visual reference resolution using attention memory for visual dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Paul Hongsuck Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Lehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning to compose dynamic tree structures for visual contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01880</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Movieqa: Understanding stories in movies through question-answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4631" to="4640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Xiaodong He, and Anton van den Hengel. Tips and tricks for visual question answering: Learnings from the 2017 challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4223" to="4232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Are you talking to me? reasoned visual dialog generation through adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="6106" to="6115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Ppr-fcn: weakly supervised visual relation detection via parallel pairwise r-fcn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zawlin</forename><surname>Kyaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4233" to="4241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Grounding referring expressions in images by variational context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4158" to="4166" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
