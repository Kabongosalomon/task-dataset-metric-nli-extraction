<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Non-Local Graph Convolutional Networks for Skeleton-Based Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shi</surname></persName>
							<email>lei.shi@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
							<email>yfzhang@nlpr.ia.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Institute of Automation Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
							<email>jcheng@nlpr.ia.ac.cn</email>
							<affiliation key="aff2">
								<orgName type="department">Institute of Automation Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Institute of Automation Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Non-Local Graph Convolutional Networks for Skeleton-Based Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Traditional deep methods for skeleton-based action recognition usually structure the skeleton as a coordinates sequence or a pseudo-image to feed to RNNs or CNNs, which cannot explicitly exploit the natural connectivity among the joints. Recently, graph convolutional networks (GCNs), which generalize CNNs to more generic non-Euclidean structures, obtains remarkable performance for skeleton-based action recognition. However, the topology of the graph is set by hand and fixed over all layers, which may be not optimal for the action recognition task and the hierarchical CNN structures. Besides, the first-order information (the coordinate of joints) is mainly used in former GCNs, while the second-order information (the length and direction of bones) is less exploited. In this work, a novel two-stream nonlocal graph convolutional network is proposed to solve these problems. The topology of the graph in each layer of the model can be either uniformly or individually learned by BP algorithm, which brings more flexibility and generality. Meanwhile, a two-stream framework is proposed to model both of the joints and bones information simultaneously, which further boost the recognition performance. Extensive experiments on two large-scale datasets, NTU-RGB+D and Kinetics, demonstrate the performance of our model exceeds the state-of-the-art by a significant margin.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Action recognition methods based on skeleton data have been widely studied and drawn considerable attention, due to their strong adaptability to the dynamic circumstance and complicated background. Conventional deep learning based methods manually structure the skeleton to a joint coordinates vector or a pseudo-image, which is then fed into RNNs or CNNs to generate the prediction. However, representing the skeleton data as a vector sequence or a 2D grid cannot fully express the dependency between correlated joints. The skeleton is naturally structured as a graph in a non-Euclidean space with the joints as vertices and the natural connections in human body as edges. The previous methods cannot explicitly exploit the graph structure of skeleton data and are difficult to generalize to skeletons with arbitrary forms. Recently, graph convolutional networks (GCNs), which generalizes convolution from image to graph, has been successfully adopted in many applications. For skeleton-based action recognition, <ref type="bibr" target="#b21">[22]</ref> firstly propose to apply GCN to directly model the skeleton data. They construct a spatial graph based on the physical connection of joints in human body and add the temporal edges between corresponding joints in consecutive frames. A distance-based sampling function is proposed for constructing graph convolution, which is then employed as the basic module to build the final spatiotemporal graph convolutional networks (ST-GCN).</p><p>However, the skeleton graph employed in ST-GCN, which is heuristically predefined, merely represents the physical structure and is not guaranteed to be suitable for action recognition task. For example, it cannot capture the dependency between two hands as they lie far away in the defined graph. Besides, the structure of CNN is hierarchical where different layers contain different-level semantic information. Nevertheless, the graph applied in ST-GCN is shared among all the layers, which lacks flexibility and is insufficient to model the multi-level semantic informa-tion contained in all of the layers. Moreover, different samples may need different graphs, which can not be satisfied in the original ST-GCN. Inspired by Non-local neural networks <ref type="bibr" target="#b20">[21]</ref>, we propose a non-local graph convolutional neural networks to solve above problems. An adaptively learned global graph structure is set as the parameter of the model, which is trained and updated jointly with other parameters. This data-driven method increases the flexibility of the model and brings more generality to adapt to various tasks. Besides, different layers are applied with different graph parameters to better fit the hierarchical structure of CNNs. Moreover, an individual graph will be calculated according to each sample. The overall architecture of the non-local GCN block is shown in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>Another notable problem of traditional methods for skeleton based action recognition is that the feature vector attached to each vertex only contain three coordinates of the joints, which can be regarded as the first-order information of the skeleton data. The second-order information, which capture the feature of bones between joints, is neglected. The skeletons are always visualized with both hinged joints and rigid bones. It is nonintuitive to only employ the joints information to classify human activities. To employ the second-order information of the skeletons, the feature of a bone is represented with a six-dimensional vector which contains its length and direction along three dimensions. Moreover, a two-stream framework is proposed to jointly model the first-order and second-order information, which is verified to be a good practice.</p><p>To verify the superiority of the proposed model, extensive experiments are performed on two large-scale datasets, NTU-RGBD <ref type="bibr" target="#b16">[17]</ref> and Kinetics <ref type="bibr" target="#b8">[9]</ref>, where our model achieves the state-of-the-art performance on both of the datasets. The main contributions of our work include:</p><p>• A non-local graph convolutional block is proposed to adaptively learn the graph structure for different layers and samples, which is trained and updated jointly with model parameters in training process and can better suit the action recognition task.</p><p>• The second-order information, i.e. the length and direction of bones, is explicitly formulated and combined with the first-order information, i.e. joint coordinates, in a two-stream framework to further improve the recognition performance.</p><p>• On two large-scale datasets for skeleton-based action recognition, our model exceed the state-of-the-art by a significant margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work 2.1. Skeleton-based action recognition</head><p>Traditional methods design hand-crafted features for skeleton-based action recognition <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b5">6]</ref>. Vemulapalli et al. <ref type="bibr" target="#b19">[20]</ref> encode the skeletons with their rotations and translations in Lie group. Fernando et al. <ref type="bibr" target="#b5">[6]</ref> leverage rank pooling method to represent the data with the parameters of ranker. With the development of deep learning, recurrent neural networks become popular due to its advantage for modeling sequence data. <ref type="bibr" target="#b3">[4]</ref> apply a hierarchical bi-directional RNN model to identify the skeleton sequence, which divides the skeleton into different parts and sends them to different subnetworks. <ref type="bibr" target="#b18">[19]</ref> embed a spatiotemporal attention module in LSTM based model, so that the network can automatically pay attention to the discriminant spatiotemporal region of the skeleton sequence. <ref type="bibr" target="#b22">[23]</ref> introduce the mechanism of view transformation in LSTM based model, which automatically translates the skeleton data to a more advantageous angle for action recognition. Recently, CNN has shown the superiority owing to its good parallel ability and easier training process compared with RNN. <ref type="bibr" target="#b9">[10]</ref> apply a one-dimensional residual CNN to identify skeleton sequence where the coordinates of joints are directly concatenated. <ref type="bibr" target="#b14">[15]</ref> manually design 10 kinds of spatiotemporal images for skeleton encoding, and enhance these images using visual and motion enhancement methods. <ref type="bibr" target="#b13">[14]</ref> employ both coordinates and motion information of joints as input, and carefully design a transformer to rearrange the order of joints. <ref type="bibr" target="#b11">[12]</ref> employ multi-scale residual networks and various data-augmentation strategies for skeleton-based action recognition. Nevertheless, both the CNNs and RNN are failed to fully represent the structure of the skeleton, as they are embedded in the form of graphs instead of a 2D or 3D grids. <ref type="bibr" target="#b21">[22]</ref> employ graph convolution to directly model the raw skeletons, which eliminates the need of hand-crafted part assignment or traversal rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Graph convolutional neural networks</head><p>There have been a lot of works for graph convolution, whose principle of constructing GCNs mainly follows two streams: spatial perspective and spectral perspective <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b2">3]</ref>. Spatial perspective methods directly perform the convolution filters on the graph vertices and their neighbors, which are extracted and normalized based on the manually designed rules <ref type="bibr" target="#b15">[16]</ref>. Different with the spatial perspective methods, spectral perspective methods utilize the eigenvalues and eigenvectors of graph Laplace matrices. It performs the graph convolution in frequency domain with the help of graph Fourier transform <ref type="bibr" target="#b17">[18]</ref>, which does not need to extract locally connected regions from graphs for each convolutional step <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Non-local neural networks</head><p>The concept of non-local was first proposed in non-local means, which computes a weighted sum of all pixels in an image. It utilize all of the pixels according to their similarity with the center point. The idea is then successfully employed in many other applications. Recently, Wang et al. <ref type="bibr" target="#b20">[21]</ref> propose the non-local neural network and achieve the remarkable performance in action recognition area. It present the non-local operations to capture long-range dependencies with deep neural networks, where each response of output feature map is calculated according to all of the features in input feature map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>In Section 3.1, we briefly introduce the original spatialtemporal graph convolutional network (ST-GCN). In Section 3.2, we present the designed non-local graph convolution block. In Section 3.3, we will describe the methods of utilizing the bone information to further boost the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Revisit the ST-GCN</head><p>In ST-GCN <ref type="bibr" target="#b21">[22]</ref>, the skeleton graph is constructed with joints as vertices and bones as edges. In adjacent frames, the corresponding joints are connected as time edges. The attribute of each vertex is the coordinate vectors of the joint. The left sketch of <ref type="figure" target="#fig_0">Fig. 1</ref> shows an example of the constructed spatial-temporal skeleton graph. Given the graph, multiple layers of spatial-temporal graph convolution operations are applied on the graph to get the high-level feature maps. The global average pooling and SoftMax classifier are then employed to predict the action categories. For spatial dimension, each graph convolution operation is formulated as:</p><formula xml:id="formula_0">f out (v i ) = vj ∈B(vi) 1 Z i (v j ) f in (v j ) · w(l i (v j ))<label>(1)</label></formula><p>where f is the feature map and v is the vertex of the graph. B is the sampling area, which is defined as the 1-distance neighbor of the target vertex. As an example, it is showed as colored points in the right sketch of <ref type="figure" target="#fig_0">Fig. 1</ref>. w is the weighting function similar to the original convolution operation, which provides a weight vector corresponding to the given input. Note that the number of weight vectors is fixed while the vertexes in B is varied. A mapping function l i is designed to map each vertexes with a unique weight vector.</p><p>In detail, ST-GCN applies a partition strategy in the frame which divides the neighbors of a vertex into three subsets: 1).the vertex itself; 2).centripetal subset: the neighboring vertexes that are closer to the gravity center of the skeleton; 3).centrifugal subset: the neighboring vertexes that are further to the gravity center. Z i (v j ) is the number of subsets to normalize the result. The right sketch of <ref type="figure" target="#fig_0">Fig. 1</ref> shows this strategy, where different colors represent different subsets and each color is corresponded with the individual learnable weight vector.</p><p>To implement the spatial graph convolution, the Eq. 1 is transformed into:</p><formula xml:id="formula_1">f out = Kv i W i (f in Λ − 1 2 i A i Λ − 1 2 i ) ⊗ M i (2)</formula><p>Here, f is the C in × T × N feature map where N denotes the number of vertexes, T denotes the temporal length and C in denotes the number of input channels. A is similar with the N × N adjacency matrix, whose element A ij indicates whether the vertex v i is in the subset of vertex v j . Λ ii j = k (A ki j ) + α is the normalized diagonal matrix. α is set to 0.001 to avoid the empty rows in A. K v denotes the kernel size of spatial dimension. With partition strategy designed above, K v is 3. A 0 = I which denotes the self-connections of vertexes. A 1 denotes the connections of centripetal subset and A 2 denotes the centrifugal subset. W j is the C out × C in × 1 × 1 weight vector of the 1 × 1 convolution operation. M is a N × N attention map which indicates the importance of each vertex. ⊗ denotes the element-wise matrix multiplication, which means it can only effect the vertexes that are connected with current target.</p><p>As for temporal dimension, because the number of neighbors for each vertex is fixed as 2 (the correspond joints in former frame and later frame), it is straightforward to perform the graph convolution similar with the classical convolution operation. Concretely, we perform a K t × 1 convolution on the output feature map calculated above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Non-local graph convolutional networks</head><p>The spatial-temporal graph convolution for skeleton data described above is calculated based on the given graph G, which is manually designed according to the natural connections of human body. However, it is hard to say this structure is the best choice for action recognition. For example, there is no connection between hand and head in the official provided graph of NTU-RGBD dataset. But for many actions such as wiping face and touching head, the relationship between hand and head may be important. So, the connection relationship should not be constrained in the adjacent joints. Besides, as the information is transferred from lower layers to higher layers, the semantic information contained in different layers is also varied. One shared graph structure can not adapt to this variety. The structure of graph should be updated along the message passing. Moreover, due to the deformation of skeleton for different classes, relations among joints in different samples should also be different.</p><p>To solve the problem, inspired by the non-local neural network, we propose the non-local graph convolution which directly focus on all of the joints to decide whether there are connections between pairs of vertexes. The graph structure is learned individually for different layers and samples in the training process in an end-to-end manner. Different with the original non-local block, our non-local graph convolution block contains three parts. The first part is the physical graph (A i in <ref type="figure" target="#fig_1">Fig. 2</ref>) same as ST-GCN. The second part is a shared graph (B i ) which is same for different samples. It can represent the common pattern of connections between joints. The third part is a individual graph (C i ) which will learn a unique graph for different samples. It will capture the unique features of each sample. All of the three parts are important, which is verified in the ablation study in Section 4.3.</p><p>In detail, according to the Eq. 2, the structure of graph is actually decided by A i . For shared graph, we add 3 N × N learnable parameter B i to represent the connections between each pair of joints. The value of each element denote not only whether there exist edge between two joints, but also the tightness of the connection or the similarly of the two joints. Instead of directly replace the original A i with B i , we make it as a residual connection which is added to A i . The value of B i is initialed with 0. In this way, it can strengthen the flexibility of model without harming the original performance. Note that it can also play the role of attention mechanism performed by M i in Eq. 2 but is more flexible, because the element with 0 will always be 0 no matter what M i is. We remove the M i to avoid the redundant parameter and found it does not effect the performance.</p><p>For individual graph, we apply the embedded Gaussian function to calculate the similarity of two joints:</p><formula xml:id="formula_2">f (v i , v j ) = e θ(vi) T φ(vj )<label>(3)</label></formula><p>using 1 × 1 convolution to represent the embedding functions, the individual graph for each input feature map is calculated by:</p><formula xml:id="formula_3">C j = sof tmax(f in T W T θj W φj f in )<label>(4)</label></formula><p>where the sof tmax operation normalizes the result of product to 0 − 1. W is the parameters of the 1 × 1 convolution and is initialed with 0. Same as the shared graph, it is also employed as a residual connection. Thus the Eq. 2 becomes: <ref type="figure" target="#fig_1">Fig. 2</ref> shows the structure of the basic non-local graph convolution block. A residual connection, similar with <ref type="bibr" target="#b6">[7]</ref>, is added for each block to allow it being inserted into any existed models without breaking its initial behavior. The convolution for temporal dimension is same as ST-GCN. Both the spatial GCN and temporal GCN are followed with the BN layer and Relu layer. One basic block is the combination of one spatial GCN, one temporal GCN and an additional dropout layer with drop rate as 0.5, showed in <ref type="figure" target="#fig_2">Fig. 3</ref>. Same as the original ST-GCN, a residual connection is added for each block. The non-local graph convolutional network (NLGCN) is the stack of these basic blocks, showed in <ref type="figure" target="#fig_3">Fig. 4</ref>. There are totally 9 blocks. The number of output channels for each unit are 64, 64, 64, 128, 128, 128, 256, 256 and 256, respectively. After that, a global average pooling layer is performed and the final output is sent to a SoftMax classifier to get the prediction. More details can be found in the code, which will be released afterwards. </p><formula xml:id="formula_4">f out = Kv i W i f in (A i + B i + C i )<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Two-stream networks</head><p>Traditional methods employ coordinates of body joints as input, which have three channels along x, y and z axises. It is the first-order information which only relies on the single joint. However, the second-order information, which represents the bones between joints, is also important for recognition but is neglected or not emphasized. In this sense, we propose to explicitly model the second-order information, the bone information, with another stream to boost the action recognition. In particular, the input graph of the new stream, which we call B-stream, has the same topological structure with the graph of original stream (Jstream). Each vertex of B-stream is a vector which represents the length and direction of the bone at the source of the current joint. Since the skeleton data has no ring, each joint can be assigned with a unique bone except for the central joint, which will be filled with 0. The bone vector is calculated by the difference between coordinates of two body joints along the x, y and z axises. The overall architecture of 2s-NLGCN is shown in <ref type="figure" target="#fig_4">Fig. 5</ref>. The sof tmax scores of two streams are added to get the fused score and the final prediction. Other fusion methods are left as the future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>To have a head-to-head comparison with ST-GCN, our experiments are conducted on the same two large-scale action recognition datasets: NTU-RGB+D <ref type="bibr" target="#b16">[17]</ref> and Kinetics <ref type="bibr" target="#b8">[9]</ref>. In particular, we first perform exhaustive ablation studies on NTU-RGB+D dataset to examine the contributions of the proposed model components to the recognition performance because it is relatively small compared with Kinetics. Then the final model is evaluated on both NTU-RGB+D and Kinetics to verify the generality and is compared with other state-of-the-art approaches. The joints and connections of two datasets are showed in <ref type="figure" target="#fig_5">Fig. 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>NTU-RGB+D: NTU-RGB+D <ref type="bibr" target="#b16">[17]</ref> is currently the largest and most widely used in-door captured action recognition dataset, which contains 56,000 action clips in 60 action classes. The clips are performed by 40 volunteers in different age groups ranging from 10 to 35. Each action is captured by 3 cameras at the same height but from different horizontal angles: −45 • , 0 • , 45 • . The dataset provides 3D joint locations of each frame detected by the Kinect depth sensors. There are 25 joints for each subject in the skeleton sequences while each clip has no more than 2 subjects. The original paper <ref type="bibr" target="#b16">[17]</ref> of the dataset recommends two benchmarks: 1). Cross-subject (X-Sub): the dataset in this benchmark is divided into training set (40,320 clips) and validation set (16,560 clips), where the actors in two subsets are different. 2).Cross-view (X-View): the training set in this benchmark contains 37,920 clips which are captured by camera 2 and 3, and the validation set contains 18,960 clips which are captured by camera 1. We follow this convention and report the top-1 accuracy on both benchmarks. Kinetics: Kinetics <ref type="bibr" target="#b8">[9]</ref> is a large-scale human action dataset which contains 300,000 videos clips in 400 classes. The video clips are sourced from YouTube videos and have a great variety. It only provides raw video clips without skeleton data. <ref type="bibr" target="#b21">[22]</ref> estimate the location of 18 joints on every frame of the clips with the public available OpenPose toolbox <ref type="bibr" target="#b1">[2]</ref>. 2 peoples are selected for multi-person clips based on the average joint confidence. We use their released data to evaluate our model. The dataset is divided into training set (240,000 clips) and validation set (20,000 clips). Follow the evaluation method in <ref type="bibr" target="#b8">[9]</ref>, we train the models on the training set and report the top-1 and top-5 accuracies on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>We examine the effectiveness of the proposed components in two-stream non-local graph convolutional network  (2s-NLGCN) in this section with the X-View benchmark on NTU-RGB+D dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Non-local GCN.</head><p>As introduced in Section 3.2, there are 3 parts for in NL-GCN block, i.e. A, B and C. We manually delete one of the part and show their performance in Tab. 4.3.1. It shows that adaptively learning the graph is benefit for action recognition and deleting anyone of the three parts will harm the performance. With all three parts together, the model get the best performance.    Right is the non-local adjacent matrix after adding Bj and Cj in Eq. 5. The color of each element Aij in matrix represents the tightness of the connection between joint i and joint j. <ref type="figure">Fig.8</ref> shows an example of edges connected to 25 th joint in NTU-RGB+D dataset learned by our model. Each circle represents one joint, whose size indicates the tightness of <ref type="figure">Figure 8</ref>. Visualization of the edges for 25 th joint of right sketch in <ref type="figure" target="#fig_5">Fig.6</ref>. The size of the circle represents the tightness of the connection. From left to right is the visualization of different layers (3 th , 5 th and 7 th layer in <ref type="figure" target="#fig_3">Fig. 4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Visualization of NLGCN block</head><p>the connection between current joint and the 25 th joint. The examples showed from left to right are the visualizations of the second subset of 3 th , 5 th and 7 th layers in <ref type="figure" target="#fig_3">Fig. 4</ref>, respectively. It shows the connection and their tightness is individual for different layers. It verified our viewpoint that different layers contain different-level semantic information, which should own distinct graphs.  <ref type="figure" target="#fig_9">Fig.9</ref> shows the edges connected to 25 th joint in NTU-RGB+D dataset for different samples. The learned parameter (adjacent matrix) is extracted from the second subset of 5 th layer in model ( <ref type="figure" target="#fig_3">Fig. 4)</ref>. It shows the model learned different connections for different samples, which confirms our point of view and is more intuitive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Two-stream.</head><p>Another important improvement is the utility of secondorder information introduced in Section 3.3. Here we compare the performance of using each kind of input data alone, showed as Js-NLGCN and Bs-NLGCN in Tab. 4.3.3, and the performance when combine them as described in Section 3.3, showed as 2s-NLGCN in Tab. 4.3.3. It shows that combining the two kinds of data as input outperforms one stream based methods, which verifies the importance of the second-order information (bones information) for skeleton based action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Accuracy (%) Js-NLGCN 93.7 Bs-NLGCN 93.2 2s-NLGCN 95.1 <ref type="table">Table 2</ref>. Recognition accuracy with different input modality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with the state-of-the-art</head><p>We compare the final model with the state-of-the-art skeleton-based action recognition methods in both NTU-RGB+D dataset and Kinetics dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">NTU-RGB+D dataset</head><p>In NTU-RGB+D dataset, our model is compared with one hand-craft feature based methods, i.e. Lie Group <ref type="bibr" target="#b19">[20]</ref>, three LSTM based methods, i.e. HBRNN <ref type="bibr" target="#b3">[4]</ref>, STA-LSTM <ref type="bibr" target="#b18">[19]</ref> and VA-LSTM <ref type="bibr" target="#b22">[23]</ref>, three CNN based methods, i.e. TCN <ref type="bibr" target="#b9">[10]</ref>, Synthesized CNN <ref type="bibr" target="#b14">[15]</ref>, Motion+Trans+CNN <ref type="bibr" target="#b12">[13]</ref>, 3scale ResNet152 <ref type="bibr" target="#b11">[12]</ref> and one graph convolution based method, i.e. ST-GCN <ref type="bibr" target="#b21">[22]</ref>. These methods are briefly introduced in Section 2. <ref type="table" target="#tab_1">Table 4</ref>.4.1 shows that the performance of deep learning based methods is generally better than hand-craft feature based methods, and CNN based methods are generally better than RNN based methods. Our model outperforms these methods even without using dataaugmentation skills, which verifies the superiority of our model for skeleton-based action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>X-Sub (%) X-View (%) Lie Group <ref type="bibr" target="#b19">[20]</ref> 50.1 82.8 HBRNN <ref type="bibr" target="#b3">[4]</ref> 59.1 64.0 STA-LSTM <ref type="bibr" target="#b18">[19]</ref> 73.4 81.2 VA-LSTM <ref type="bibr" target="#b22">[23]</ref> 79.2 87.7 Temporal Conv. <ref type="bibr" target="#b9">[10]</ref> 74.3 83.1 Synthesized CNN <ref type="bibr" target="#b14">[15]</ref> 80.0 87.2 Motion+Trans+CNN 83.2 89.3 3scale ResNet152 <ref type="bibr" target="#b11">[12]</ref> 85.0 92.3 ST-GCN <ref type="bibr" target="#b21">[22]</ref> 81.5 88.3 2s-NLGCN (ours) 88.5 95.1 <ref type="table">Table 3</ref>. Compare with the state-of-the-art methods in NTU-RGB+D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Kinetics dataset</head><p>In Kinetics, our model is compared with one hand-crafted features based method, i.e. "Feature Encoding" <ref type="bibr" target="#b5">[6]</ref>, one LSTM base methods, i.e. Deep LSTM <ref type="bibr" target="#b16">[17]</ref>, one CNN based methods, i.e. TCN <ref type="bibr" target="#b9">[10]</ref> and one graph convolution based method, i.e. ST-GCN <ref type="bibr" target="#b21">[22]</ref>. These methods are briefly introduced in Section 2. The top-1 and top-5 recognition accu-racies are reported in <ref type="table" target="#tab_1">Table 4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Conclusion</head><p>In this work, we propose a non-local graph convolution block for skeleton based action recognition, which can overcome the weakness of manual design of graph in ST-GCN. Furthermore, we found that not only the joint but also the bone information is important for action recognition. So we represent the bone information with bone vector and propose a two-stream network to separately model the two input. The final model is evaluated on two large-scale action recognition datasets, NTU-RGB+D and Kinetics, and achieves the state-of-the-art performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>(a).The spatial-temporal graph of skeleton. (b).The partition strategy. Different colors denote different subsets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>(a).The spatial-temporal graph of skeleton. (b).The partition strategy. Different colors denote different subsets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The basic non-local GCN block. Conv s represent the spatial GCN and conv t represent the temporal GCN, both are followed with the BN and Relu layers. Besides, a residual connection is added for each block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>The structure of the NLGCN network. There are totally 9 NLGCN layers. The number of each layer represent the number of input channels, the number of output channels and the stride. GAP represent the global average pooling layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>The overall architecture of 2s-NLGCN. The scores of two stream are added to get the final prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Left is the joint label of Kinetics dataset and right is the joint label of NTU-RGB+D dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7</head><label>7</label><figDesc>shows an example of the learned adjacent matrix by our model for second subset. The color of each element A ij in matrix represents the tightness of the connection between</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>joint i and joint j. The left is the original matrix employed in ST-GCN, which is same for different layers and different samples. The right is the learned matrix of the first layer of one sample. It is obviously different from the original matrix, which is more flexible and not constrained to the physical connections of the human body.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Left is the original adjacent matrix for second subset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 .</head><label>9</label><figDesc>Visualization of different samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of the recognition accuracies when adding NLGCN block with or without A, B and C.</figDesc><table><row><cell>Methods</cell><cell>Accuracy (%)</cell></row><row><cell>STGCN</cell><cell>92.7</cell></row><row><cell>NLGCN wo/A</cell><cell>93.4</cell></row><row><cell>NLGCN wo/B</cell><cell>93.3</cell></row><row><cell>NLGCN wo/C</cell><cell>93.4</cell></row><row><cell>NLGCN</cell><cell>93.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 4 .</head><label>4</label><figDesc>.4.2, where our model outperforms the other methods with a large margin. Compare with the state-of-the-art methods in Kinetics.</figDesc><table><row><cell>Methods</cell><cell cols="2">Top-1 (%) Top-5 (%)</cell></row><row><cell>Feature Enc. [6]</cell><cell>14.9</cell><cell>25.8</cell></row><row><cell>Deep LSTM [17]</cell><cell>16.4</cell><cell>35.3</cell></row><row><cell>TCN [10]</cell><cell>20.3</cell><cell>40.0</cell></row><row><cell>ST-GCN [22]</cell><cell>30.7</cell><cell>52.8</cell></row><row><cell>Js-NLGCN (ours)</cell><cell>35.1</cell><cell>57.1</cell></row><row><cell>Bs-NLGCN (ours)</cell><cell>33.3</cell><cell>55.7</cell></row><row><cell>2s-NLGCN (ours)</cell><cell>36.1</cell><cell>58.7</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Spectral Networks and Locally Connected Networks on Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Realtime multiperson 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional Networks on Graphs for Learning Molecular Fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Modeling video evolution for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mogrovejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Antonio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings CVPR 2015</title>
		<meeting>CVPR 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5378" to="5387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep convolutional networks on graph-structured data</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The Kinetics Human Action Video Dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Interpretable 3d human action analysis with temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1623" to="1631" />
		</imprint>
	</monogr>
	<note>2017 IEEE Conference on. bibtex: kim2017interpretable bibtex[organization=IEEE</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<idno>arXiv: 1609.02907</idno>
		<imprint>
			<date type="published" when="2016-09" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Skeleton based action recognition using translation-scale invariant image mapping and multi-scale deep CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="601" to="604" />
		</imprint>
	</monogr>
	<note>Multimedia &amp; Expo Workshops</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="597" to="600" />
		</imprint>
	</monogr>
	<note>Multimedia &amp; Expo Workshops</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08106</idno>
		<idno>arXiv: 1705.08106</idno>
		<title level="m">Two-Stream 3d Convolutional Neural Network for Skeleton-Based Action Recognition</title>
		<imprint>
			<date type="published" when="2017-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Enhanced skeleton visualization for view invariant human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="346" to="362" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>bibtex: liu2017enhanced bibtex[publisher=Elsevier</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">NTU RGB+D: A Large Scale Dataset for 3d Human Activity Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The Emerging Field of Signal Processing on Graphs: Extending High-Dimensional Data Analysis to Networks and Other Irregular Domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">I</forename><surname>Shuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1211.0053</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="83" to="98" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An Endto-End Spatio-Temporal Attention Model for Human Action Recognition from Skeleton Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Human action recognition by representing 3d skeletons as points in a lie group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="588" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07971</idno>
		<idno>arXiv: 1711.07971</idno>
		<title level="m">Non-local Neural Networks</title>
		<imprint>
			<date type="published" when="2017-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">2018</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">View Adaptive Recurrent Neural Networks for High Performance Human Action Recognition From Skeleton Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
