<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchically Structured Meta-learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxiu</forename><surname>Yao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wei</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhui</forename><surname>Li</surname></persName>
						</author>
						<title level="a" type="main">Hierarchically Structured Meta-learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In order to learn quickly with few samples, metalearning utilizes prior knowledge learned from previous tasks. However, a critical challenge in meta-learning is task uncertainty and heterogeneity, which can not be handled via globally sharing knowledge among tasks. In this paper, based on gradient-based meta-learning, we propose a hierarchically structured meta-learning (HSML) algorithm that explicitly tailors the transferable knowledge to different clusters of tasks. Inspired by the way human beings organize knowledge, we resort to a hierarchical task clustering structure to cluster tasks. As a result, the proposed approach not only addresses the challenge via the knowledge customization to different clusters of tasks, but also preserves knowledge generalization among a cluster of similar tasks. To tackle the changing of task relationship, in addition, we extend the hierarchical structure to a continual learning environment. The experimental results show that our approach can achieve state-of-the-art performance in both toy-regression and few-shot image classification problems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Learning quickly with a few samples is one of the key characteristics of human intelligence, while it remains a daunting challenge for artificial intelligence. Learning to learn (a.k.a., meta-learning) <ref type="bibr" target="#b16">(Braun et al., 2010)</ref>, as a common practice to address this challenge, leverages the transferable knowledge learned from previous tasks to improve the learning effectiveness in a new task. There have been several lines of meta-learning algorithms, including recurrent network based methods <ref type="bibr" target="#b46">(Ravi &amp; Larochelle, 2016)</ref>, optimizer based † Part of the work was done when the author interned in Tencent AI Lab. <ref type="bibr">1</ref>  methods <ref type="bibr" target="#b14">(Andrychowicz et al., 2016)</ref>, nearest neighbours based methods <ref type="bibr" target="#b50">Vinyals et al., 2016)</ref> and gradient descent based methods , which instantiate the transferable knowledge as latent representations, an optimizer, a metric space, and parameter initialization, respectively.</p><p>Despite their early success in few-shot image classification <ref type="bibr" target="#b46">(Ravi &amp; Larochelle, 2016)</ref> and machine translation <ref type="bibr" target="#b28">(Gu et al., 2018)</ref>, most of the existing meta-learning algorithms assume the transferable knowledge to be globally shared across all tasks. As a consequence, they suffer from handling a sequence of tasks originated from different distributions. At the other end of the spectrum, recently, a few research works <ref type="bibr" target="#b56">Yoon et al., 2018a;</ref><ref type="bibr" target="#b34">Lee &amp; Choi, 2018)</ref> try to fix the problem by tailoring the transferable knowledge to each task. Yet the downside of such methods lies in the impaired knowledge generalization among closely correlated tasks (e.g., the tasks sampled from the same distribution).</p><p>Hence we are motivated to pursue a meta-learning framework to effectively balance generalization and customization. The inspiration comes from a hypothesis which has been formulated and tested by psychological researchers <ref type="bibr" target="#b25">(Gershman et al., 2010;</ref>. The hypothesis suggests that the key to human beings' capability of solving a task with little training data is the way how human beings organize the learned knowledge from tasks. As bits of tasks impinge on us, we human beings cluster the tasks into several states based on task similarity, so that the learning occurs within each cluster instead of across cluster boundaries. Thus, when a new task arrives, it can either quickly take advantage of the knowledge learned within the cluster it belongs to or initiate a new cluster if it is wildly different from any existing clusters.</p><p>Inspired by this, we propose a novel meta-learning framework called Hierarchically Structured Meta-Learning (HSML). The key idea of the HSML is to enhance the meta-learning effectiveness by promoting knowledge customization to different clusters of tasks but simultaneously preserving knowledge generalization among a cluster of closely related tasks. In this paper, without loss of generality, we ground HSML on a gradient based meta learning algorithm  with the transferable knowl-arXiv:1905.05301v2 <ref type="bibr">[cs.</ref>LG] 15 Nov 2019</p><p>Hierarchically Structured Meta-learning task 1 task 2 task 3 task 4 t (a) : globally shared task 1 task 2 task 3 task 4 t (b) : task specific task 4 task 1 task 3 task 2 t (c) : HSML <ref type="figure">Figure 1</ref>. Pictorial illustration of the difference between the proposed HSML and the other two representative lines of gradient based meta-learning algorithms. edge instantiated as parameter initializations. Specifically, first, the HSML resorts to a hierarchical clustering structure to perform soft clustering on tasks. The representation of each task is learned from either of the two proposed candidate aggregators, i.e., pooling autoencoder aggregator and recurrent autoencoder aggregator, and is passed to the hierarchical clustering structure to obtain the clustering result of this task. The sequentially incoming tasks, in turn, update the clustering structure. Especially, if the existing structure does not fit the task, we dynamically expand the structure. Secondly, a globally shared parameter initialization is tailored to each cluster via a parameter gate, to serve as the initializations for all tasks belonging to the cluster.</p><p>Again we would highlight the contribution of the proposed HSML: 1) it achieves a better balance between generalization and customization of the transferable knowledge, so that it empirically outperforms state-of-the-art meta-learning algorithms in both toy regression and few-shot image classification problems; 2) it is interpretable in terms of the task relationship; 3) it has been theoretically proved to be superior than existing gradient-based meta-learning algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Meta-learning, allowing machines to learn new skills or adapt to new environments rapidly with a few training examples, has demonstrated success in both supervised learning such as few-shot image classification and reinforcement learning settings. There are four common approaches: 1) use a recurrent neural network equipped with either external or internal memory storing and querying meta-knowledge <ref type="bibr" target="#b41">(Munkhdalai &amp; Yu, 2017;</ref><ref type="bibr" target="#b47">Santoro et al., 2016;</ref><ref type="bibr" target="#b42">Munkhdalai et al., 2018;</ref><ref type="bibr" target="#b40">Mishra et al., 2018)</ref>; 2) learn a meta-optimizer which can quickly optimize the model parameters <ref type="bibr" target="#b46">(Ravi &amp; Larochelle, 2016;</ref><ref type="bibr" target="#b14">Andrychowicz et al., 2016;</ref><ref type="bibr" target="#b35">Li &amp; Malik, 2016)</ref>; 3) learn an effective distance metric between examples <ref type="bibr" target="#b50">Vinyals et al., 2016;</ref><ref type="bibr" target="#b54">Yang et al., 2018)</ref>; 4) learn an appropriate initialization from which the model parameters can be updated within a few gradient steps <ref type="bibr" target="#b34">Lee &amp; Choi, 2018)</ref>.</p><p>HSML falls into the fourth aforementioned category named as gradient-based meta-learning. Most of the gradient-based meta-learning algorithms <ref type="bibr" target="#b36">Li et al., 2017;</ref><ref type="bibr" target="#b23">Flennerhag et al., 2018</ref>) assume a globally shared initialization across all tasks, as shown in <ref type="figure">Figure 1a</ref>. To accommodate dynamically changing tasks, as illustrated in <ref type="figure">Figure 1b</ref>, recent studies tailor the global shared initialization to each task by taking advantage of probabilistic models <ref type="bibr" target="#b27">Grant et al., 2018;</ref><ref type="bibr" target="#b56">Yoon et al., 2018a</ref>) and incorporating task-specific information <ref type="bibr" target="#b34">(Lee &amp; Choi, 2018;</ref><ref type="bibr" target="#b51">Vuorio et al., 2018)</ref>. However, our proposed HSML outlined in <ref type="figure">Figure 1c</ref> customizes the global shared initialization to each cluster using a hierarchical clustering structure, which enjoys not only knowledge customization but also generalization (e.g., between task 1 and 3). Better yet, HSML right fit to a continual learning scenario with evolving clustering structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminaries</head><p>The Meta-Learning Problem Suppose that a sequence of tasks {T1, ..., TN t } are sampled from an environment which is a probability distribution E on tasks <ref type="bibr" target="#b15">(Baxter, 1998)</ref>. In each task Ti ∼ E, we have a few examples {xi,j, yi,j} n tr j=1 to constitute the training set D tr T i and the rest as the test set D te T i . Given a base learner f with θ as parameters, the optimal parameters θT i are learned to make accurate predictions, i.e.,</p><formula xml:id="formula_0">f θ T i (xi,j) → yi,j. The effectiveness of such a base learner on D tr T i is evaluated by the loss function L(f θ T i , D tr T i ), which equals the mean square error (x i,j ,y i,j )∈D tr T i f θ T i (xi,j)−</formula><p>yi,j 2 2 for regression problems or the cross entropy loss − (x i,j ,y i,j )∈D tr T i log p(yi,j|xi,j, f θ T i ) for classification problems.</p><p>The goal of meta-learning is to learn from previous tasks a well-generalized meta-learner M(·) which can facilitate the training of the base learner in a future task with a few examples. In fulfillment of this, meta-learning involves two stages, i.e., meta-training and meta-testing. During metatraining, the parameters of the base learner for all tasks, i.e.,</p><formula xml:id="formula_1">{θT i } N t i=1</formula><p>, and the meta-learner M(·) are optimized alternatingly. In virtue of M, the parameters {θT i } N t i=1 are learned to minimize the expected empirical loss over training sets of all Nt historical tasks, i.e.,</p><formula xml:id="formula_2">min {θ T i } N t i=1 N t i=1 L(M(f θ T i ), D tr T i ).</formula><p>In turn, a well-generalized M can be obtained by minimizing the expected empirical loss over test sets, i.e.,</p><formula xml:id="formula_3">minM N t i=1 L(M(f θ T i ), D te T i ).</formula><p>When it comes to the metatesting phase, provided with a future task Tt, the learning effectiveness and efficiency are improved by applying the meta-learner M and solving min</p><formula xml:id="formula_4">θ T t L(M(f θ T t ), D tr T t ).</formula><p>Gradient-based Meta-Learning Here we give an overview of the representative algorithm, model-agnostic metalearning (MAML) . MAML instantiates the meta-learner M as a well-generalized initialization for the parameters of a base learner from which a few gradient descent steps can be performed to reach the optimal θT i for the task Ti, which means</p><formula xml:id="formula_5">M(f θ T i ) = f θ 0 −α∇ θ L(f θ ,D tr T i ) .</formula><p>As a result, the optimization of M during meta-training is formulated as (one gradient step as exemplary):</p><formula xml:id="formula_6">min θ 0 N t i=1 L(f θ 0 −α∇ θ L(f θ ,D tr T i ) , D te T i ).</formula><p>(1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methodology</head><p>In this section, we detail the proposed HSML algorithm whose framework is presented in <ref type="figure">Figure 2</ref>. The HSML aims to adapt the transferable knowledge learned from previous tasks, namely the initialization for parameters of the base learner in gradient based meta-learning (θ 0 here), to the task T i in a cluster-specific manner, so that the optimal parameters θ Ti can be achieved in as few gradient descent steps as possible. As shown in the part (c) of <ref type="figure">Figure 2</ref>, the possibilities to adaptation are completely dictated by the hierarchical task clustering structure in part (b), and the eventual path for adaptation follows the clustering result on the task T i (i.e., θ B1 ). By this means, the HSML balances between customization and generalization: the transferable knowledge is adapted to different clusters of tasks, while it is still shared among closely related tasks pertaining to the same cluster.</p><p>To perform hierarchical clustering on tasks, we learn the representation of a task using the proposed task embedding network, i.e., the part (a). Next we will introduce the three stages, i.e., task representation learning, hierarchical task clustering, and knowledge adaptation, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Task Representation Learning</head><p>Learning the representation of a task Ti with the whole training set D tr T i as input is much more challenging than the common representation learning over examples, which bears a striking similarity to the connection between sentence embeddings and word embeddings in natural language processing. Inspired by common practices in learning sentence embeddings <ref type="bibr" target="#b18">(Conneau et al., 2017)</ref>, we tackle the challenge by aggregating representations of all examples {xi,j, yi,j} n tr j=1 ∈ D tr T i . The desiderata of an ideal aggregator include 1) high representational capacity, and 2) permutational invariance to its inputs. In light of these, we propose two candidate aggregators, i.e., pooling autoencoder aggregator (PAA) and recurrent autoencoder aggregator (RAA). Pooling Autoencoder Aggregator To meet the first desideratum, foremost, we resort to an autoencoder that learns highly effective representation for each example. The recontruction loss for training the autoencoder is as follows,</p><formula xml:id="formula_7">Lr(D tr T i ) = n tr j=1 FC dec (gi,j) − F (x tr i,j , y tr i,j ) 2 2 ,<label>(2)</label></formula><p>where gi,j = FCenc(F(x tr i,j , y tr i,j )) is the representation for the j-th example. In order to characterize the joint distribution instead of the marginal distribution only, we use F(·, ·) to preliminarily embed both features and predictions of an example. The definition of F varies from dataset to dataset, which we will detail in supplementary material C. FCenc and FC dec stand for the encoder composed of a stack of fully connected layers and the decoder consisting of two fully connected layers with ReLU activation, respectively. Consequently, the aggregation satisfying the permutational invariance follows,</p><formula xml:id="formula_8">gi = Pool n tr j=1 (gi,j),<label>(3)</label></formula><p>where gi ∈ R d is the desired representation of task Ti. Pool denotes a max or mean pooling operator over examples. Recurrent Autoencoder Aggregator Motivated by recent success of the recurrent embedding aggregation in orderinvariant problems such as graph embedding <ref type="bibr" target="#b29">(Hamilton et al., 2017)</ref>, we also consider a recurrent autoencoder aggregator which demonstrates more remarkable expressivity especially for a task with few examples. Different from the pooling autoencoder, examples are sequentially fed into the recurrent autoencoder, i.e.,</p><formula xml:id="formula_9">F(x tr i,1 , y tr i,1 ) → gi,1 → · · · → g i,n tr → d i,n tr → · · · → di,1,<label>(4)</label></formula><p>where ∀j, gi,j = RNNenc(F(x tr i,j , y tr i,j ), gi,j−1) and di,j = RNN dec (di,j+1) represent the learned representation and the reconstruction of the j-th example, respectively. Here RNNenc and RNN dec stand for a recurrent encoder (LSTM or GRU) and a recurrent decoder, respectively. The reconstruction loss is similar to Eqn. (2), except that FC dec (gi,j) is replaced with di,j. Thereupon, the task representation is aggregated over representations of all examples, i.e., gi = 1 n tr n tr j (gi,j).</p><p>Regrettably, the sequential feeding of examples makes the final task representation to be permutation sensitive, which violates the second prerequisite of an ideal aggregator. We address the problem by applying the recurrent aggregator to random permutations of examples <ref type="bibr" target="#b29">(Hamilton et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Hierarchical Task Clustering</head><p>Given the representation of a task, we propose a hierarchical task clustering structure to locate the cluster the task belongs to. Before proceeding to detail the structure, we first explicate why the hierarchical clustering is preferred over flat clustering: a single level of task groups is likely insufficient to model complex task relationship in real-world applications; for example, to identify the cross-talks between gene expressions of multiple species, the study <ref type="bibr" target="#b30">(Kim &amp; Xing, 2010)</ref> suggests multi-level clustering of such gene interaction.</p><p>The hierarchical clustering, following the tradition of clustering, proceeds by alternating between two steps, i.e., assignment step and update step, in a layer-wise manner. Assignment step: Each task receives a cluster assignment score on each hierarchical level, and the assignment that it  <ref type="figure">Figure 2</ref>. The framework of the proposed HSML involving three essential stages. (a) Task representation learning: we learn the representation for the task Ti using an autoencoder aggregator (e.g., pooling aggregator, recurrent aggregator). (b) Hierarchical task clustering: provided with the task representation, we learn the soft clustering assignment with this differentiable hierarchical clustering structure. Darker nodes signify more likely assigned clusters (e.g., the cluster 1 in the first level and the cluster B in the second level). (c) Knowledge adaptation: we next use a parameter gate to adapt the transferable knowledge (θ0) to a cluster-specific initialization (θB1) from which only a few gradient descent steps are required to achieve the optimal parameters θT i . receives in a particular level is a function of its representation in the previous level. Thus, we assign a task represented in the k l -th cluster of the l-th level, i.e., h k l i ∈ R d , to the k l+1 -th cluster in the (l + 1)-th level. Note that we conduct soft assignment for the following two reasons: (1) task groups have been demonstrated to overlap, since there is always a continuum in the sharing between tasks <ref type="bibr" target="#b31">(Kumar &amp; Daumé III, 2012)</ref>; (2) the soft instead of hard assignment guarantees the differentiability, so that the full HSML framework can still be trained in an end-to-end fashion. In particular, for each task Ti, the soft-assignment p k l →k l+1 i is computed by applying softmax over Euclidean distances between h k l i and the learnable cluster centers</p><formula xml:id="formula_11">{c k l+1 } K l+1 k l+1 =1 , i.e., p k l →k l+1 i = exp (− (h k l i − c k l+1 )/σ l 2 2 /2) K l+1 k l+1 =1 exp (− (h k l i − c k l+1 )/σ l 2 2 /2) ,<label>(6)</label></formula><p>where σ l is a scaling factor in the l-th level and K l+1 denotes the number of clusters in the (l+1)-th level.</p><p>Update step: As a result of assignment, the representation of a task in the k l+1 -th cluster of the (l +1)-th level, i.e., h k l+1 i , can be updated with the following weighted average,</p><formula xml:id="formula_12">h k l+1 i = K l k l =1 p k l →k l+1 i tanh (W k l+1 h k l i + b k l+1 ),<label>(7)</label></formula><p>where W k l+1 ∈ R d×d and b k l+1 ∈ R d are learned to transform from representations of the l-th to those of the (l+1)-th level.</p><p>The full pipeline of clustering starts from l = 0, where the initialization for h k 0 i equals the task representation gi and K 0 = 1, and ends at K L = 1. We would especially discuss the cluster centers. The meta-learning scenario where training tasks come sequentially poses a unique challenge which requires the hierarchical clustering structure to be accordingly online. Therefore, the cluster centers are parameterized and learned as the learning proceeds. Each center is randomly initialized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Knowledge Adaptation</head><p>The final representation h L i , which encrypts the hierarchical clustering result, is believed to be cluster specific. Previous works <ref type="bibr" target="#b53">(Xu et al., 2015;</ref><ref type="bibr" target="#b34">Lee &amp; Choi, 2018)</ref> suggest that similar tasks activate similar meta-parameters (e.g., initialization) while different tasks trigger disparate ones. Inspired by this finding, we design a cluster-specific parameter gate,</p><formula xml:id="formula_13">oi = FC σ Wg (gi ⊕ h L i ),<label>(8)</label></formula><p>where the fully connected layer FC σ Wg is parameterized by Wg and activated by a sigmoid function σ. It is worth mentioning here that concatenating the task representation gi together with h L i not only preserves but also reinforces the cluster-specific property of the parameter gate. Most importantly, the globally transferable knowledge, i.e., the initial parameters θ 0 , is adapted to the cluster-specific initial parameters θ 0i via the parameter gate, i.e., θ0i = θ0 • oi.</p><p>Recalling the objectives for a meta-learning algorithm in Section 3, we reach the optimization problem for HSML:</p><formula xml:id="formula_14">min Θ N t i=1 L(f θ 0i −α∇ θ L(θ,D tr T i ) , D te T i ) + ξLr(D tr T i ),<label>(9)</label></formula><p>where L defined in Section 3 measures the empirical risk over D te Ti and L r measures the reconstruction error as defined in Eqn. (2). ξ is used to balance the importance of these two items. Θ represents all learnable parameters including the global transferable initialization θ 0 , the parameters for clustering, and those for knowledge adaptation (i.e., Wg). Continual Adaptation We especially pay attention to the case where a new task does not fit any of the learned task clusters, which implies that additional clusters should be introduced to the hierarchical clustering structure. Incrementally adding model capacity <ref type="bibr" target="#b57">(Yoon et al., 2018b;</ref><ref type="bibr" target="#b19">Daniely et al., 2015)</ref>, has been the common practice to handle distribution drift without initially introducing excessive parameters. The key lies in the criterion when to expand the clustering structure. Since the loss values of </p><formula xml:id="formula_15">L(f θ T i , D te T i ) fluctuate</formula><formula xml:id="formula_16">Sample D tr T i , D te T i from Ti 9:</formula><p>Compute gi in Eqn.</p><p>(3) or Eqn. <ref type="formula" target="#formula_10">(5)</ref>, h L i in Eqn. <ref type="formula" target="#formula_12">(7)</ref>, and reconstruction error Lr(D tr</p><formula xml:id="formula_17">T i ) 10: Compute oi in Eqn. (8) and evaluate ∇ θ L(θ, D tr T i ) 11:</formula><p>Update parameters with gradient descent (taking one step as an example):</p><formula xml:id="formula_18">θT i = θ0i −α∇ θ L(θ, D tr T i ) 12: end for 13: Update Θ ← Θ − β∇Θ N t i=1 L(f θ T i , D te T i ) + ξLr(D tr T i ) 14:</formula><p>ComputeLnew and saveL old for every Q rounds 15: end while across different tasks during the online meta-training process, setting the loss value as threshold would obviously be futile. Instead, for every Q training tasks, we compute the average loss valueL. If the new average valueLnew is more than µ times the previous valueL old (i.e.,Lnew &gt; µL old ), the number of clusters will be increased, and the parameters for new clusters are randomly initialized. The whole algorithm of our proposed model is detailed in Alg. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Analysis</head><p>The core of HSML is to adapt a globally shared initialization of stochastic gradient descent (SGD) to be cluster specific via the proposed hierarchical clustering structure. Hence, in this section, we theoretically analyze the advantage of such adaptation in terms of the generalization bound.</p><p>For a task Ti ∼ E, we assume both training and testing examples are i.i.d. drawn from a distribution Si , i.e., D tr</p><formula xml:id="formula_19">T i ∼ Si and D te T i ∼ Si. According to Theorem 2 in (Kuzborskij &amp; Lampert, 2017), a base learner f θ T i is (Si, θ0)-on-average stable if its generalization is bounded by (Si, θ0), i.e., ES i E f θ T i [R(f θ T i (D tr T i ))−R D tr T i (f θ T i (D tr T i ))] ≤ (Si, θ0). θ0 is the initialization of SGD to reach θT i , and R(·) andR D tr T i (·)</formula><p>denote the expected and empirical risk on D tr T i , respectively. Transferring the globally shared initialization θ0 (i.e., MAML) to the target task Tt is equivalent to transferring a hypothesis f θ 0 learned from meta-training tasks like <ref type="bibr" target="#b33">(Kuzborskij &amp; Orabona, 2017)</ref>. For HSML, the initialization can be represented as θ0t = K k=1B k θ0, which we demonstrate in the supplementary material A. In the following two theorems, provided with an initialization θ0t , we derive according to <ref type="bibr" target="#b32">(Kuzborskij &amp; Lampert, 2017)</ref> the generalization bounds of the base learner f θ T t when the loss L is convex and non-convex, respectively.</p><formula xml:id="formula_20">Theorem 1 Assume that L is convex and f θ T t optimized using SGD is St, θ0t -on-average stable. Then St, θ0t is bounded by, O R D tr T t (θ0t) + 1 n tr .<label>(10)</label></formula><p>Theorem 2 Assume that L ∈ [0, 1] is η-smooth and has a ρ-Lipschitz Hessian. The step size at the u-step αu = c/u satisfying c ≤ min{ 1 η , 1 4(2η ln U ) 2 } with total steps U = n tr and</p><formula xml:id="formula_21">γ ± = 1 n tr n tr j=1 ∇ 2 L(θ0t, (xt,j, yt,j)) 2 + R D tr T t (θ0t) ± 1 4</formula><p>√ n tr and then <ref type="bibr">(St,</ref><ref type="bibr">θ0t)</ref> is bounded by,</p><formula xml:id="formula_22">O 1 + 1 cγ − R D tr T t (θ0t) cγ + 1+cγ + 1 (n tr ) 1 1+cγ + .<label>(11)</label></formula><p>Though some standard base learners (e.g., 4 convolutional layers in few-shot image classification ) with ReLU do not meet the property of Lipschitz Hessian, following <ref type="bibr" target="#b43">(Nguyen &amp; Hein, 2018)</ref>, a softplus function</p><formula xml:id="formula_23">f (x) = 1 κ log(1 + exp(κx))</formula><p>can arbitrarily well approximate ReLU by adjusting κ and thus Theorem 2 holds. In both cases, MAML can be regarded as the special case of HSML, i.e., ∀k,B k = I, where I is an identity matrix. Remarkably, by proving</p><formula xml:id="formula_24">∃{B k } K k=1 , s.t.,R D tr T t (θ0t) ≤R D tr T t<label>(</label></formula><p>θ0), we conclude that HSML achieves a tighter generalization bound than MAML and thereby is much more favored. Consider the optimization process starting from θ 0 , through the negative gradient direction,θ0 = (I − α∇L(θ0)(θ0I) −1 )θ0 and</p><formula xml:id="formula_25">R D tr T t (θ0) ≤R D tr T t<label>(</label></formula><p>θ0). Thus, we can find a K k=1B k = I − α∇L(θ0)(θ0I) −1 . We provide more details about analysis in supplementary material A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>In this section, we evaluate the effectiveness of HSML. The goal of our experimental evaluation is to answer the following questions: (1) Can our approach outperform other meta-learning algorithms in toy regression and few-shot image classification tasks? (2) Can our approach discover reasonable task clusters? (3) Can our approach update the clustering structure in the continual learning manner and achieve better performance?</p><p>We study these questions on toy regression and few-shot image classification problems. For gradient-based metalearning algorithms, we select the following as baselines: (1) globally shared models including MAML  and Meta-SGD <ref type="bibr" target="#b36">(Li et al., 2017</ref>); (2) task specific models including MT-Net <ref type="bibr" target="#b34">(Lee &amp; Choi, 2018)</ref>, BMAML <ref type="bibr" target="#b56">(Yoon et al., 2018a)</ref> and MUMOMAML .</p><p>The empirical results indicate that recurrent autoencoder aggregator (RAA) is on average better than PAA for task representation, so that RAA is used as the default aggregator. We also provide a comparison of RAA and PAA on fewshot classification problem in supplementary material G. All the baselines use the same neural network structure (base learner). For hierarchical task clustering, like <ref type="bibr" target="#b55">(Ying et al., 2018)</ref>, the number of clusters in a high layer is half of that in its consecutive lower layer. We specify the hyperparameters for meta-training in supplementary material C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Toy Regression</head><p>Dataset and Experimental Settings In the toy regression problem, different tasks are sampled from different family of functions. In this paper, the underlying family functions are (1) Sinusoids:</p><formula xml:id="formula_26">y(x) = Asin(wx) + b, A ∼ U [0.1, 5.0], w ∼ U [0.8, 1.2] and b ∼ U [0, 2π]; (2) Line: y(x) = A l x + b l , A l ∼ U [−3.0, 3.0] and b l ∼ U [−3.0, 3.0]; (3) Cubic: y(x) = A c x 3 +b c x 2 +c c x+d c , A c ∼ U [−0.1, 0.1], b c ∼ U [−0.2, 0.2], c c ∼ U [−2.0, 2.0] and d c ∼ U [−3.0, 3.0]; (4) Quadratic: y(x) = A q x 2 +b q x+c q , A q ∼ U [−0.2, 0.2], b q ∼ U [−2.0, 2.0] and c q ∼ U [−3.0, 3.0]. U [·, ·]</formula><p>represents a uniform distribution. Each individual is randomly sampled from one of the four underlying functions. The input x ∼ U [−5.0, 5.0] for both training and testing tasks. We train all models for 5-shot and 10-shot regression. Mean square error (MSE) is used as evaluation metric. In hierarchical clustering, we set the number of layers to be three with 4, 2, 1 clusters in each layer, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results of Regression Performance</head><p>The results of 5-shot and 10-shot regression are shown in <ref type="table" target="#tab_2">Table 1</ref>. HSML improves the performance of global models (e.g., MAML) and task specific models (e.g., MUMOMAML), indicating the effectiveness of task clustering. Task Clustering Analysis in Toy Regression In order to show the power of HSML for detecting task clusters, we randomly select six tasks (more results are shown in supplementary material I) of 5-shot regression scenario and show soft-assignment values in <ref type="figure" target="#fig_1">Figure 3</ref>(a), i.e., the value of {p k 0 →k 1 i |∀k 1 } in Eqn. (6). Darker color stands for higher probability. The qualitative results of each task are shown in <ref type="figure" target="#fig_1">Figure 3</ref> respectively. Qualitative results of MAML, MUMOMAML (best baseline), HSML are shown in different colors.</p><p>As shown in the heatmap, sinusoids and linear with positive slope activate cluster 1 and 3, respectively. Both quadratic 1 and 2 activate cluster 2, while quadratic 1 also activates cluster 1 and quadratic 2 also activates cluster 3. From the qualitative results, we can see the shape of quadratic 2 is similar to that of linear with positive slope, while quadratic 1 has more apparent curvature. Similar findings also verify in cubic cases. The shape of cubic 2 is very similar to sinusoids, thus cluster 1 is activated. Different from cubic 2, cubic 1 mainly activates cluster 4, whose shape is similar to linear with negative slope. The results indicate that the main cluster criteria of HSML is the shapes of tasks despite the underlying family functions. Furthermore, according to the qualitative results, HSML fits better than other baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results of Continual Adaptation</head><p>To demonstrate the effectiveness of HSML under the continual learning scenario (HSML-D), we add more underlying functions during metatraining. First, we generate tasks from sinusoids and linear, and quadratic and cubic functions are added after 15,000 and 30,000 training rounds, respectively. For comparison, one baseline is HSML with 2 fixed clusters (HSML-S(2C)), and the other is HSML with 10 fixed clusters with much more representational capability (HSML-S(10C)). The metatraining loss curve and the meta-testing performance (MSE) are shown in <ref type="figure" target="#fig_2">Figure 4</ref>. We can see that HSML-D outperforms as expected. Especially, HSML-D performs better than HSML-S(10C) which are prone to overfit and stuck at local optima at early stages.    , we divide each dataset to meta-training, meta-validation and metatesting classes. Following the protocol in <ref type="bibr" target="#b46">Ravi &amp; Larochelle, 2016;</ref><ref type="bibr" target="#b50">Vinyals et al., 2016)</ref>, we adopt N-way classification with K-shot samples. Each task samples classes from one of the four datasets. Compared with previous benchmarks (e.g., MiniImagenet) that the tasks are constructed within a single dataset, the new benchmark is more heterogeneous and closer to the real-world image classification. Like , the base learner is a standard four-block convolutional architecture. The number of layers in hierarchical clustering structure is set as 3 with 4, 2, 1 clusters in each layer. Note that, in this section, for the tables without confidence interval, we provide the full results in supplementary material F. In addition, we provide the comparison to MiniImagenet benchmark in supplementary material D. Note that, the sampled tasks from MiniImagenet do not have obvious heterogeneity and uncertainty. Our approach achieves comparable results among gradient-based meta-learning methods.</p><p>Results of Classification Performance For each dataset, we report the averaged accuracy over 1000 tasks of 5-way 1-shot/5-shot classification in <ref type="table" target="#tab_5">Table 2</ref>. HSML consistently outperforms the other baselines on each dataset, which demonstrates the power of modeling hierarchical clustering structure. To verify the effectiveness of our proposed three components (i.e., task representation, hierarchical task clustering, knowledge adaptation), we also propose some variants of HSML. The detailed description of these variants and their corresponding results can be found in the supplementary material H, which further enhance the contribution of each component. In addition, we design another challeng-ing leave-one-out experiment in this benchmark. We use one dataset for meta-testing and the rest three for meta-training. The results are reported in the supplementary material E and the HSML still achieves the best performance. Task Clustering Analysis in Few-shot Classification Like the analysis of toy regression, we select four tasks in 5-way 1-shot classification and show their soft-assignment in <ref type="figure" target="#fig_3">Figure 5</ref> (more results are shown in the supplementary material J). Darker color means higher probability. Furthermore, in <ref type="figure" target="#fig_3">Figure 5</ref>, we show the learned hierarchical clustering of each task. In each layer, the top activated clusters are shown in darker color and then the activation paths are generated. From <ref type="figure" target="#fig_3">Figure 5</ref>, we can see different datasets mainly activate different clusters: bird→cluster 2, texture→cluster 4, aircraft→cluster 1, fungi→cluster 3. It is also interesting to find the clustering across different tasks via the second largest activated cluster which further promote knowledge transfer between tasks. The correlation may represent the similarity of shape (bird and aircraft), environment (fungi and bird), surface texture (texture and fungi). Note that, aircraft is correlated to texture because the classification of aircraft variant is mainly based on their shape and texture. The clustering can be further verified in the learned activated path. In the second layer, the left node, which may represent the environment, is activated by cluster 2 (activated by bird) and 3 (activated by fungi). The right node that reflects surface texture is activated by cluster 1 (activated by aircraft) and 4 (activated by texture). In <ref type="figure" target="#fig_4">Figure 6</ref>, in addition, we randomly select 1000 tasks from each dataset, and show the t-SNE <ref type="bibr" target="#b38">(Maaten &amp; Hinton, 2008)</ref> visualization of the gated weight, i.e., θ 0i , in Eqn. (9). Compared with MUMO-MAML, the results indicate that our clustering structure are able to identify the tasks in different clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results of Continual Adaptation</head><p>In few-shot classification task, we conduct the experiments for continual adaptation in the 5-way 1-shot scenario. Initially, the tasks are generated from bird and texture datasets. Then, aircraft and fungi datasets are added after approximately meta-training round 15000 and 25000, respectively. We show the average meta-training accuracy curve and meta-testing accuracy in <ref type="figure" target="#fig_5">Figure 7</ref>, where MUMOMAML, HSML-S(2C) and HSML-    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Cluster Numbers</head><p>We further analyze the effect of cluster numbers. The results are shown in <ref type="table" target="#tab_7">Table 3</ref>. The cluster numbers from bottom layer to top layer are saved in a tuple. We can see that too few clusters may not enough to learn the task clustering characteristic (e.g., case (2,2,1)). In this dataset, increasing layers (e.g., case (8,4,4,1)) achieves similar performance compared with case (4,2,1). However, the former introduces more parameters. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion and Discussion</head><p>In this paper, we introduce HSML to improve the metalearning effectiveness, which simultaneously customizing task knowledge and preserving knowledge generalization via hierarchical clustering structure. Compared with several baselines, experiments demonstrated the effectiveness and interpretability of our algorithm in both toy regression and few-shot classification problems.</p><p>Although our method is widely applicable, there are some limitations and interesting future directions. (1) In this paper, we provide a simple version for continual learning, where tasks from new underlying groups are added continually. However, to construct a more reliable lifelong learning system, it is will be necessary to consider more complex evolution relations between tasks (e.g., relationship forgetting); (2) Another interesting direction is to combining active learning with task relation learning for automatically exploring evolutionary task relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Detailed Theoretical Analysis</head><p>Proof of Theorem 1 Assuming a task T i is sampled from E, its training and testing samples are i.i.d. drawn from distribution S i , i.e., D tr Ti ∼ S i and D te Ti ∼ S i . According to Theorem 3 in <ref type="bibr" target="#b32">(Kuzborskij &amp; Lampert, 2017)</ref></p><formula xml:id="formula_27">, if L is convex, the base learner f θ T i SGD is (S i , θ 0 )-on-average-stable with (S i , θ 0 ) = O c(R(θ 0 ) − R * ) 4 √ T n tr + cσ √ T n tr ,<label>(12)</label></formula><p>where R * = inf θ∈H R(θ).</p><p>For a new task T t , we first prove that the initialization can be approximately represented as θ 0t = K k=1B k θ 0 . Wihtout loss of generality, here we consider a hierarchy C − L − 1 in HSML.</p><formula xml:id="formula_28">θ 0t = θ 0 • o t = diag(o t )θ 0 = diag(FC σ Wg (g t ⊕ h t ))θ 0 = diag(FC σ Wg (g t ⊕ h t ))θ 0 ≈ diag{a 1 [W g (g t ⊕ h t )] + a 2 }θ 0 = diag[W g (g t ⊕ h t ) + a 2 ]θ 0 = diag W gg g t ⊕ W gh L l=1 p l tanh W C c=1 p cl tanh(W l h c t + b l ) + b + a 2 ≈ diag W gg g t ⊕ W gh L l=1 p l W C c=1 p cl (W l h c t + b l ) + b + a 2 = diag L l=1 C c=1 1 LC W gg g t ⊕ p l W gh p cl WW l h c t + p cl Wb l + b C + a 2 LC θ 0 = K k=1B k θ 0 ,<label>(13)</label></formula><p>where K = CL andB (l−1) * C+c = 1 LC W gg g t ⊕ p l W gh p cl WW l h c t + p cl Wb l + b C + a2 LC . Note that the first equality holds by converting the Hadamard product into matrix multiplication, and the first and the second approximations come from first-order taylor series of sigmoid and hybolic functions. In addition, in the C − L − 1 hierarchical structure, ∀l, p l = 1.</p><p>From Eqn. 12, we can see that (S t , θ 0 ) depends R(θ 0 ). Like <ref type="bibr" target="#b32">(Kuzborskij &amp; Lampert, 2017)</ref>, when the optimization process for task T t starts from the equivalent form that θ 0t = K k=1B k θ 0 , we can bound (S t , θ 0t ) by using Hoeffding bound as:</p><formula xml:id="formula_29">(S t , θ 0t ) ≤ O R D tr T t (θ 0t ) + 1 n tr .<label>(14)</label></formula><p>Thus, we reach the conclusion.</p><p>Proof of Theorem 2 In non-convex case, we assume L is η-smooth and has ρ-Lipschitz Hessian. According to the Corollary 1 and Proposition 1 in <ref type="bibr" target="#b32">(Kuzborskij &amp; Lampert, 2017)</ref>, for task T t , we define: </p><formula xml:id="formula_30">γ = O E (xt,j ,yt,j )∼D tr T t [ ∇ 2 L(θ 0t , (x t,j , y t,j )) 2 ] + R (θ 0t ) ,<label>(15)</label></formula><formula xml:id="formula_31">∇ 2 L(θ 0t , (x t,j , y t,j )) 2 + R D tr T t (θ 0t ).<label>(16)</label></formula><p>Then, we use Hoeffding inequality and get</p><formula xml:id="formula_32">|γ −γ| ≤ O( 1 4 √ n tr ).<label>(17)</label></formula><p>Finally, letγ ± =γ ± 1/ 4 √ n tr , (S t , θ 0t ) can be bounded as:</p><formula xml:id="formula_33">(S t , θ 0t ) ≤ O 1 + 1 cγ − R D tr T t (θ 0t ) cγ + 1+cγ + 1 (n tr ) 1 1+cγ + .<label>(18)</label></formula><p>Thus, we reach our conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Existance of</head><formula xml:id="formula_34">K k=1B k</formula><p>Here, we provides more details about the analysis of existence of K k=1B k , i.e.,</p><formula xml:id="formula_35">∃{B k } K k=1 , s.t.,R D tr T t (θ 0t ) ≤R D tr T t (θ 0 )</formula><p>. Though the negative gradient descent, we can get</p><formula xml:id="formula_36">θ 0 = θ 0 − α∇L θ = (I − α∇L(θ 0 )(θ 0 I) −1 )θ 0 .<label>(19)</label></formula><p>Then, we can find a K k=1B k = I − α∇L(θ 0 )(θ 0 I) −1 . It can also be verified in <ref type="figure">Figure 8</ref>. Assume θ 0 is in the red contour, we can find a better parameterθ 0 inside the contour through its negative gradient direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Detailed Description of the New Few-shot Classification Benchmark</head><p>The new benchmark consists of four image classification datasets. All images are resized to 84 × 84 × 3. Here, we briefly introduce each of them as follows:</p><p>• Caltech-UCSD Birds-200-2011 (CUB-200-2011) <ref type="bibr" target="#b52">(Wah et al., 2011</ref>) is a bird image dataset which contains 11,788 photos of 200 bird species. In this paper, we randomly select 100 species with 60 photos in each species. We split the meta-training/meta-validation/meta-testing sets as 64/16/20 species.</p><p>• Describable Textures Dataset (DTD) <ref type="bibr" target="#b17">(Cimpoi et al., 2014</ref>) is a texture image dataset which contains 5640 images from 47 classes. Each class contains 120 images. Meta-training/Meta-validation/Meta-testing contains 30/7/10 classes respectively.</p><p>- <ref type="bibr">Meta-training: pitted, woven, crosshatched, crystalline, sprinkled, lacelike, bubbly, marbled, dotted, bumpy, striped, zigzagged, lined, smeared, pleated, stratified, waffled, knitted, gauzy, porous, spiralled, grooved, banded, potholed, stained, veined, swirly, frilly, freckled, studded. -Meta-validation: wrinkled, grid, perforated, cobwebbed, honeycombed, cracked, blotchy. -Meta-testing: fibrous, matted, scaly, chequered, flecked, paisley, braided, polka-dotted, interlaced, meshed.</ref> • Fine-Grained Visual Classification of Aircraft (FGVC-Aircraft) <ref type="bibr" target="#b39">(Maji et al., 2013</ref>) is a image dataset for fine grained visual categorization of aircraft. The dataset contains 102 different aircraft variants. In this paper, we randomly select 100 variants with 100 images in each variant. We split the meta-training/meta-validation/meta-testing to 64/16/20 variants respectively. • FGVCx-Fungi (Fungi) (Fun, 2018) contains over 100,000 fungi images of nearly 1,500 wild mushroom species. We first filter the species with less than 150 images and then randomly select 100 species with 150 images in each species. We split the meta-training/meta-validation/meta-testing to 64/16/20 species respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Hyperparameters &amp; Additional Experiment Settings</head><p>We summarize the hyperparameters in this paper in <ref type="table" target="#tab_9">Table 4</ref>. Like , we compute the full Hessian-vector products for MAML. All cluster centers are randomly initialized. Note that, in few-shot classification problem, we use the change of averaged training accuracy to determine whether to increase clusters. Thus, µ &lt; 1 in this problem. For toy regression task, the pre-aggregator embedding F(·, ·) is a fully connected layer. Following , the base learner has two hidden layers with 40 neurons in each. For few-shot image classification task, the pre-aggregator embedding F(·, ·) is a block of two convolutional layers with two fully connected layers. The base learner is a standard base learner with 4 standard convolutional blocks. For continual scenario, we add one cluster every time. All the experiments are implemented using Tensorflow <ref type="bibr" target="#b13">(Abadi et al., 2016)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Results of MiniImagenet</head><p>In this part, we present the additional comparison on MiniImagenet dataset. Similar to the analysis in , the sampled tasks in this benchmark do not have obvious heterogeneity and uncertainty. Thus, the goal is to compare our approach with gradient-based meta-learning methods and other previous models. The expressive capacity of each model is controlled by using 4 standard convolutional layers and the results are shown in <ref type="table">Table 5</ref>. With the same expressive capacity, our model can achieve comparable performance with MAML-based models and other previous models in meta-learning field. <ref type="table">Table 5</ref>. Comparison between our approach and prior few-shot learning techniques on the 5-way, 1-shot MiniImagenet benchmark. For MT-Net <ref type="bibr" target="#b34">(Lee &amp; Choi, 2018)</ref>, we remove the T-block since it introduces several 1 × 1 convolutional layers which increases the expressive capacity of base learner <ref type="bibr" target="#b37">(Lin et al., 2013)</ref>. For BMAML <ref type="bibr" target="#b56">(Yoon et al., 2018a)</ref>, 24 classes are used for meta-testing in their original paper, while other methods use 20 classes. Since they have not released their code, we are not able to know the used classes. Thus, we implement it and report their performance on the standard classes (i.e., 20 classes for testing). Like , we bold methods whose highest scores that overlap in their confidence intervals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Leave-one-out Experiments on Few-shot Image Classification</head><p>In this part, we design a more difficult experiment for few-shot image classification. For each dataset, we use three datasets for meta-training and the remaining dataset for meta-testing. For example, we use texture, bird and aircraft datasets for meta-training, and fungi dataset for meta-testing. Different from all the previous meta-learning settings which only use different classes for meta-testing, the leave-one-out experiment use a totally different dataset to test the generalization performance, which is more challenging.</p><p>The results of 5-way 1-shot classification are shown in <ref type="table" target="#tab_10">Table 6</ref>. We compare our methods with MAML and MUMOMAML (the best baseline in few-shot classification). We can see all results are significantly worse than the results without the leave-one-out technique, which shows the difficulty of this experiment. However, by capturing task clustering structure, our method can still achieves better performance than MAML and MUMOMAML. F. Additional Results of Few-shot Classification <ref type="table">Table 7</ref> and <ref type="table">Table 8</ref> contain the full results (accuracy with 95% confident interval) of few-shot image classfiation. <ref type="table">Table 7</ref> shows the full results of the bottom table in <ref type="figure" target="#fig_5">Figure 7</ref> (in paper). <ref type="table">Table 8</ref> contains the full results of Table 3 (in paper).  In our experiment, we found that the recurrent aggregator performs the best. To give more quantitative insight about the choice of aggregator, we compare these two aggregators with different shots in <ref type="table" target="#tab_12">Table 9</ref>. We can see that recurrent aggregator significantly outperforms in 1-shot scenario. With the increase of the size of training samples, the performances of the two aggregators become more similar. Therefore, compared with recurrent aggregator, training a better mean pooling aggregator may require more data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Ablation Studies</head><p>To investigate the contribution of different components of HSML (i.e., task representation, hierarchical task clustering, knowledge adaptation), we conduct the following ablation studies from four perspectives in <ref type="table" target="#tab_2">Table 10</ref>, where 5-way, 1-shot results on image classification are reported. The detailed ablations are provided in follows:</p><p>• (A1) We train four MAMLs for four clusters, i.e., bird, texture, aircraft and fungi, by assigning a task to its groundtruth cluster. The results can be regarded as an upper-bound application of MAML with task clustering, provided with groundtruth clusters of all tasks which are unfortunately absent in real-world applications. HSML outperforms as the soft and hierarchical clustering not only accurately captures the task relationship but also encourages knowledge transfer across clusters.</p><p>• (A2) We investigate different variants of task representation learning in <ref type="formula" target="#formula_7">(A2a)</ref> and <ref type="formula" target="#formula_7">(A2b)</ref>. In (A2a), we first use reconstruction loss to train task embeddings. Next, we fix the parameters of the task representation learning component and backpropagate meta-gradients to only train the other two components. The results are inferior, showing that metalearning gradients further optimize task embeddings. In (A2b), we replace our task embedding with the last hidden state of the encoder. The results higher than MUMOMAML show the contribution of hierarchical clustering, while they worse than ours further justify the capability of our task embedding.</p><p>• (A3) We analyze the effect of hierarchical clustering in (A3). In (A3a), we remove the hierarchical task clustering component. In (A3b), we consider the flat instead of hierarchical task clustering. The results of (A3a) lower than (A3b) consolidate our motivation of knowledge generalization with a cluster.</p><p>• (A4) We also study three variants of knowledge adaptation in (A4a)-(A4c). In (A4a), we revise Eqn. (8) by only using the clustering representation. The results still compete with state-of-the-art baselines, but empirically the combination with the task representation yields the best performance. In (A4b), we replace the parameter gate o i with FiLM <ref type="bibr" target="#b45">(Perez et al., 2018)</ref>, where the comparable results verify the primary contribution of hierarchical clustering. In order to validate the effectiveness of parameter gate, in (A4c), we directly learn the initialization from the task representation and the cluster representation instead of using the parameter gate to mask a shared initialization. The poor results show that the parameter gate masking a shared set of parameters θ 0 may 1) prevent the curse of dimensionality and constrain the optimization space, given the high dimensionality of parameters; 2) serve as the warm-start for a new cluster of tasks in continual learning. Ablation Bird Texture Aircraft Fungi (A1): Train a MAML for each cluster, e.g., bird, by assigning a task to its groundtruth cluster.</p><p>58.67 ± 1.49% 33.46 ± 1.34% 55.81 ± 1.38% 43.50 ± 1.38% (A2a): Use reconstruction loss to pretrain the task representation learning component, and then fix the paramters of it and backpropagate meta-gradients to only train the other two components.</p><p>56.97 ± 1.44% 29.12 ± 1.30% 45.71 ± 1.38% 40.92 ± 1.39% (A2b): Replace our task embedding with the last hidden state of the encoder.</p><p>58.25 ± 1.49% 34.53 ± 1.36% 55.73 ± 1.37% 43.59 ± 1.39% (A3a): Remove the hierarchical task clustering component.</p><p>58.22 ± 1.48% 33.30 ± 1.36% 55.35 ± 1.38% 42.68 ± 1.40% (A3b): Consider only flat rather than hierarchical task clustering.</p><p>58.08 ± 1.45% 34.26 ± 1.35% 56.11 ± 1.38% 43.38 ± 1.39% (A4a): Infer the parameter gate with the clustering representation only.</p><p>59.01 ± 1.50% 33.69 ± 1.35% 56.69 ± 1.39% 42.88 ± 1.40% (A4b): Replace the parameter gate with FiLM <ref type="bibr" target="#b45">(Perez et al., 2018)</ref>.</p><p>61.02 ± 1.47% 34.87 ± 1.37% 56.53 ± 1.40% 44.56 ± 1.38% (A4c): Learn the initialization directly from task and cluster representations rather than using the parameter gate.</p><p>53.95 ± 1.47% 32.35 ± 1.35% 52.15 ± 1.37% 42.31 ± 1.40%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Additional Task Clustering Results of Toy Regression Tasks</head><p>In <ref type="figure">Figure I</ref>, we show the additional results of task clustering analysis of toy regression. In this figure, we further verify that tasks can be clustered by their shapes. Clusters 1 reflects the fluctuation mode curve (e.g., Sin a1-a4, Cubic a1-a4), while cluster 2 reflects an arc (e.g., Quad a2-a4). Cluster 3 mainly reflects a linear shape with positive slope (e.g. Line a1, Line a2, Quad a1, Quad a2, Cubic a1). Cluster 4 mainly reflects a linear shape with negative slope (e.g., Line a3, Line a4, Cubic a4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sin a1</head><p>Sin <ref type="formula" target="#formula_7">a2</ref>  Heatmap of soft-assignment </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J. Additional Task Clustering Analysis of Few-shot Classification</head><p>In <ref type="figure" target="#fig_9">Figure 10</ref>, we show the additional results of task clustering analysis. The soft-assignment heatmap with their training images and activation paths of twelve tasks are illustrated. The conclusion is similar to that we draw previously in the paper. Tasks from different datasets mainly activate different clusters: bird→cluster 2, texture→cluster 4, aircraft→cluster 1, fungi→cluster 3. The left cluster and right cluster in the second layer may represent environment and surface texture, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fungi a3</head><p>Heatmap of Soft-assignment </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>(b). The ground truth underlying functions and the data samples D tr Ti are shown as red lines and green stars, (a) The visualization of soft-assignment in Eqn. (6) of six selected tasks. Darker color represents higher probability. (b)The corresponding fitting curves. The ground truth underlying a function is shown in red line with data samples marked as green stars. C1-4 mean cluster 1-4, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>The performance comparison for the 5-shot toy regression problem in the continual adaptation scenario. The curve of MSE in meta-training process is shown in the top figure and the performance of meta-testing is reported in the bottom</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>(a) The visualization of soft-assignment in Eqn. (6) of four selected tasks. (b) Learned hierarchical structure of each task. In each layer, top activated cluster is shown in dark color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6</head><label>6</label><figDesc>. t-SNE visualization of gated weight, i.e., θ0i, in Eqn.(9)S(10C) are used as baselines. As shown inFigure 7, HSML-D consistently achieves better performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>The performance comparison for the 5-way 1-shot fewshot classification problem in the continual adaptation scenario. The top figure and bottom table show the meta-training accuracy curves and the meta-testing accuracy, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Figure 8. Illustration of Existance of K k=1B k .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>-</head><label></label><figDesc>Meta-training: MD-90, 737-600, A310, An-12, DR-400, Falcon-900, DC-3, Challenger-600, Fokker-70, Cessna-172, 747-400,  ERJ-145, Dornier-328, A330-300, A319, Model-B200, E-170, A340-500, BAE-125, Metroliner, 747-300, C-130, DH-82, Hawk- T1, 727-200, 767-300, DC-10, Spitfire, E-195, BAE-146-300, F-16A-B, Beechcraft-1900, 747-200, Boeing-717, Falcon-2000, DC-8, Global-Express, DHC-1, CRJ-200, A340-300, DC-9-30, CRJ-900, A320, 737-300, Eurofighter-Typhoon,SR-20, E-190, Saab-340, C-47, Il-76, MD-87, 757-300, DHC-6, Tu-154, 777-200,  767-200, A318, 757-200, A300B4.  -Meta-validation: 737-900, A340-600, 737-800, 737-400, L-1011, A330-200, Gulfstream-V, 737-500, A340-200, ATR-72,  MD-11, CRJ-700, EMB-120, Fokker-100, DC-6, 737-700.  -Meta-testing: 707-320, PA-28, Cessna-208, F-A-18, DHC-8-300, ERJ-135, Tornado, BAE-146-200, A321, ATR-42, Saab-2000 Tu-134, Fokker-50, A380, MD-80, Gulfstream-IV, Yak-42, 747-100, 767-400, Embraer-Legacy-600.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .</head><label>9</label><figDesc>Additional results of task clustering analysis of toy regression problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 .</head><label>10</label><figDesc>Additional results of task clustering analysis of few-shot image classification problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>College of Information Science and Technology, Pennsylvania State University, PA, USA 2 Tencent AI Lab, Shenzhen, China. Correspondence to: Ying Wei &lt;judyweiy-ing@gmail.com&gt;. Proceedings of the 36 th International Conference on Machine Learning, Long Beach, California, PMLR 97, 2019. Copyright 2019 by the author(s).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Algorithm 1 Meta-training of HSMLRequire: E: distribution over tasks; {K 1 , · · · , K L }: # of clusters in each layer; α, β: stepsizes; µ: threshold 1: Randomly initialize Θ 2: while not done do</figDesc><table><row><cell>3:</cell><cell>ifLnew &gt; µL old then</cell></row><row><cell>4:</cell><cell>Increase the number of clusters</cell></row><row><cell>5:</cell><cell>end if</cell></row><row><cell>6:</cell><cell>Sample a batch of tasks Ti ∼ E</cell></row><row><cell>7:</cell><cell>for all Ti do</cell></row><row><cell>8:</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Performance of MSE ± 95% confidence intervals on toy regression tasks, averaged over 4,000 tasks. Both 5-shot and 10shot results are reported.</figDesc><table><row><cell>Model</cell><cell>5-shot</cell><cell>10-shot</cell></row><row><cell>MAML</cell><cell>2.205 ± 0.121</cell><cell>0.761 ± 0.068</cell></row><row><cell>Meta-SGD</cell><cell>2.053 ± 0.117</cell><cell>0.836 ± 0.065</cell></row><row><cell>MT-Net</cell><cell>2.435 ± 0.130</cell><cell>0.967 ± 0.056</cell></row><row><cell>BMAML</cell><cell>2.016 ± 0.109</cell><cell>0.698 ± 0.054</cell></row><row><cell>MUMOMAML</cell><cell>1.096 ± 0.085</cell><cell>0.256 ± 0.028</cell></row><row><cell>HSML (ours)</cell><cell cols="2">0.856 ± 0.073 0.161 ± 0.021</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 .</head><label>2</label><figDesc>Comparison between HSML and other gradient-based meta-learning methods on the 5-way, 1-shot/5-shot image classification problem, averaged over 1000 tasks for each dataset. Accuracy ± 95% confidence intervals are reported.</figDesc><table><row><cell></cell><cell>Model</cell><cell>Bird</cell><cell>Texture</cell><cell>Aircraft</cell><cell>Fungi</cell><cell>Average</cell></row><row><cell></cell><cell>MAML</cell><cell>53.94 ± 1.45%</cell><cell>31.66 ± 1.31%</cell><cell>51.37 ± 1.38%</cell><cell>42.12 ± 1.36%</cell><cell>44.77%</cell></row><row><cell></cell><cell>Meta-SGD</cell><cell>55.58 ± 1.43%</cell><cell>32.38 ± 1.32%</cell><cell>52.99 ± 1.36%</cell><cell>41.74 ± 1.34%</cell><cell>45.67%</cell></row><row><cell>5-way</cell><cell>MT-Net</cell><cell>58.72 ± 1.43%</cell><cell>32.80 ± 1.35%</cell><cell>47.72 ± 1.46%</cell><cell>43.11 ± 1.42%</cell><cell>45.59%</cell></row><row><cell>1-shot</cell><cell>BMAML</cell><cell>54.89 ± 1.48%</cell><cell>32.53 ± 1.33%</cell><cell>53.63 ± 1.37%</cell><cell>42.50 ± 1.33%</cell><cell>45.89%</cell></row><row><cell></cell><cell>MUMOMAML</cell><cell>56.82 ± 1.49%</cell><cell>33.81 ± 1.36%</cell><cell>53.14 ± 1.39%</cell><cell>42.22 ± 1.40%</cell><cell>46.50%</cell></row><row><cell></cell><cell>HSML (ours)</cell><cell cols="5">60.98 ± 1.50% 35.01 ± 1.36% 57.38 ± 1.40% 44.02 ± 1.39% 49.35%</cell></row><row><cell></cell><cell>MAML</cell><cell>68.52 ± 0.79%</cell><cell>44.56 ± 0.68%</cell><cell>66.18 ± 0.71%</cell><cell>51.85 ± 0.85%</cell><cell>57.78%</cell></row><row><cell></cell><cell>Meta-SGD</cell><cell>67.87 ± 0.74%</cell><cell>45.49 ± 0.68%</cell><cell>66.84 ± 0.70%</cell><cell>52.51 ± 0.81%</cell><cell>58.18%</cell></row><row><cell>5-way</cell><cell>MT-Net</cell><cell>69.22 ± 0.75%</cell><cell>46.57 ± 0.70%</cell><cell>63.03 ± 0.69%</cell><cell>53.49 ± 0.83%</cell><cell>58.08%</cell></row><row><cell>5-shot</cell><cell>BMAML</cell><cell>69.01 ± 0.74%</cell><cell>46.06 ± 0.69%</cell><cell>65.74 ± 0.67%</cell><cell>52.43 ± 0.84%</cell><cell>58.31%</cell></row><row><cell></cell><cell>MUMOMAML</cell><cell>70.49 ± 0.76%</cell><cell>45.89 ± 0.69%</cell><cell>67.31 ± 0.68%</cell><cell>53.96 ± 0.82%</cell><cell>59.41%</cell></row><row><cell></cell><cell>HSML (ours)</cell><cell cols="5">71.68 ± 0.73% 48.08 ± 0.69% 73.49 ± 0.68% 56.32 ± 0.80% 62.39%</cell></row><row><cell>Bird</cell><cell>Texture</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Fungi is added Aircraft is added</head><label></label><figDesc></figDesc><table><row><cell>Model</cell><cell>Bird</cell><cell>Texture</cell><cell>Aircraft</cell><cell>Fungi</cell></row><row><cell cols="2">MUMOMAML 56.66%</cell><cell>33.68%</cell><cell>45.73%</cell><cell>40.38%</cell></row><row><cell>HSML-S (2C)</cell><cell>60.77%</cell><cell>33.41%</cell><cell>51.28%</cell><cell>40.78%</cell></row><row><cell>HSML-S (10C)</cell><cell>59.16%</cell><cell>34.48%</cell><cell>52.30%</cell><cell>40.56%</cell></row><row><cell>HSML-D</cell><cell cols="4">61.16% 34.53% 54.50% 41.66%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 .</head><label>3</label><figDesc>Comparison of different cluster numbers. The numbers in first column represents the number of clusters from bottom layer to top layer. Accuracy for 5-way 1-shot classification are reported.</figDesc><table><row><cell>Num. of Clu.</cell><cell>Bird</cell><cell>Texture</cell><cell>Aircraft</cell><cell>Fungi</cell></row><row><cell>(2, 2, 1)</cell><cell>58.37%</cell><cell>33.18%</cell><cell>56.15%</cell><cell>42.90%</cell></row><row><cell>(4, 2, 1)</cell><cell cols="2">60.98% 35.01%</cell><cell>57.38%</cell><cell>44.02%</cell></row><row><cell>(6, 3, 1)</cell><cell>60.55%</cell><cell>34.02%</cell><cell>55.79%</cell><cell>43.43%</cell></row><row><cell>(8, 4, 2, 1)</cell><cell>59.55%</cell><cell>34.74%</cell><cell cols="2">57.84% 44.18%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 .</head><label>4</label><figDesc>Hyperparameter summary</figDesc><table><row><cell>Hyperparameters</cell><cell cols="3">Toy Regreesion miniImageNet Multi-Datasets (New Benchmark)</cell></row><row><cell>Input Scale (only for image data)</cell><cell>/</cell><cell>84 × 84 × 3</cell><cell>84 × 84 × 3</cell></row><row><cell>Meta-batch Size (task batch size)</cell><cell>25</cell><cell>4</cell><cell>4</cell></row><row><cell>Inner loop learning rate (α)</cell><cell>0.001</cell><cell>0.001</cell><cell>0.001</cell></row><row><cell>Outer loop learning rate (β)</cell><cell>0.001</cell><cell>0.01</cell><cell>0.01</cell></row><row><cell>Filters of CNN (only for image data)</cell><cell>/</cell><cell>32</cell><cell>32</cell></row><row><cell>Meta-training adaptation steps</cell><cell>5</cell><cell>5</cell><cell>5</cell></row><row><cell>Task representation size</cell><cell>40</cell><cell>128</cell><cell>128</cell></row><row><cell>Reconstruction loss weight (γ)</cell><cell>0.01</cell><cell>0.01</cell><cell>0.01</cell></row><row><cell>Image Embedding Size (before aggregator)</cell><cell>/</cell><cell>64</cell><cell>64</cell></row><row><cell>Continual Training Threshold (τ )</cell><cell>1.25</cell><cell>/</cell><cell>0.85</cell></row><row><cell># epoch (Q) for computing loss</cell><cell>1000</cell><cell>/</cell><cell>100</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 .</head><label>6</label><figDesc>Comparison of leave-one-out experiments on 5-way 1-shot classification. 4000 tasks are used to test the performance. For each dataset, the performance is reported when this dataset is used for meta-testing.</figDesc><table><row><cell>Model</cell><cell>Bird</cell><cell>Texture</cell><cell>Aircraft</cell><cell>Fungi</cell><cell>Average</cell></row><row><cell>MAML</cell><cell>40.76 ± 0.68%</cell><cell>29.50 ± 0.65%</cell><cell>29.54 ± 0.63%</cell><cell>29.94 ± 0.64%</cell><cell>32.43%</cell></row><row><cell>MUMOMAML</cell><cell>41.58 ± 0.68%</cell><cell>30.24 ± 0.68%</cell><cell>30.69 ± 0.66%</cell><cell>30.63 ± 0.66%</cell><cell>33.28%</cell></row><row><cell>HSML-RTG</cell><cell cols="5">42.54 ± 0.67% 30.90 ± 0.67% 31.23 ± 0.64% 32.98 ± 0.68% 34.41%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 .Table 8 .</head><label>78</label><figDesc>Comparison of online update results on few-shot image classification 5-way 1-shot scenario (FullTable). ± 1.42% 34.53 ± 1.35% 54.50 ± 1.36% 41.66 ± 1.41% 47.96% Comparison of different cluster numbers (FullTable).</figDesc><table><row><cell>Model</cell><cell>Bird</cell><cell>Texture</cell><cell>Aircraft</cell><cell>Fungi</cell><cell>Average</cell></row><row><cell>MUMOMAML</cell><cell>56.66 ± 1.43%</cell><cell>33.68 ± 1.37%</cell><cell>45.73 ± 1.39%</cell><cell>40.38 ± 1.40%</cell><cell>44.11%</cell></row><row><cell>HSML-Static (2C)</cell><cell>60.77 ± 1.43%</cell><cell>33.41 ± 1.40%</cell><cell>51.28 ± 1.37%</cell><cell>40.78 ± 1.34%</cell><cell>46.56%</cell></row><row><cell>HSML-Static (10C)</cell><cell>59.16 ± 1.49%</cell><cell>34.48 ± 1.36%</cell><cell>52.30 ± 1.35%</cell><cell>40.56 ± 1.39%</cell><cell>46.63%</cell></row><row><cell cols="2">HSML-Dynamic 61.16 Num. of Clus. Bird</cell><cell>Texture</cell><cell>Aircraft</cell><cell>Fungi</cell><cell>Average</cell></row><row><cell>(2, 2, 1)</cell><cell>58.37 ± 1.42%</cell><cell>33.18 ± 1.34%</cell><cell>56.15 ± 1.36%</cell><cell>42.90 ± 1.41%</cell><cell>47.65%</cell></row><row><cell>(4, 2, 1)</cell><cell cols="2">60.98 ± 1.50% 35.01 ± 1.36%</cell><cell>57.38 ± 1.40%</cell><cell>44.02 ± 1.39%</cell><cell>49.35%</cell></row><row><cell>(6, 3, 1)</cell><cell>60.55 ± 1.45%</cell><cell>34.02 ± 1.34%</cell><cell>55.79 ± 1.38%</cell><cell>43.43 ± 1.39%</cell><cell>48.45%</cell></row><row><cell>(8, 4, 4, 1)</cell><cell>59.55 ± 1.46%</cell><cell>34.74 ± 1.37%</cell><cell cols="3">57.83 ± 1.39% 44.18 ± 1.38% 49.08%</cell></row><row><cell cols="2">G. Effect of Different Aggregator</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 .</head><label>9</label><figDesc>Comparison of different aggregator on different shot, where HSML-RAA and HSML-MPAA represent HSML with recurrent autoencoder aggregator and mean pooling autoencoder aggregator, respectively.</figDesc><table><row><cell></cell><cell>Model</cell><cell>Bird</cell><cell>Texture</cell><cell>Aircraft</cell><cell>Fungi</cell><cell>Average</cell></row><row><cell>1-shot</cell><cell>HSML-MPAA HSML-RAA</cell><cell cols="5">57.87 ± 1.48% 60.98 ± 1.50% 35.01 ± 1.36% 57.38 ± 1.40% 44.02 ± 1.39% 49.35% 32.07 ± 1.36% 53.76 ± 1.41% 40.88 ± 1.37% 46.14%</cell></row><row><cell>3-shot</cell><cell>HSML-MPAA HSML-RAA</cell><cell cols="5">67.80 ± 0.91% 68.01 ± 0.88% 45.07 ± 0.87% 68.59 ± 0.82% 53.51 ± 0.96% 58.80% 44.33 ± 0.82% 67.73 ± 0.83% 52.45 ± 0.94% 58.07%</cell></row><row><cell>5-shot</cell><cell cols="2">HSML-MPAA 71.80 ± 0.70% HSML-RAA 71.68 ± 0.73%</cell><cell cols="4">48.02 ± 0.68% 48.08 ± 0.69% 73.49 ± 0.68% 56.32 ± 0.80% 62.39% 71.79 ± 0.74% 54.01 ± 0.82% 61.40%</cell></row><row><cell>8-shot</cell><cell cols="3">HSML-MPAA 75.75 ± 0.62% 52.90 ± 0.57% HSML-RAA 75.52 ± 0.63% 51.52 ± 0.59%</cell><cell>73.03 ± 0.55% 75.33 ± 0.53%</cell><cell cols="2">58.20 ± 0.73% 64.97% 57.68 ± 0.71% 65.01%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 .</head><label>10</label><figDesc>Ablation Studies. Results of 5-way, 1-shot image classification are reported.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Eared Grebe, Blue headed Vireo, White necked Raven, Frigatebird, Horned Lark, Tree Sparrow, Red bellied Woodpecker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baltimore</forename><surname>Meta-Training ; Hooded Warbler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scarlet</forename><surname>Oriole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cerulean</forename><surname>Tanager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Downy</forename><surname>Warbler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woodpecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>White Warbler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canada</forename><surname>Kingbird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blue</forename><surname>Warbler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Red breasted Merganser</title>
		<imprint>
			<publisher>American Crow</publisher>
		</imprint>
		<respStmt>
			<orgName>Savannah Sparrow, Dark eyed Junco, Black footed Albatross, Henslow Sparrow, Cape Glossy Starling, Black throated Sparrow, Northern Waterthrush ; Great Crested Flycatcher, Blue Grosbeak, White breasted Kingfisher, White eyed Vireo, Purple Finch, Cliff Swallow</orgName>
		</respStmt>
	</monogr>
	<note>Elegant Tern, Groove billed Ani, Mallard, European Goldfinch. Scissor tailed Flycatcher, Harris Sparrow, Western Grebe, Gadwall, American Goldfinch, Pine Warbler</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pomarine</forename><surname>Ovenbird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaeger</surname></persName>
		</author>
		<imprint>
			<pubPlace>Indigo Bunting, Blue winged Warbler, Chipping Sparrow, Horned Grebe, Fox Sparrow, Green Violetear, Nashville Warbler, Least Tern, Marsh Wren</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Meta-validation: Mockingbird, Vermilion Flycatcher, Cape May Warbler, Prothonotary Warbler, White crowned Sparrow</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Hierarchically Structured Meta-learning</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Ring billed Gull, Great Grey Shrike, White breasted Nuthatch, Mourning Warbler, Sage Thrasher, Horned Puffin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meta-Testing ;</forename><surname>Vinyals</surname></persName>
		</author>
		<idno>43.56 ± 0.34% meta-learner LSTM</idno>
		<imprint>
			<date type="published" when="2016" />
			<pubPlace>Pied Kingfisher, Shiny Cowbird, Scott Oriole, Red eyed Vireo, Song Sparrow, Winter Wren</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Rose breasted Grosbeak, Nighthawk, Long tailed Jaeger, Bronzed Cowbird, California Gull, Ivory Gull, Northern Fulmar, Brown Pelican</orgName>
		</respStmt>
	</monogr>
	<note>MiniImagenet 5-way 1-shot Accuracy Matching Nets. Ravi &amp; Larochelle, 2016) 43.44 ± 0.77%</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prototypical</forename><surname>Network</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Snell</surname></persName>
		</author>
		<idno>46.61 ± 0.78%</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snail (mishra</surname></persName>
		</author>
		<idno>50.44 ± 0.82%</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gnn (garcia &amp;amp; Bruna</surname></persName>
		</author>
		<idno>2017) 50.33 ± 0.36%</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maml (finn</surname></persName>
		</author>
		<idno>2017) 49.40 ± 1.83%</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bmaml (yoon</surname></persName>
		</author>
		<idno>2018a) 50.01 ± 1.86%</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mt-Net</surname></persName>
		</author>
		<idno>49.75 ± 1.83%</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Lee &amp; Choi</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mumomaml (vuorio</surname></persName>
		</author>
		<idno>49.97 ± 0.32%</idno>
	</analytic>
	<monogr>
		<title level="j">Nichol &amp; Schulman</title>
		<imprint>
			<biblScope unit="page">49</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>86 ± 1.85% Reptile</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Metasgd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<idno>2017) 50.47 ± 1.87%</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">2018) classification challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Platipus (finn</surname></persName>
		</author>
		<ptr target="https://www.kaggle.com/c/fungi-challenge-fgvc-2018" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Tensorflow: a system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to learn by gradient descent by gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3981" to="3989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Theoretical models of learning to learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baxter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning to learn</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="71" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Structure learning in action</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Wolpert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioural brain research</title>
		<imprint>
			<biblScope unit="volume">206</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="165" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Describing textures in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Supervised learning of universal sentence representations from natural language inference data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="670" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Strongly adaptive online learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Daniely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1405" to="1411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meta-Learning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Universality</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.11622</idno>
		<title level="m">Deep representations and gradient descent can approximate any learning algorithm</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Model-agnostic metalearning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Probabilistic modelagnostic meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02817</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Transferring knowledge across learning processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Flennerhag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">G</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Damianou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01054</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Few-shot learning with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04043</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Context, learning, and extinction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">197</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Statistical computations underlying the dynamics of memory updating</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radulescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">1003939</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Recasting gradient-based meta-learning as hierarchical bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Griffiths</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.08930</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Metalearning for low-resource neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">O</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.08437</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Tree-guided group lasso for multitask regression with structured sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="543" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning task grouping and overlap in multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1723" to="1730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Data-dependent stability of stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kuzborskij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01678</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fast rates by transferring from auxiliary hypotheses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kuzborskij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Orabona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="171" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Gradient-based meta-learning with learned layerwise metric and subspace</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2933" to="2942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01885</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Learning to optimize. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meta-Sgd</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09835</idno>
		<title level="m">Learning to learn quickly for few shot learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<title level="m">Network in network</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Fine-grained visual classification of aircraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">A simple neural attentive meta-learner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Meta</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Networks</surname></persName>
		</author>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2554" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Rapid adaptation with conditionally shifted neurons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3661" to="3670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Optimization landscape and expressivity of deep cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<title level="m">Reptile: a scalable metalearning algorithm</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Film: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Meta-learning with memory-augmented neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1842" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Few-shot learning through an information retrieval lens</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2255" to="2265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Toward multimodal model-agnostic meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vuorio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.07172</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<title level="m">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4805" to="4815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Bayesian model-agnostic meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Dia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahn</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7343" to="7353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Lifelong learning with dynamically expandable networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
