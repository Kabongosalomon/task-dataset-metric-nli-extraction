<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Plugin Networks for Inference under Partial Evidence</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Koperski</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tooploox Ltd</orgName>
								<address>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Konopczyński</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tooploox Ltd</orgName>
								<address>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Radiation Oncology</orgName>
								<orgName type="institution" key="instit1">University Medical Center Mannheim</orgName>
								<orgName type="institution" key="instit2">Heidelberg University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Interdisciplinary Center for Scientific Computing (IWR)</orgName>
								<orgName type="institution">Heidelberg University</orgName>
								<address>
									<settlement>Germany</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafał</forename><surname>Nowak</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tooploox Ltd</orgName>
								<address>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Institute of Computer Science</orgName>
								<orgName type="institution">University of Wroclaw</orgName>
								<address>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Semberecki</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tooploox Ltd</orgName>
								<address>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Wroclaw University of Science and Technology</orgName>
								<address>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Trzciński</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tooploox Ltd</orgName>
								<address>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution">Warsaw University of Technology</orgName>
								<address>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Plugin Networks for Inference under Partial Evidence</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we propose a novel method to incorporate partial evidence in the inference of deep convolutional neural networks. Contrary to the existing, top performing methods, which either iteratively modify the input of the network or exploit external label taxonomy to take the partial evidence into account, we add separate network modules ("Plugin Networks") to the intermediate layers of a pretrained convolutional network. The goal of these modules is to incorporate additional signal, i.e. information about known labels, into the inference procedure and adjust the predicted output accordingly. Since the attached plugins have a simple structure, consisting of only fully connected layers, we drastically reduced the computational cost of training and inference. At the same time, the proposed architecture allows to propagate information about known labels directly to the intermediate layers to improve the final representation. Extensive evaluation of the proposed method confirms that our Plugin Networks outperform the state-of-the-art in a variety of tasks, including scene categorization, multi-label image annotation and semantic segmentation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Visual recognition tasks, e.g. scene categorization or multi-label image annotation, have attracted a significant amount of research interest in recent years <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b4">5]</ref>. One of the reasons, which sparked this attention was the availability of evaluation datasets created for benchmarking given visual tasks, such as ImageNet <ref type="bibr" target="#b1">[2]</ref>, VOC Pascal <ref type="bibr" target="#b3">[4]</ref> or COCO <ref type="bibr" target="#b16">[17]</ref>. Although sensible for comparison purposes, * Equal contribution  <ref type="figure">Figure 1</ref>: Plugin Networks -neural networks attached to the intermediate layers of a pre-trained convolutional neural network, allow to exploit partial evidence labels at the inference to predict unknown labels with higher accuracy. This simple, yet effective approach significantly reduces train and test time, while outperforming competitive results on three challenging benchmarks.</p><p>single-task evaluation protocols are often far from real-life use-cases, where additional information, e.g. related to location or time of photo capture, is available. The availability of partial information (partial evidence) about an image, made available at test time, can improve accuracy of pre-trained networks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b26">27]</ref>, and we will follow this scenario. More specifically, we assume that a set of labels corresponding to a given image is known during inference, while the task at hand is to improve the performance of the model on the original task, e.g. image classification, object detection, semantic segmentation. This corresponds to a real life application, where, for instance, we know that the image was captured in a forest or in a cave, which drastically reduces the likelihood of detecting a skyscraper. Similarly, information that a given ob-ject appears in an image can greatly improve its localization or segmentation. Since partial evidence can be available in multiple forms and modalities, the main prediction system, e.g. a convolutional neural network (CNN), is trained to perform a general purpose prediction with no assumption about the existence of partial evidence or lack thereof. Neural architectures such as CNNs are not modular, thus any modification such as new inputs (partial evidence) or new outputs (new tasks) are difficult to apply without repeating the full training procedure. Otherwise, phenomena such as the catastrophic forgetting may occur. Our objective is to enable the model to incorporate additional available information without re-training the main system while exploiting this information to increase the quality of predictions.</p><p>Several methods were proposed in the literature to address this problem, among them <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b26">[27]</ref> are the most recent. In paper <ref type="bibr" target="#b10">[11]</ref> authors proposed to exploit external taxonomy of the labels by modelling correspondences between scene attributes and categories, by feeding this data into the main neural network at inference. On the other hand, paper <ref type="bibr" target="#b26">[27]</ref> introduces a feedback-prop approach that iteratively modifies input and network activations to ensure the response of the network corresponds to the distribution of known labels. Although, those methods provide effective ways to exploit partial evidence, they require complex reasoning, that concerns relationships between labels or computationally expensive iterative adaptation mechanism.</p><p>In this work, we reduce the complexity and propose the Plugin Networks -a simple, yet effective main network extension that allows to incorporate partial evidence during the inference. We show that by using a set of fullyconnected (FC) side networks attached to intermediate layers of the main network (see <ref type="figure">Fig. 1</ref>), we are able to not only avoid costly optimization process, but also exploit the assumption about the existence of partial evidence in the offline training stage. More specifically, the proposed Plugin Networks, connected to the backbone neural network, adjust their activations at the time of inference, depending on available known labels. Due to the simplicity of the Plugin Networks, their training converges quickly, while remaining robust to overfitting, as we show in this paper. The inference of the proposed model consists of a quick feed-forward propagation of the main model. Plugin Networks offer a significant speedup with respect to the state-of-the-art feedback-prop method <ref type="bibr" target="#b26">[27]</ref>. Last but not least, the proposed Plugin Networks outperform all of the existing methods on three challenging benchmark applications: hierarchical scene categorization on the SUN397 dataset <ref type="bibr" target="#b28">[29]</ref>, multilabel image annotation on the COCO 2014 dataset <ref type="bibr" target="#b16">[17]</ref>, and semantic segmentation on Pascal VOC 2011 <ref type="bibr" target="#b3">[4]</ref>.</p><p>To summarize, our contributions are as follows:</p><p>• We propose novel neural network model extensions called Plugin Networks, which allows us to take partial information available at test time into account. Plugin Networks adjust the activations of the pre-trained base network. They are fast to train and efficient at test time.</p><p>• We show how to attach the proposed Plugin Networks to different types of neural network layers and investigate the influence of those variants on final results.</p><p>• We provide an extensive evaluation of the proposed approach on three challenging tasks: hierarchical scene categorization, multi-label image annotation and scene segmentation.</p><p>We make our code available for the public 1 .</p><p>In the remainder of this paper, we first give an overview of related publications. In Sec. 3 we formally introduce the proposed approach, explain how to use it and discuss its properties. Sec. 4 provides an extensive evaluation of our method and we conclude this paper in Sec. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Using context in visual tasks: Exploiting additional contextual cues in visual recognition tasks gained a lot of attention from the computer vision community <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b12">13]</ref>. Contextual information related to semantics was used to improve object detection <ref type="bibr" target="#b19">[20]</ref>. Social media meta-data was also used in a context of multilabel image annotation in <ref type="bibr" target="#b12">[13]</ref>. Although, adding context proved to be successful in increasing the quality of visual recognition tasks, all of the above mentioned methods used the context in conjunction with the input uni-modal (visual) image during the training of the entire system. In this work, we propose a fundamentally different approach since the context (in the form of known labels) is learned only after the training of the main model is finished and our approach allows to extend this pre-trained model with additional information a posteriori. Rosenfeld et al. <ref type="bibr" target="#b23">[24]</ref> proposed a method where detection and segmentation is conditioned on the presence of a given object category. To achieve it, they propose to use a set of linear modulators. This method shares common features such as offline training with the Plugin Networks, but a model capacity of linear modulators is not enough to learn complex functions. This leads to more complicated training procedures with data oversampling. Perez et al. <ref type="bibr" target="#b21">[22]</ref> proposed FiLM (Feature-wise Linear Modulation) for visual question answering, where textual information serves as a context to a CNN model. In particular FiLM introduces new layers (named FiLM blocks or Resblocks), which are incorporated into the base model. FiLM blocks are later modulated using affine fusion operator. Plugin Networks on the other hand do not introduce any alternations to the base model architecture and directly modulate activations of the base model. Thanks to that, we do not introduce so much noise as the FiLM Resblocks to the base model which results in a more stable training. Using label structure: Some authors proposed to model the co-occurrence of labels available at training time to improve recognition performance <ref type="bibr" target="#b18">[19]</ref>. <ref type="bibr" target="#b0">[1]</ref> on the other hand uses a special structure to store the relations between the labels using a graph designed specifically to capture semantic similarities between the labels. Other forms of external knowledge can be found in <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b11">[12]</ref> where they use the Word-Net taxonomy of tags to increase the accuracy of their visual recognition systems. <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b17">[18]</ref> also used social media meta-data to improve the quality of the results obtained for image recognition tasks. Finally, <ref type="bibr" target="#b20">[21]</ref> estimated entry-level labels of visual objects by exploiting image captions. Contrary to our method, the above-mentioned approaches focus on finding the relationships between the labels and driving the training algorithm to encompass those structures. In this work, we do not explicitly model any label structures -the only input related to labels we give to the network is a set of known labels related to an image with no information about their relationship with the others. Multi-task learning: Somehow related to our work is the thriving area of multi-task learning. Motivated by the phenomenon of catastrophic forgetting, multi-task learning tries to address the problem of lifelong learning and adaptation of a neural network to a set of changing tasks while preserving the network's structure. In <ref type="bibr" target="#b15">[16]</ref>, Lee et al. aim to solve this problem by the continuous matching of network distribution. In <ref type="bibr" target="#b22">[23]</ref> the same problem is solved through residual adapters -neural network modules plugged into a network, similarly to our Plugin Networks -which are the only structures trained for the tasks while the base network remains untouched. Although, we do not aim to solve multitask learning problem in this work, our approach is inspired by the above-mentioned methods, which focus on designing robust network architecture that can dynamically adjust to additional data point sources unseen during training. Inference with Partial Evidence: Finally, the most relevant to the work presented in this paper are two methods proposed by Hu et al. <ref type="bibr" target="#b10">[11]</ref> and Wang et al. <ref type="bibr" target="#b26">[27]</ref>. Both of them address the problem of visual tasks in the presence of partial evidence.</p><p>Hu et al. <ref type="bibr" target="#b10">[11]</ref> tackles this challenge by proposing a Structured Inference Neural Network (SINN). The SINN method is designed to discover the hierarchical structure of labels, but it can also be used in a partial evidence setup, if labels in a given hierarchy are clamped at inference. However, the SINN model, which uses CNN and LSTM to discover label relations, has a large amount of learnable parameters, which makes model training difficult. To solve this issue authors use the positive and negative correlations of labels as prior knowledge, which is inferred from the WordNet rela-tions. We compare our method with SINN and show that we achieve significantly better performance with a much simpler model.</p><p>The FeedbackProp proposed by Wang et al. <ref type="bibr" target="#b26">[27]</ref>, uses an iterative procedure, which is applied at inference time. The idea is to modify network activations to maximize the probabilities of labels under the partial evidence. The method does not require to re-train the base model. However, due to the iterative procedure introduced at inference time, it requires more computational effort. In addition, they introduced hyperparameters, like a number of iterations and learning rate to the inference phase. Finally, in the case of FeedbackProp, the partial evidence labels can only be a subset of labels that the base model can recognize. Our method, however, can accept any kind of labels as partial evidence. Moreover, our method introduce negligible computations, and not extra parameters to the inference phase. The comparison shows that our method outperforms FeedbackProp while being significantly faster at inference phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Plugin Networks</head><p>In this section, we first introduce the Plugin Networks and define them formally. We then describe how to attach the Plugin Networks to the existing base network at the linear and convolutional layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Definition</head><p>Let's assume that we have a CNN model F (x; w), where x is an input image and w are the parameters. The model F is already trained on some task (e.g. single or multi-label classification, scene segmentation). The parameters w were trained on input images X and input labels Y . Now let's assume that some labelsȲ are available and known at inference time. In the following definitions without loosing generality, we will assume that only one Plugin Network is attached to the base model. We define the Plugin Network model F p with parameters w p as</p><formula xml:id="formula_0">r = F p (ȳ; w p ).</formula><p>(1)</p><p>The model takes the partial evidenceȳ ∈Ȳ as an input. The output r of the plugin can be attached to the output vector z of some layer of the base model F :</p><formula xml:id="formula_1">z = z ⊕ r,<label>(2)</label></formula><p>where the sign ⊕ can have the following meaning:</p><p>• additive:z = z + r,</p><p>• affine:z = r a z + r b , r = r a r b ,</p><p>• multiplicative:z = z * r,</p><formula xml:id="formula_2">• residual:z = z + z * r,</formula><p>where is the concatenation operator. In this way the Plugin Network F p adapts the output vector z of the base model F under presence of available partial evidence. The eq. (2) defines how the Plugin Network F p is attached to the base network F , thus a joint model can be defined as:F (x,ȳ; w, w pi ).</p><p>(</p><p>In general, several Plugin Networks can be attached simultaneously to a number of layers of the base model F . Note that the output of a Plugin Network r can be attached to either the output of a fully connected layer or a convolutional layer. In the following sections, we explain how both operations are performed in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Connection with Linear Layers</head><p>The Plugin Network, which is attached to the linear layer, has to compute the vector r of the same dimension as the vector z. Then the operator ⊕ in eq. (2) is well defined. The adjusted vectorz is then processed by the following layers of the base model. The value z is the output of a layer before a non-linear function (e.g. ReLU).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Connection with Convolutional Layers</head><p>When a Plugin Network is attached to a convolutional layer, it adjusts the feature map obtained from a given convolutional filter. Thus, it has to compute vector r, which has to be of the same dimension as the number of channels in the tensor z. Then all the considered operators ⊕ in eq. (2) have elementwise meaning:</p><formula xml:id="formula_4">z c = z c ⊕ r c =    z 11 . . . z 1j . . . . . . z i1 z ij    c ⊕ r c ,<label>(4)</label></formula><p>where c indicates the number of channel. Adding a scalar value to each channel of a feature map greatly reduces the number of Plugin Network parameters that have to be learned. Learning different values for each elements of a feature map requires w × h × c output values, where w, h stands for width and height of a feature map, respectively. Since we only add scalar value to each feature map, we require only c output values from a Plugin Network. <ref type="figure" target="#fig_1">Fig. 2</ref> illustrates how Plugin Network is attached to convolutional layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Plugin Network Architecture</head><p>Overall, the Plugin Networks can be generalized to any model that can be trained with backpropagation. In our case, the Plugin Network is a FC neural network. Each FC layer is followed by a ReLU activation except for the last layer. We chose a fully connected architecture because partial evidence vectorȳ can be interpreted as a feature vector, which is used to compute a non-linear transform of outputs of the base model. This task can be well handled by a fully connected neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Training</head><p>The eq. (3) defines joint model of a base network F with parameters w and the Plugin Network F p with parameters w p . In the training procedure we are optimizing only w p parameters, thus the base model F is not altered. To optimize the parameters of the Plugin Networks, the original loss function is used, i.e. the same loss function that was used to train the base model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Properties</head><p>One important property of the Plugin Networks is that the function, which modifies a base model, is trained in an offline phase (see Section 3.5). Thus, the testing phase requires only a single feed-forward propagation through base model and the Plugin Networks. Thanks to the single forward pass, our model is fast, and forward propagation overhead of the Plugin Networks is negligible. Thus, our method is significantly faster than the model proposed in <ref type="bibr" target="#b26">[27]</ref>, where iterative optimization process is applied at the inference phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section we evaluate the Plugin Networks on three challenging computer vision tasks. We consider a hierarchical, multi-label classification and semantic segmentation problems. To stay consistent with the previous work on these subjects, we conduct the experiments in the same setups as <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27]</ref>. Therefore, we use SUN397 <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b27">28]</ref>, COCO'14 <ref type="bibr" target="#b16">[17]</ref> and Pascal VOC 2011 <ref type="bibr" target="#b3">[4]</ref> datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Hierarchical Scene Categorization</head><p>We apply our method on SUN397 dataset <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b27">28]</ref>. The dataset is annotated with 3 coarse categories, 16 general scene categories and 397 fine-grained scene categories. Our  We believe that it is characteristic for classification task.</p><p>task is to classify fine-grained categories, given true values for coarse categories, as it was performed in Hu et al. <ref type="bibr" target="#b10">[11]</ref> and Wang et al. <ref type="bibr" target="#b26">[27]</ref>. Thus, coarse categories serve as the partial evidence. We follow same experimental setup as <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b26">27]</ref>: we split dataset into train, validation, and test split with 50, 10 and 40 images per scene category. To allow fair comparison to <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b26">27]</ref>, we use the AlexNet <ref type="bibr" target="#b14">[15]</ref> with Softmax trained on fine-grained categories. It will serve as the base model for the Plugin Networks in this experiment.</p><p>To evaluate our method we compute mean average precision (mAP), multi-class accuracy (MC Acc) and intersectionover-union accuracy (IoU Acc).</p><p>Ablation study: Plugin Network can be attached to different layers of the base model. Furthermore, one can attach more than one Plugin Network simultaneously. Now, we analyze different combinations of the above choices, the results are summarized in  <ref type="table">Table 2</ref>: Performance of the Plugin Network (MC Acc) with respect to different number of hidden layers on SUN397. We consider two cases. Former is the Plugin Network being attached to the conv5 layer, while the latter -attached to the fc3 layer. The experiment shows that simple linear transformation (0 hidden layers) is not sufficient.</p><p>the performance gain differs between chosen layers. Thus, couple of observations can be drawn. It is more effective to attach a Plugin Network to an FC layer rather than a conv layer. Secondly, the Plugin Network connected to deeper layers tends to obtain better performance. The results are aligned with the intuition, that in case of the classification task, deeper layers carry more abstract information. Thus, the Plugin Network can converge to better solution when attached to deeper layers. Moreover, in classification task spatial information is ignored, thus modification of conv layers, which carry spatial information is less important. Note, that in case of semantic segmentation task described in Sec. 4.3, it is more important to modify conv layers. These experiments show that the Plugin Networks are generic and can solve various tasks. We do not observe further performance improvements, if more than one Plugin Network is attached simultaneously. In case of classification task, the Plugin Network mainly learn relationship between partial evidence and output labels. Thus, fc3 outputs carry enough information to find such a relationship.</p><p>In the second ablation study, we consider different Plugin Network architectures. We evaluate the number of hidden layers of the Plugin Network. We check from 0 to 4 hidden layers. The results are in Tab. 2. The best results are obtained by network with 3 hidden layers, which is still a quite shallow architecture that allows efficient training and does not add significant overhead in the inference. The results also show that a linear function (model with 0 hidden layers) has not enough capacity to adjust base model outputs.</p><p>In the third ablation study, we compare different fusion operators from Eq. 2. The results are presented in <ref type="figure" target="#fig_2">Fig. 3</ref> and show that the best performance and convergence rate is obtained by the additive operator, which is a particular case of affine operator where r a = 1. Because we use the ReLU activation function, the translation operation performed by the additive operator is sufficient to alter channel activation values by translating them to either positive or negative halfplane. Scaling (in the multiplicative operator) can achieve similar effect when r a ≈ 0, but negative r a may switch sign    <ref type="table">Table 4</ref>: Plugin Network performance on SUN397 dataset. Our method outperforms state-of-the-art on all reported metrics. To allow fair comparison, we also show the performance of base model used in <ref type="bibr" target="#b26">[27]</ref> as well as the performance of base model trained by us.</p><p>of activation values, resulting in amplification of unwanted responses. Finally we can observe that all proposed fusion operators achieve state-of-the-art results. As the additive operator achieved consistently the best performance, in all further experiments we are using the additive operator.</p><p>In the final model we use AlexNet CNN + Softmax as the base model. The CNN was pretrained on the Places365 dataset <ref type="bibr" target="#b29">[30]</ref>. The base model was chosen to allow a fair comparison with <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b10">11]</ref>. We use the Plugin Network with 3 hidden layers, attached to the fc3 layer from the base model. The model was trained for 15 epochs using the Adam <ref type="bibr" target="#b13">[14]</ref> optimizer with learning rate set to 10 −3 , which was reduced to 10 −4 and 10 −5 after 5 and 10 epochs. Training analysis: In <ref type="table" target="#tab_3">Table 3</ref> we report the performance of our method w.r.t. to the percentage of available training data. We trained Plugin Network with 20%, 40%, 60%, and 80% percent of the training data. The results in <ref type="table" target="#tab_3">Table 3</ref> show that our model achieves the state-of-the-art performance even when trained on 20% of the data. Comparison with the state-of-the-art: In <ref type="table">Table 4</ref> we report the performance of our Plugin Network. The results are averaged over 5 runs to mitigate the randomness in validation set sampling, also standard deviation is computed. We report performance of base model, which does not use partial evidence information as a reference. We also report performance of SINN network <ref type="bibr" target="#b10">[11]</ref> and FeedbackProp <ref type="bibr" target="#b26">[27]</ref>. The results in <ref type="table">Table 4</ref> show that Plugin Networks outperform state-of-the-art methods in terms of MC Acc, mAP and IoU. In addition, our method is easier to train than SINN model and allows faster inference than FeedbackProp. Observations: In <ref type="figure" target="#fig_3">Fig. 4</ref> we show 5 images from the SUN397 test set. For each example we show: ground-truth label for fine-grained category, both classification results from the base and our model. We also report coarse category (partial evidence). The examples show that our method can recover many errors thanks to the presence of partial evidence information. For instance, in top left example, partial evidence that "parking lot" belongs to "Outdoor man made" category helped to correct the classification error. The base model classified example as "Anechoic Chamber", which belongs to "Indoor" category. If we look at the "Anechoic Chamber" examples, one can notice that cars in the parking lot can mimic patterns of the "Anechoic Chamber" walls. Representation analysis: we analyze how the intermediate output of base model was changed, after Plugin Network was added. First, we projected the activations of base  model for each example in a test set to 2D plane using t-SNE method <ref type="bibr" target="#b25">[26]</ref>. <ref type="figure" target="#fig_5">Figure 5</ref> (A) Base Model shows the representation of the base model, while (B) Plugin Network shows the representation of our method. The figure shows that our model learns much better representation, as partial evidence categories are clearly separable. It is also interesting that our model manages to learn such representation, because the loss function that we optimize considers only fine grained labels and ignores error on partial evidence categories. The results show that our model learns dependency between partial evidence categories and fine-grained labels, while not being directly guided by the loss function.</p><p>We also tried to incorporate loss on partial evidence categories, but we did not observe an increase in model performance. Thus we decided to optimize loss function only on unknown labels. Such solution has another advantage -partial evidence categories do not have to be a subset of labels that base model recognizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Multi-label Image Annotation</head><p>In the evaluation of our method for the multi-label image annotation task, we use the COCO 2014 dataset <ref type="bibr" target="#b16">[17]</ref>. It contains 120,000 images, each annotated with 5 caption sentences. Again, for consistency, we follow the same experimental setup as <ref type="bibr" target="#b26">[27]</ref>. Namely, we use the provided 82,783 training data instances as our training set, and randomly split the remaining provided validation data into 20,000 validation set and 20,504 test set images.</p><p>The task is to predict a predefined set of words explaining an image. The words are referred as visual concepts in Fang et al. in their visual concept classifier <ref type="bibr" target="#b4">[5]</ref>. We define them as the 1,000 most frequent words in the captions of the COCO dataset. We use the same tokenization, lemmatization, and stop-word removals as Wang et al. <ref type="bibr" target="#b26">[27]</ref>. As a result, each image is annotated by a vector of 1,000 elements corresponding to an occurrence of words in the captions.</p><p>For the task of reasoning under partial evidence, we ran-domly divide the target vector into a fixed 500 known and 500 unknown classes. The model performance is measured on the unknown set only, while the known set is used as the partial evidence. The base network is first trained as in Fang et al. <ref type="bibr" target="#b4">[5]</ref> on the multi-labeled task using the entire 1,000 classes. It is done by minimization of the binary cross entropy between the predicted and target vector of concepts. For this experiment, for the base network we choose to use the ResNet-18 architecture <ref type="bibr" target="#b9">[10]</ref> to stay consistent with <ref type="bibr" target="#b26">[27]</ref>. Additionally, we also make an ablation study for deeper base network architectures: ResNet-50 and ResNet-101.</p><p>Hyperparameters selection: The architecture of the plugin has been chosen based on the validation scores from Figs. 6 and 7. As indicated before, we first train the base network on the given task. Then, we freeze the base network's weights and add a number of plugins. We train each plugin for 36 epochs with a starting learning rate of 1e−3, which decrease with the number of epochs to 1e−4. We use the Adam <ref type="bibr" target="#b13">[14]</ref> optimizer with the Xavier initialization <ref type="bibr" target="#b6">[7]</ref>. During the hyperparameters selection, we verified all combinations of attaching a plugin to the conv13, conv17 and FC layers of the ResNet-18. We report, that a single attachment to the last FC layers results in the highest mAP. Using any earlier layer still improves the baseline, but such plugin overfits much easier. Using a combination of the FC layer and any other convolutional layer leads to a lower performance comparing to a single attachment to the FC layer. due to the overfitting;[MK] see <ref type="figure" target="#fig_6">Fig. 6</ref>. This results are aligned with the conclusions from Section 4.1.</p><p>In next experiment we search for the best architecture of a single plugin. We consider different number of layers and neurons in the Plugin Networks. See <ref type="figure">Fig. 7</ref> for details. The highest score is achieved by using the two layered architectures with 500 and 2048 neurons respectively. The two and three layered networks get to the plateau after around 20 epochs, while the four-layered networks start to overfit. Comparison with the state-of-the-art: We compare our-    deeper architectures, such as ResNet50 and ResNet101; see Tab. 5. We notice an improvement for each base network. As for the ResNet-18, we achieve the highest score when only one plugin is used at the last FC layer.</p><p>In this section we evaluate the Plugin Networks on multicue object class segmentation task. We take fully convolutional network (FCN) <ref type="bibr" target="#b24">[25]</ref> as a starting point. Next, we experiment by adding several plugins into its architecture. For the baseline we take the pre-trained FCN-8s model 3 trained on the SBD dataset <ref type="bibr" target="#b8">[9]</ref>. We use the same dataset when training the Plugin Networks. We validate our models on the Pascal VOC 2011 segmentation challenge dataset <ref type="bibr" target="#b3">[4]</ref>. We follow <ref type="bibr" target="#b24">[25]</ref> and take Pascal VOC 2011 segval 4 validation split in order to avoid overlapping images between these two datasets. Thus, our training and validation datasets consist of 8498 and 736 images, respectively. Objects in the image are assigned to one of 21 classes.</p><p>The base model (FCN-8s without plugins) results in IoU score of 65.5%. For this scenario, we assume that we have the knowledge of classes present in the image at the inference time, which constitutes partial evidence. Therefore, our partial evidence is a vector of 21 elements. The goal of the Plugin Network is to improve the output segmentation masks of the base network. We experiment with attaching Model # of plugins mean-IoU Baseline 0 65.5 conv1-3 3 65.7 conv1-5 5 70.5 deconv1-3 3 71.1 deconv1-5 5 71.2 conv1-5, deconv1-5 10 72.2 <ref type="table">Table 7</ref>: Scores obtained when attaching plugins at different layers of the FCN architecture for the task of semantic segmentation. conv stands for a convolutional and deconv for transposed convolutional layers. several plugins to the FCN-8s model. We report the results in Tab. 7. In contrary to findings from previous experiments, the Plugin Networks provide the highest increase in the IoU, when multiple of them are used. We achieve the highest gain of 72.2% of IoU when attaching 5 plugins into all of the convolutional layers and all of the transposed convolutional layers. We also outperform the previously proposed method <ref type="bibr" target="#b23">[24]</ref>, which achieved 69.2%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Scene semantic segmentation</head><p>Using partial evidence through the Plugin Networks, we are able to soften the wrong feature maps and strengthen the expected ones. It results in major improvement of the base network. When multiple objects are present in the scene, the base model may have a problem to consistently assign a proper label to a particular object. As expected, when the baseline network makes a mistake by assigning a wrong label of a class that is not present in the image, plugins correct these with a correct class. The examples are shown in <ref type="figure" target="#fig_7">Fig. 8</ref>. For instance in the last row, the pixels belonging to a bottle are inconsistently assigned to different classes by the base model. Using the Plugin Networks fixes the problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this work, we introduced the Plugin Networks -a simple, yet effective method to exploit the availability of a partial evidence in the context of visual recognition tasks. Plugin Networks are integrated directly with the intermediate layers of pre-trained convolutional neural networks and thanks to their lightweight design can be trained efficiently with low computational cost and limited amount of data. Results presented on three challenging tasks and various datasets show superior performance of the proposed method with respect to the state-of-the-art approaches.</p><p>We believe that this work can open novel research directions related to solving visual recognition tasks with partial evidence, as our Plugin Networks are agnostic to the input signal and can accommodate arbitrary modality of the input data, including audio or textual cues. Therefore, their multimodal nature can allow richer contextual cues to be taken into account in the inference procedure, leading to more effective and efficient visual recognition models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>If we add Plugin Network to a convolutional layer, then each element of the output vector is added to a corresponding channel of feature maps. For instance, if convolutional layer has c channels, then output from the Plugin Network has also c elements.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Plugin Networks -fusion operators ablation study on SUN397 dataset. Black dashed line marks state-of-theart.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of results (best viewed in color</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Comparison of representation learned by the base model and the Plugin Network. The output of the layer where Plugin Network is attached, is projected to 2 dimensions using t-SNE. Each point in the figure refers to one example from the test set of SUN397. Our model finds better separation w.r.t. partial evidence categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Ablation study of Plugin Network attachment layer. Base network is ResNet-18. Plugins are attached to 17th and 13th conv layer and the last FC layer. The plugins architecture, consists of 2 layered FC network with 500, and 2048 neurons. The black solid line indicates mAP achieved by ResNet-18 w/o Plugin Network. Attachment to the last fc layer only results in the highest improvement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Examples of semantic segmentation masks predicted by the base model with and without Plugin Networks. The images are taken from the Pascal VOC 2011 validation set. In rows we show different examples, and in columns, from left to right: input image, ground-truth (all classes), base model, FCN-8s with 10 plugins. Our method show clear improvement of the output segmentation masks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>Layer</cell><cell>MC Acc</cell><cell></cell><cell></cell></row><row><cell>no plugin</cell><cell>53.15</cell><cell>Layer</cell><cell>MC Acc</cell></row><row><cell>conv1</cell><cell>54.08</cell><cell>fc1</cell><cell>53.90</cell></row><row><cell>conv2</cell><cell>53.88</cell><cell>fc2</cell><cell>56.47</cell></row><row><cell>conv3</cell><cell>53.75</cell><cell>fc3</cell><cell>57.51</cell></row><row><cell>conv4</cell><cell>54.30</cell><cell>fc1-3</cell><cell>57.08</cell></row><row><cell>conv5</cell><cell>54.86</cell><cell>conv3-5, fc1-3</cell><cell>57.16</cell></row><row><cell>conv3-5</cell><cell>56.88</cell><cell></cell><cell></cell></row></table><note>: Performance of the Plugin networks w.r.t. the num- ber of plugins and layers at which they are attached to the AlexNet. The comparison is done on SUN397 dataset. The performance is better if plugin is attached to deeper lay- ers. Plugins attached to FC layers perform better. The most effective is plugin attached to fc3 layer, although it outper- forms slightly models where 3 and 6 plugins were attached.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>.41 54.06 54.18 54.28 53.90 fc3 54.53 56.65 57.18 57.51 56.56</figDesc><table><row><cell>Layer</cell><cell>0</cell><cell># of hidden layers 1 2 3</cell><cell>4</cell></row><row><cell cols="2">conv5 53</cell><cell></cell><cell></cell></row><row><cell>hand,</cell><cell></cell><cell></cell><cell></cell></row></table><note>Table 1. The results show, that Plugin Network improve performance of the base model re- gardless to which layer it is attached. On the other</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>). We pick 5 representative images from the SUN397 test set and visualize the predicted fine grained categories from our method. We compare them with the predictions from base model (CNN+Softmax). Correct predictions are marked in green, incorrect in red. Failure cases are shown in the rightmost column. "PE" stands for partial evidence.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">% of training examples</cell></row><row><cell></cell><cell>20</cell><cell>40</cell><cell>60</cell><cell>80</cell><cell>100</cell></row><row><cell cols="6">MC Acc 56.58 57.07 57.26 57.34 57.51</cell></row><row><cell>mAP</cell><cell cols="5">60.75 61.19 61.26 61.27 61.37</cell></row><row><cell>IoU</cell><cell cols="5">38.98 39.39 39.42 39.60 39.62</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performance of the Plugin Network model trained on a fraction of training data. The results show that model trained even with 20% of available data, achieves state-ofthe-art performance on all metrics. The results are reported on SUN397 dataset. Ours) 57.59±0.24 61.55±0.43 39.26±0.38</figDesc><table><row><cell></cell><cell>MC Acc</cell><cell>mAP</cell><cell>IoU Acc</cell></row><row><cell>Base model [27]</cell><cell cols="3">52.83±0.24 56.17±0.21 35.90±0.22</cell></row><row><cell>SINN [11, 27]</cell><cell cols="3">54.30±0.35 58.34±0.32 37.28±0.34</cell></row><row><cell>F. Prop (LF) [27]</cell><cell cols="3">54.94±0.42 58.52±0.34 37.86±0.39</cell></row><row><cell>F. Prop (RF) [27]</cell><cell cols="3">55.01±0.35 58.70±0.26 37.95±0.33</cell></row><row><cell>Base (Ours)</cell><cell cols="3">53.30±0.29 56.36±0.21 34.39±0.31</cell></row><row><cell>Plugin Net (</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Ablation study of Plugin Network architecture. Base network is ResNet-18. Plugin network is attached to the last layer. Numbers in brackets indicate the number of neurons at consecutive layers. The black solid line mAP achieved by ResNet-18 w/o Plugin Network. If the plugin has too many layers, it starts to overfit. We report the highest performance by using a two-layered network.selves to the Layer-wise Feedback-prop (LF) and Residual Feedback-prop (RF) Inference proposed by<ref type="bibr" target="#b26">[27]</ref>. Results presented in this work are based on the open sourced online implementation 2 provided by the authors of RF and LF. Due to the random choice of the known and unknown labels, the baseline may differ, but the overall gain stays similar. We show, that in terms of the mAP, we achieve the state-of-theart with a significant margin. Furthermore, as expected, the inference phase is much faster compared to the Feedbackprop methods. Please refer to Tab. 6 for results.Finally, we verify the usage of the Plugin Networks for</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>no</cell><cell>FC,</cell><cell>FC,</cell><cell>FC</cell></row><row><cell></cell><cell></cell><cell></cell><cell>plugin</cell><cell>RL4, RL3</cell><cell>RL4</cell></row><row><cell></cell><cell></cell><cell>ResNet18</cell><cell>23.00</cell><cell>27.56</cell><cell>27.61 27.97</cell></row><row><cell></cell><cell></cell><cell>ResNet50</cell><cell>25.84</cell><cell>29.85</cell><cell>29.65 29.93</cell></row><row><cell></cell><cell></cell><cell>ResNet101</cell><cell>26.56</cell><cell>29.49</cell><cell>29.79 30.13</cell></row><row><cell></cell><cell>28</cell><cell></cell><cell></cell></row><row><cell></cell><cell>27</cell><cell></cell><cell></cell></row><row><cell></cell><cell>26</cell><cell></cell><cell></cell></row><row><cell>mAP</cell><cell>23 24 25</cell><cell>[500, 512] [500, 1024] [500, 2048] [500, 512, 1024] [500, 1024, 2048] [500, 2048, 2048] [500, 512, 2048, 2048] [500, 1024, 1024, 2048]</cell><cell></cell></row><row><cell></cell><cell></cell><cell>1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 Epochs</cell><cell></cell></row><row><cell cols="3">Figure 7: 2 github.com/uvavision/feedbackprop</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Scores obtained when attaching plugins at different places to different versions of ResNet. We consider attachments to the last FC layer, end of the third residual layer (RL3) and fourth residual layer (RL4). Plugins always consist of 500 and 2048 neurons. For this task, we always achieve the highest score when applying a single plugin to the last layer of the base network.</figDesc><table><row><cell></cell><cell cols="2">mAP Inference time [s]</cell></row><row><cell>Base model [5]</cell><cell>23.00</cell><cell>25.64</cell></row><row><cell cols="2">F. Prop (LF) [27] 25.26</cell><cell>93.36</cell></row><row><cell cols="2">F. Prop (RF) [27] 25.70</cell><cell>103.27</cell></row><row><cell>Plugins (Ours)</cell><cell>27.97</cell><cell>25.72</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Results on the COCO'14. The baseline is ResNet-18 trained for the multi-label experiment. Our method not only achieves the state-of-the-art in means of the mAP, but also is the fastest during inference, being barely slower than the base network.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/tooploox/plugin-networks</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">github.com/wkentaro/pytorch-fcn 4 github.com/shelhamer/fcn.berkeleyvision.org</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research was partially supported by the Polish National Science Centre grant no. UMO-2016/21/D/ST6/01946.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Large-scale object classification using label relation graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An empirical study of context in object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2011/workshop" />
		<title level="m">The PASCAL Visual Object Classes Challenge 2011 (VOC2011) Results</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">From captions to visual concepts and back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Object categorization using co-occurrence, location and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Galleguillos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning a tree of metrics with disjoint visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<editor>J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. Pereira, and K. Q. Weinberger, editors, NIPS</editor>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning structured inference neural networks with label relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semantic kernel forests from multiple taxonomies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ballan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<title level="m">Love thy neighbors: Image annotation by exploiting image metadata. ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<editor>F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger</editor>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Overcoming catastrophic forgetting by incremental moment matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/1703.08475</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<editor>D. Fleet, T. Pajdla, B. Schiele, and T. Tuytelaars</editor>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Image labeling on a network: Using social-network metadata for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<editor>A. Fitzgibbon, S. Lazebnik, P. Perona, Y. Sato, and C. Schmid</editor>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learnable pooling with context gating for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<idno>abs/1706.06905</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-G</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">From large scale image categorization to entry-level categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">FiLM: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning multiple visual domains with residual adapters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Priming neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Biparva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="640" to="651" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Feedback-prop: Convolutional neural network inference under partial evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sun database: Exploring a large collection of scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="3" to="22" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sun database: Large-scale scene recognition from abbey to zoo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
