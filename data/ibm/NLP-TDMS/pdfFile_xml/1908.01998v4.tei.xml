<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Few-Shot Object Detection with Attention-RPN and Multi-Relation Detector</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Fan</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhuo</surname></persName>
							<email>wei.zhuowx@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><forename type="middle">Tai</forename><surname>Tencent</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">HKUST</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<settlement>Tencent</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Few-Shot Object Detection with Attention-RPN and Multi-Relation Detector</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Chi-Keung Tang HKUST</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conventional methods for object detection typically require a substantial amount of training data and preparing such high-quality training data is very labor-intensive. In this paper, we propose a novel few-shot object detection network that aims at detecting objects of unseen categories with only a few annotated examples. Central to our method are our Attention-RPN, Multi-Relation Detector and Contrastive Training strategy, which exploit the similarity between the few shot support set and query set to detect novel objects while suppressing false detection in the background. To train our network, we contribute a new dataset that contains 1000 categories of various objects with high-quality annotations. To the best of our knowledge, this is one of the first datasets specifically designed for few-shot object detection. Once our few-shot network is trained, it can detect objects of unseen categories without further training or finetuning. Our method is general and has a wide range of potential applications. We produce a new state-of-the-art performance on different datasets in the few-shot setting. The dataset link is https://github.com/fanq15/Few-Shot-Object-Detection-Dataset. * Both authors contributed equally.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Existing object detection methods typically rely heavily on a huge amount of annotated data and require long training time. This has motivated the recent development of few-shot object detection. Few-shot learning is challenging given large variance of illumination, shape, texture, etc, in real-world objects. While significant research and progress have been made <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>, all of these methods focus on image classification rarely tapping into the problem of few-shot object detection, most probably because transferring from few-shot classification to few-shot object detection is a non-trivial task.</p><p>Central to object detection given only a few shots is how to localize an unseen object in a cluttered background, which in hindsight is a general problem of object localiza- <ref type="bibr">Figure 1</ref>. Given different objects as supports (top corners above), our approach can detect all objects in the same categories in the given query image.</p><p>tion from a few annotated examples in novel categories. Potential bounding boxes can easily miss unseen objects, or else many false detections in the background can be produced. We believe this is caused by the inappropriate low scores of good bounding boxes output from a region proposal network (RPN) making a novel object hard to be detected. This makes the few-shot object detection intrinsically different from few-shot classification. Recent works for few-shot object detection <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref> on the other hand all require fine-tuning and thus cannot be directly applied on novel categories.</p><p>In this paper, we address the problem of few-shot object detection: given a few support images of novel target object, our goal is to detect all foreground objects in the test set that belong to the target object category, as shown in <ref type="figure">Fig. 1</ref>. To this end, we propose two main contributions:</p><p>First, we propose a general few-shot object detection model that can be applied to detect novel objects without retraining and fine-tuning. With our carefully designed contrastive training strategy, attention module on RPN and detector, our method exploits matching relationship between object pairs in a weight-shared network at multiple network stages. This enables our model to perform online detection on objects of novel categories requiring no finetraining or further network adaptation. Experiments show that our model can benefit from the attention module at the early stage where the proposal quality is significantly enhanced, and from the multi-relation detector module at the later stage which suppresses and filters out false detection in the confusing background. Our model achieves new stateof-the-art performance on the ImageNet Detection dataset and MS COCO dataset in the few-shot setting.</p><p>The second contribution consists of a large wellannotated dataset with 1000 categories with only a few examples for each category. Overall, our method achieves significantly better performance by utilizing this dataset than existing large-scale datasets, e.g. COCO <ref type="bibr" target="#b12">[13]</ref>. To the best of our knowledge, this is one of the first few-shot object detection datasets with an unprecedented number of object categories (1000). Using this dataset, our model achieves better performance on different datasets even without any fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>General Object Detection. Object detection is a classical problem in computer vision. In early years, object detection was usually formulated as a sliding window classification problem using handcrafted features <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>. With the rise of deep learning <ref type="bibr" target="#b16">[17]</ref>, CNN-based methods have become the dominant object detection solution. Most of the methods can be further divided into two general approaches: proposal-free detectors and proposal-based detectors. The first line of work follows a one-stage training strategy and does not explicitly generate proposal boxes <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref>. On the other hand, the second line, pioneered by R-CNN <ref type="bibr" target="#b22">[23]</ref>, first extracts class-agnostic region proposals of the potential objects from a given image. These boxes are then further refined and classified into different categories by a specific module <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27</ref>]. An advantage of this strategy is that it can filter out many negative locations by the RPN module which facilitates the detector task next. For this sake, RPN-based methods usually perform better than proposal-free methods with state-of-the-art results <ref type="bibr" target="#b26">[27]</ref> for the detection task. The methods mentioned above, however, work in an intensive supervision manner and are hard to extend to novel categories with only several examples. Few-shot learning. Few-shot learning in a classical setting <ref type="bibr" target="#b27">[28]</ref> is challenging for traditional machine learning algorithms to learn from just a few training examples. Earlier works attempted to learn a general prior <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref>, such as hand-designed strokes or parts which can be shared across categories. Some works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref> focus on metric learning in manually designing a distance formulation among different categories. A more recent trend is to design a general agent/strategy that can guide supervised learning within each task; by accumulating knowledge the network can capture the structure variety across different tasks. This research direction is named meta-learning in general <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref>. In this area, a siamese network was proposed in <ref type="bibr" target="#b36">[37]</ref> that consists of twin networks sharing weights, where each network is respectively fed with a support image and a query. The distance between the query and its support is naturally learned by a logistic regression. This matching strategy captures inherent variety between support and query regardless of their categories. In the realm of matching framework, subsequent works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b39">40]</ref> had focused on enhancing feature embedding, where one direction is to build memory modules to capture global contexts among the supports. A number of works <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref> exploit local descriptors to reap additional knowledge from limited data. In <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref> the authors introduced Graph Neural Network (GNN) to model relationship between different categories. In <ref type="bibr" target="#b44">[45]</ref> the given entire support set was traversed to identify task-relevant features and to make metric learning in high-dimensional space more effective. Other works, such as <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b45">46]</ref>, dedicate to learning a general agent to guide parameter optimization.</p><p>Until now, few-shot learning has not achieved groundbreaking progress, which has mostly focused on the classification task but rarely on other important computer vision tasks such as semantic segmentation <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49]</ref>, human motion prediction <ref type="bibr" target="#b49">[50]</ref> and object detection <ref type="bibr" target="#b8">[9]</ref>. In <ref type="bibr" target="#b50">[51]</ref> unlabeled data was used and multiple modules were optimized alternately on images without box. However, the method may be misled by incorrect detection in weak supervision and requires re-training for a new category. In LSTD <ref type="bibr" target="#b8">[9]</ref> the authors proposed a novel few-shot object detection framework that can transfer knowledge from one large dataset to another smaller dataset, by minimizing the gap of classifying posterior probability between the source domain and the target domain. This method, however, strongly depends on the source domain and is hard to extend to very different scenarios. Recently, several other works for few-shot detection <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref> have been proposed but they learn category-specific embeddings and require to be fine-tuned for novel categories.</p><p>Our work is motivated by the research line pioneered by the matching network <ref type="bibr" target="#b36">[37]</ref>. We propose a general few-shot object detection network that learns the matching metric between image pairs based on the Faster R-CNN framework equipped with our novel attention RPN and multi-relation detector trained using our contrastive training strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">FSOD: A Highly-Diverse Few-Shot Object Detection Dataset</head><p>The key to few-shot learning lies in the generalization ability of the pertinent model when presented with novel categories. Thus, a high-diversity dataset with a large number of object categories is necessary for training a general model that can detect unseen objects and for performing convincing evaluation as well. However, existing datasets <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55]</ref> contain very limited categories and they are not designed in the few-shot evaluation setting. Thus we build a new few-shot object detection dataset. Dataset Construction. We build our dataset from existing large-scale object detection datasets for supervised learning i.e. <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b55">56]</ref>. These datasets, however, cannot be used directly, due to 1) the label system of different datasets are <ref type="figure">Figure 2</ref>. Dataset label tree. The ImageNet categories (red circles) are merged with Open Image categories (green circles) where the superclasses are adopted. <ref type="figure">Figure 3</ref>. The dataset statistics of FSOD. The category image number are distributed almost averagely. Most classes (above 90%) has small or moderate amount of images (in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr">108]</ref>), and the most frequent class still has no more than 208 images. inconsistent where some objects with the same semantics are annotated with different words in the datasets; 2) large portion of the existing annotations are noisy due to inaccurate and missing labels, duplicate boxes, objects being too large; 3) their train/test split contains the same categories, while for the few-shot setting we want the train/test sets to contain different categories in order to evaluate its generality on unseen categories.</p><p>To start building the dataset, we first summarize a label system from <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b55">56]</ref>. We merge the leaf labels in their original label trees, by grouping those in the same semantics (e.g., ice bear and polar bear) into one category, and removing semantics that do not belong to any leaf categories. Then, we remove the images with bad label quality and those with boxes of improper size. Specifically, removed images have boxes smaller than 0.05% of the image size which are usually in bad visual quality and unsuitable to serve as support examples. Next, we follow the few-shot learning setting to split our data into training set and test set without overlapping categories. We construct the training set with categories in MS COCO dataset <ref type="bibr" target="#b12">[13]</ref> in case researchers prefer a pretraining stage. We then split the test set which contains 200 categories by choosing those with the largest distance with existing training categories, where the distance is the shortest path that connects the meaning of two phrases in the is-a taxonomy <ref type="bibr" target="#b56">[57]</ref>. The remaining categories are merged into the training set that in total contains 800 categories. In all, we construct a dataset of 1000 categories with unambiguous category split for training and testing, where 531 categories are from ImageNet dataset <ref type="bibr" target="#b55">[56]</ref> and 469 from Open Image dataset <ref type="bibr" target="#b53">[54]</ref>. Dataset Analysis. Our dataset is specifically designed for few-shot learning and for evaluating the generality of a model on novel categories, which contains 1000 categories with 800/200 split for training and test set respectively, around 66,000 images and 182,000 bounding boxes in to- tal. Detailed statistics are shown in <ref type="table">Table 1</ref> and <ref type="figure">Fig. 3</ref>. Our dataset has the following properties:</p><p>High diversity in categories: Our dataset contains 83 parent semantics, such as mammal, clothing, weapon, etc, which are further split to 1000 leaf categories. Our label tree is shown in <ref type="figure">Fig. 2</ref>. Due to our strict dataset split, our train/test sets contain images of very different semantic categories thus presenting challenges to models to be evaluated.</p><p>Challenging setting: Our dataset contains objects with large variance on box size and aspect ratios, consisting of 26.5% images with no less than three objects in the test set. Our test set contains a large number of boxes of categories not included in our label system, thus presenting great challenges for a few-shot model.</p><p>Although our dataset has a large number of categories, the number of training images and boxes are much less than other large-scale benchmark datasets such as MS COCO dataset, which contains 123,287 images and around 886,000 bounding boxes. Our dataset is designed to be compact while effective for few-shot learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Our Methodology</head><p>In this section, we first define our task of few-shot detection, followed by a detailed description of our novel fewshot object detection network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Problem Definition</head><p>Given a support image s c with a close-up of the target object and a query image q c which potentially contains objects of the support category c, the task is to find all the target objects belonging to the support category in the query and label them with tight bounding boxes. If the support set contains N categories and K examples for each category, <ref type="figure">Figure 4</ref>. Network architecture. The query image and support image are processed by the weight-shared network. The attention RPN module filters out object proposals in other categories by focusing on the given support category. The multi-relation detector then matches the query proposals and the support object. For the N -way training, we extend the network by adding N − 1 support branches where each branch has its own attention RPN and multi-relation detector with the query image. For K-shot training, we obtain all the support feature through the weight-shared network and use the average feature across all the supports belonging to the same category as its support feature. the problem is dubbed N -way K-shot detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Deep Attentioned Few-Shot Detection</head><p>We propose a novel attention network that learns a general matching relationship between the support set and queries on both the RPN module and the detector. <ref type="figure">Fig. 4</ref> shows the overall architecture of our network. Specifically, we build a weight-shared framework that consists of multiple branches, where one branch is for the query set and the others are for the support set (for simplicity, we only show one support branch in the figure). The query branch of the weight-shared framework is a Faster R-CNN network, which contains RPN and detector. We utilize this framework to train the matching relationship between support and query features, in order to make the network learn general knowledge among the same categories. Based on the framework, we introduce a novel attention RPN and detector with multi-relation modules to produce an accurate parsing between support and potential boxes in the query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Attention-Based Region Proposal Network</head><p>In few-shot object detection, RPN is useful in producing potentially relevant boxes for facilitating the following task of detection. Specifically, the RPN should not only distinguish between objects and non-objects but also filter out negative objects not belonging to the support category. However, without any support image information, the RPN will be aimlessly active in every potential object with high objectness score even though they do not belong to the support category, thus burdening the subsequent classification task of the detector with a large number of irrelevant objects. To address this problem, we propose the attention RPN ( <ref type="figure">Fig. 5</ref>) which uses support information to enable filtering out most background boxes and those in non-matching categories. Thus a smaller and more precise set of candidate proposals is generated with high potential containing target objects.</p><p>We introduce support information to RPN through the attention mechanism to guide the RPN to produce relevant <ref type="figure">Figure 5</ref>. Attention RPN. The support feature is average pooled to a 1 × 1 × C vector. Then the depth-wise cross correlation with the query feature is computed whose output is used as attention feature to be fed into RPN for generating proposals.</p><p>proposals while suppressing proposals in other categories. Specifically, we compute the similarity between the feature map of support and that of the query in a depth-wise manner. The similarity map then is utilized to build the proposal generation. In particular, we denote the support features as X ∈ t S×S×C and feature map of the query as Y ∈ t H×W ×C , the similarity is defined as</p><formula xml:id="formula_0">G h,w,c = i,j X i,j,c · Y h+i−1,w+j−1,c , i, j ∈ {1, ..., S}</formula><p>where G is the resultant attention feature map. Here the support features X is used as the kernel to slide on the query feature map <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b58">59]</ref> in a depth-wise cross correlation manner <ref type="bibr" target="#b59">[60]</ref>. In our work, we adopt the features of top layers to the RPN model, i.e. the res4 6 in ResNet50. We find that a kernel size of S = 1 performs well in our case. This fact is consistent with <ref type="bibr" target="#b24">[25]</ref> that global feature can provide a good object prior for objectness classification. In our case, the kernel is calculated by averaging on the support feature map. The attention map is processed by a 3 × 3 convolution followed by the objectiveness classification layer and box regression layer. The attention RPN with loss L rpn is trained jointly with the network as in <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Multi-Relation Detector</head><p>In an R-CNN framework, an RPN module will be followed by a detector whose important role is re-scoring proposals and class recognition. Therefore, we want a detector to have a strong discriminative ability to distinguish different categories. To this end, we propose a novel multi-relation detector to effectively measure the similarity between proposal boxes from the query and the support objects, see <ref type="figure" target="#fig_0">Fig. 6</ref>. The detector includes three attention modules, which are respectively the global-relation head to learn a deep embedding for global matching, the local-correlation head to learn the pixel-wise and depth-wise cross correlation between support and query proposals and the patch-relation head to learn a deep non-linear metric for patch matching. We experimentally show that the three matching modules can complement each other to produce higher performance. Refer to the supplemental material for implementation details of the three heads. Which relation heads do we need? We follow the Nway K-shot evaluation protocol proposed in RepMet <ref type="bibr" target="#b60">[61]</ref> to evaluate our relation heads and other components. <ref type="table" target="#tab_1">Table 2</ref> shows the ablation study of our proposed multi-relation detector under the naive 1-way 1-shot training strategy and 5-way 5-shot evaluation on the FSOD dataset. We use the same evaluation setting hereafter for all ablation studies on the FSOD dataset. For individual heads, the local-relation head performs best on both AP 50 and AP 75 evaluations. Surprisingly, the patch-relation head performs worse than other relation heads, although it models more complicated relationship between images. We believe that the complicated relation head makes the model difficult to learn. When combining any two types of relation head, we obtain better performance than that of individual head. By combining all relation heads, we obtain the full multi-relation detector and achieve the best performance, showing that the three proposed relation heads are complementary to each other for better differentiation of targets from non-matching objects. All the following experiments thus adopt the full multi-relation detector. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Two-way Contrastive Training Strategy</head><p>A naive training strategy is matching the same category objects by constructing a training pair (q c , s c ) where the query image q c and support image s c are both in the same c-th category object. However a good model should not only match the same category objects but also distinguish different categories. For this reason, we propose a novel 2-way contrastive training strategy.</p><p>According to the different matching results in <ref type="figure" target="#fig_1">Fig. 7</ref>, we propose the 2-way contrastive training to match the same category while distinguishing different categories. We randomly choose one query image q c , one support image s c containing the same c-th category object and one other support image s n containing a different n-th category object, to construct the training triplet (q c , s c , s n ), where c = n. In the training triplet, only the c-th category objects in the query image are labeled as foreground while all other objects are treated as background.</p><p>During training, the model learns to match every proposal generated by the attention RPN in the query image with the object in the support image. Thus the model learns to not only match the same category objects between (q c , s c ) but also distinguish objects in different categories between (q c , s n ). However, there are a massive amount of background proposals which usually dominate the training, especially with negative support images. For this reason, we balance the ratio of these matching pairs between query proposals and supports. We keep the ratio as 1:2:1 for the foreground proposal and positive support pairs (p f , s p ), background proposal and positive support pairs (p b , s p ), and proposal (foreground or background) and negative support pairs (p, s n ). We pick all N (p f , s p ) pairs and select top 2N (p b , s p ) pairs and top N (p, s n ) pairs respectively according to their matching scores and calculate the matching loss on the selected pairs. During training, we use the multi-task loss on each sampled proposal as L = L matching + L box with the bounding-box loss L box as defined in <ref type="bibr" target="#b23">[24]</ref> and the matching loss being the binary cross-entropy. Which training strategy is better? Refer to <ref type="table" target="#tab_2">Table 3</ref>. We train our model with the 2-way 1-shot contrastive training strategy and obtain 7.9% AP 50 improvement compared with the naive 1-way 1-shot training strategy, which indicates the importance in learning how to distinguish different categories during training. With 5-shot training, we achieve further improvement which was also verified in <ref type="bibr" target="#b0">[1]</ref> that few- shot training is beneficial to few-shot testing. It is straightforward to extend our 2-way training strategy to multi-way training strategy. However, from <ref type="table" target="#tab_2">Table 3</ref>, the 5-way training strategy does not produce better performance than the 2-way training strategy. We believe that only one negative support category suffices in training the model for distinguishing different categories. Our full model thus adopts the 2-way 5-shot contrastive training strategy. Which RPN is better? We evaluate our attention RPN on different evaluation metrics. To evaluate the proposal quality, we first evaluate the recall on top 100 proposals over 0.5 IoU threshold of the regular RPN and our proposed attention RPN. Our attention RPN exhibits better recall performance than the regular RPN (0.9130 vs. 0.8804). We then evaluate the average best overlap ratio (ABO <ref type="bibr" target="#b61">[62]</ref>) across ground truth boxes for these two RPNs. The ABO of attention RPN is 0.7282 while the same metric of regular RPN is 0.7127. These results indicate that the attention RPN can generate more high-quality proposals. <ref type="table" target="#tab_2">Table 3</ref> further compares models with attention RPN and those with the regular RPN in different training strategies. The model with attention RPN consistently performs better than the regular RPN on both AP 50 and AP 75 evaluation. The attention RPN produces 0.9%/2.0% gain in the 1-way 1-shot training strategy and 2.0%/2.1% gain in the 2-way 5shot training strategy on the AP 50 /AP 75 evaluation. These results confirm that our attention RPN generates better proposals and benefits the final detection prediction. The attention RPN is thus adopted in our full model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In the experiments, we compare our approach with stateof-the-art (SOTA) methods on different datasets. We typically train our full model on FSOD training set and directly evaluate on these datasets. For fair comparison with other methods, we may discard training on FSOD and adopt the same train/test setting as these methods. In these cases, we use a multi-way 1 few-shot training in the fine-tuning stage with more details to be described.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Training Details</head><p>Our model is trained end-to-end on 4 Tesla P40 GPUs using SGD with a batch size of 4 (for query images). The learning rate is 0.002 for the first 56000 iterations and 0.0002 for later 4000 iterations. We observe that pretraining on ImageNet <ref type="bibr" target="#b55">[56]</ref> and MS COCO <ref type="bibr" target="#b12">[13]</ref> can provide stable low-level features and lead to a better converge point. Given this, we by default train our model from the pretrained ResNet50 on <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b55">56]</ref> unless otherwise stated. During training, we find that more training iterations may damage performance, where too many training iterations make the model over-fit to the training set. We fix the weights of Res1-3 blocks and only train high-level layers to utilize lowlevel basic features and avoid over-fitting. The shorter side of the query image is resized to 600 pixels; the longer side is capped at 1000. The support image is cropped around the target object with 16-pixel image context, zero-padded and then resized to a square image of 320 × 320. For few-shot training and testing, we fuse feature by averaging the object features with the same category and then feed them to the attention RPN and the multi-relation detector. We adopt the typical metrics <ref type="bibr" target="#b20">[21]</ref>, i.e. AP, AP 50 and AP 75 for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison with State-of-the-Art Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">ImageNet Detection dataset</head><p>In <ref type="table">Table 4</ref>, we compare our results with those of LSTD <ref type="bibr" target="#b8">[9]</ref> and RepMet <ref type="bibr" target="#b60">[61]</ref> on the challenging ImageNet based 50way 5-shot detection scenario. For fair comparison, we use their evaluation protocol and testing dataset and we use the same MS COCO training set to train our model. We also use soft-NMS <ref type="bibr" target="#b62">[63]</ref> as RepMet during evaluation. Our approach produces 1.7% performance gain compared to the state-ofthe-art (SOTA) on the AP 50 evaluation.</p><p>To show the generalization ability of our approach, we directly apply our model trained on FSOD dataset on the test set and we obtain 41.7% on the AP 50 evaluation which is surprisingly better than our fine-tuned model ( can be directly applied on the test set without fine-tuning to achieve SOTA performance. Furthermore, although our model trained on FSOD dataset has a slightly better AP 50 performance than our fine-tuned model on the MS COCO dataset, our model surpasses the fine-tuned model by 6.4% on the AP 75 evaluation, which shows that our proposed FSOD dataset significantly benefits few-shot object detection. With further fine-tuning our FSOD trained model on the test set, our model achieves the best performance, while noting that our method without fine-tuning already works best compared with SOTA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">MS COCO dataset</head><p>In <ref type="table">Table 5</ref>, we compare our approach 1 with Feature Reweighting <ref type="bibr" target="#b9">[10]</ref> and Meta R-CNN <ref type="bibr" target="#b11">[12]</ref> on MS COCO minival set. We follow their data split and use the same evaluation protocol: we set the 20 categories included in PASCAL VOC as novel categories for evaluation, and use the rest 60 categories in MS COCO as training categories.</p><p>Our fine-tuned model with the same MS COCO training dataset outperforms Meta R-CNN by 2.4%/1.3%/4.0% on AP /AP 50 /AP 75 metrics. This demonstrates the strong learning and generalization ability of our model, as well as that, in the few-shot scenario, learning general matching relationship is more promising than the attempt to learn category-specific embeddings <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12]</ref>. Our model trained on FSOD achieves more significant improvement of 7.9%/12.2%/9.5% on AP /AP 50 /AP 75 metrics. Note that our model trained on FSOD dataset are directly applied on the novel categories without any further fine-tuning while all other methods use 10 supports for fine-tuning to adapt to the novel categories. Again, without fine-tuning our FSODtrained model already works the best among SOTAs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Realistic Applications</head><p>We apply our approach in different real-world application scenarios to demonstrate its generalization capability. <ref type="figure" target="#fig_2">Fig. 8</ref> shows qualitative 1-shot object detection results on novel categories in our test set. We further apply our approach on the wild penguin detection <ref type="bibr" target="#b63">[64]</ref> and show sample qualitative 5-shot object detection results in <ref type="figure">Fig. 9</ref>. Novel Category Detection. Consider this common realworld application scenario: given a massive number of images in a photo album or TV drama series without any labels, the task is to annotate a novel target object (e.g., a rocket) in the given massive collection without knowing which images contain the target object, which can be in different sizes and locations if present. In order to reduce manual labor, one solution is to manually find a small number of images containing the target object, annotate them, and then apply our method to automatically annotate the rest in the image collection. Following this setting, we perform the evaluation as follows: We mix all test images of FSOD dataset, and for each object category, we pick 5 images that contain the target object to perform this novel category object detection in the entire test set. Note that different from the standard object detection evaluation, in this evaluation, the model evaluates every category separately and has no knowledge of the complete categories.</p><p>We compare with LSTD <ref type="bibr" target="#b8">[9]</ref> which needs to be trained on novel categories by transferring knowledge from the source to target domain. Our method, however, can be applied to detect object in novel categories without any further retraining or fine-tuning, which is fundamentally different from LSTD. To compare empirically, we adjust LSTD to base on Faster R-CNN and re-train it on 5 fixed supports for each test category separately in a fair configuration. Results are shown in <ref type="table" target="#tab_4">Table 6</ref>. Our method outperforms LSTD by 3.3%/5.9% and its backbone Faster R-CNN by 4.5%/6.5% on all 200 testing categories on AP 50 /AP 75 metrics. More specifically, without pre-training on our dataset, the performance of Faster R-CNN significantly drops. Note that because the model only knows the support category, the finetuning based models need to train every category separately which is time-consuming. Wild Car Detection. We apply our method 2 to wild car detection on KITTI <ref type="bibr" target="#b51">[52]</ref> and Cityscapes <ref type="bibr" target="#b64">[65]</ref> datasets which are urban scene datasets for driving applications, where the images are captured by car-mounted video cameras. We  <ref type="figure">Figure 9</ref>. Our application results on the penguin dataset <ref type="bibr" target="#b63">[64]</ref>. Given 5 penguin images as support, our approach can detect all penguins in the wild in the given query image. evaluate the performance of Car category on KITTI training set with 7481 images and Cityscapes validation set with 500 images. DA Faster R-CNN <ref type="bibr" target="#b65">[66]</ref> uses massively annotated data from source domains (KITTI/Cityscapes) and unlabeled data from target domains (Cityscapes/KITTI) to train the domain adaptive Faster R-CNN, and evaluated the performance on target domains. Without any further retraining or fine-tuning, our model with 10-shot supports obtains comparable or even better AP 50 performance (37.0% vs. 38.5% on Cityscapes and 67.4% vs. 64.1% on KITTI) on the wild car detection task. Note that DA Faster R-CNN are specifically designed for the wild car detection task and they use much more training data in similar domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">More Categories vs. More Samples?</head><p>Our proposed dataset has a large number of object categories but with few image samples in each category, which we claim is beneficial to few-shot object detection. To confirm this benefit, we train our model on MS COCO dataset, which has more than 115,000 images with only 80 categories. Then we train our model on FSOD dataset with different category numbers while keeping similar number of training image. <ref type="table">Table 7</ref> summarizes the experimental results, where we find that although MS COCO has the most training images but its model performance turns out to be the worst, while models trained on FSOD dataset have better performance as the number of categories incremen- tally increases while keeping similar number of training images, indicating that a limited number of categories with too many images can actually impede few-shot object detection, while large number of categories can consistently benefit the task. Thus, we conclude that category diversity is essential to few-shot object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We introduce a novel few-shot object detection network with Attention-RPN, Multi-Relation Detectors and Contrastive Training strategy. We contribute a new FSOD which contains 1000 categories of various objects with high-quality annotations. Our model trained on FSOD can detect objects of novel categories requiring no pre-training or further network adaptation. Our model has been validated by extensive quantitative and qualitative results on different datasets. This paper contributes to few-shot object detection and we believe worthwhile and related future work can be spawn from our large-scale FSOD dataset and detection network with the above technical contributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A: Implementation Details of Multi-Relation Detector</head><p>Given the support feature f s and query proposal feature f q with the size of 7 × 7 × C, our multi-relation detector is implemented as follows. We use the sum of all matching scores from the three heads as the final matching scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global-Relation</head><p>Head We concatenate f s and f q to the concatenated feature f c with the size of 7 × 7 × 2C. Then we average pool f c to a 1 × 1 × 2C vector. We then use an MLP with two fully connected (fc) layers with ReLU and a final fc layer to process f c and generate matching scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Local-Relation Head</head><p>We first use a weight-shared 1 × 1 × C convolution to process f s and f q separately. Then we calculate the depth-wise similarity using the equation in Section 4.2.1 of the main paper with S = H = W = 7. Then we use a fc layer to generate matching scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Patch-Relation Head</head><p>We first concatenate f s and f q to the concatenated feature f c with the size of 7×7×2C. Then f c is fed into the patch-relation module, whose structure is shown in <ref type="table" target="#tab_6">Table 8</ref>. All the convolution layers followed by ReLU and pooling layers in this module have zero padding to reduce the feature map size from 7 × 7 to 1 × 1. Then we use a fc layer to generate matching scores and a separate fc layer to generate bounding box predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B: More Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Training and Fine-tuning details</head><p>Here we show more details for the experiments in Section 5.2 of the main paper.</p><p>In Section 5.2, we follow other methods to train our model on MS COCO dataset <ref type="bibr" target="#b12">[13]</ref> and fine-tune on the target datasets. When we train our model on MS COCO, we remove the images with boxes smaller than the size of 32×32. Those boxes are usually in bad visual quality and hurt the training when they serve as support examples. When we fine-tune our model on the target datasets, we follow the same setting of other methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref> for fair comparison. Specifically, LSTD <ref type="bibr" target="#b8">[9]</ref> and RepMet <ref type="bibr" target="#b10">[11]</ref> use 5 support images per category where each image contains one or more object instances, and the Feature Reweighting <ref type="bibr" target="#b9">[10]</ref> and Meta R-CNN <ref type="bibr" target="#b11">[12]</ref> use a strict rule to adopt 10 object instances per category for fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Evaluation details</head><p>There are two evaluation settings in the main paper. Evaluation setting 1: The ablation experiments adopt the episode-based evaluation protocol defined in RepMet <ref type="bibr" target="#b10">[11]</ref>, where the setting is borrowed from the few-shot classification task <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4]</ref>. There are 600 random evaluation episodes in total, which guarantee every image in the test set can be evaluated in a high probability. In each episode, for N -way K-shot evaluation, there are K support images for each of the N categories, and there are 10 query images for each category where each query image containing at least one instance belonging to this category. So there are K × N supports and 10 × N query images in each episode. Note that all these categories and images are randomly chosen in each episode. Evaluation setting 2: Other comparison experiments with baselines adopt the standard object detection evaluation protocol, which is a full-way, N-shot evaluation. During evaluation, the support branches in our model can be discarded once the support features are attained, then the support features serve as model weights for the forward process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D: FSOD Dataset Class Split</head><p>Here we describe the training/testing class split in our proposed FSOD Dataset. This split was used in our experiments.</p><p>Training Class Split lipstick, sandal, crocodile, football helmet, umbrella, houseplant, antelope, woodpecker, palm tree, box, swan, miniskirt, monkey, cookie, scissors, snowboard, hedgehog, penguin, barrel, wall clock, strawberry, window blind, butterfly, television, cake, punching bag, picture frame, face powder, jaguar, tomato, isopod, balloon, vase, shirt, waffle, carrot, candle, flute, bagel, orange, wheelchair, golf ball, unicycle, surfboard, cattle, parachute, candy, turkey, pillow, jacket, dumbbell, dagger, wine glass, guitar, shrimp, worm, hamburger, cucumber, radish, alpaca, bicycle wheel, shelf, pancake, helicopter, perfume, sword, ipod, goose, pretzel, coin, broccoli, mule, cabbage, sheep, apple, flag, horse, duck, salad, lemon, handgun, backpack, printer, mug, snowmobile, boot, bowl, book, tin can, football, human leg, countertop, elephant, ladybug, curtain, wine, van, envelope, pen, doll, bus, flying disc, microwave oven, stethoscope, burrito, mushroom, teddy bear, nail, bottle, raccoon, rifle, peach, laptop, centipede, tiger, watch, cat, ladder, sparrow, coffee table, plastic bag, brown bear, frog, jeans, harp, accordion, pig, porcupine, dolphin, owl, flowerpot, motorcycle, calculator, tap, kangaroo, lavender, tennis ball, jellyfish, bust, dice, wok, roller skates, mango, bread, computer monitor, sombrero, desk, cheetah, ice cream, tart, doughnut, grapefruit, paddle, pear, kite, eagle, towel, coffee, deer, whale, cello, lion, taxi, shark, human arm, trumpet, french fries, syringe, lobster, rose, human hand, lamp, bat, ostrich, trombone, swim cap, human <ref type="figure">Figure 10</ref>. Qualitative 1-shot object detection results on our test set. We visualize the bounding boxes with score larger than 0.8. <ref type="figure">Figure 11</ref>. Qualitative results of our 1-shot object detection on test set. We visualize the bounding boxes with score larger than 0.8. <ref type="figure">Figure 12</ref>. Qualitative results of our 5-shot car detection on Cityscapes. We visualize the bounding boxes with score larger than 0.8. The first image is a training example. <ref type="figure">Figure 13</ref>. Qualitative results of our 5-shot car detection on KITTI. We visualize the bounding boxes with score larger than 0.8.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 6 .</head><label>6</label><figDesc>Multi-Relation Detector. Different relation heads model different relationships between the query and support image. The global relation head uses global representation to match images; local relation head captures pixel-to-pixel matching relationship; patch relation head models one-to-many pixel relationship.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 7 .</head><label>7</label><figDesc>The 2-way contrastive training triplet and different matching results. Only the positive support has the same category with the target ground truth in the query image. The matching pair consists of the positive support and foreground proposal, and the non-matching pair has three categories: (1) positive support and background proposal, (2) negative support and foreground proposal and (3) negative support and background proposal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 8 .</head><label>8</label><figDesc>Qualitative 1-shot detection results of our approach on FSOD test set. Zoom in the figures for more visual details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Experimental results for different relation head combinations in the 1-way 1-shot training strategy.</figDesc><table><row><cell cols="2">Global R Local R Patch R AP50 AP75</cell></row><row><cell>47.7</cell><cell>34.0</cell></row><row><cell>50.5</cell><cell>35.9</cell></row><row><cell>45.1</cell><cell>32.8</cell></row><row><cell>49.6</cell><cell>35.9</cell></row><row><cell>53.8</cell><cell>38.0</cell></row><row><cell>54.6</cell><cell>38.9</cell></row><row><cell>55.0</cell><cell>39.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Experimental results for training strategy and attention RPN with the multi-relation detector.</figDesc><table><row><cell cols="3">Training Strategy Attention RPN AP50 AP75</cell></row><row><cell>1-way 1-shot</cell><cell>55.0</cell><cell>39.1</cell></row><row><cell>1-way 1-shot</cell><cell>55.9</cell><cell>41.1</cell></row><row><cell>2-way 1-shot</cell><cell>63.8</cell><cell>42.9</cell></row><row><cell>2-way 5-shot</cell><cell>65.4</cell><cell>43.7</cell></row><row><cell>2-way 5-shot</cell><cell>67.5</cell><cell>46.2</cell></row><row><cell>5-way 5-shot</cell><cell>66.9</cell><cell>45.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 )Table 5 .</head><label>45</label><figDesc>. It should be noted that our model trained on FSOD datasetTable 4. Experimental results on ImageNet Detection dataset for 50 novel categories with 5 supports. † means that the testing categories are removed from FSOD training dataset. ImageN et means the model is fine-tuned on ImageNet Detection dataset. Experimental results on MS COCO minival set for 20 novel categories with 10 supports. † means that the testing categories are removed from FSOD training dataset.</figDesc><table><row><cell>Method</cell><cell>dataset</cell><cell cols="2">fine-tune</cell><cell cols="2">AP50 AP75</cell></row><row><cell>LSTD [9]</cell><cell cols="3">COCO ImageN et</cell><cell>37.4</cell><cell>-</cell></row><row><cell cols="4">RepMet [11] COCO ImageN et</cell><cell>39.6</cell><cell>-</cell></row><row><cell>Ours</cell><cell cols="3">COCO ImageN et</cell><cell>41.3</cell><cell>21.9</cell></row><row><cell>Ours</cell><cell>FSOD  †</cell><cell></cell><cell></cell><cell>41.7</cell><cell>28.3</cell></row><row><cell>Ours</cell><cell cols="3">FSOD  † ImageN et</cell><cell>44.1</cell><cell>31.0</cell></row><row><cell>Method</cell><cell cols="3">dataset fine-tune AP</cell><cell cols="2">AP50 AP75</cell></row><row><cell>FR [10]</cell><cell>COCO</cell><cell>coco</cell><cell>5.6</cell><cell>12.3</cell><cell>4.6</cell></row><row><cell cols="2">Meta [12] COCO</cell><cell>coco</cell><cell>8.7</cell><cell>19.1</cell><cell>6.6</cell></row><row><cell>Ours</cell><cell>COCO</cell><cell>coco</cell><cell>11.1</cell><cell>20.4</cell><cell>10.6</cell></row><row><cell>Ours</cell><cell>FSOD  †</cell><cell></cell><cell>16.6</cell><cell>31.3</cell><cell>16.1</cell></row></table><note>coco means the model is fine-tuned on MS COCO dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>Experimental results on FSOD test set for 200 novel categories with 5 supports evaluated in novel category detection.f sod means the model is fine-tuned on FSOD dataset.</figDesc><table><row><cell>Method</cell><cell cols="3">FSOD pretrain fine-tune AP50 AP75</cell></row><row><cell>FRCNN [25]</cell><cell>f sod</cell><cell>11.8</cell><cell>6.7</cell></row><row><cell>FRCNN [25]</cell><cell>f sod</cell><cell>23.0</cell><cell>12.9</cell></row><row><cell>LSTD [9]</cell><cell>f sod</cell><cell>24.2</cell><cell>13.5</cell></row><row><cell>Ours</cell><cell>trained directly</cell><cell>27.5</cell><cell>19.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 .</head><label>8</label><figDesc>Architecture of the patch-relation module.</figDesc><table><row><cell>Type</cell><cell cols="2">Filter Shape Stride/Padding</cell></row><row><cell>Avg Pool</cell><cell>3x3x4096</cell><cell>s1/p0</cell></row><row><cell>Conv</cell><cell>1x1x512</cell><cell>s1/p0</cell></row><row><cell>Conv</cell><cell>3x3x512</cell><cell>s1/p0</cell></row><row><cell>Conv</cell><cell>1x1x2048</cell><cell>s1/p0</cell></row><row><cell>Avg Pool</cell><cell>3x3x2048</cell><cell>s1/p0</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The fine-tuning stage benefits from more ways during the multi-way training, so we use as many ways as possible to fill up the GPU memory.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Since Feature Reweighting and Meta R-CNN are evaluated on MS COCO, in this subsection we discard pre-training on<ref type="bibr" target="#b12">[13]</ref> for fair comparison to follow the same experimental setting as described.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We also discard the MS COCO pretraining in this experiment.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Meta-learning with memory-augmented neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Modelagnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Memory matching networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenggang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dynamic few-shot visual learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung Yongxin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Lstd: A low-shot transfer detector for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Hao Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoyou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Few-shot object detection via feature reweighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Repmet: Representative-based metric learning for classification and few-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Shtok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sivan</forename><surname>Harary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Amit Aides, Rogerio Feris, Raja Giryes, and Alex M Bronstein</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Meta r-cnn : Towards general solver for instance-level low-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anni</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navneet</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vioda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Receptive field block net for accurate and fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sniper: Efficient multi-scale training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahyar</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Is learning the n-th thing any easier than learning the first?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">One-shot learning of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="594" to="611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">One shot learning of simple visual concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brenden</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Cognitive Science Society</title>
		<meeting>the Annual Meeting of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">One-shot learning by inverting a compositional causal process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brenden M Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Brenden M Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">One shot learning via compositions of meaningful patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Tadam: Task dependent adaptive metric for improved few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Pau Rodríguez López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lacoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Few-shot learning through an information retrieval lens</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Low-shot visual recognition by shrinking and hallucinating features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Yu</surname></persName>
		</author>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Rapid adaptation with conditionally shifted neurons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soroush</forename><surname>Mehri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Low-shot learning from imaginary data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Revisiting local descriptor based image-to-class measure for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Dense classification and implanting for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lifchitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvaine</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Bursuc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Edge-labeling graph neural network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><forename type="middle">D</forename><surname>Sungwoong Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo Jongmin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesup</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Generating classification weights with gnn denoising autoencoders for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Finding task-relevant features for fewshot learning by category traversal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Modelagnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Few-shot semantic segmentation with prototype learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanqing</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">One-shot segmentation in clutter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Attention-based multi-context guiding for few-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiliang</forename><surname>Pengwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Few-shot human motion prediction via metalearning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Liang-Yan Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José</forename><forename type="middle">M F</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Few-example object detection with model communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1641" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00982</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Fully-convolutional siamese networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Class-agnostic counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erika</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Siamrpn++: Evolution of siamese visual tracking with very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Repmet: Representative-based metric learning for classification and one-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Shtok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sivan</forename><surname>Harary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mattias</forename><surname>Marder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharathchandra</forename><surname>Pankanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raja</forename><surname>Giries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jasper Rr Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Koen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Soft-nms improving object detection with one line of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navaneeth</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Counting in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Arteta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Domain adaptive faster r-cnn for object detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
