<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Few-Shot Learning with Global Class Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiange</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aoxue</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
							<email>t.xiang@surrey.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">University of Surrey</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>Huang</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
							<email>wanglw@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Few-Shot Learning with Global Class Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose to tackle the challenging fewshot learning (FSL) problem by learning global class representations using both base and novel class training samples. In each training episode, an episodic class mean computed from a support set is registered with the global representation via a registration module. This produces a registered global class representation for computing the classification loss using a query set. Though following a similar episodic training pipeline as existing meta learning based approaches, our method differs significantly in that novel class training samples are involved in the training from the beginning. To compensate for the lack of novel class training samples, an effective sample synthesis strategy is developed to avoid overfitting. Importantly, by joint base-novel class training, our approach can be easily extended to a more practical yet challenging FSL setting, i.e., generalized FSL, where the label space of test data is extended to both base and novel classes. Extensive experiments show that our approach is effective for both of the two FSL settings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep learning has achieved great success in various recognition tasks <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b32">33]</ref>. However, with a large number of parameters, deep neural networks need large amounts of labeled data from each class for model training. This severely limits their scalability -for many rare classes, collecting a large number of training samples is infeasible or even impossible. In contrast, humans can easily recognize a new object class after only seeing it once. Inspired by the few-shot learning ability of humans, there has been increasing interest recently on few-shot learning (FSL) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b9">10]</ref>. In the FSL problem, we are provided with a set of base classes with ample training samples per class, and a set of novel classes with only a * Equal contribution. <ref type="figure" target="#fig_1">Figure 1</ref>. An illustration of our approach. The first block shows a base class and a novel class in an embedding space. The base class contains sufficient labeled data while the novel class has only a few labeled data. The two classes have intersections, and we aim to learn global representations for each class which are used for recognizing test data. The second block illustrates the two key components of the proposed model. First, we generate new samples (orange cross) to increase intra-class variance for novel classes. Second, a registration module is proposed to encourage sample to 'pull' its global representation to itself and 'push' other global representations away. Similarly, global representations would influence the sample. The last block shows the results after learning global representations jointly using both base and novel class samples. The two classes become more separable, and the global representations are more distinguishable. few labeled samples (shots) per class. FSL aims to learn a classifier for the novel classes with few shots by transferring knowledge from the based classes.</p><p>Most existing FSL approaches are based on meta learning. During a meta learning phase, the base classes are sampled to simulate the few-shot learning condition for the novel classes. Transferable knowledge is then learned from the source classes in the form of good initial conditions <ref type="bibr" target="#b8">[9]</ref>, embeddings <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b30">31]</ref> or optimization strategies <ref type="bibr" target="#b22">[23]</ref>. After the meta learning phase, the target few-shot learning problem is solved by fine-tuning <ref type="bibr" target="#b8">[9]</ref> with the learned optimization strategy <ref type="bibr" target="#b22">[23]</ref> or computed in a feed-forward pass <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b28">29]</ref> without updating network weights. However, there is a fundamental limitation in these meta learning based approaches: the model (initial condition, embedding or optimization strategy) are mostly learned with the source data only. This offers no guarantee for the model to generalize well on the target data, even after the fine-tuning step.</p><p>In this paper, we propose a novel approach to FSL by representing each class, base or novel, as a single point in an embedding space. Since the representation is learned jointly using both base and novel class training samples, it is called a global representation. We argue that only by involving the novel class data at the very beginning of the model training, we can ensure that the learned FSL model is suited for the novel classes.</p><p>A critical obstacle for learning such a global class representation is the imbalanced training sample numbers across the base and novel classes. We overcome this problem by two means. First, we use sample synthesis to increase the intra-class variation for novel classes. By randomly sampling data points from a subspace of samples in the same classes, our synthesis strategy can effectively increase intraclass variance (see orange crosses in <ref type="figure" target="#fig_1">Figure 1</ref>). Second, we introduce episodic training to balance the base and novel class samples. In each training episode, an episodic class mean computed from a support set is registered with the global representation via a registration module. This produces a registered global class representation for computing the classification loss using a query set. By learning to compare each data against all global class representations, our registration module forces each data to 'pull' the global representation of its class toward itself and 'push' other global representations away in the embedding space (see blue arrows in <ref type="figure" target="#fig_1">Figure 1</ref>). After the training, the learned global representations are used for recognizing test data.</p><p>Since the base and novel classes are involved simultaneously in every step of the training process, the learned global representations are naturally able to distinguish both the base and novel classes. This means that our approach can be easily extended to a more realistic yet more challenging FSL setting (i.e., Generalized FSL) where the label space of test data covers both base and novel classes. This is as opposed to the standard setting where the test data contain novel class samples only. Under this setting, there is no way that one can tell whether the learned global representations have been biased towards to the base classes.</p><p>Our main contributions are as follows: 1) We propose a novel FSL approach that recognizes novel class data by learning global representations using both base and novel classes training samples. 2) Our approach can be easily extended to the more realistic generalized FSL setting. Extensive experiments on two FSL benchmarks show that the proposed approach is effective under both the standard and generalized settings. Importantly, the improvement is even bigger under the generalized FSL setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Recently, few-shot object recognition has become topical. With the success of deep learning-based approaches in the data-rich many-shot setting <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b10">11]</ref>, there has been a surge of interest in generalizing such deep learning approaches to the few-shot learning setting, so that visual recognition can truly scale to a large number of classes (e.g., millions). Most of the recent deep learning based approaches use a meta-learning or learning-to-learn strategy. With meta learning, these models extract transferrable knowledge from a set of auxiliary tasks via episodic training, which then helps them to solve the target few-shot classifier training for the target novel classes.</p><p>Existing meta-learning based FSL approaches can be grouped into three categories: 1) The first category address the FSL problem by "learning to fine-tune". These approaches aim to learn good model initialization (i.e., parameters of a network) so that the classifiers for novel classes can be learned with a limited number of labeled examples and a small number of gradient update steps <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b19">20]</ref>. 2) The second category of models tackles the FSL by "learning a good metric to compare". The intuition is that if a model can determine the similarity of two images, it can classify an unseen input image with the labeled instances <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>. To learn an effective comparison model, these methods make their prediction conditioned on distances to few labeled instances during the training process <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b1">2]</ref>. These instances are sampled from base classes designed to simulate the test scenario where only a few shots from the novel classes are available. 3) The third category of models deals with the FSL problem by "learning an optimizer". These models attempt to modify the classical gradient-based optimization (e.g., stochastic gradient descent) to fit into the meta-learning scenario <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b18">19]</ref>. Despite their dominance in the recent literature, a fundamental problem remains for these meta-learning-based models: in the training stage, only the base classes samples are involved, making them vulnerable to overfitting to the base classes.</p><p>The most related approach to ours is the prototypical networks <ref type="bibr" target="#b30">[31]</ref>, which aims to learn a class representation, or prototype by feeding the feature mean of a few shots of the class to a fully-connected layer. Compared with <ref type="bibr" target="#b30">[31]</ref>, our model has two vital differences: (1) We learn global class representation rather than episodic one as in <ref type="bibr" target="#b30">[31]</ref>. (2) Both base and novel class training samples are used to jointly learn the representation. This ensures that a class represen-tation can be learned with a global consistency rather than a local one. Also related is the recent feature hallucination based approach <ref type="bibr" target="#b36">[37]</ref>. These are orthogonal to ours -we can employ any one of them in our model for synthesizing novel class samples to tackle the class imbalance problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>The key idea of our model is joint class representation learning using both base and novel class training samples. To overcome the class imbalance problem, we employ representation registration and novel class sample synthesis. In this section, we first introduce these two key modules. After that, we describe how to integrate the two modules into our FSL framework to recognize unlabeled data from novel classes. Finally, we extend the proposed approach to the generalized FSL setting, where unlabeled data are from both base and novel classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Registration Module</head><p>Suppose we have a set of classes C total = {c 1 , ..., c N }, where N denotes the total number of classes. These include both base and novel classes. We are given a training set D train whose label space is C total (i.e., both base and novel classes are used for training) and a test set D test . Our registration module compares the training samples against global representations of all training classes and selects the corresponding global representation. A registration loss is defined to jointly optimize global representations and the registration module.</p><p>Concretely, first, a sample x i in the training set is fed into a feature extractor F to obtain its visual feature, denoted as f i = F (x i ). Then, visual feature of this sample and all global class representations G = {g cj , c j ∈ C total } are fed into registration module R. For each visual feature f i , the registration module R produces a vector</p><formula xml:id="formula_0">V i = [v c1 i , ...., v c N i ] T ,</formula><p>where the j-th element is the similarity score between f i and the global representation g cj for the class c j . In this paper, we compute similarity score in an embedding space.</p><formula xml:id="formula_1">v c j i = exp d c j i c j ∈C total exp d c j i d c j i = − θ(fi) − φ(gc j ) 2 (1)</formula><p>where θ(·) and φ(·) are embeddings for visual feature of samples and global class representations, respectively.</p><p>Therefore, we define a registration loss L reg for sample x i (with its label y i ) to make the sample nearest to its global class representation in the embedding space, where CE denotes a cross entropy loss.</p><formula xml:id="formula_2">Lreg = CE(yi, Vi)<label>(2)</label></formula><p>By comparing sample against global representations of all classes in C total in the embedding space, our registration module makes each global representations close to samples within its class, and away from the extra-class samples. Note that both the representation and the feature extraction network are end-to-end trainable and jointly optimized. Specifically, with well trained global class representations, the feature extractor is optimized to cluster samples around these class representations; given feature extractor, each global class representation is optimized to be closer to samples in its class and away from other representations.</p><p>When integrating the registration module into the FSL framework (to be detailed later), we feed labeled data into the registration module to select global representations for classifying query images. The loss of classifying query images will also optimize global representations, together with the registration loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Sample Synthesis Module</head><p>To address the class imbalance issue caused by the limited data (few shots) in the novel classes, we propose a sample synthesis strategy to synthesize samples for novel classes denoted as C novel . In this paper, we synthesize samples by two steps: 1) we generate new samples with original samples. 2) we synthesize a new sample by using all samples obtained by the first step.</p><p>Specifically, we first generate new samples with original samples by using random cropping, random flipping and data hallucination <ref type="bibr" target="#b36">[37]</ref>. These methods take a single example of a class as input and produce variants of this example. Using the three methods on the original few training samples, we will obtain a total of k t samples for every novel class. After that, we further synthesize new samples from the k t samples per class. In particular, for a novel class c j , we first randomly select k r samples out of the k t samples. Then, we synthesize a new sample by randomly selecting a data point from the subspace spanned by the k r visual features of the samples {f 1 , .., f kr }. Concretely, we sample k r values {ν 1 , ..., ν kr } from a uniform distribution ranging from 0 to 1. Then, we weight sum these visual features by using random numbers as weights. A new sample r cj for the novel class c j is defined in Equation 3. With the proposed strategy, the intra-class variation thus increases and the limited data issue is alleviated (as validated in <ref type="figure" target="#fig_3">Figure  3</ref>).</p><formula xml:id="formula_3">r cj = kr i=1 ν i j ν j f i , wherey i = c ĵ k r ∼ U(0, k t ), k r = k r , ν i ∼ U(0, 1),<label>(3)</label></formula><p>where r cj denotes the synthesized sample for novel class c j . U(a, b) denotes a uniform distribution ranging from a to b. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Few Shot Learning By Registration</head><p>Now we can describe our full FSL framework. In FSL, the class set C total consists of two disjoint sets: a set of base classes C base and a set of novel classes C novel . In the training set, each base class has sufficient labeled data, while each novel class is given only n f ew (n f ew ≤ 5) labeled samples. In the test set, samples are from classes in C novel under the standard FSL setting. We first obtain an initial class representation for each class in C total by simply averaging visual features of all samples from the class. Our model aims to learn the global class representation for each novel class given the simple initialization.</p><p>To alleviate the severe data imbalance issue in the training set, apart from the data synthesis strategy described earlier, we define an episodic learning strategy commonly adopted by many existing meta learning models <ref type="bibr" target="#b30">[31]</ref>. In each training iteration, an episode/mini-batch is obtained by the following three steps: 1) We first randomly select n train classes from the whole class set C total to form a training episode class set C train ; 2) n s samples of each of the classes in C train are randomly selected from the training set to form a support set S = {(x i , y i ), i = 1, ..., n s × n train }.</p><p>3) We select n q samples of each of the classes in C train from the training set to form a query set Q = {(x k , y k ), k = 1, ..., n q × n train }. Note that, each novel class has only n f ew labelled samples in training set, where n f ew is usually smaller than n s + n q . Therefore, we first augment the n f ew original samples to n s + n q samples by using the synthesis method proposed in Section 3.2, then split them into n s samples and n q samples, and put them into the support set and the query set, respectively. In each testing iteration, an episode is different from a training episode in three parts: 1) C test consists of n test randomly selected from novel classes only. 2) We use the labeled images of the n test novel classes in the training set as the support set (i.e., the few shots).</p><p>3) The query set is selected from D test instead of D train . If there is n f ew labeled samples per novel class in the training set, the FSL problem is called n f ewshot FSL. If a model predicts the label of a test image from n test candidate classes during the test stage, the FSL problem is called n test -way FSL.</p><p>With the episodic learning strategy, we first integrate the proposed sample synthesis module into the FSL framework. Specifically, in each training iteration, the images {x i , i = 1, ..., n s × n train } in the support set S are first fed into the trainable feature extractor F to obtain their visual features {f i = F (x i ), i = 1, ..., n s × n train }. Second, we construct a episodic representation for each class in the support set, denoted as {r ci , c i ∈ C train }. This episodic representation r ci integrates information of class c i in the support set S for the current mini-batch; it is thus a local class representation rather than global. For base classes, we average the visual features in the same class to obtain the episodic class representations, similar to that in Prototypical Nets <ref type="bibr" target="#b30">[31]</ref>. For novel classes, we exploit the synthesis strategy proposed in Section 3.2 to synthesize a new sample for each class, with visual features from this class in the support set as input (see 'Sample Synthesis' in the <ref type="figure" target="#fig_0">Figure  2</ref>). Such episodic novel class representations are more diverse than original labeled samples.</p><p>Then, the registration module is integrated into our FSL framework to select global representation according to its episodic representation. The selected global representations are then used to classify query images. Specifically, we feed episodic representations of classes in the support set {r cj , c j ∈ C train } and all global class representations G = {g cj , c j ∈ C total } into the registration module R to compute similarity score between each episodic class representation and all global class representations according to Equation 1. The similarity score will be used to select global class representations for query images. To make the global class representations more separable, our registration module defines a registration loss to impose the similarity score of its to be bigger than those of other global representations. The more separable global class representations can enhance the ability to recognize unlabeled images.</p><p>According to Equation 2, the registration loss of one class episodic representation r ci is formulated as follows:</p><formula xml:id="formula_4">Lreg(rc i ) = CE(ci, Vi), v c j i = exp d c j i c j ∈C total exp d c j i d c j i = − θ(rc i ) − φ(gc j ) 2 (4) where V i = [v c1</formula><p>i , ...., v c N i ] T denotes the similarity scores between the episodic representation r ci for class c i in C train and all global class representations {g cj , c j ∈ C total }. Finally, with the similarity score obtained by the third step, we select a global class representation for each class in C train as its class representation and recognize query images by performing the nearest neighbor search using the selected global class representations as references. However, it is nondifferentiable when the selection is an argmax operation. Therefore, we select the class representation in a soft manner: taking the probability distribution V i as the weight, we estimate class representations of the i-th class in C train (denoted as ξ i ) as a weighted sum of global representations of all classes. That is, ξ i = V i G. In our experiments (see Section 4.1.4), the soft manner shows almost the same performance as the argmax operation. Now we obtain the corresponding global class representation set {ξ i , i = 1, ..., n train } for classes in C train . The classifica-tion loss of query samples {x k , y k } ∈ Q is formulated as follows:</p><formula xml:id="formula_5">L f sl (x k ) = CE(y k , W k ), w i k = exp d i k i∈C train exp (d i k ) d i k = − F (x k ) − ξi 2<label>(5)</label></formula><p>where F denotes the feature extractor, and W k = [w 1 k , ...., w ntrain k ] T denotes the similarity between the selected global class representation ξ i and the query sample x k .</p><p>By combining the registration loss and classification loss of query images together, the total loss function for a training iteration is given as in <ref type="bibr">Equation 6</ref>, and the outline of computing training episode loss is given in Algorithm 1. The loss will update all learnable components including global representations, and the parameters of the registration module and feature extractor.</p><formula xml:id="formula_6">L total (S, Q) = c i ∈S Lreg(rc i ) + k∈Q L f sl (x k )<label>(6)</label></formula><p>During the test, we use the same procedure to predict the label of unlabeled data. That is, we first feed support set to feature extractor and get episodic class representations for each class. Then, episodic class representations are used to register the corresponding global class representation via the registration module. After that, we perform the nearest neighbor search by computing the Euclidean distance between the feature vector of a test sample and the selected global representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Extension to Generalized FSL</head><p>Although the proposed approach is originally designed for standard FSL, it can be easily extended to generalized </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Discussion</head><p>In this section, we evaluate our approach by conducting three groups of experiments: 1) standard FSL setting where the label space of test data is restricted to a few novel classes at each test iteration, 2) generalized FSL setting where the label space of test data is extended to both base classes and novel classes, and 3) ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Standard Few-Shot Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Datasets and Settings</head><p>Under the standard FSL setting adopted by all FSL works so far, we evaluate our approach on the most popular benchmarks: Omniglot and miniImageNet. Omniglot <ref type="bibr" target="#b15">[16]</ref> contains 32,460 images of hand-written characters. It is composed of 1,623 different characters within 50 alphabets. Each character has 20 images. We follow the most common split in <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b30">31]</ref>, taking 1,200 characters for training and the rest 423 for testing. Moreover, we adopt the same data preprocessing as in <ref type="bibr" target="#b35">[36]</ref>: each image is resized to 28 × 28 pixels and rotated by multiples of 90 degrees as data augmentation. The miniImageNet dataset is a recent collection of ImageNet for FSL. It consists of 100 classes randomly selected from ImageNet <ref type="bibr" target="#b26">[27]</ref> and each class contains 600 images with the size of 84 × 84 pixels. Following the widely used setting in prior works <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b30">31]</ref>, we take 64 classes for training, 16 for validation and 20 for testing, respectively. During the training stage, the 64 training classes and 16 validation classes are respectively regarded as base classes and novel classes to decide the hyperparameters of our approach. The reported performance is obtained by our approach trained with 64 training classes as base classes and 20 test classes as novel classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Implementation Details</head><p>Network architecture: Our feature extractor F mirrors the architecture used by <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b35">36]</ref> and consists of four convolutional blocks. Each block comprises a 64-filter 3 × 3 convolution, batch normalization layer <ref type="bibr" target="#b11">[12]</ref>, a ReLU nonlinearity and a 2 × 2 max-pooling layer. When applied to the 28 × 28 Omniglot images, this architecture results in 64-dimensional output space. When applied to the 84 × 84 miniImageNet images, this architecture results in 1600dimensional output space. We use the same feature extractor on images in both the support set and query set. The two embeddings θ and φ in our registration module use the same architecture: a fully-connected layer followed by a batch normalization layer and a ReLU non-linearity layer. The output channels of the fully-connected layer are 512. Training procedure: We first train our feature extractor F for simple classification task by using all base class. Each global class representation is then initialized by first using the pretrained F to extract visual features of images from its class and then averaging these visual features. The data hallucinator <ref type="bibr" target="#b36">[37]</ref> used in Section 3.2 is pretrained with the pretrained F as feature extractor. The registration module is trained from scratch with random Gaussian initialization. After initializing the feature extractor, global representations, the data hallucinator, and the registration module, we train them together in an end-to-end manner. Stochastic gradient descent (SGD) <ref type="bibr" target="#b16">[17]</ref> with momentum is used for model training with a base learning rate of 0.001 and a momentum of 0.9. The learning rate is annealed by 1/10 for every 3,000 episodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Results on Omniglot</head><p>Following the standard setting adopted by most existing few-shot learning works, we conduct 5-way 1-shot/5-shot and 20-way 1-shot/5-shot classification on the Omniglot dataset. In the four FSL tasks, each training episode contains 60 classes and each test episode contains n test classes (n test = 20 for 20-way scenario and n test = 5 for 5-way scenario). In 1-shot and 5-shot scenarios, each query set has 5 images per class, while each support set contains 1 and 5 image(s) per class, respectively. For a training episode, images in the support sets and query sets are randomly selected MLSTM <ref type="bibr" target="#b22">[23]</ref> 43.44 ± 0.77 60.60 ± 0.71 MN <ref type="bibr" target="#b35">[36]</ref> 43.56 ± 0.84 55.31 ± 0.73 MA <ref type="bibr" target="#b8">[9]</ref> 48.70 ± 1.84 63.11 ± 0.92 PN <ref type="bibr" target="#b30">[31]</ref> 49.42 ± 0.78 68.20 ± 0.66 DLM <ref type="bibr" target="#b34">[35]</ref> 50.28 ± 0.80 63.70 ± 0.70 RN <ref type="bibr" target="#b30">[31]</ref> 50.44 ± 0.82 65.32 ± 0.70 MG <ref type="bibr" target="#b37">[38]</ref> 52.71 ± 0.64 68.63 ± 0.67 MMN <ref type="bibr" target="#b2">[3]</ref> 53.37 ± 0.48 66.97 ± 0.35</p><p>Ours 53.21 ± 0.40 72.34 ± 0.32 <ref type="table">Table 2</ref>. Comparative results for FSL on the miniImageNet dataset. The averaged accuracy (%) on 600 test episodes is given followed by the standard deviation (%).</p><p>from the whole training set. In a test episode, images in the support sets are randomly selected from the training set, while images in the query sets are randomly selected from the test set. The evaluation metric is defined as the classification accuracies on randomly selected 1000 test episodes.</p><p>The comparative results on the Omniglot dataset are provided in <ref type="table">Table 1</ref>. It can be observed that our approach has achieved a new state-of-the-art performance. This validates the effectiveness of our approach due to its unique global class representation learning strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Results on miniImageNet</head><p>Following previous works <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b30">31]</ref>, we conducted 5-way 1shot and 5-way 5-shot classification on the miniImageNet dataset. The 5-way 1-shot and 5-way 5-shot on the miniIm-ageNet dataset is similar to that on the Omniglot dataset, except three differences: 1) In 5-way 1-shot FSL, each training episode contains 30 classes; 2) In 5-way 5-shot FSL, each training episode contains 20 classes; 3) Each query set has 5 images per class in training and test episodes. The evaluation metric is defined as the classification accuracy on randomly selected 600 test episodes. <ref type="table">Table 2</ref> provides comparative results for FSL on the miniImageNet dataset. We can see that our approach significantly outperforms other FSL alternatives on 5-way 5-shot setting and achieves the joint best results under 5-way 1-shot setting. Our registration module yields 100% registration accuracy on the test data and the similarity scores are close to their onehot labels. This indicates that the registration module can accurately select the corresponding global representations for episodic class representations of support sets. That is, the soft manner of registering proposed in Section 3.3 has achieved the same performance as the 'argmax' operation on similarity score.  <ref type="table">Table 3</ref>. Comparative results (%) on the miniImageNet dataset under the generalized FSL setting. In this setting, test examples are from both the base and novel classes and each approach has to predict labels from the joint label space. Notations: acca -the accuracy of classifying the all test samples to all the classes (both base and novel). acc b -the accuracy of classifying the data samples from the base classes to all the classes. accun -the accuracy of classifying the data samples from the novel classes to all the classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Generalized Few-Shot Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Dataset and Settings</head><p>To further evaluate the effectiveness of our approach, we test our approach in a more challenging yet practical setting, i.e., generalized FSL, where the label space of test data is extended to both base and novel classes. We conduct experiments on 5-way 5-shot FSL on the miniIma-geNet dataset with a new data split. Concretely, we use the same class split as the original miniImageNet (i.e., training/validation/test: 64/16/20), with a new sample split: we randomly select 500 images of the total of 600 images per base class and a few samples per novel class to form a new training set. We select 100 images per base/novel class from the remaining data to form a new test set. The hyperparameter selection strategy is the same as that in standard FSL. Inspired by generalized ZSL <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b5">6]</ref>, we define three evaluation metrics for generalized FSL : 1) acc a -the accuracy of classifying the all test samples to all the classes. 2) acc b -the accuracy of classifying the data samples from the base classes to all the classes (both base and novel). 3) accu n -the accuracy of classifying the data samples from the novel classes to all the classes. Note that, test examples are from both the base and novel classes and each approach has to predict labels from the joint label space.</p><p>We compare our model with three recent approaches 1 : 1) PN <ref type="bibr" target="#b30">[31]</ref> which recognizes unlabeled images based on distances from each class mean in a learned embedding space. 2) MN <ref type="bibr" target="#b35">[36]</ref> which recognizes unlabeled data by a soft nearest neighbor mechanism with the outputs of a contextual embedding as references. The contextual embedding is trained with images from support sets and query sets to emphasize features that are relevant for the particular query class. 3) RN <ref type="bibr" target="#b31">[32]</ref>    <ref type="bibr" target="#b31">[32]</ref> and <ref type="bibr" target="#b35">[36]</ref>, respectively. <ref type="table">Table 3</ref> provides the comparative results of generalized FSL on the miniImageNet dataset. We can observe that: 1) Our approach achieves the best results on all evaluation metrics, with bigger margins than those under the standard setting. This shows that our model has the strongest generalization ability under this more challenging setting. 2) Our approach outperforms the PN and RN, because we learn global class representation for each class, while they estimate episodic class representations. 3) MN yields much lower results than our approaches. It is expected: context embedding encodes examples of all classes; with so many base class examples, they overwhelm those in the novel classes, making context embedding fail to emphasize novel class features. Our sample synthesis strategy increases intra-class variance and thus alleviates the data scarcity issue in novel classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Components Analysis</head><p>We compare our full model with a number of stripped-down versions to evaluate the effectiveness of the key components of our approach. Before introducing our methods for ablation study, we denote the key components of our approach as follows: 'B' -Averaging the visual features in the same class to obtain class representations; 'S1' -The first step of the method proposed in Section 3. The ablation study results in <ref type="figure" target="#fig_3">Figure 3</ref> show that: leveraging our sample synthesis strategy or registration module alone cannot well learn the global representation of classes (see 'B+S1' vs. 'B', 'B+S1+S2' vs. 'B' and 'B+R' vs. 'B'). However, when both of the two methods are used simultaneously to learn global class representations, performance has been significantly improved (see 'B+S1+R' vs. 'B' and 'B+S1+S2+R' vs. 'B'). It is expected because: 1) When applying sample synthesis strategy alone, we use an episodic representation as a global representation. Although the synthesis strategy can increase intra-class variance, the episodic representation loses the global class consistency, which limits performance improvement. 2) When applying the registration module alone, the severe class imbalance issue will limit performance improvement. 3) By integrating both of the two methods into the FSL framework, our approach can address the above two issues and the performance thus will significant improve. These results clearly illustrate the effectiveness of these key components in our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">New Novel Classes</head><p>Our method is flexible to adapt to new unseen novel classes: the model learns the global representations of new unseen novel classes only with its model parameters and the global representations of base and seen novel classes fixed. This means that our method has a low cost for adding new unseen classes. To validate this, we have conducted an additional experiment: we extend Mini-ImageNet (whole 100 classes) with another 20 ImageNet classes as the new unseen novel classes. Each new unseen novel class has 5 training samples and 100 test samples. We test our model under the generalized FSL setting described in Section 4.2, where the model has to predict labels from the joint label space of all 120 classes, which include 100 Mini-ImageNet classes and 20 new novel classes. <ref type="table" target="#tab_3">Table 4</ref> below shows that even when the 100 seen class global representations are fixed, our model is still able to beat Prototypical Net by a large margin. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We proposed to solve the challenging FSL problem by learning a global class representation using both base and novel class training samples. In each training episode, an episodic class mean computed from a support set is registered with the global representation via a registration module. This produces a registered global class representation for computing the classification loss using a query set. Our approach can be easily extended to the more challenging generalized FSL setting. Our approach is shown to be effective on both standard FSL and generalized FSL.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Overview of the whole framework. First, we propose a sample synthesis method to synthesize episodic representation for each class in the support set. Second, the registration module is leveraged to select global representation according to their episodic representation, and the selected global representations are then used to classify query images. The classification loss and registration loss are used to jointly optimize the global representations, the registration module, and the feature extractor. (Best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1</head><label>1</label><figDesc>Training episode loss computation. Input: Whole class set C total , base class set C base , novel class set C novel , training set D train , test set D test , feature extractor F , registration module R and global class representations G = {g cj , c j ∈ C total }. Output: The loss for a randomly generated training episode. 1. Randomly sample n train classes from C total to form C train ; 2. Randomly sample n s images per class in C train to form a support set S = {(x i , y i ), i = 1, .., n s × n train }; 3. Randomly sample n q images per class in C train to form a query set Q = {(x j , y j ), j = 1, .., n q × n train )};4. Compute visual features of images in S by using the feature extractor F , and obtain visual features {f i = F (x i ), i = 1, ..., n q × n train }; 5. Construct episodic representations {r ci , c i ∈ C train } by using the features within their own classes and the sample synthesis module. 6. Compute the similarity score vector V i = [v c1 i , ...., v c N i ] T between each episodic representation r ci and all global class representations G = {g cj , c j ∈ C total } according to Equation 4; 7. Compute the registration loss according to Equation 4; 8. Select corresponding global class representation {ξ i , i = 1, .., n train } by using ξ i = V i G; 9. Compute the classification loss of query images according to Equation 5; 10. Compute the total loss according to Equation 6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>which recognizes unlabeled images by using a Relation Network learned with training set to compute relation scores between query images and Ablation study for FSL on the miniImageNet dataset under the standard FSL setting. Different methods to obtain global class representations are denoted as follows: 'B' -Averaging the visual features in the same class to obtain the class representations; 'S1' -The first step of the method proposed in Section 3.2; 'S2' -The second step of the method proposed in Section 3.2; 'R'registration module proposed in Section 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>2; 'S2' -The second step of the method proposed in Section 3.2; 'R' -registration module proposed in Section 3.1. By combining the key components, we compare six FSL models, each of them uses the same FSL framework and differs only in how to learn global class representations: 1) 'B'-The global representations of both base/novel class are obtained by 'B'; 2) 'B+S1' -The global representations of base classes are obtained by 'B' ; while the global representations of novel classes are obtained by 'S1'; 3) 'B+S1+S2' -The global representations of base classes are obtained by 'B' ; while global representations of novel classes are obtained by 'S1' followed by 'S2'; 4) 'B+R' -The episodic representation of each base/novel class is obtained by 'B', and the global class representations of both base and novel classes are learned by 'R' with these episodic representations as inputs;. 5) 'B+S1+R' -The episodic representation of each base class is obtained by 'B', while the episodic representation of each novel class is obtained by 'S1'. The global class representations of both base and novel classes are learned by 'R' with these episodic representations as inputs; 6) 'B+S1+S2+R' -Our full model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>] 99.28±0.08 99.77±0.04 97.16±0.10 98.93±0.05 MG [38] 99.67±0.18 99.86±0.11 97.64±0.17 99.21±0.10 Ours 99.72±0.06 99.90±0.10 99.63±0.09 99.32±0.04</figDesc><table><row><cell>Model</cell><cell cols="2">5 way Acc. 1 shot 5 shot</cell><cell cols="2">20 way Acc. 1 shot 5 shot</cell></row><row><cell>MN [36]</cell><cell>97.9</cell><cell>98.7</cell><cell>93.5</cell><cell>98.7</cell></row><row><cell>APL [22]</cell><cell>97.9</cell><cell>99.9</cell><cell>97.2</cell><cell>97.6</cell></row><row><cell>DLM [35]</cell><cell>98.8</cell><cell>95.4</cell><cell>99.6</cell><cell>98.6</cell></row><row><cell>PN [31]</cell><cell>98.8</cell><cell>99.7</cell><cell>96.0</cell><cell>98.9</cell></row><row><cell>MA [9]</cell><cell cols="4">98.7±0.4 99.9±0.1 95.8±0.3 98.9±0.2</cell></row><row><cell>RN [32]</cell><cell cols="4">99.6±0.2 99.8±0.1 97.6±0.2 99.1±0.1</cell></row><row><cell cols="5">MMN [3Table 1. Comparative results for FSL on the Omniglot dataset. The</cell></row><row><cell cols="5">averaged accuracy (%) on 1,000 test episodes is given followed by</cell></row><row><cell cols="2">the standard deviation (%).</cell><cell></cell><cell></cell><cell></cell></row></table><note>FSL: simply including test data from both base and novel classes, and their labels are predicted from all N classes in C total in the test stage. This setting is much more challeng- ing and realistic than the standard FSL, where test data are from only novel classes. Note that, our registration module inherently is a classifier for generalized FSL setting. Our registration module not only optimizes novel class repre- sentations but updates base class representations as well. By comparing each test sample against global representa- tions of both base classes and novel classes, our registration module can directly predict the probability of the test im- ages belong to each class in C total .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>1.    class mean. These three methods can be easily extended to generalized FSL. Concretely, the model training is almost the same as standard FSL setting, except that new data split is used and few-shot samples from novel classes are included in the training set. During the test stage, we formulate the generalized FSL as a 100-way FSL problem, and all test data are classified into the joint space of both base and novel classes. In<ref type="bibr" target="#b30">[31]</ref>, we average features of all samples within a class as a class mean to recognize test data. Similarly, features of test samples and average features of all samples within each class are feed into the Relation Network and Context embedding in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Comparative results under generalized FSL setting on 100 Mini-ImageNet classes and 20 new novel classes. Notations: accua -the accuracy of classifying all test samples to all the classes (both 100 Mini-ImageNet classes and 20 new novel classes). accu b -the accuracy of classifying the data samples from the 100 Mini-ImageNet classes to all the classes. accunthe accuracy of classifying the data sampled from the 20 new novel classes to all the classes.</figDesc><table><row><cell>Model</cell><cell>accua</cell><cell>accu b</cell><cell>accun</cell></row><row><cell>PN [31]</cell><cell>26.80%</cell><cell>31.62%</cell><cell>1.07%</cell></row><row><cell>Ours</cell><cell>30.36%</cell><cell>40.20%</cell><cell>12.60%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The results of these three approaches are obtained by training the original code provided in their papers using our new split of miniImageNet.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable closed-form solvers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning feed-forward oneshot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joo</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="523" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Memory matching networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao Mei Chenggang</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Synthesized classifiers for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soravit</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5327" to="5336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Predicting visual exemplars of unseen classes for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soravit</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3476" to="3485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An empirical study and analysis of generalized zeroshot learning for object recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soravit</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="52" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Good semi-supervised learning that requires a bad gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6510" to="6520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Towards a neural statistician</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harrison</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><forename type="middle">J</forename><surname>Storkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Modelagnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dynamic few-shot visual learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4367" to="4375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to remember rare events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Nachum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semantic autoencoder for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elyor</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3174" to="3183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">One shot learning of simple visual concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brenden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Cognitive Science Society</title>
		<meeting>the Annual Meeting of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><forename type="middle">E</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donnie</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">D</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><forename type="middle">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Large-scale few-shot learning: Knowledge transfer with class hierarchy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aoxue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiange</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7212" to="7220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Meta networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2554" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<title level="m">Reptile: a scalable metalearning algorithm</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A unified approach for conventional zero-shot, generalized zero-shot and few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafin</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Salman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Porikli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.08653</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adaptive posterior learning: few-shot learning with a surprise-based memory module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Garnelo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Meta-learning for semi-supervised fewshot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">One-shot generalization in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1521" to="1529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An embarrassingly simple approach to zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardino</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2152" to="2161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Oriol Vinyals, Razvan Pascanu, Simon Osindero, and Raia Hadsell. Meta-learning with latent embedding optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Andrei A Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sygnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">One-shot learning with memory-augmented neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.06065</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Few-shot learning with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garcia</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><forename type="middle">Bruna</forename><surname>Satorras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Estrach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Swersky Kevin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Torr Tao Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1199" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke Sergey Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4278" to="4284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Few-shot learning through an information retrieval lens</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Low-shot learning from imaginary data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick1</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Metagan: An adversarial approach to few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
