<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">STAR-Net: Action Recognition using Spatio-Temporal Activation Reprojection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Mcnally</surname></persName>
							<email>wmcnally@uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Systems Design Engineering</orgName>
								<orgName type="institution">University of Waterloo Waterloo</orgName>
								<address>
									<postCode>N2L 3G1</postCode>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Wong</surname></persName>
							<email>a28wong@uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Systems Design Engineering</orgName>
								<orgName type="institution">University of Waterloo Waterloo</orgName>
								<address>
									<postCode>N2L 3G1</postCode>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Mcphee</surname></persName>
							<email>mcphee@uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Systems Design Engineering</orgName>
								<orgName type="institution">University of Waterloo Waterloo</orgName>
								<address>
									<postCode>N2L 3G1</postCode>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">STAR-Net: Action Recognition using Spatio-Temporal Activation Reprojection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>action recognition</term>
					<term>convolutional neural network</term>
					<term>spatio-temporal</term>
					<term>3D convolution</term>
					<term>human pose estimation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While depth cameras and inertial sensors have been frequently leveraged for human action recognition, these sensing modalities are impractical in many scenarios where cost or environmental constraints prohibit their use. As such, there has been recent interest on human action recognition using low-cost, readily-available RGB cameras via deep convolutional neural networks. However, many of the deep convolutional neural networks proposed for action recognition thus far have relied heavily on learning global appearance cues directly from imaging data, resulting in highly complex network architectures that are computationally expensive and difficult to train. Motivated to reduce network complexity and achieve higher performance, we introduce the concept of spatiotemporal activation reprojection (STAR). More specifically, we reproject the spatio-temporal activations generated by human pose estimation layers in space and time using a stack of 3D convolutions. Experimental results on UTD-MHAD and J-HMDB demonstrate that an end-to-end architecture based on the proposed STAR framework (which we nickname STAR-Net) is proficient in single-environment and small-scale applications. On UTD-MHAD, STAR-Net outperforms several methods using richer data modalities such as depth and inertial sensors.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Human action recognition has been a popular research focus for several decades due to its wide range of applications in intelligent video surveillance <ref type="bibr" target="#b0">[1]</ref>, sports analytics <ref type="bibr" target="#b1">[2]</ref>, and human-computer interaction <ref type="bibr" target="#b2">[3]</ref>. With recent innovations in sensor technology, new data modalities such as 3D skeletal coordinates obtained from depth cameras, and wearable inertial sensors, have been explored for the purpose of enhancing action recognition performance <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. However, the practical limitations of these sensors render many applications infeasible. Depth cameras are severely limited by their working range, often fail in outdoor scenes due to sunlight interference <ref type="bibr" target="#b6">[7]</ref>, and are not as widely available or economically viable as RGB cameras. Wearable inertial sensors must be worn by the subject, prohibiting "in-the-wild" applications like sports analytics and intelligent surveillance. For these reasons, performing action recognition strictly using RGB images remains highly desirable.</p><p>Recently, the ubiquity of RGB video data from online  sources has fostered large-scale action recognition datasets that have spurred the use of deep learning and eliminated the need for engineered features. With the success of deep convolutional neural networks (CNNs) for visual recognition tasks <ref type="bibr" target="#b7">[8]</ref>, RGB-based action recognition has shifted towards using deep CNNs to data-mine spatio-temporal features in image sequences. Incorporating temporal information into CNNs has been accomplished using 3D convolutions <ref type="bibr" target="#b0">[1]</ref>, recurrent networks <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, or by fusing spatial and temporal features from multiple streams (i.e., RGB and optical flow) <ref type="bibr" target="#b10">[11]</ref>. Still, these action recognition models have limitations. First, they rely heavily on global appearance cues and could potentially underperform in situations where multiple unique actions exist within a single environment (e.g., in a sports match). Second, they are highly complex and contain a large number of parameters. As a result, training requires large amounts of data and is computationally taxing.</p><p>In a parallel line of computer vision research, CNNs have been used extensively to infer 2D human pose from RGB images <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. Although these two streams of research share many similarities, utilizing the spatial activations generated within human pose estimation networks for action recognition remains largely unexplored.</p><p>Motivated by this, we introduce the concept of spatiotemporal activation reprojection (STAR) for action recognition. More specifically, spatio-temporal activations generated by a stack of pose estimation layers are reprojected in space and time using a stack of 3D convolutions that have been learned directly from the data (see <ref type="figure" target="#fig_1">Fig. 1</ref>). By leveraging spatio-temporal activations that are linked to human pose, the STAR framework is not influenced by global appearance cues, making it better suited for single-environment applications where variation in human movement is critical. Moreover, initializing the network with pretrained pose estimation layers shortcuts a large portion of complex spatial learning, permitting a network built around the STAR framework to be trained quickly and with limited data. We empirically demonstrate that superior performance under said conditions can be achieved using an end-to-end network architecture based on the STAR framework (which we call STAR-Net) through evaluation on UTD-MHAD <ref type="bibr" target="#b14">[15]</ref> and J-HMDB <ref type="bibr" target="#b15">[16]</ref>, two small-scale action recognition datasets. On UTD-MHAD, STAR-Net outperforms several methods using richer data modalities, including methods leveraging depth images, inertial sensor data, and 3D skeletal coordinates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK A. Action Recognition using RGB Images</head><p>Before the prevalence of CNNs for computer vision tasks, action recognition was performed by extracting features from RGB image sequences using traditional image processing techniques. Blank et al. <ref type="bibr" target="#b16">[17]</ref> regarded human actions as 3D shapes by extracting silhouettes from each frame and forming space-time volumes. Sch√ºldt et al. <ref type="bibr" target="#b17">[18]</ref> integrated local space-time features with support vector machines. Many generic image descriptors have been extended to video for the purpose of action recognition, including 3D-SIFT <ref type="bibr" target="#b18">[19]</ref>, HOG3D <ref type="bibr" target="#b19">[20]</ref>, extended SURF <ref type="bibr" target="#b20">[21]</ref>, and Local Trinary Patterns <ref type="bibr" target="#b21">[22]</ref>. Optical flow techniques employing dense feature trajectories have also demonstrated success for action recognition applications <ref type="bibr" target="#b22">[23]</ref>.</p><p>Recently, CNNs have been shown to be extremely proficient at image recognition <ref type="bibr" target="#b7">[8]</ref>. Successful CNN architectures have been adapted to accommodate video using 3D convolutions <ref type="bibr" target="#b0">[1]</ref>, long-short term memory (LSTM) <ref type="bibr" target="#b8">[9]</ref>, and two-stream approaches using RGB and optical flow <ref type="bibr" target="#b10">[11]</ref>.</p><p>Three-dimensional CNNs (3D CNNs) extract features from both the spatial and temporal dimensions by convolving 3D filters across temporally-stacked image sequences, thereby capturing motion information across adjacent frames. Compared to LSTM and two-stream models, 3D CNNs are attractive because they directly create hierarchical representations of spatio-temporal data. Ji et al. <ref type="bibr" target="#b0">[1]</ref> used 3D CNNs for intelligent surveillance and achieved superior performance in comparison to baseline methods. Tran et al. <ref type="bibr" target="#b23">[24]</ref> used 3D CNNs for large-scale video classification and found that small 3x3x3 filters were most effective. <ref type="bibr" target="#b24">[25]</ref> demonstrated how 2D CNNs could be "inflated" to 3D, making it possible to extract spatio-temporal features from video while leveraging proven 2D CNN architectures and even their parameters. However, 3D CNN architectures cannot be warm-started on ImageNet. Moreover, 3D CNNs contain many more parameters than their 2D counterpart due to the extra filter dimension. For these reasons, monolithic 3D CNNs are prone to overfitting and require enormous amounts of video data to achieve good results <ref type="bibr" target="#b24">[25]</ref>. Training difficulties are mitigated in this work using a shallow 3D CNN that takes refined spatio-temporal information as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Carreira and Zisserman</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Action Recognition using Pose Data</head><p>Three-dimensional skeletal coordinates obtained from depth cameras have frequently been leveraged to perform action recognition using both hand-crafted and machinelearned features. Examples of hand-crafted features include the relative positioning between joints <ref type="bibr" target="#b25">[26]</ref>, covariance matrices of joint trajectories <ref type="bibr" target="#b26">[27]</ref>, and view-invariant histograms <ref type="bibr" target="#b27">[28]</ref>. With the increasing popularity of deep learning approaches, recurrent neural networks employing LSTM blocks have been used extensively to encode temporal sequences of 3D skeletal data <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. In other works, CNNs were used to recognize joint trajectory maps <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>. The major drawback of these methods is that they rely on 3D skeletal coordinates obtained from the Microsoft Kinect, rendering them impractical in many scenarios.</p><p>There are but a few very recent works that perform human action recognition using pose information extracted from RGB images. Yan et al. <ref type="bibr" target="#b30">[31]</ref> used a 2D human pose estimation model <ref type="bibr" target="#b12">[13]</ref> as a standalone toolbox to extract joint coordinates and use them as input to a graph convolutional network. Liu et al. <ref type="bibr" target="#b31">[32]</ref> used a standalone 2D human pose estimation model to recognize actions as the evolution of pose estimation maps and achieved stateof-the-art results on UTD-MHAD. These methods required that the "natural" connectivity between joints was chosen a priori. In the proposed method, biomechanical connectivity is not strictly encoded, enabling the exploration of complex spatial relationships between joints, and similarly, temporal relationships between frames. Furthermore, a more seamless integration of pose information is desirable to enable endto-end training for the task for action recognition.</p><p>To this end, Choutas et al. <ref type="bibr" target="#b32">[33]</ref> integrated pose information more coherently using the confidence maps generated by a pose estimation network as the input to a 2D CNN classifier. Pose data returned from such networks is generally 4D, having the shape (frame, height, width, keypoint). Thus, Choutas et al. manually reduced the dimensionality of the pose data by summing the individual keypoint activations temporally and encoding with color, such that the resulting clip-level representation could be processed using a 2D CNN. McNally et al. <ref type="bibr" target="#b33">[34]</ref> explored a different clip-level encoding technique that involved summing the keypoints spatially and compressing the temporal dimension. Although these architectures integrated pose information in a more seamless manner, end-to-end training was never explored in these works. Finally, Luvizon et al. <ref type="bibr" target="#b34">[35]</ref> demonstrated that the tasks of human action recognition and pose estimation could be jointly-learned in an end-to-end multitask framework.</p><p>Most similar to our model are those of Choutas et al. and McNally et al. The proposed STAR framework differs from these approaches in three key aspects encompassing the main contributions of this work:</p><p>‚Ä¢ We introduce the idea of leveraging 3D convolutions to reproject spatio-temporal activations generated by pose estimation layers within a network in a direct manner. This reprojection strategy avoids potential losses of information associated with a manual dimensionality reduction. ‚Ä¢ The STAR framework integrates a stack of pose estimation layers within a seamless network architecture, which enables end-to-end training. We further perform pose estimation layer ablation experiments in this work. ‚Ä¢ We use top-down pose estimation and a sliding window, which permits simultaneous localization and detection of multiple actions from multiple subjects in a single video clip.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>In this section we detail the proposed STAR framework. The components of the framework are described as follows. Using a sliding window approach, a person detector is used to locate the subject. The detections are cropped, padded, and resized in accordance with the input size of 256√ó192. Within an end-to-end network architecture (STAR-Net), a sequence of RGB input images are fed through a stack of pose estimation layers, resulting in a 4D set of spatiotemporal activations. The spatio-temporal activations are then reprojected via a stack of inflated inception convolutional layers <ref type="bibr" target="#b24">[25]</ref>. Finally, predictions of human actions are obtained via a final stack consisting of an average pooling layer, a point-wise convolutional layer, and a softmax layer. The STAR-Net architecture is illustrated in <ref type="figure" target="#fig_2">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Person Detection</head><p>As with all top-down pose estimation models, a person detector is generally required. It was found that a simple histogram of oriented gradients (HOG) person detector missed many detections on the J-HMDB dataset. With concerns that missed detections may be detrimental to recognition performance, a more robust deep learning-based detector was chosen, namely the MobileNetV2 Single-shot Multibox Detector <ref type="bibr" target="#b35">[36]</ref> (SSD). Using the bounding boxes returned by the SSD, the images were cropped, padded to an aspect ratio of 4:3, and then resized in accordance with the 256√ó192 input size of the first pose estimation layer using bilinear interpolation. Even using the SSD, detections were occasionally missed. In the case of a missed detection, the bounding box from the previous frame was used.</p><p>Compared to the bottom-up clip-level representation used by Choutas et al. <ref type="bibr" target="#b32">[33]</ref>, our top-down sliding window approach enables the simultaneous detection and localization of multiple actions in a single video sample. This is an important feature for many applications, particularly for sports, where it is beneficial to identify when and where actions occur.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Pose Estimation Layers</head><p>The pose estimation layers used in the STAR-Net architecture are derived from the Cascaded Pyramid Network <ref type="bibr" target="#b13">[14]</ref> (CPN) architecture, which placed first in the 2017 and 2018 Microsoft COCO Keypoints Challenge. We chose the CPN 1 more for its efficient network architecture than its high precision. In fact, in Section IV-B we empirically show that high-precision keypoint localization is not a prerequisite for accurate action classification.</p><p>Prior to the release of the CPN, hourglass networks were prevalent for human pose estimation <ref type="bibr" target="#b11">[12]</ref>. The principle of the hourglass architecture lies in repeated bottom-up, top-down processing to consolidate features across multiple scales and encode the local and global context required for the spatial relationships of the human body. These hourglass modules were then stacked with intermediate supervision to improve performance. However, there are computational inefficiencies associated with hourglass stacking as performance gains drop after two stages, leading to wasteful computations in subsequent stages <ref type="bibr" target="#b13">[14]</ref>. The CPN was designed to mitigate these inefficiencies using a feature pyramid network <ref type="bibr" target="#b36">[37]</ref> with a ResNet-50 <ref type="bibr" target="#b37">[38]</ref> backbone. The feature pyramid, which the authors refer to as GlobalNet, makes liberal use of 1√ó1 convolutions and intermediate supervision. The CPN also uses an adjacent network called RefineNet to refine keypoint predictions. The RefineNet efficiently combines features across multiple scales and as a result, the CPN outperforms an 8-stage hourglass network at less than a third of the computational cost <ref type="bibr" target="#b13">[14]</ref>. In this paper, we refer to the output feature maps of the feature pyramid as C 2 through C 5 (see <ref type="figure" target="#fig_2">Fig. 2</ref>).</p><p>In Section IV-B, a pose layer ablation study is performed to assess the trade-off between action recognition performance and the use of activations from various depths of the CPN. The computational efficiency of the action recognition model can be improved if the RefineNet or feature pyramid blocks can be removed without sacrificing classification accuracy. pose estimation layers input images <ref type="bibr" target="#b31">(32,</ref><ref type="bibr">256,</ref><ref type="bibr">192,</ref><ref type="bibr" target="#b2">3)</ref> .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Spatio-Temporal Activation Reprojection Layers</head><p>The set of 4D spatio-temporal activations (frame, height, width, keypoint) are reprojected in space and time via a stack of 3D convolutional layers. In essence, the 4D spatiotemporal activations capture the pixel-wise confidence values for the presence of keypoints over time and space. The pose estimation layers were trained to detect the eyes, ears, nose, shoulders, elbows, wrists, hips, knees and ankles (i.e., 17 keypoints in total). The 3D spatio-temporal activation reprojection subarchitecture shown in <ref type="figure" target="#fig_2">Fig. 2</ref> begins with a 7√ó3√ó3 convolution with a stride of 2, where the filter dimensions are in the format frame√óheight√ówidth. Following the initial convolution is a series of inflated inception modules <ref type="bibr" target="#b24">[25]</ref> and max pooling layers.</p><p>The inflated inception modules used in the 3D reprojection subarchitecture were based on those introduced in I3D <ref type="bibr" target="#b24">[25]</ref> and follow a similar structure as the modules in the original Inception network <ref type="bibr" target="#b38">[39]</ref>, except with filters inflated to three dimensions. One of the principles behind the inception module is to judiciously reduce dimensionality wherever the computational requirements would increase too much otherwise <ref type="bibr" target="#b38">[39]</ref>. In 2D inception modules, this is accomplished using 1√ó1 convolutions, whereas in the 3D inception modules it is accomplished using 1√ó1√ó1 convolutions. Inception modules reduce the total number of parameters and permit deeper networks to be trained more effectively with less data. Therefore, inception modules help achieve better performance on the small-scale datasets evaluated in Section IV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Action Recognition Prediction Layers</head><p>Following the 3D spatio-temporal activation reprojection layers, an average pooling layer with a window of 2√ó8√ó6 and stride of 1 is applied, which collapses the spatial dimension. A final 1√ó1√ó1 convolution is applied with the output channels equal to the number of action classes. The final size of the temporal dimension will vary based on the number of input frames. Using a 32-frame input results in a temporal feature size of 3. The temporal features are averaged to reach a final prediction, which provides flexibility for the temporal breadth of the input. This characteristic is critical during testing, when long-duration video samples with varying number of frames must be classified using a single label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Implementation Details</head><p>Each convolutional layer is followed by batch normalization <ref type="bibr" target="#b39">[40]</ref> and the ReLU non-linearity. The pose estimation layers were initialized with weights pretrained on MSCOCO <ref type="bibr" target="#b40">[41]</ref>, and the weights of the reprojection and prediction layers were initialized using Gaussian noise. During training, dropout <ref type="bibr" target="#b41">[42]</ref> was applied at a rate of 50% following the average pooling layer. Because the pose estimation subarchitecture accepts batches of frames rather than batches of videos, the spatio-temporal activations were generated prior to training to facilitate batch training of the reprojection layers. We maximize the computational resources of a single NVIDIA Titan Xp GPU using a window size of 32 frames and a batch size of 32. As in <ref type="bibr" target="#b24">[25]</ref>, the start frame was randomly selected during training and, if the number of frames in the video sample was less than 32, the video was looped as many times as necessary. Randomly selecting the start frame served as a form of data augmentation. Other forms of data augmentation included random rotations and horizontal flipping. When flipping the spatial activations, the left and right indices were also switched. For a fair comparison with the similar method of Choutas et al. <ref type="bibr" target="#b32">[33]</ref>, only horizontal flipping was used to produce the results on J-HMDB. During testing, predictions are made on the fulllength video samples.</p><p>For training, we use the Adam <ref type="bibr" target="#b42">[43]</ref> optimizer and a constant learning rate of 0.001. On a small dataset like UTD-MHAD, the reprojection and prediction layers of STAR-Net can be trained effectively from scratch in just 1000 iterations, and training takes just a few minutes. This is in stark contrast to most state-of-the-art approaches that often require extensive pretraining and large datasets, which can take days of training on multiple GPUs. For example, I3D was trained on Kinetics-400 using 32 GPUs <ref type="bibr" target="#b24">[25]</ref>, although the GPU hours were not reported.</p><formula xml:id="formula_0">input image C 2</formula><p>RefineNet C 3 C 4 C 5 <ref type="figure">Figure 3</ref>. Activations at various depths of the CPN architecture, including the outputs of the feature pyramid blocks C 2 to C 5 , and the final RefineNet output. For display purposes, the eyes and ears were excluded and the activations were summed along the keypoint axis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section we demonstrate the efficacy of STAR-Net for small-scale and single-environment applications through evaluation on UTD-MHAD <ref type="bibr" target="#b14">[15]</ref> and J-HMDB <ref type="bibr" target="#b15">[16]</ref>, two small-scale action recognition datasets containing 27 and 21 action classes, respectively. We compare the results of STAR-Net with other state-of-the-art methods using RGB and other data modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>The action classes in UTD-MHAD comprise high-level body segment movements with no scene interaction (e.g., wave, squat, lunge, etc); thus UTD-MHAD is a suitable dataset for pose-based action recognition. The actions were performed by 8 subjects, four male and four female, with each subject performing an action four times. After removing three corrupted samples, the dataset includes a total of 861 action samples. We evaluate according to the frequentlyused cross-subject protocol, where subjects 1, 3, 5, 7 are used for training and subjects 2, 4, 6, 8 are used for testing.</p><p>J-HMDB is a 21-class subset of the larger HMDB <ref type="bibr" target="#b43">[44]</ref> dataset. Its action classes consist primarily of body segment movements (e.g., golf swing, baseball swing, pull-up, etc.) where, for the most part, the full body is visible. For these reasons, J-HMDB is a suitable dataset for pose-based action recognition. J-HMDB contains 928 samples and uses three train/test splits. The results reported on J-HMDB are averaged over the three splits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. STAR-Net Results</head><p>In this section, we evaluate STAR-Net on the aforementioned datasets and investigate the effects of pose layer ablation and end-to-end training. Pose Layer Ablation. For the pose layer ablation, multiple STAR-Net classifiers were trained using pre-generated activations from various levels of the CPN architecture. Specifically, the classifiers were trained using activations from the feature pyramid blocks C 2 through C 5 , and the final output of the RefineNet. The deeper activations with smaller spatial resolutions were upsampled to 64√ó48. <ref type="figure">Fig. 3</ref> illustrates the appearance of the activations at different depths of the CPN architecture. Interestingly, the C 2 and C 3 activations are qualitatively similar to the final output activations of the RefineNet. Motivated by this visual observation, we hypothesize that activations at shallower depths may be leveraged within STAR-Net to enable greater computational efficiency without sacrificing classification accuracy.</p><p>The performances of STAR-Net variants trained on activations from various depths of the CPN are reported in <ref type="table" target="#tab_1">Table I</ref>. On UTD-MHAD, the performance using the RefineNet activations was matched using activations from the feature pyramid blocks C 2 and C 3 . Furthermore, classification accuracy decreased less than 1% using the relatively high-level pose information encoded in C 4 and C 5 . This result suggests that high-precision pose estimation is not essential within the STAR framework for the purpose of action recognition. This is in contrast to other pose-based approaches that saw significant performance improvements using ground-truth keypoint locations <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>. The inference speeds of each end-to-end architecture are also reported in <ref type="table" target="#tab_1">Table I</ref>. As expected, removing pose estimation layers significantly improves computational efficiency. By removing the RefineNet, STAR-Net becomes 28% faster (STAR-Net-C 2 versus STAR-Net-RN). By removing feature pyramid blocks, STAR-Net becomes 42% faster with minimal loss in performance (STAR-Net-C 5 versus STAR-Net-RN).</p><p>End-to-end Training. Considering the significant improvement in forward propagation time of STAR-Net-C 2 compared to STAR-Net-RN, and the apparent negligible loss in classification accuracy, we chose to investigate the finetuning of STAR-Net-C 2 in an end-to-end training manner on UTD-MHAD. To accommodate batch training of the 3D reprojection layers, multiple 32-frame video samples were concatenated along the batch dimension and used as input to the pose estimation layers. Upon reaching the 3D reprojection, the spatio-temporal activations were reshaped to 5D, effectively separating the video samples. Due to limited GPU memory, it was only possible to batch two 32-frame video samples at a time. For this fine-tuning, the input images were additionally augmented using random horizontal and vertical scaling in the range of 0.75 to 1.25 with a probability of 50%. The intent of the scaling was to account for inter-subject variation in body types. Random scaling was not practical when the spatio-temporal activations were pre-generated, as it would have altered the spatial footprint of the activations, which should remain the   same regardless of the scale of the input image.</p><p>The fine-tuned STAR-Net-C 2 , which we will refer to as simply STAR-Net moving forward, managed a marginal improvement in classification accuracy on UTD-MHAD, reaching 90.0%. It is suspected that greater improvements may be achievable with additional GPU memory resources to accommodate a larger video sample batch size. Still, a particular phenomenon worth discussing is the change in the spatial activations generated by the pose estimation layers of STAR-Net following end-to-end training for action recognition. <ref type="figure" target="#fig_4">Fig. 4</ref> illustrates that the spatial activations of keypoints that contributed most to the action, such as motions of the arms and feet were greatly accentuated in the fine-tuned STAR-Net. Conversely, the non-moving keypoints such as the knees and hips were attenuated. This interesting phenomenon can potentially be leveraged to gain new insights into discovering what the key defining characteristics are for particular actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison with the State-of-the-art</head><p>UTD-MHAD was chosen with the intent of assessing the ability of STAR-Net to be trained with limited data and recognize actions effectively in single-environments. In single-environment applications, there is minimal variance in the overall appearance of the videos, and therefore an action classifier cannot rely on global appearance cues to make predictions. State-of-the-art RGB-based action recognition models such as I3D <ref type="bibr" target="#b24">[25]</ref> are typically evaluated on largescale datasets like UCF-101 <ref type="bibr" target="#b46">[47]</ref> and Kinetics-400 <ref type="bibr" target="#b47">[48]</ref>. Generally speaking, the action classes in these datasets have unique environments (e.g., sky diving, soccer, skiing). Hence, we suspect that models trained on these datasets benefit greatly from global appearance cues. As such, the effectiveness of action recognition models trained on largescale datasets for single-environment applications remains an open research question. To this end, we elected to fine-tune I3D <ref type="bibr" target="#b24">[25]</ref> on UTD-MHAD. The same detections were used, except we cropped and resized the detections to 224√ó224 in accordance with the input size of I3D. The fine-tuned I3D results on UTD-MHAD are reported in <ref type="table" target="#tab_1">Table II</ref> along with other published methods. STAR-Net outperforms the fine-tuned I3D model by 12.8%. The significant performance gap between the RGB and optical flow streams of I3D is an indication that global appearance is less effective in the single environment. On UCF-101 and Kinetics-400, the performance gap between I3D's two streams is much less (1.1% and 2.3%, respectively, in favour of optical flow) <ref type="bibr" target="#b24">[25]</ref>. This insight supports the hypothesis that RGB action recognition networks developed for largescale datasets rely heavily on global appearance cues, thus causing them to underperform in single-environments. However, the authors acknowledge that the network capacity of I3D is likely too great for UTD-MHAD, and that overfitting may have been a factor contributing to the poor results.</p><p>In <ref type="bibr" target="#b32">[33]</ref>, it was demonstrated that pose-based features were complementary to those produced by I3D. To this end, the bottom rows of <ref type="table" target="#tab_1">Table II</ref> show the result of combining the STAR-Net predictions with the two streams of I3D using equal weights. On UTD-MHAD, the effect of combining the models decreased performance. The results in <ref type="table" target="#tab_1">Table II</ref> also show that STAR-Net outperforms several methods using richer data modalities, but surrenders 2.8% to the state-of-the-art pose-based method of Liu et al. <ref type="bibr" target="#b31">[32]</ref>, who used spatial rank pooling to encode the evolution of 2D  <ref type="bibr" target="#b32">[33]</ref> 57.0 Chron et al. (P-CNN) <ref type="bibr" target="#b44">[45]</ref> 61.1 Gkioxari et al. (Action Tubes) <ref type="bibr" target="#b48">[49]</ref> 62.5 Peng et al. (MR TS R-CNN) <ref type="bibr" target="#b9">[10]</ref> 71.1 Zolfaghari et al. (Chained) <ref type="bibr" target="#b45">[46]</ref> 76.1 STAR-Net 64.3 pose images and averaged pose heatmaps (i.e., as separate streams). Interestingly, the performance of each stream alone was 85.6% and 74.9%, respectively, indicating that these streams were highly complementary. Provided that STAR-Net was able to achieve 89.8% using only pose heatmaps (before fine-tuning), it is possible that incorporating 2D pose images as a separate stream could be advantageous. This is a proposed area for future research.</p><p>In <ref type="table" target="#tab_1">Table III</ref>, STAR-Net is compared to state-of-the-art methods on J-HMDB. The method of Choutas et al. (Po-Tion) <ref type="bibr" target="#b32">[33]</ref> is the most similar to STAR-Net, in that is uses a manual dimensionality reduction and 2D convolutions in place of 3D convolutions. Notably, the use of 3D convolutions has lead to significant performance gains over the former. STAR-Net also outperforms popular action recognition models such as P-CNN <ref type="bibr" target="#b44">[45]</ref> and Action Tubes <ref type="bibr" target="#b48">[49]</ref>, but falls short to Peng et al. <ref type="bibr" target="#b9">[10]</ref> and the state-of-the-art method of Zolfaghari et al. <ref type="bibr" target="#b45">[46]</ref>, who used a Markov chain model to sequentially refine predictions from three 3D CNN streams encoding pose, optical flow, and RGB. It is worth noting that in the model of Zolfaghari et al., the pose stream on its own yields a classification accuracy of 45.5%, which is much lower than STAR-Net (64.3%). This suggests that the integration of optical flow and RGB streams via the Markov chain was highly effective. STAR-Net may benefit from a similar framework, and thus incorporating optical flow and RGB streams via a Markov chain is another targeted area for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>This paper introduces the concept of spatio-temporal activation reprojection for human action recognition. The STAR framework seamlessly integrates human pose estimation with 3D convolutional reprojection in a coherent end-to-end architecture to efficiently detect actions in video. Empirical results on multiple action recognition datasets demonstrated that precise pose estimation is not essential within the STAR framework. This facilitated the compression of the pose estimation subarchitecture, leading to improved inference speed. Through evaluation on a multimodal action recognition dataset, it was shown that our approach is superior to many action recognition methods utilizing multiple richer data modalities, including depth and inertial sensors. Using a sliding window approach with top-down pose estimation, our method permits the simultaneous detection and localization of actions in continuous video streams.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>The STAR framework. Input frames are extracted from a video sample using a sliding window. Spatio-temporal activations representing the motions of joints (left wrist shown in the figure) are generated using a stack of pose estimation layers. The activations are then reprojected in space and time using a stack of 3D convolutions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>STAR-Net. Within an end-to-end network architecture, a sequence of RGB input images are fed through a stack of pose estimation layers, resulting in a 4D set of spatio-temporal activations. The spatio-temporal activations are then reprojected via a stack of inflated inception<ref type="bibr" target="#b24">[25]</ref> convolutional layers. Finally, predictions of human action are obtained via a final stack consisting of an average pooling layer, a point-wise convolutional layer, and a softmax layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Spatial activations before and after end-to-end fine-tuning of STAR-Net for action recognition. A very interesting observation that can be made is that the keypoints contributing most to the action, such as motions of the arms and feet, are greatly accentuated in the spatial activations of the fine-tuned STAR-Net.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>..</figDesc><table><row><cell cols="2">spatio-temporal activations</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">(32, 64, 48, 17)</cell><cell></cell><cell></cell><cell cols="2">3D reprojection</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>7x3x3 conv stride 2</cell><cell>Inc.</cell><cell>3x3x3 max pool stride 2</cell><cell>Inc.</cell><cell>3x3x3 max pool stride 2</cell><cell>Inc.</cell><cell>2x8x6 avg pool stride 1</cell><cell>1x1x1 conv stride 1</cell><cell>softmax</cell></row><row><cell>first frame</cell><cell>frame final</cell><cell>keypoints</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table I</head><label>I</label><figDesc></figDesc><table><row><cell cols="4">CLASSIFICATION ACCURACY OF STAR-NET USING ACTIVATIONS FROM</cell></row><row><cell cols="4">DIFFERENT POSE ESTIMATION DEPTHS.</cell></row><row><cell>Method</cell><cell cols="2">UTD-MHAD J-HMDB</cell><cell>ms/sample  *</cell></row><row><cell>STAR-Net-RN</cell><cell>88.8</cell><cell>64.3</cell><cell>84.0</cell></row><row><cell>STAR-Net-C 2</cell><cell>88.8</cell><cell>64.0</cell><cell>60.4</cell></row><row><cell>STAR-Net-C 3</cell><cell>88.8</cell><cell>61.5</cell><cell>52.9</cell></row><row><cell>STAR-Net-C 4</cell><cell>87.4</cell><cell>61.6</cell><cell>49.7</cell></row><row><cell>STAR-Net-C 5</cell><cell>87.9</cell><cell>61.6</cell><cell>48.9</cell></row><row><cell>STAR-Net  ‚Ä†</cell><cell>90.0</cell><cell>-</cell><cell>60.4</cell></row><row><cell cols="4">*  Forward propagation of 32-frame video sample, excluding detection,</cell></row><row><cell cols="4">I/O, and preprocessing. Averaged over 1000 trials using videos from</cell></row><row><cell cols="4">UTD-MHAD.  ‚Ä† End-to-end fine-tuned STAR-Net using the STAR-Net-C 2</cell></row><row><cell>architecture.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table II RESULTS</head><label>II</label><figDesc>ON UTD-MHAD CROSS-SUBJECT EXPERIMENT</figDesc><table><row><cell>Method</cell><cell>Accuracy (%)</cell></row><row><cell>I3D (RGB)</cell><cell>61.9</cell></row><row><cell>I3D (Flow)</cell><cell>71.9</cell></row><row><cell>I3D (RGB+Flow)</cell><cell>77.2</cell></row><row><cell>McNally et al. [34] (RGB)</cell><cell>76.1</cell></row><row><cell>Chen et al. [4] (Depth+Inertial)</cell><cell>79.1</cell></row><row><cell>Hussein et al. [27] (3D Skeletal)</cell><cell>85.6</cell></row><row><cell>Wang et al. [29] (3D Skeletal)</cell><cell>85.8</cell></row><row><cell>Hou et al. [30] (3D Skeletal)</cell><cell>87.0</cell></row><row><cell>Liu et al. [32] (RGB)</cell><cell>92.8</cell></row><row><cell>STAR-Net</cell><cell>90.0</cell></row><row><cell>STAR-Net+I3D RGB</cell><cell>83.7</cell></row><row><cell>STAR-Net+I3D Flow</cell><cell>88.1</cell></row><row><cell>STAR-Net+I3D RGB+Flow</cell><cell>88.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table III RESULTS</head><label>III</label><figDesc>ON J-HMDB (AVERAGED OVER 3 SPLITS)</figDesc><table><row><cell>Method</cell><cell>Accuracy (%)</cell></row><row><cell>Choutas et al. (PoTion)</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">TensorFlow CPN implementation available at https://github.com/ chenyilun95/tf-cpn</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>We would like to acknowledge financial support from the Canada Research Chairs program and the Natural Sciences and Engineering Research Council of Canada (NSERC). We would also like to acknowledge the NVIDIA GPU Grant Program for the Titan Xp GPU donation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hockey action recognition via integrated stacked hourglass network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Neher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Clausi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zelek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Gesture recognition: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Acharya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fusion of depth, skeleton, and inertial data for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kehtarnavaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Spatio-temporal lstm with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Vnect: Realtime 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-region two-stream r-cnn for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Realtime multiperson 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Utd-mhad: A multimodal dataset for human action recognition utilizing a depth camera and a wearable inertial sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kehtarnavaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Towards understanding action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Actions as space-time shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gorelick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recognizing human actions: a local svm approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schuldt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A 3-dimensional sift descriptor and its application to action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Scovanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM international conference on Multimedia</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A spatio-temporal descriptor based on 3d-gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klaser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marsza≈Çek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An efficient dense and scale-invariant spatio-temporal interest point detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Willems</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Local trinary patterns for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yeffet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mining actionlet ensemble for action recognition with depth cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Human action recognition using a temporal hierarchy of covariance descriptors on 3d joint locations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Torki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Gowayyed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>El-Saban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">View invariant human action recognition using histograms of 3d joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Action recognition based on joint trajectory maps using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM on Multimedia Conference</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Skeleton optical spectra-based action recognition using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Recognizing human actions as the evolution of pose estimation maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Potion: Pose motion representation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Action recognition using deep convolutional neural networks and compressed spatio-temporal pose encodings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Mcnally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcphee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">2d/3d pose estimation and action recognition using multitask deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tabia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Mobilenetv2: Inverted residuals and linear bottlenecks,&quot; in CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll√°r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll√°r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Hmdb: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">P-CNN: Pose-based CNN Features for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ch√©ron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Chained multi-stream networks exploiting pose, motion, and appearance for action classification and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sedaghat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Finding action tubes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
