<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Automatic Stance Detection Using End-to-End Memory Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitra</forename><surname>Mohtarami</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">MIT Computer Science and Artificial Intelligence Laboratory</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramy</forename><surname>Baly</surname></persName>
							<email>baly@csail.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">MIT Computer Science and Artificial Intelligence Laboratory</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
							<email>glass@csail.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">MIT Computer Science and Artificial Intelligence Laboratory</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
							<email>pnakov@hbku.edu.qa</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Qatar Computing Research Institute</orgName>
								<orgName type="institution" key="instit2">HBKU</orgName>
								<address>
									<settlement>Doha</settlement>
									<country key="QA">Qatar</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
						</author>
						<title level="a" type="main">Automatic Stance Detection Using End-to-End Memory Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a novel end-to-end memory network for stance detection, which jointly (i) predicts whether a document agrees, disagrees, discusses or is unrelated with respect to a given target claim, and also (ii) extracts snippets of evidence for that prediction. The network operates at the paragraph level and integrates convolutional and recurrent neural networks, as well as a similarity matrix as part of the overall architecture. The experimental evaluation on the Fake News Challenge dataset shows state-of-the-art performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, an unprecedented amount of false information has been flooding the Internet with aims ranging from affecting individual people's beliefs and decisions <ref type="bibr">(Mihaylov et al., 2015a,b;</ref><ref type="bibr" target="#b13">Mihaylov and Nakov, 2016)</ref> to influencing major events such as political elections <ref type="bibr" target="#b26">(Vosoughi et al., 2018)</ref>. Consequently, manual fact checking has emerged with the promise to support accurate and unbiased analysis of public statements.</p><p>As manual fact checking is a very tedious task, automatic fact checking has been proposed as an alternative. This is often broken into intermediate steps in order to alleviate the task complexity. One such step is stance detection, which is also useful for human experts as a stand-alone task. The aim is to identify the relative perspective of a piece of text with respect to a claim, typically modeled using labels such as agree, disagree, discuss, and unrelated. <ref type="figure">Figure 1</ref> shows some examples.</p><p>Here, we address the problem using a novel model based on end-to-end memory networks <ref type="bibr" target="#b22">(Sukhbaatar et al., 2015)</ref>, which incorporates convolutional and recurrent neural networks, as well as a similarity matrix. * This work was carried out when the authors were scientists at the Qatar Computing Research Institute, HBKU.</p><p>Claim: Robert Plant Ripped up $800M Led Zeppelin Reunion Contract.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stance</head><p>Snippet agree</p><p>Led Zeppelin's Robert Plant turned down £500m to reform supergroup... disagree Robert Plant's publicist has described as "rubbish" a Daily Mirror report that he rejected a £500m Led Zeppelin reunion... discuss Robert Plant reportedly tore up an $800 million Led Zeppelin reunion deal...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>unrelated</head><p>Richard Branson's Virgin Galactic is set to launch SpaceShipTwo today... <ref type="figure">Figure 1</ref>: Examples of snippets of text and their stances with respect to a given claim.</p><p>Our model jointly addresses the problems of predicting the stance of a text document with respect to a given claim, and of extracting relevant text snippets as support for the prediction of the model. We further introduce a similarity matrix, which we use at inference time in order to improve the extraction of relevant snippets.</p><p>The experimental results on the Fake News Challenge benchmark dataset show that our model, which is very feature-light, performs similarly to the state of the art, which is achieved by more complex systems. Our contributions can be summarized as follows: (i) We apply a novel memory network model enhanced with CNN and LSTM networks for stance detection. (ii) We further propose a novel extension of the general architecture based on a similarity-based matrix, which we use at inference time, and we show that this extension offers sizable performance gains. (iii) Finally, we show that our model is capable of extracting meaningful snippets from the input text document, which is useful not only for stance detection, but more importantly can be useful for human experts who need to decide on the factuality of a given claim.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>Long-term memory is necessary in order to determine the stance of a long document with respect to a claim, as relevant parts of a document -paragraphs or text snippets-can indicate the perspective of a document with respect to a claim. Memory networks were designed to remember past information <ref type="bibr" target="#b22">(Sukhbaatar et al., 2015)</ref> and they can be particularly well-suited for stance detection since they can use a variety of inference strategies alongside their memory component.</p><p>In this section, we present a novel memory network (MN) for stance detection. It contains a new inference component that incorporates a similarity matrix to extract, with better accuracy, textual snippets that are relevant to the input claims.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview of the network</head><p>A memory network is a 5-tuple {M, I, G, O, R}, where the memory M is a sequence of objects or representations, the input I is a component that maps the input to its representation, the generalization component G <ref type="bibr" target="#b22">(Sukhbaatar et al., 2015)</ref> updates the memory with respect to new input, the output O generates an output for each new input and the current memory state, and finally, the response R converts the output into a desired response format, e.g., a textual response or an action. These components can potentially use many different machine learning models.</p><p>Our new memory network for stance detection is a 6-tuple {M, I, F, G, O, R}, where F represents the new inference component. It takes an input document d as evidence and a textual statement s as a claim and converts them into their corresponding representations in the input I. Then, it passes them to the memory M . Next, the relevant parts of the input are identified in F , and afterwards they are used by G to update the memory. Finally, O generates an output from the updated memory, and converts it to a desired response format with R. The network architecture is depicted in <ref type="figure" target="#fig_0">Figure 2</ref>. We describe the components below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Input Representation Component</head><p>The input to the stance detection algorithm is a document d and a textual statement s as a claim: see lines 2 and 3 in <ref type="table" target="#tab_2">Table 1</ref>. Each d is segmented into paragraphs x j of varied lengths, where each x j is considered as a potential piece of evidence for stance detection. Inputs: 2</p><p>(1) A document (d) as a set of evidence (xj) 3</p><p>(2) A textual statement containing a claim (s) 4 Outputs: 5</p><p>(1) predicting the relative perspective (or stance) of a pair of (d, s) to a claim as agree, disagree, discuss and unrelated. 6</p><p>Inference outputs: 7</p><p>(2) Top K evidence xj with their similarity scores 8</p><p>(3) Top K snippets of xj with their similarity scores 9 Memory Network Model: 10 1. Input memory representation (I):  Indeed, a paragraph usually represents a coherent argument, unified under one or more inter-related topics. The input component in our model converts each d into a set of potential pieces of evidence in a three-dimensional (3D) tensor space as shown below (see line 11 in <ref type="table" target="#tab_2">Table 1)</ref>:</p><formula xml:id="formula_0">11 d → (X, W, E) 12 (X, W, E) T imeDistributed(LST M ) − −−−−−−−−−−−−−−−− → {m1, ..., mn} 13 (X, W, E) T imeDistributed(CN N ) − −−−−−−−−−−−−−−− → {c1, .., cn} 14 s LST M,CN N −−−−−−−−→ slstm,</formula><formula xml:id="formula_1">d = (X, W, E)<label>(1)</label></formula><p>where X = {x 1 , ..., x n } is a set of paragraphs considered as potential pieces of evidence, such that each x j is represented by a set of words W = {w 1 , ..., w v }-global vocabulary of size v-and a set of neural representations E = {e 1 , ..., e v } for words in W . This 3D space is illustrated as a cube in <ref type="figure" target="#fig_0">Figure 2</ref>. Each x j is encoded from the 3D space into a semantic representation at the input component using a Long Short-Term Memory (LSTM) network. The lower left component in <ref type="figure" target="#fig_0">Figure 2</ref> shows our LSTM network, which operates on our input as follows (see also line 12 in <ref type="table" target="#tab_2">Table 1)</ref>: where m j is the LSTM representation of x j , and TimeDistributed() indicates a wrapper that enables training the LSTM over all pieces of evidence by applying the same LSTM model to each time-step of a 3D input tensor, i.e., (X, W, E).</p><formula xml:id="formula_2">(X, W, E) T imeDistributed(LST M ) − −−−−−−−−−−−−−−− → {m 1 , ..., m n }<label>(2)</label></formula><p>While LSTM networks are designed to effectively capture and memorize their inputs <ref type="bibr" target="#b23">(Tan et al., 2016)</ref>, Convolutional Neural Networks (CNNs) emphasize the local interaction between the words in the input word sequence, which is important for obtaining an effective representation. We use a CNN to encode each x j into its representation c j as shown in Equation 3 (see line 13 in <ref type="table" target="#tab_2">Table 1)</ref>.</p><formula xml:id="formula_3">(X, W, E) T imeDistributed(CN N ) − −−−−−−−−−−−−−− → {c 1 , .., c n }</formula><p>(3) The left-top of <ref type="figure" target="#fig_0">Figure 2</ref> shows that this representation is passed as a new input to the component M of our memory network.</p><p>We keep track of the computed n-grams from the CNN, so that we can use them later in the inference and in the response components (see Sections 2.3 and 2.6). For this purpose, we use a Maxout layer <ref type="bibr" target="#b6">(Goodfellow et al., 2013)</ref> to take the maximum across k affine feature maps computed by the CNN, i.e., pooling across channels. Previous work has investigated the combination of convolutional and recurrent representations, which is then fed to the other network as input <ref type="bibr" target="#b23">(Tan et al., 2016;</ref><ref type="bibr" target="#b4">Donahue et al., 2015;</ref><ref type="bibr" target="#b29">Zuo et al., 2015;</ref><ref type="bibr" target="#b19">Sainath et al., 2015)</ref>. In contrast, we feed their individual outputs into our memory network separately, and let the network decide which representation helps the target task better. We show the effectiveness of this choice below.</p><p>Similarly, we convert each input claim s to its representation using the corresponding LSTM and CNN networks, as follows:</p><formula xml:id="formula_4">s LST M,CN N − −−−−−−− → s lstm , s cnn<label>(4)</label></formula><p>where s lstm and s cnn are the representations of s computed using LST M and CN N networks, respectively. Note that these are separate networks with different parameters from those used to encode the pieces of evidence. Lines 10-14 of <ref type="table" target="#tab_2">Table 1</ref> describe the above steps in representing I in our memory network. We encode each input document d into a set of pieces of evidence {x j }∀j: it computes LSTM and CNN representations, m j and c j , respectively, for each x j , and LSTM and CNN representations, s lstm and s cnn , for each claim s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Inference Component</head><p>The resulting representations are used to compute semantic similarity between claims and pieces of evidence. We define the similarity P j lstm between s and x j as follows (see also line 17 in <ref type="table" target="#tab_2">Table 1)</ref>:</p><formula xml:id="formula_5">P j lstm = s lstm × M × m j , ∀j<label>(5)</label></formula><p>where s lstm ∈ R q and m j ∈ R d are LSTM representations of s and x j , respectively, and M ∈ R q×d is a similarity matrix capturing their similarity. For this purpose, M maps s and x j into the same space as shown in <ref type="figure" target="#fig_1">Figure 3</ref>. M is a set of q × d parameters of the network, which are optimized during training.</p><p>In a similar fashion, we compute the similarity P j cnn between x j and s using the CNN representations as follows (see line 19 of <ref type="table" target="#tab_2">Table 1)</ref>: where s cnn ∈ R q and c j ∈ R d are the representations of s and x j obtained with CNN, respectively. The similarity matrix M ∈ R q ×d is a set of q × d parameters of the network and is optimized during training. P j lstm and P j cnn indicate the claim-evidence similarity vectors computed based on the LSTM and on the CNN representations of s and x j , respectively.</p><formula xml:id="formula_6">P j cnn = s cnn × M × c j , ∀j<label>(6)</label></formula><p>The rationale behind using the similarity matrix is that in our memory network model, as <ref type="figure" target="#fig_1">Figure 3</ref> shows, we look for a transformation of the input claim s such that s = M × s in order to obtain the closest facts to the claim.</p><p>In fact, the relevant parts of the input document with respect to the input claim can be captured at a different level, e.g., using M for the n-gram level or using the claim-evidence P j lstm or P j cnn , ∀j at the paragraph level. We note that (i) P j lstm uses LSTM to take the word order and long-length dependencies into account, and (ii) P j cnn exploits CNN to take n-grams and local dependencies into account, as explained in sections 2.2 and 2.3. Additionally, we compute another semantic similarity vector, P j tfidf , by applying a cosine similarity between the TF.IDF <ref type="bibr" target="#b21">(Spärck Jones, 2004)</ref> representation of x j and s. This is particularly useful for stance detection as it can help detect the unrelated pieces of evidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Memory and Generalization Components</head><p>The information flow and updates in the memory is as follows: first, the representation vector {m j }∀j is passed to the memory and updated using the claim-evidence similarity vector {P j tfidf }:</p><formula xml:id="formula_7">m j = m j P j tfidf , ∀j<label>(7)</label></formula><p>The goal is to filter out most unrelated evidence. The updated m j in conjunction with s lstm are used by the inference component-component F to compute {P j lstm } as explained in Section 2.3.</p><p>Then, {P j lstm } is used to update the new input set {c j }∀j to the memory:</p><formula xml:id="formula_8">c j = c j P j lstm , ∀j<label>(8)</label></formula><p>Finally, the updated c j in conjunction with s cnn are used to compute P j cnn as explained in Sec. 2.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Output Representation Component</head><p>In memory networks, the memory output depends on the final goal, which, in our case, is to detect the relative perspective of a document to a claim. For this purpose, we apply the following equation:</p><formula xml:id="formula_9">o = mean({c j }); max({P j cnn }); mean({P j cnn }) ; max({P j lstm }); mean({P j lstm }) ; max({P j tfidf }); mean({P j tfidf })<label>(9)</label></formula><p>where mean({c j }) is the average vector of the c j representations.</p><p>Then, we compute the maximum and the average similarity between each piece of evidence and the claim using P j tfidf , P j lstm and P j cnn , which are computed for each evidence and claim in the inference component F . The maximum similarity identifies the part of document x j that is most similar to the claim, while the average similarity measures the overall similarity between the document and the claim.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Response and Output Generation</head><p>This component computes the final stance of a document with respect to a claim. For this purpose, the concatenation of vectors o, s lstm and s lstm , are fed into a Multi-Layer Perceptron (MLP), where a softmax predicts the stance of the document with respect to the claim, as shown below (see also lines 22-23 in <ref type="table" target="#tab_2">Table 1)</ref>:</p><formula xml:id="formula_10">[o; s lstm ; s cnn ] M LP −−−→ δ<label>(10)</label></formula><p>where δ is a softmax function. In addition to the resulting stance, we extract snippets from the input document that best indicates the perspective of the document with respect to the claim. For this purpose, we use P j lstm , P j cnn and M as explained in Section 2.3 (see also lines 24-26 of <ref type="table" target="#tab_2">Table 1</ref>).</p><p>The overall model is shown in <ref type="figure" target="#fig_0">Figure 2</ref> and a summary of the model is presented in <ref type="table" target="#tab_2">Table 1</ref>. All model parameters, including those of (i) CNN and LSTM in I, (ii) the similarity matrices M and M in F , and (iii) the MLP in R, are jointly learned during the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data</head><p>We use the dataset provided by the Fake News Challenge, 1 where each example consists of a claim-document pair with the following possible relationship: agree (the document agrees with the claim), disagree (the document disagrees with the claim), discuss (the document discusses the same topic as the claim, but does not take a stance with respect to the claim), unrelated (the document discusses a different topic). The data includes a total of 75.4K claim-document pairs, which link 2.5K unique articles with 2.5K unique claims, i.e., each claim is associated with 29.8 articles on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Settings</head><p>We use 100-dimensional word embeddings from GloVe <ref type="bibr" target="#b15">(Pennington et al., 2014)</ref>, which were pretrained on two billion tweets. We use Adam as an optimizer and categorical cross entropy as a loss function. We further use 100-dimensional units for the LSTM embeddings, and 100 feature maps with filter width of size 5 for the CNN. We consider the first p=9 paragraphs for each document, where p is the median of the number of paragraphs.</p><p>We optimize the hyper-parameters of the models using the same validation dataset (20% of the training data). Finally, as the data is largely imbalanced towards the unrelated class, during training we randomly select an equal number of instances from each class for each epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation Measures</head><p>We use the following evaluation measures:</p><p>Accuracy: Number of correctly classified examples divided by the total number of examples. It is equivalent to micro-averaged F 1 .</p><p>Macro-F1: We calculate F 1 for each class, and then we average across all classes.</p><p>Weighted Accuracy: This is a weighted, twolevel scoring scheme, which is applied to each test example. First, if the example is from the unrelated class and the model correctly predicts it, the score is incremented by 0.25; otherwise, if the example is related and the model predicts agree, disagree, or discuss, the score is incremented by 0.25. Second, there is a further increment by 0.75 for each related example if the model correctly predicts the correct label: agree, disagree, or discuss.</p><p>1 Available at www.fakenewschallenge.org Finally, the score is normalized by dividing it by the total number of test examples. The rationale behind this metric is that the binary related/unrelated classification task is expected to be much easier, while also being arguably less relevant to fake news detection, than the actual stance detection task, which aims to further classify the relevant instances as agree, disagree, or discuss. Therefore, the weighted accuracy metric gives more weight to the former distinction and less weight to the latter one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Baselines</head><p>Given the imbalanced nature of our data, we use two baselines, in which we label all testing examples with the same label: (a) unrelated and (b) discuss. The former is the majority class baseline, which is a reasonable baseline for Accuracy and macro-F 1 , while the latter is a potentially better baseline for Weighted Accuracy.</p><p>We further use CNN and LSTM models, as well as combinations thereof, as baselines since they form components of our model, and also because they yield state-of-the-art results for text, image, and video classification <ref type="bibr" target="#b23">(Tan et al., 2016;</ref><ref type="bibr" target="#b4">Donahue et al., 2015;</ref><ref type="bibr" target="#b29">Zuo et al., 2015;</ref><ref type="bibr" target="#b19">Sainath et al., 2015)</ref>.</p><p>Finally, we include the official baseline from the challenge, which is a Gradient Boosting classifier with word and n-gram overlap features, as well as indicators for refutation and polarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Our Models</head><p>sMemNN: This is our model presented in <ref type="figure">Figure</ref> 2. Note that unlike the CNN+LSTM and the LSTM+CNN baselines above, which feed the output of one network into the other one, the sMemNN model feeds the individual outputs of both the CNN and the LSTM networks into the memory network, and lets it decide how much to rely on each of them. This consideration also facilitates reasoning and explaining model predictions, as we will discuss in more detail below. sMemNN (dotProduct): This is a version of sMemNN, where the similarity matrices are replaced by the dot product between the representation of the claims and of the evidence. For this purpose, we first project the claim representation to a dense layer that has the same size as the representation of each piece of evidence, and then we compute the dot product between the resulting representation and the representation of the evidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Total Parameters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Trainable Parameters</head><p>Weighted Accuracy</p><p>Macro-F1 Accuracy  sMemNN (with TF): Since our LSTM and CNN networks only use a limited number of starting paragraphs 2 for an input document, we enrich our model with the BOW representation of documents and claims as well as their TF.IDF-based cosine similarity. These vectors are concatenated with the memory outputs (section 2.5) and passed to the R component (section 2.6) of sMemNN. We expect these BOW vectors to provide useful additional information. <ref type="table" target="#tab_4">Table 2</ref> reports the performance of all models on the test dataset. The All-unrelated and the Alldiscuss baselines perform poorly across the evaluation measures, except for All-unrelated, which achieves high accuracy, which is due to unrelated being by far the dominant class in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Results</head><p>Next, we can see that LSTM consistently outperforms CNN across all evaluation measures. Although the larger number of parameters of the LSTM can play a role, we believe that its superiority comes from it being able to remember previously-observed relevant pieces of text.</p><p>Next, we see systematic improvements for the combinations of CNN and LSTM: CNN+LSTM is better than CNN alone, and LSTM+CNN is better than LSTM alone. Better performance is achieved by LSTM+CNN, that is, when claims and evidence are first processed by an LSTM network, and then fed into a CNN. The Gradient Boosting model achieves sizable improvement over the above baseline neural models. However, we should note that these neural models do not use the rich hand-crafted features that were used in the Gradient Boosting model. Row 9 shows the results for our memory network model (sMemNN), which consistently outperforms all other baseline models across all evaluation metrics, achieving 10.62 and 3.77 points of absolute improvement in terms of Macro-F1 and Weighted Accuracy, respectively, over the best baseline (Gradient Boosting). We believe that this is due to the memory network's capturing good text snippets. As we will see below, these snippets are also useful for explaining the model's predictions. Comparing row 9 to row 8, we can see the importance of our proposed similarity matrix: replacing that matrix by a simple dot product hurts the performance of the model considerably across all evaluation measures, thus lowering it to the level of the Gradient Boosting model.</p><p>Finally, row 10 shows the results for our memory network model enriched by a BOW representation. As we expected, it outperforms sMemNN, probably due to being able to capture useful information from paragraphs beyond the starting few.</p><p>To put the results of sMemNN in perspective, we should mention that the best system at the Fake News Challenge achieved a macro-F1 of 57.79, which is not significantly different from the performance of our full model at the 0.05 significance level <ref type="bibr">(p-value=0.53</ref>). Yet, they have an ensemble combining the feature-rich Gradient Boosting system with neural networks.</p><p>Further analysis of the output of the different systems (e.g., the confusion matrices) reveals the following general trends: (i) the unrelated examples are easy to detect, and most models show high performance for this class, (ii) the agree and the disagree examples are often mislabeled as discuss by the baselines, and (iii) the disagree examples are the most difficult ones for all models, probably because they represent by far the smallest class.</p><p>Claim 1: man saved from bear attack -thanks to his justin bieber ringtone Evidence Id P j cnn Evidence Snippet 2069-3 0.89 ... fishing in the yakutia republic , russia , igor vorozhbitsyn is lucky to be alive after his justin bieber ringtone , baby , scared off a bear that was attacking him 0.41 ... 2069-7 1.0 ... but as the bear clawed vorozhbitsyn ' s face and back his mobile phone rang , the ringtone selected was justin bieber ' s hit song baby . rightly startled 1.00 , the bear retreated back into 0.39 the forest ... true label: agree; predicted label: agree Claim 2: 50ft crustacean , dubbed crabzilla , photographed lurking beneath the waters in whitstable Evidence Id P j cnn Evidence Snippet 24835-1 0.0046 ... a marine biologist has killed off claims -0.0008 that a giant crab is 0.0033 living on the kent coast -insisting the image is probably a well -doctored hoax 0.0012 ... 24835-7 -0.0008 ... i don ' t know what the currents are like around that harbour or what sort of they might produce in the sand , but i think it ' s more conceivable that someone is playing 0.0007 about with the photo ... true label: disagree; predicted label: disagree <ref type="table">Table 3</ref>: Examples of highly ranked snippets of evidence for an input claim, which were automatically extracted by our inference component for claim-document pairs. The P j cnn column and the values in the top-right corner of the highlighted snippets show the similarity between the claim and a piece of evidence, and between the claim and an evidence snippet, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training Data Coverage</head><p>As discussed previously, we balance the data at each training iteration by randomly selecting z instances from each of the four target classes, where z is the size of the class with the minimum number of training instances. In this experiment, we investigate what proportion of the training data got actually used when following our sampling procedure. For this purpose, at each training iteration, we report the proportion of the training instances from each class that were used so far, either at the current or at any of the previous iterations.</p><p>As <ref type="figure" target="#fig_2">Figure 4</ref> shows, our random data sampling procedure eventually used almost all training examples. Since the disagree class was the smallest, its examples remained fully covered throughout the process. Moreover, almost all other related examples, i.e., agree and discuss, were observed during training, as well as a large fraction of the dominating unrelated examples. Note that the model achieved its best (lowest) loss on the validation dataset at iteration 31, when almost all related instances had already been observed. This happened while the corresponding fraction for the unrelated pairs was around 50%, i.e., a considerable number of the unrelated instances were not really needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Explainability</head><p>A major advantage of our model, compared to the baselines and to most related work, is that it can explain its predictions: as we explained in section 2.3, our inference component predicts the similarity between each piece of evidence x j and the claim s at the n-grams-level using the claimevidence similarity vector P j cnn . <ref type="table">Table 3</ref> shows examples of two claims and the snippets extracted as evidence. Column P j cnn shows the overall similarity between the evidence and the corresponding claim as computed by the inference component of our model. The highlighted texts are snippets with the highest similarity (the value is shown next to each snippet) to the claim as extracted by the inference component.</p><p>Note that the snippets are of fixed length, namely 5-grams, but in case of consecutive n-grams with similar scores, we combine them into a single snippet and we report the average value, e.g., see the snippet for evidence 2069-3. The lower half of <ref type="table">Table 3</ref> shows an example where the similarity values associated with the snippets are either too small or negative, e.g., see the value for biologist has killed off claims. In all cases, the model could accurately predict the stance of these pieces of evidence with respect to the corresponding claims.</p><p>Next, we conducted an experiment to quantify the performance of our memory network at explaining its predictions: we randomly sampled 100 agree/disagree claim-document examples from our gold data, and we manually evaluated the top five pieces of evidence that our model provided. In 76 cases, the model correctly classified the agree/disagree examples, and provided arguably adequate snippets. <ref type="figure" target="#fig_3">Figure 5</ref>(a) shows the performance of our model at explaining its predictions when each supporting/opposing piece of evidence is an n-gram snippet of fixed length (n = 5) for the agree and the disagree classes, and their combinations at the topk ranks, k = {1, . . . , 5}. It achieved precision of 0.28, 0.32, 0.35, 0.25, and 0.33 at ranks 1-5. Moreover, we found that it could accurately identify, as part of the identified n-grams, key phrases such as officials declared the video, according to previous reports, believed will come, president in his tweets as supporting pieces of evidence, and proved a hoax, shot down a cnn report, would be skeptical as opposing pieces of evidence.</p><p>Note that the above low precision is mainly due to the unsupervised nature of this task as no gold snippets supporting the document's stance are available for training in the FNC dataset. 3 Furthermore, our evaluation setup was at the n-gram level in <ref type="figure" target="#fig_3">Figure 5(a)</ref>. However, if we conduct a more coarse-grained evaluation where we combine consecutive n-grams with similar scores into a single snippet, the precision for these new snippets improves to 0.4, 0.38, 0.42, 0.38, and 0.42 at ranks 1-5, as <ref type="figure" target="#fig_3">Figure 5(b)</ref> shows. If we further extend the evaluation to the sentence level, the precision jumps to 0.6, 0.58, 0.55, 0.62, and 0.57 at ranks 1-5, as we can see on <ref type="figure" target="#fig_3">Figure 5</ref>(c).</p><p>3 Some other recent datasets, to be presented at this same HLT-NAACL'2018 conference, do have such gold evidence annotations <ref type="bibr" target="#b1">(Baly et al., 2018;</ref><ref type="bibr" target="#b24">Thorne et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>While stance detection is an interesting task in its own right, e.g., for media monitoring, it is also an important component for fact checking and veracity inference. 4 Automatic fact checking was envisioned by <ref type="bibr" target="#b25">Vlachos and Riedel (2014)</ref> as a multistep process that (i) identifies check-worthy statements <ref type="bibr" target="#b8">(Hassan et al., 2015;</ref><ref type="bibr" target="#b5">Gencheva et al., 2017;</ref><ref type="bibr" target="#b9">Jaradat et al., 2018)</ref>, (ii) generates questions to be asked about these statements <ref type="bibr" target="#b10">(Karadzhov et al., 2017)</ref>, (iii) retrieves relevant information to create a knowledge base <ref type="bibr" target="#b20">(Shiralkar et al., 2017)</ref>, and (iv) infers the veracity of these statements, e.g., using text analysis <ref type="bibr" target="#b2">(Banerjee and Han, 2009;</ref><ref type="bibr" target="#b3">Castillo et al., 2011;</ref><ref type="bibr" target="#b17">Rashkin et al., 2017)</ref> or information from external sources <ref type="bibr" target="#b10">(Karadzhov et al., 2017;</ref><ref type="bibr" target="#b16">Popat et al., 2017)</ref>.</p><p>There have been some nuances in the way researchers have defined the stance detection task. SemEval-2016 Task 6 <ref type="bibr" target="#b14">(Mohammad et al., 2016)</ref> targets stances with respect to some target proposition, e.g., entities, concepts or events, as infavor, against, or neither. The winning model in the task was based on transfer learning: a Recurrent Neural Network trained on a large Twitter corpus was used to predict task-relevant hashtags and to initialize a second recurrent neural network trained on the provided dataset for stance prediction <ref type="bibr" target="#b27">(Zarrella and Marsh, 2016)</ref>. Subsequently, <ref type="bibr" target="#b28">Zubiaga et al. (2016)</ref> detected the stance of tweets toward rumors and hot topics using linear-chain conditional random fields (CRFs) and tree CRFs that analyze tweets based on their position in treelike conversational threads.</p><p>Most commonly, stance detection is defined with respect to a claim, e.g., as in the 2017 Fake News Challenge. The best system was an ensemble of gradient-boosted decision trees with rich features and CNNs <ref type="bibr" target="#b0">(Baird et al., 2017)</ref>. The second system was a multi-layer neural network with similarity features, word n-grams, and latent semantic analysis <ref type="bibr" target="#b7">(Hanselowski et al., 2017)</ref>. The third one was a neural network with similarity features <ref type="bibr" target="#b18">(Riedel et al., 2017)</ref>.</p><p>Unlike the above work, we use a feature-light memory network that jointly infers the stance and highlights relevant snippets of evidence. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We studied the problem of stance detection, which aims to predict whether a document supports, challenges, or just discusses a given claim. The nature of the task clearly shows that, in order to go beyond simple matching between stance (short text) and evidence (longer text, e.g., an entire document), a machine learning model needs to focus on the relevant paragraphs of the evidence. Moreover, in order to understand whether a paragraph supports a claim, there is a need to refer to information available in other paragraphs. CNNs and LSTMs are not well-suited for this task as they cannot model complex dependencies such as semantic relationships with respect to entire previous paragraphs. In contrast, memory networks are exactly designed to remember previous information. However, given the large size of documents and paragraphs, basic memory networks do not handle well irrelevant and noisy information, which we confirmed in our experimental results.</p><p>Thus, we proposed a novel extension of the basic memory networks, which is based on a similarity matrix and a stance filtering component, which we apply at inference time, and we have shown that this extension offers sizable performance gains, making memory networks competitive. Moreover, our model can extract meaningful snippets from documents that can explain the factuality of a given claim.</p><p>In future work, we plan to extend the inference component to select an optimal set of explanations for each prediction, and to explain the model as a whole, not only at the instance level.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The architecture of our Memory Network model for stance detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Matching a claim s and a piece of evidence x j using a similarity matrix M . Here, s lstm and s cnn are LSTM and CNN representations of s, whereas m j and c j are LSTM and CNN representations of x j .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Effect of data coverage. The y-axis shows the fraction of data observed during training (coverage), while the x-axis shows the loss during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Prediction explainability. Sub-figures (a)-(c) show the precision of our model explaining its prediction when the pieces of evidence are (a) fixed-length n-grams (n = 5), (b) combinations of several consecutive n-grams with similar scores, or (c) the entire sentence, if it includes at least one extracted n-gram snippet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Summary of our Memory Network algorithm for stance detection.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Evaluation results on the test data.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Due to the long length of some documents, it is impractical to consider all paragraphs when training LSTM and CNN.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Yet, stance detection and fact checking are typically supported by separate datasets. Two notable upcoming exceptions, both appearing in this HLT-NAACL'2018, are<ref type="bibr" target="#b24">(Thorne et al., 2018)</ref> for English and<ref type="bibr" target="#b1">(Baly et al., 2018)</ref> for Arabic.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>We would like to thank the members of the MIT Spoken Language Systems group and the anonymous reviewers for their helpful comments.</p><p>This research was carried out in collaboration between the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) and the Qatar Computing Research Institute (QCRI), HBKU.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Talos targets disinformation with fake news challenge victory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Baird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Sibley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxi</forename><surname>Pan</surname></persName>
		</author>
		<ptr target="https://blog.talosintelligence.com/2017/06/talos-fake-news-challenge.html" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Integrating stance detection and fact checking in a unified corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramy</forename><surname>Baly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitra</forename><surname>Mohtarami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL</title>
		<meeting>HLT-NAACL<address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Answer credibility: A language modeling approach to answer validation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Protima</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyoil</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL</title>
		<meeting>HLT-NAACL<address><addrLine>Boulder, CO, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="157" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Information credibility on Twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcelo</forename><surname>Mendoza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Poblete</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WWW</title>
		<meeting>WWW<address><addrLine>Hyderabad, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="675" to="684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A context-aware approach for detecting worthchecking claims in political debates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pepa</forename><surname>Gencheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Koychev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of RANLP. Varna</title>
		<meeting>RANLP. Varna<address><addrLine>Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="267" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Maxout networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML<address><addrLine>Atlanta, GA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1319" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Team Athene on the fake news challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Hanselowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pvs</forename><surname>Avinesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Schiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Caspelherr</surname></persName>
		</author>
		<ptr target="https://medium.com/@andre134679/team-athene-on-the-fake-news-challenge-28a5cf5e017b" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Detecting check-worthy factual claims in presidential debates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naeemul</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengkai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Tremayne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings CIKM. Melbourne</title>
		<meeting>CIKM. Melbourne<address><addrLine>Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1835" to="1838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Claim-Rank: Detecting check-worthy claims in Arabic and English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Israa</forename><surname>Jaradat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pepa</forename><surname>Gencheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL. New Orleans</title>
		<meeting>HLT-NAACL. New Orleans<address><addrLine>LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fully automated fact checking using external sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgi</forename><surname>Karadzhov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Koychev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of RANLP. Varna</title>
		<meeting>RANLP. Varna<address><addrLine>Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="344" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Finding opinion manipulation trolls in news community forums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgi</forename><surname>Georgiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL. Beijing</title>
		<meeting>CoNLL. Beijing<address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="310" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exposing paid opinion manipulation trolls</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Koychev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgi</forename><surname>Georgiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of RANLP. Hissar</title>
		<meeting>RANLP. Hissar<address><addrLine>Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="443" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hunting for troll comments in news community forums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SemEval-2016 task 6: Detecting stance in tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saif</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parinaz</forename><surname>Sobhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Dan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval</title>
		<meeting>SemEval<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="31" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Glove: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Where the truth lies: Explaining the credibility of emerging claims on the web and social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashyap</forename><surname>Popat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhabrata</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jannik</forename><surname>Strötgen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WWW</title>
		<meeting>WWW<address><addrLine>Perth, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1003" to="1012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Truth of varying shades: Analyzing language in fake news and political fact-checking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Hannah Rashkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">Yea</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svitlana</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Volkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2931" to="2937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A simple but tough-to-beat baseline for the Fake News Challenge stance detection task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Augenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Georgios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Spithourakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Riedel</surname></persName>
		</author>
		<idno>ArXiv:1707.03264</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Convolutional, long short-term memory, fully connected deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haşim</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
		<meeting>ICASSP<address><addrLine>Brisbane, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4580" to="4584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Finding streams in knowledge graphs to support fact checking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prashant</forename><surname>Shiralkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Flammini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filippo</forename><surname>Menczer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Luca</forename><surname>Ciampaglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICDM</title>
		<meeting>ICDM<address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="859" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">IDF term weighting and IR research lessons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen Spärck</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of documentation</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="521" to="523" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improved representation learning for question answer matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Cicero Dos Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="464" to="473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">FEVER: A large-scale dataset for fact extraction and VERification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL</title>
		<meeting>HLT-NAACL<address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Christos Christodoulopoulos, and Arpit Mittal</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fact checking: Task definition and dataset construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science</title>
		<meeting>the ACL 2014 Workshop on Language Technologies and Computational Social Science<address><addrLine>Baltimore, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="18" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The spread of true and false news online</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soroush</forename><surname>Vosoughi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deb</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinan</forename><surname>Aral</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">359</biblScope>
			<biblScope unit="issue">6380</biblScope>
			<biblScope unit="page" from="1146" to="1151" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guido</forename><surname>Zarrella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Marsh</surname></persName>
		</author>
		<idno>1606.03784</idno>
		<title level="m">MITRE at SemEval-2016 task 6: Transfer learning for stance detection</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stance classification in rumours as a sequential task exploiting the tree structure of social media conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arkaitz</forename><surname>Zubiaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Kochkina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Liakata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Procter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Lukasik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2438" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Convolutional recurrent neural networks: Learning spatial dependencies for image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yushi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="18" to="26" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
