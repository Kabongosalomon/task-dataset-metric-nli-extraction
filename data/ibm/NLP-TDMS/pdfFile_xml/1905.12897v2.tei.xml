<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Compare-Aggregate Model with Latent Clustering for Answer Selection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghyun</forename><surname>Yoon</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
							<email>franck.dernoncourt@adobe.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doo</forename><forename type="middle">Soon</forename><surname>Kim</surname></persName>
							<email>dkim@adobe.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trung</forename><surname>Bui</surname></persName>
							<email>bui@adobe.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyomin</forename><surname>Jung</surname></persName>
							<email>kjung@snu.ac.kr</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Seoul National University Seoul</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Adobe Research San Jose</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Adobe Research San Jose</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Adobe Research San Jose</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Compare-Aggregate Model with Latent Clustering for Answer Selection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>question answering</term>
					<term>natural language processing</term>
					<term>information re- trieval</term>
					<term>deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a novel method for a sentence-level answer-selection task that is a fundamental problem in natural language processing. First, we explore the effect of additional information by adopting a pretrained language model to compute the vector representation of the input text and by applying transfer learning from a large-scale corpus. Second, we enhance the compare-aggregate model by proposing a novel latent clustering method to compute additional information within the target corpus and by changing the objective function from listwise to pointwise. To evaluate the performance of the proposed approaches, experiments are performed with the WikiQA and TREC-QA datasets. The empirical results demonstrate the superiority of our proposed approach, which achieve state-of-the-art performance for both datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Automatic question answering (QA) is a primary objective of artificial intelligence. Recently, research on this task has taken two major directions based on the answer span considered by the model. The first direction (i.e., the fine-grained approach) finds an exact answer to a question within a given passage <ref type="bibr" target="#b6">[7]</ref>. The second direction (i.e., the coarse-level approach) is an information retrieval (IR)-based approach that provides the most relevant sentence from a given document in response to a question. In this study, we are interested in building a model that computes a matching score between two text inputs. In particular, our model is designed to undertake an answer-selection task that chooses the sentence that is most relevant to the question from a list of answer candidates. This task has been extensively investigated by researchers because it is a fundamental task that can be applied to other QA-related tasks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>However, most previous answer-selection studies have employed small datasets <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b16">17]</ref> compared with the large datasets employed for other natural language processing (NLP) tasks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7]</ref>. Therefore, * Work conducted while the author was an intern at Adobe Research. the exploration of sophisticated deep learning models for this task is difficult.</p><p>To fill this gap, we conduct an intensive investigation with the following directions to obtain the best performance in the answerselection task. First, we explore the effect of additional information by adopting a pretrained language model (LM) to compute the vector representation of the input text. Recent studies have shown that replacing the word-embedding layer with a pretrained language model helps the model capture the contextual meaning of words in the sentence <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6]</ref>. Following this study, we select an ELMo <ref type="bibr" target="#b5">[6]</ref> language model for this study. We investigate the applicability of transfer learning (TL) using a large-scale corpus that is created for a relevant-sentence-selection task (i.e., question-answering NLI (QNLI) dataset <ref type="bibr" target="#b12">[13]</ref>). Second, we further enhance one of the baseline models, Comp-Clip <ref type="bibr" target="#b0">[1]</ref> (refer to the discussion in 3.1), for the target QA task by proposing a novel latent clustering (LC) method. The LC method computes latent cluster information for target samples by creating a latent memory space and calculating the similarity between the sample and the memory. By an endto-end learning process with the answer-selection task, the LC method assigns true-label question-answer pairs to similar clusters. In this manner, a model will have further information for matching sentence pairs, which increases the total model performance. Last, we explore the effect of different objective functions (listwise and pointwise learning). In contrast to previous research <ref type="bibr" target="#b0">[1]</ref>, we observe that the pointwise learning approach performs better than the listwise learning approach when we apply our proposed methods. Extensive experiments are conducted to investigate the efficacy and properties of the proposed methods and show the superiority of our proposed approaches for achieving state-of-the-art performance with the WikiQA and TREC-QA datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Researchers have investigated models based on neural networks for question-answering tasks. One study employs a Siamese architecture that utilizes an encoder (e.g., RNN or CNN) to compute vector representations of the question and the answer. The affinity score is calculated based on these vector representations <ref type="bibr" target="#b3">[4]</ref>. To improve the model performance by enabling the use of information from one sentence (e.g., a question or an answer) in computing the representation of another sentence, researchers included the attention mechanism in their models <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16]</ref>. 1: <ref type="figure">Figure 1</ref>: The architecture of the model. The dotted box on the right shows the process through which the latent-cluster information is computed and added to the answer. This process is also performed in the question part but is omitted in the figure. The latent memory is shared in both processes.</p><p>Another line of research includes the compare-aggregate framework <ref type="bibr" target="#b14">[15]</ref>. In this framework, first, vector representations of each sentence are computed. Second, these representations are compared. Last, the results are aggregated to calculate the matching score between the question and the answer <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>In this study, unlike the previous research, we employ a pretrained language model and a latent-cluster method to help the model understand the information in the question and the answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODS 3.1 Comp-Clip Model</head><p>In this paper, we are interested in estimating the matching score f (y|Q, A), where y, Q = {q 1 , ..., q n } and A = {a 1 , ..., a m } represent the label, the question and the answer, respectfully. We select the model from <ref type="bibr" target="#b0">[1]</ref>, which is referred to as the Comp-Clip model, as our baseline model. The model consists of the following four parts:</p><p>Context representation: The question Q ∈ R d ×Q and answer A ∈ R d ×A , (where d is a dimensionality of word embedding and Q and A are the length of the sequence in Q and A, respectively), are processed to capture the contextual information and the word as follows:</p><formula xml:id="formula_0">Q = σ (W i Q) ⊙ tanh(W u Q), A = σ (W i A) ⊙ tanh(W u A),<label>(1)</label></formula><p>where ⊙ denotes element-wise multiplication, and σ is the sigmoid function. The W ∈ R l ×d is the learned model parameter.</p><p>Attention: The soft alignment of each element in Q ∈ R l ×Q and A ∈ R l ×A are calculated using dynamic-clip attention <ref type="bibr" target="#b0">[1]</ref>. We obtain the corresponding vectors H Q ∈ R l ×A and H A ∈ R l ×Q .</p><formula xml:id="formula_1">H Q = Q · softmax((W q Q) ⊺ A), H A = A · softmax((W a A) ⊺ Q).<label>(2)</label></formula><p>Comparison: A comparison function is used to match each word in the question and answer to a corresponding attention-applied vector representation:</p><formula xml:id="formula_2">C Q = A ⊙ H Q , (C Q ∈ R l ×A ), C A = Q ⊙ H A , (C A ∈ R l ×Q ),<label>(3)</label></formula><p>where ⊙ denotes element-wise multiplication.</p><p>Aggregation: We aggregate the vectors from the comparison layer using CNN <ref type="bibr" target="#b2">[3]</ref> with n-types of filters and calculate the matching score between Q and A.</p><formula xml:id="formula_3">R Q = CNN(C Q ), R A = CNN(C A ), score = σ ([R Q ; R A ] ⊺ W),<label>(4)</label></formula><formula xml:id="formula_4">where [;] denotes concatenation of each vector R Q ∈ R nl and R A ∈ R nl . The W ∈ R 2nl ×1 is the learned model parameter.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Proposed Approaches</head><p>To achieve the best performance in the answer-selection task, we propose four approaches: adding a pretrained LM; adding the LC information of each sentence as auxiliary knowledge; applying TL to benefit from large-scale data; and modifying the objective function from listwise to pointwise learning. <ref type="figure">Figure 1</ref> depicts the total architecture of the proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pretrained Language Model (LM):</head><p>Recent studies have shown that replacing the word embedding layer with a pretrained LM helps the model capture the contextual meaning of the words in the sentence <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6]</ref>. We select an ELMo <ref type="bibr" target="#b5">[6]</ref> language model and replace the previous word embedding layer with the ELMo model as follows: L Q = ELMo(Q), L A = ELMo(A). These new representations-L Q and L A -are substituted for Q and A, respectively, in equation <ref type="formula" target="#formula_0">(1)</ref>.</p><p>Latent Clustering (LC) Method: We assume that extracting the LC information of the text and using it as auxiliary information will help the neural network model analyze the corpus. The dotted box in <ref type="figure">figure 1</ref> shows the proposed LC method. We create n-many latent memory vectors M 1:n and calculate the similarity between the sentence representation and each latent memory vector. The latentcluster information of the sentence representation will be obtained using a weighted sum of the latent memory vectors according to the calculated similarity as follows:</p><formula xml:id="formula_5">p 1:n = s ⊺ WM 1:n , p 1:k = k-max-pool(p 1:n ), α 1:k = softmax(p 1:k ), M LC = k α k M k ,<label>(5)</label></formula><p>where s ∈ R d is a sentence representation, M 1:n ∈ R d ′ ×n indicates the latent memory, and W∈ R d ×d ′ is the learned model parameter. We apply the LC method and extract cluster information from each question and answer. This additional information is added to each of the final representations in the comparison part (see 3.1) as follows:</p><formula xml:id="formula_6">M Q LC = f (( i q i )/n), q i ⊂ Q 1:n , M A LC = f (( i a i )/m), a i ⊂ A 1:m , C Q new = [C Q ; M Q LC ], C A new = [C A ; M A LC ],<label>(6)</label></formula><p>where f is the LC method (in equation 5) and [;] denotes the concatenation of each vector. These new representations-C Q new and C A new -are substituted for C Q and C A in equation <ref type="bibr" target="#b3">(4)</ref>. Note that we average word-embedding to obtain sentence representation in the previous equation.</p><p>Transfer Learning (TL): To observe the efficacy in a large dataset, we apply transfer learning using the question-answering NLI (QNLI) corpus <ref type="bibr" target="#b12">[13]</ref>. We train the CompClip model with the QNLI corpus and then fine-tune the model with target corpora, such as the Wik-iQA and TREC-QA datasets.</p><p>Pointwise Learning to Rank: Previous research adopts a listwise learning approach. With a dataset that consists of a question, Q, a related answer set, A = {A 1 , ..., A N }, and a target label, y = {y 1 , ..., y N }, a matching score is computed using equation <ref type="formula" target="#formula_3">(4)</ref>. This approach applies KL-divergence loss to train the model as follows: score i = model(Q, A i ), S = softmax([score 1 , ..., score i ]), loss = N n=1 KL(S n ||y n ),</p><p>where i is the number of answer candidates for the given question and N is the total number of samples employed during training. In contrast, we pair each answer candidate to the question and compute the cross-entropy loss to train the model as follows: loss = − N n=1 y n log (score n ), </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EMPIRICAL RESULTS</head><p>We regard all tasks as relevant answer selections for the given questions. Following the previous study, we report the model performance as the mean average precision (MAP) and the mean reciprocal rank (MRR) <ref type="bibr" target="#b0">1</ref> . To test the performance of the model, we utilize the TREC-QA, WikiQA and QNLI datasets <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>WikiQA <ref type="bibr" target="#b16">[17]</ref> is an answer selection QA dataset constructed from real queries of Bing and Wikipedia. Following the literature <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9]</ref>, we use only questions that contain at least one correct answer among the list of answer candidates. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>To implement the Comp-Clip model, we apply a context projection weight matrix with 100 dimensions that are shared between the question part and the answer part (eq. 1). In the aggregation part, we use 1-D CNN with a total of 500 filters, which involves five types of filters K ∈ R {1,2,3,4,5}×100 , 100 per type. This CNN is independently applied to the question part and answer part. For the LC method, we perform additional hyper-parameter searching  <ref type="bibr" target="#b14">[15]</ref> 0.743* 0.699* 0.754* 0.708* ----Comp-Clip (2017) <ref type="bibr" target="#b0">[1]</ref> 0.732* 0.718* 0.738* 0.732* -0.821 -0.899 IWAN (2017) <ref type="bibr" target="#b8">[9]</ref> 0.738* 0.692* 0.749* 0.705* -0.822 -0.899 IWAN + sCARNN (2018) <ref type="bibr" target="#b11">[12]</ref> 0.719* 0.716* 0.729* 0.722* -0.829 -0.875 MCAN (2018) <ref type="bibr">[</ref> experiments to select the best parameters. We select k (for the kmax-pool in equation 5) as 6 and 4 for the WikiQA and TREC-QA case, respectively. In both datasets, we apply 8 latent clusters. The vocabulary size in the WiKiQA, TREC-QA and QNLI dataset are 30,104, 56,908 and 154,442, respectively. When applying the TL, the vocabulary size is set to 154,442, and the dimension of the context projection weight matrix is set to 300. We use the Adam optimizer, including gradient clipping, by the norm at a threshold of 5. For the purpose of regularization, we applied a dropout with a ratio of 0.5. <ref type="table" target="#tab_2">Table 2</ref> shows the model performance for the WikiQA and TREC-QA datasets. For the Compare-Aggregate (2016), Comp-Clip (2017), IWAN (2017) and IWAN+sCARNN (2018) models, we measure the performance on the WikiQA dataset using the authors' implementations (marked by * in the table). Unlike previous studies, we report our results for both the dev dataset and the test dataset because we note a performance gap between these datasets. While training the model, we apply an early stop that is based on the performance of the dev dataset and measure the performance on the test dataset. Because Comp-Clip [1] is our baseline model, we implement it from scratch and achieve a performance that is similar to that of the original paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with Other Methods</head><p>WikiQA: For the WikiQA dataset, the pointwise learning approach shows a better performance than the listwise learning approach. We combine LM with the base model (Comp-Clip +LM) and observe a significant improvement in performance in terms of MAP (0.714 to 0.746 absolute). When we add the LC method (Comp-Clip +LM +LC), the best previous results are surpassed in terms of MAP (0.718 to 0.764 absolute). We achieve a vast improvement in performance </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TREC-QA:</head><p>The pointwise learning approach also shows excellent performance with the TREC-QA dataset. As shown in table 1, the TREC-QA dataset has a larger number of answer candidates per question. We assume that this characteristic prevents the model from handling the dataset with a listwise learning approach. As in the WikiQA case, we achieve additional performance gains in terms of the MAP as we apply LM, LC, and TL (0.850, 0.868 and 0.875, respectively). In particular, our model outperforms the best previous result when we add LC method, (Comp-Clip +LM +LC) in terms of MAP (0.865 to 0.868).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Impact of Latent Clustering</head><p>To evaluate the impact of latent clustering method (Comp-Clip +LM +LC) in a larger dataset environment, we perform QNLI evaluation. <ref type="table" target="#tab_4">Table 3</ref> shows the performance of the model (Comp-Clip +LM +LC) for the QNLI dataset with a variant number of clusters.</p><p>Note that the QNLI dataset is created from the SQuAD <ref type="bibr" target="#b6">[7]</ref> dataset, which only provides train and dev subsets. Consequently, we report the model performances for the dev dataset. As shown in the table, we achieve the best results with 8 clusters in listwise learning and 16 clusters in pointwise learning. In both cases, we achieve no additional performance gain after 16 clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this study, our proposed method achieves state-of-the-art performance for both the WikiQA dataset and TREC-QA dataset. We show that leveraging a large amount of data is crucial for capturing the contextual representation of input text. In addition, we show that the proposed latent clustering method with a pointwise objective function significantly improves the model performance in the sentence-level QA task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Properties of the dataset.</figDesc><table><row><cell>Dataset</cell><cell cols="3">Listwise pairs</cell><cell cols="3">Pointwise pairs</cell></row><row><cell></cell><cell>train</cell><cell>dev</cell><cell>test</cell><cell>train</cell><cell>dev</cell><cell>test</cell></row><row><cell>WikiQA</cell><cell>873</cell><cell>126</cell><cell>243</cell><cell>8.6k</cell><cell>1.1k</cell><cell>2.3k</cell></row><row><cell>TREC-QA</cell><cell>1.2k</cell><cell>65</cell><cell>68</cell><cell>53k</cell><cell>1.1k</cell><cell>1.4k</cell></row><row><cell>QNLI</cell><cell>86k</cell><cell>10k</cell><cell>-</cell><cell cols="2">428k 169k</cell><cell>-</cell></row><row><cell cols="7">where N is the total number of samples used during training. Using</cell></row><row><cell cols="7">this approach, the number of training instances for a single iteration</cell></row><row><cell cols="3">increases, as shown in table 1.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>QNLI<ref type="bibr" target="#b12">[13]</ref> is a modified version of the SQuAD dataset<ref type="bibr" target="#b6">[7]</ref> that allows for sentence selection QA. The context paragraph in SQuAD is split into sentences, and each sentence is paired with the question. The true label is given to the question-sentence pairs when the sentence contains the answer. There are 86,308/10,385 questions and 428,998/169,435 question-answer pairs for train/dev split. Considering the large size of this dataset, we use it to train the base model for transfer learning; it is also used to evaluate the proposed model performance in a large dataset environment.</figDesc><table><row><cell>There are 873/126/243 ques-</cell></row><row><cell>tions and 8,627/1,130/2,351 question-answer pairs for train/dev/test</cell></row><row><cell>split.</cell></row><row><cell>TREC-QA [14] is another answer selection QA dataset created</cell></row><row><cell>from the TREC Question-Answering tracks. In this study, we use</cell></row><row><cell>the clean dataset that removed questions from the dev and test</cell></row><row><cell>datasets that did not have answers or had only positive/negative</cell></row><row><cell>answers. There are 1,229/65/68 questions and 53,417/1,117/1,442</cell></row><row><cell>question-answer pairs for train/dev/test split.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Model performance (the top 3 scores are marked in bold for each task). We evaluate model<ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15]</ref> on the WikiQA corpus using author's implementation (marked by *). For TREC-QA case, we present reported results in the original papers.</figDesc><table><row><cell></cell><cell cols="2">WikiQA</cell><cell></cell><cell></cell><cell cols="2">TREC-QA</cell><cell></cell></row><row><cell>Model</cell><cell>MAP</cell><cell>MRR</cell><cell></cell><cell>MAP</cell><cell></cell><cell>MRR</cell><cell></cell></row><row><cell>dev</cell><cell>test</cell><cell>dev</cell><cell>test</cell><cell>dev</cell><cell>test</cell><cell>dev</cell><cell>test</cell></row><row><cell>Compare-Aggregate (2017)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Model (Comp-Clip +LM +LC) performance on the QNLI corpus with a variant number of clusters (top score marked as bold). LM + LC + TL).</figDesc><table><row><cell># Clusters</cell><cell cols="4">Listwise Learning Pointwise Learning</cell></row><row><cell></cell><cell>MAP</cell><cell>MRR</cell><cell>MAP</cell><cell>MRR</cell></row><row><cell>1</cell><cell>0.822</cell><cell>0.819</cell><cell>0.842</cell><cell>0.841</cell></row><row><cell>4</cell><cell>0.839</cell><cell>0.840</cell><cell>0.846</cell><cell>0.845</cell></row><row><cell>8</cell><cell>0.841</cell><cell>0.842</cell><cell>0.846</cell><cell>0.846</cell></row><row><cell>16</cell><cell>0.840</cell><cell>0.842</cell><cell>0.847</cell><cell>0.846</cell></row><row><cell cols="5">in terms of the MAP (0.764 to 0.834 absolute) by including the TL</cell></row><row><cell cols="2">approach (Comp-Clip +</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://aclweb.org/aclwiki/Question_Answering_(State_of_the_art)</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A compareaggregate model with dynamic-clip attention for answer selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM. ACM</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1987" to="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks for Sentence Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nissan</forename><surname>Pow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGDIAL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="285" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Integrating Question Classification and Deep Learning for improved Answer Selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Harish Tayyar Madabushi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barnden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3283" to="3294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep Contextualized Word Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ Questions for Machine Comprehension of Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santos</forename><surname>Cicero Dos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.03609</idno>
		<title level="m">Attentive pooling networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Inter-weighted alignment network for sentence pair modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gehui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunlun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1179" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Lstmbased deep learning models for non-factoid answer selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Cicero Dos Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04108</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-Cast Attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu Cheung</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD. ACM</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2299" to="2308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The Context-dependent Additive Recurrent Neural Net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan</forename><surname>Quan Hung Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingrid</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trung</forename><surname>Zukerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1274" to="1283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel R</forename><surname>Bowman</surname></persName>
		</author>
		<idno>EMNLP. 353</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">What is the Jeopardy model? A quasi-synchronous grammar for QA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teruko</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A compare-aggregate model for matching text sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bilateral multi-perspective matching for natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wael</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 26th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4144" to="4150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Wikiqa: A challenge dataset for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yih</forename><surname>Wen-Tau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
