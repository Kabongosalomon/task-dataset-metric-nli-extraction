<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Inductive Graph-Level Representation Learning via Graph-Graph Proximity</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsheng</forename><surname>Bai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ding</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Purdue University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Qiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agustin</forename><surname>Marinovic</surname></persName>
							<email>amarinovic@ucla.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Gu</surname></persName>
							<email>ken.qgu@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
							<email>weiwang@cs.ucla.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Inductive Graph-Level Representation Learning via Graph-Graph Proximity</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a novel approach to graph-level representation learning, which is to embed an entire graph into a vector space where the embeddings of two graphs preserve their graph-graph proximity. Our approach, UGRAPHEMB, is a general framework that provides a novel means to performing graph-level embedding in a completely unsupervised and inductive manner. The learned neural network can be considered as a function that receives any graph as input, either seen or unseen in the training set, and transforms it into an embedding. A novel graph-level embedding generation mechanism called Multi-Scale Node Attention (MSNA), is proposed. Experiments on five real graph datasets show that UGRAPHEMB achieves competitive accuracy in the tasks of graph classification, similarity ranking, and graph visualization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent years we have witnessed the great popularity of graph representation learning with success in not only node-level tasks such as node classification <ref type="bibr">[Kipf and Welling, 2016a]</ref> and link prediction <ref type="bibr">[Zhang and Chen, 2018</ref>], but also graphlevel tasks such as graph classification <ref type="bibr">[Ying et al., 2018]</ref> and graph similarity/distance computation <ref type="bibr" target="#b0">[Bai et al., 2019]</ref>.</p><p>There has been a rich body of work <ref type="bibr" target="#b0">[Belkin and Niyogi, 2003;</ref><ref type="bibr">Tang et al., 2015;</ref><ref type="bibr">Qiu et al., 2018]</ref> on node-level embeddings that turn each node in a graph into a vector preserving node-node proximity (similarity/distance). Most of these models are unsupervised and demonstrate superb performance in node classification and link prediction. It is natural to raise the question: Can we embed an entire graph into a vector in an unsupervised way, and how? However, most existing methods for graph-level embeddings assume a supervised model <ref type="bibr">[Ying et al., 2018;</ref><ref type="bibr">Zhang and Chen, 2019]</ref>, with only a few exceptions, such as GRAPH KERNELS <ref type="bibr">[Yanardag and Vishwanathan, 2015]</ref> and GRAPH2VEC <ref type="bibr">[Narayanan et al., 2017]</ref>. GRAPH KERNELS typically count subgraphs for a given graph and can be slow. GRAPH2VEC is transductive (non-inductive), i.e. it does not naturally generalize to unseen graphs outside the training set.</p><p>A key challenge facing designing an unsupervised graphlevel embedding model is the lack of graph-level signals in the training stage. Unlike node-level embedding which has a long history in utilizing the link structure of a graph to embed nodes, there lacks such natural proximity (similarity/distance) information between graphs. Supervised methods, therefore, typically resort to graph labels as guidance and use aggregation based methods, e.g. average of node embeddings, to generate graph-level embeddings, with the implicit assumption that good node-level embeddings would automatically lead to good graph-level embeddings using only "intra-graph information" such as node attributes, link structure, etc.</p><p>However, this assumption is problematic, as simple aggregation of node embeddings can only preserve limited graphlevel properties, which is, however, often insufficient in measuring graph-graph proximity ("inter-graph" information). Inspired by the recent progress on graph proximity modeling <ref type="bibr">[Ktena et al., 2017;</ref><ref type="bibr" target="#b0">Bai et al., 2019]</ref>, we propose a novel framework, UGRAPHEMB ( Unsupervised Graph-level Embbedding) that employs multi-scale aggregations of nodelevel embeddings, guided by the graph-graph proximity defined by well-accepted and domain-agnostic graph proximity metrics such as Graph Edit Distance (GED) <ref type="bibr">[Bunke, 1983]</ref>, Maximum Common Subgraph (MCS) <ref type="bibr">[Bunke and Shearer, 1998</ref>], etc.</p><p>The goal of UGRAPHEMB is to learn high-quality graphlevel representations in a completely unsupervised and inductive fashion: During training, it learns a function that maps a graph into a universal embedding space best preserving graphgraph proximity, so that after training, any new graph can be mapped to this embedding space by applying the learned function. Inspired by the recent success of pre-training methods in the text domain, such as <ref type="bibr">ELMO [Peters et al., 2018]</ref>, <ref type="bibr">BERT [Devlin et al., 2018]</ref>, and <ref type="bibr">GPT [Radford et al., 2018]</ref>. we further fine-tune the model via incorporating a supervised loss, to obtain better performance in downstream tasks, including but not limited to:</p><p>• Graph classification. The embeddings can be fed into any classification model such as logistic regression for graph classification.</p><p>• Graph similarity ranking. The embeddings learned by UGRAPHEMB preserve the graph-graph proximity by design, and for a graph query, a ranked list of graphs that arXiv:1904.01098v2 <ref type="bibr">[cs.</ref>LG] 2 Jun 2019 are similar to the query can be retrieved.</p><p>• Graph visualization. The embeddings can be projected into a 2-D space for graph visualization, where each graph is a point. It renders human insights into the dataset and facilitates further database analysis.</p><p>In summary, our contributions are three-fold:</p><p>1. We formulate the problem of unsupervised inductive graph-level representation learning, and make an initial step towards pre-training methods for graph data. We believe that, given the growing amount of graph datasets of better quality, this work would benefit future works in pre-training methods for graphs.</p><p>2. We provide a novel framework, UGRAPHEMB, to generate graph-level embeddings in a completely unsupervised and inductive fashion, well preserving graph proximity. A novel Multi-Scale Node Attention (MSNA) mechanism is proposed to generate graph-level embeddings.</p><p>3. We conduct extensive experiments on five real network datasets to demonstrate the superb quality of the embeddings by UGRAPHEMB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Proposed Approach: UGRAPHEMB</head><p>We present the overall architecture of our unsupervised inductive graph-level embedding framework UGRAPHEMB in <ref type="figure" target="#fig_3">Figure 4</ref>. The key novelty of UGRAPHEMB is the use of graph-graph proximity. To preserve the proximity between two graphs, UGRAPHEMB generates one embedding per graph from node embeddings using a novel mechanism called Multi-Scale Node Attention (MSNA), and computes the proximity using the two graph-level embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Inductive Graph-Level Embedding</head><p>Node Embedding Generation For each graph, UGRAPHEMB first generates a set of node embeddings. There are two major properties that the node embedding model has to satisfy:</p><p>• Inductivity. The model should learn a function such that for any new graph unseen in the training set, the learned function can be applied to the graph, yielding its node embeddings.</p><p>• Permutation-invariance. The same graph can be represented by different adjacency matrices by permuting the order of nodes, and the node embedding model should not be sensitive to such permutation.</p><p>Among various node embedding models, neighbor aggregation methods based on Graph Convolutional Networks (GCN) <ref type="bibr">[Kipf and Welling, 2016a]</ref> are permutation-invariant and inductive. The reason is that the core operation, graph convolution, updates the representation of a node by aggregating the embedding of itself and the embeddings of its neighbors. Since the aggregation function treats the neighbors of a node as a set, the order does not affect the result.</p><p>A series of neighbor aggregation methods have been proposed with different ways to aggregate neighbor information, e.g. <ref type="bibr">GRAPHSAGE [Hamilton et al., 2017]</ref>, <ref type="bibr">GAT [Velickovic et al., 2018]</ref>, <ref type="bibr">GIN [Xu et al., 2019]</ref>, etc. Since UGRAPHEMB is a general framework for graph-level embeddings, and all these models satisfy the two properties, any one of these methods can be integrated. We therefore adopt the most recent and state-of-the-art method, Graph Isomorphism Network (GIN) <ref type="bibr">[Xu et al., 2019]</ref>, in our framework:</p><formula xml:id="formula_0">u (k) i = MLP (k) W k   1 + (k) · u (k−1) i + j∈N (i) u (k−1) j   (1) where u i is the representation of node i, N (i) is the set of neighbors of node i, MLP (k)</formula><p>W k denotes multi-layer perceptrons for the k-th GIN layer with learnable weights W k , and is a scalar that can either be learned by gradient descent or be a hyperparameter. GIN has been proven to be theoretically the most powerful GNN under the neighbor aggregation framework <ref type="bibr">[Xu et al., 2019]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Embedding Generation</head><p>After node embeddings are generated, UGRAPHEMB generates one embedding per graph using these node embeddings. Existing methods are typically based on aggregating node embeddings, by either a simple sum or average, or some more sophisticated way to aggregate.</p><p>However, since our goal is to embed each graph as a single point in the embedding space that preserves graph-graph proximity, the graph embedding generation model should:</p><p>• Capture structural difference at multiple scales. Applying a neighbor aggregation layer on nodes such as GIN cause the information to flow from a node to its direct neighbors, so sequentially stacking K layers would cause the final representation of a node to include information from its K-th order neighbors. However, after many neighbor aggregation layers, the learned embeddings could be too coarse to capture the structural difference in small local regions between two similar graphs. Capturing structural difference at multiple scales is therefore important for UGRAPHEMB to generate high-quality graph-level embeddings. • Be adaptive to different graph proximity metrics.</p><p>UGRAPHEMB is a general framework that should be able to preserve the graph-graph proximity under any graph proximity metric, such as GED and MCS. A simple aggregation of node embeddings without any learnable parameters limits the expressive power of existing graphlevel embedding models. To tackle both challenges in the graph embedding generation layer, we propose the following Multi-Scale Node Attention (MSNA) mechanism. Denote the input node embeddings of graph G as U G ∈ R N ×D , where the n-th row, u n ∈ R D is the embedding of node n. The graph level embedding is obtained as follows:</p><formula xml:id="formula_1">h G = MLP W K k=1 ATT Θ (k) (U G )<label>(2)</label></formula><p>...  <ref type="figure">Figure 1</ref>: Overview of UGRAPHEMB. (a) Given a set of graphs, (b) UGRAPHEMB first computes the graph-graph proximity scores (normalized distance scores in this example), (c) yielding a "hyper-level graph" where each node is a graph in the dataset, and each edge has a proximity score associated with it, representing its weight/strength. UGRAPHEMB then trains a function that maps each graph into an embedding which preserves the proximity score. The bottom flow illustrates the details of graph-level embedding generation. (d) After embeddings are generated, similarity ranking can be performed. The green "+" sign denotes the embedding of an example query graph. Colors of dots indicate how similar a graph is to the query based on the ground truth (from red to blue, meaning from the most similar to the least similar). (e) Finally, UGRAPHEMB can perform fine-tuning on the proximity-preserving graph-level embeddings, adjusting them for the task of graph classification. Different colors represent different graph labels in the classification task.</p><p>where denotes concatenation, K denotes the number of neighbor aggregation layers, ATT denotes the following multihead attention mechanism that transforms node embeddings into a graph-level embedding, and MLP W denotes multilayer perceptrons with learnable weights W applied on the concatenated attention results. The intuition behind Equation 2 is that, instead of only using the node embeddings generated by the last neighbor aggregation layer, we use the node embeddings generated by each of the K neighbor aggregation layers.</p><p>ATT is defined as follows:</p><formula xml:id="formula_2">ATT Θ (U G ) = N n=1 σ(u T n ReLU(Θ( 1 N N m=1 u m )))u n . (3)</formula><p>where N is the number of nodes, σ is the sigmoid function σ(x) = 1 1+exp (−x) , and Θ (k) ∈ R D×D is the weight parameters for the k-th node embedding layer.</p><p>The intuition behind Equation 3 is that, during the generation of graph-level embeddings, the attention weight assigned to each node should be adaptive to the graph proximity metric. To achieve that, the weight is determined by both the node embedding u n , and a learnable graph representation. The learnable graph representation is adaptive to a particular graph proximity via the learnable weight matrix Θ (k) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Unsupervised Loss via Inter-Graph Proximity Preservation Definition of Graph Proximity</head><p>The key novelty of UGRAPHEMB is the use of graph-graph proximity. It is important to select an appropriate graph proximity (similarity/distance) metric. We identify three categories of candidates:</p><p>• Proximity defined by graph labels.</p><p>For graphs that come with labels, we may treat graphs of the same label to be similar to each other. However, such proximity metric may be too coarse, unable to distinguish between graphs of the same label. • Proximity given by domain knowledge or human experts.</p><p>For example, in drug-drug interaction detection <ref type="bibr">[Ma et al., 2018]</ref>, a domain-specific metric to encode compound chemical structure can be used to compute the similarities between chemical graphs. However, such metrics do not generalize to graphs in other domains. Sometimes, this information may be very expensive to obtain. For example, to measure brain network similarities, a domainspecific preprocessing pipeline involving skull striping, band-pass filtering, etc. is needed. The final dataset only contains networks from 871 humans <ref type="bibr">[Ktena et al., 2017]</ref>. • Proximity defined by domain-agnostic and wellaccepted metrics. Metrics such as Graph Edit Distance (GED) <ref type="bibr">[Bunke, 1983]</ref> and Maximum Common Subgraph (MCS) <ref type="bibr">[Bunke and Shearer, 1998</ref>] have been widely adopted in graph database search <ref type="bibr">[Yan et al., 2005;</ref><ref type="bibr">Liang and Zhao, 2017]</ref>, are well-defined and general to any domain. In this paper, we use GED as an example metric to demonstrate UGRAPHEMB. GED measures the minimum number of edit operations to transform one graph to the other, where an edit operation on a graph is an insertion or deletion of a node/edge or relabelling of a node. Thus, the GED metric takes both the graph structure and the node labels/attributes into account. The supplementary material contain more details on GED.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prediction of Graph Proximity</head><p>Once the proximity metric is defined, and the graph-level embeddings for G i and G j are obtained, denoted as h Gi and h Gj , we can compute the similarity/distance between the two graphs.</p><p>Multidimensional scaling (MDS) is a classic form of dimensionality reduction <ref type="bibr">[Williams, 2001]</ref>. The idea is to embed data points in a low dimensional space so that their pairwise distances are preserved, e.g. via minimizing the loss function</p><formula xml:id="formula_3">L(h i , h j , d ij ) = (||h i − h j || 2 2 − d ij ) 2 (4)</formula><p>where h i and h j are the embeddings of points i and j, and d ij is their distance. Since GED is a well-defined graph distance metric, we can minimize the difference between the predicted distance and the ground-truth distance:</p><formula xml:id="formula_4">L = E (i,j)∼D (d ij − d ij ) 2 (5) = E (i,j)∼D (||h Gi − h Gj || 2 2 − d ij ) 2 .<label>(6)</label></formula><p>where (i, j) is a graph pair sampled from the training set and d ij is the GED between them. Alternatively, if the metric is similarity, such as in the case of MCS, we can use the following loss function:</p><formula xml:id="formula_5">L = E (i,j)∼D (ŝ ij − s ij ) 2 (7) = E (i,j)∼D (h T Gi h Gj − s ij ) 2 .<label>(8)</label></formula><p>After training, the learned neural network can be applied to any graph, and the graph-level embeddings can facilitate a series of downstream tasks, and can be fine-tuned for specific tasks. For example, for graph classification, a supervised loss function can be used to further enhance the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We evaluate our model, UGRAPHEMB, against a number of state-of-the-art approaches designed for unsupervised node and graph embeddings, to answer the following questions: Q1 How superb are the graph-level embeddings generated by UGRAPHEMB, when evaluated with downstream tasks including graph classification and similarity ranking? Q2 Do the graph-level embeddings generated by UGRAPHEMB provide meaningful visualization for the graphs in a graph database? Q3 Is the quality of the embeddings generated by UGRAPHEMB sensitive to choices of hyperparamters? Datasets We evaluate the methods on five real graph datasets, PTC, IMDBM, WEB, NCI109, and REDDIT12K. The largest dataset, REDDIT12K, has 11929 graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task 1: Graph Classification</head><p>Intuitively, the higher the quality of the embeddings, the better the classification accuracy. Thus, we feed the graph-level embeddings generated by UGRAPHEMB and the baselines into a logistic regression classifier to evaluate the quality: For GRAPH KERNELS, we also try using the kernel matrix and SVM classifier as it is the standard procedure outlined  <ref type="table">Table 1</ref>: Graph classification accuracy in percent. "-" indicates that the computation did not finish after 72 hours. We highlight the top 2 accuracy in bold.</p><p>in <ref type="bibr">[Yanardag and Vishwanathan, 2015]</ref>, and report the better accuracy of the two. For <ref type="formula">(3)</ref> and <ref type="formula">(4)</ref>, we try different averaging schemes on node embeddings to obtain the graph-level embeddings and report their best accuracy. As shown in <ref type="table">Table 1</ref>, UGRAPHEMB without fine-tuning, i.e. using only the unsupervised "inter-graph" information, can already achieve top 2 on 3 out of 5 datasets and demonstrates competitive accuracy on the other datasets. With finetuning (UGRAPHEMB-F), our model can achieve the best result on 4 out of 5 datasets. Methods specifically designed for graph-level embeddings (GRAPH KERNELS, GRAPH2VEC, and UGRAPHEMB) consistently outperform methods designed for node-level embeddings (NETMF and GRAPHSAGE), suggesting that good node-level embeddings do not naturally imply good graph-level representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Task 2: Similarity Ranking</head><p>For each dataset, we split it into training, validation, and testing sets by 6:2:2, and report the averaged Mean Squared Error (mse), Kendall's Rank Correlation Coefficient (τ ) <ref type="bibr">[Kendall, 1938]</ref>, and Precision at 10 (p@10) to test the ranking performance. <ref type="table">Table 2</ref> shows that UGRAPHEMB achieves state-of-theart ranking performance under all settings except one. This should not be a surprise, because only UGRAPHEMB utilizes the ground-truth GED results collectively determined by <ref type="bibr">BEAM [Neuhaus et al., 2006]</ref>, <ref type="bibr">HUNGARIAN [Riesen and</ref><ref type="bibr">Bunke, 2009], and</ref><ref type="bibr">VJ [Fankhauser et al., 2011]</ref>. UGRAPHEMB even outperforms HED <ref type="bibr">[Fischer et al., 2015]</ref>, a state-of-the-art approximate GED computation algorithm, under most settings, further confirming its strong ability to generate proximity-preserving graph embeddings by learning from a specific graph proximity metric, which is GED in this case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Task 3: Embedding Visualization</head><p>Visualizing the embeddings on a two-dimensional space is a popular way to evaluate node embedding methods <ref type="bibr">[Tang et al., 2015]</ref>. However, we are among the first to investigate the question: Are the graph-level embeddings generated by a model like UGRAPHEMB provide meaningful visualization?</p><p>We feed the graph emebddings learned by all the methods into the visualization tool t-SNE <ref type="bibr">[Maaten and Hinton, 2008]</ref>. The three deep graph kernels, i.e. DGK, DSP, and WDL,  <ref type="table">Table 2</ref>: Similarity ranking performance. BEAM, HUNGARIAN, and VJ are three approximate GED computation algorithms returning upper bounds of exact GEDs. We take the minimum GED computed by the three as ground-truth GEDs for training and evaluating all the methods on both Task 1 and 2. Their results are labeled with " * ". HED is another GED solver yielding lower bounds. "-" indicates that the computation did not finish after 72 hours.</p><formula xml:id="formula_6">Method PTC IMDBM WEB NCI109 REDDIT12K τ p@10 τ p@10 τ p@10 τ p@10 τ p@10 GK 0.</formula><p>generate the same embeddings as the non-deep versions, but use additional techniques <ref type="bibr">[Yanardag and Vishwanathan, 2015]</ref> to modify the similarity kernel matrices, resulting in different classification and ranking performance. From <ref type="figure">Figure 2</ref>, we can see that UGRAPHEMB can separate the graphs in IMDBM into multiple clusters, where graphs in each cluster share some common substructures.</p><p>Such clustering effect is likely due to our use of graphgraph proximity scores, and is thus not observed in NETMF or GRAPHSAGE. For GRAPH KERNELS and GRAPH2VEC though, there are indeed clustering effects, but by examining the actual graphs, we can see that graph-graph proximity is not well-preserved by their clusters (e.g. for WL graph 1, 2 and 9 should be close to each other; for GRAPH2VEC, graph 1, 2, and 12 should be close to each other), explaining their worse similarity ranking performance in <ref type="table">Table 2</ref> compared to UGRAPHEMB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Parameter Sensitivity of UGRAPHEMB</head><p>We evaluate how the dimension of the graph-level embeddings and the percentage of graph pairs with ground-truth GEDs used to train the model can affect the results. We report the graph classification accuracy on IMDBM.</p><p>As can be seen in <ref type="figure" target="#fig_1">Figure 3</ref>, the performance becomes marginally better if larger dimensions are used. For the percentage of training pairs with ground-truth GEDs, the performance drops as less pairs are used. Note that the x-axis is in log-scale. When we only use 0.001% of all the training graph pairs (only 8 pairs with ground-truth GEDs), the performance is still better than many baseline methods, exhibiting impressive insensitivity to data sparsity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Unsupervised graph representation learning has a long history. Classic works including NETMF <ref type="bibr">[Qiu et al., 2018]</ref>, <ref type="bibr">LINE [Tang et al., 2015]</ref>, DeepWalk <ref type="bibr">[Perozzi et al., 2014]</ref>, etc., which typically generate an embedding for each node in one graph. Theoretical analysis shows that many of these works cannot handle embeddings for multiple graphs in the sense that the node embeddings in one graph are not comparable to those in another graph in any straightforward way <ref type="bibr">[Heimann and Koutra, 2017]</ref>. A simple permutation of node indices could cause the node embedding to be very different.</p><p>More recently, some of the methods based on Graph Convolutional Networks (GCN) <ref type="bibr">[Defferrard et al., 2016;</ref><ref type="bibr">Kipf and Welling, 2016a]</ref>, such as VGAE <ref type="bibr">[Kipf and Welling, 2016b]</ref>, satisfy the desired permutation-invariance property. Categorized as "graph autoencoders" <ref type="bibr">[Wu et al., 2019]</ref>, they also belong to the family of graph neural network methods. Although satisfying the permutation-invariance requirement, these autoencoders are still designed to generate unsuperised node embeddings.</p><p>Methods designed for unsupervised graph-level embeddings include GRAPH2VEC <ref type="bibr">[Narayanan et al., 2017]</ref> and GRAPH <ref type="bibr">KERNELS [Yanardag and Vishwanathan, 2015]</ref>, which however are either not based on learning or not inductive. Unlike node-level information which is reflected in the neighborhood of a node, graph-level information is much more limited. A large amount of graph neural network models resort to graph labels as a source of such information, making the models supervised aiming to improve graph classification accuracy specifically, such as DIFFPOOL <ref type="bibr">[Ying et al., 2018]</ref>, CAPS-GNN [Zhang and Chen, 2019], etc., while UGRAPHEMB learns a function that maps each graph into an embedding that can be used to facilitate many downstream tasks. 1 2 3 4 56 7 8 9 10 11 12 1 2 3 4 5 6 7 8 9 10 11 12 12 3 4 5 6 7 8 9 10 11 12 (a) GK (b) SP Figure 2: Visualization of the IMDBM dataset. From (a) to (g), for each method, 12 graphs are plotted. For (h) to (l), we focus on UGRAPHEMB: 5 clusters are highlighted in red circles. 12 graphs are sampled from each cluster and plotted to the right.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Comparison with Existing Frameworks</head><p>To better see the novelty of our proposed framework, UGRAPHEMB, we present a detailed study on two related existing frameworks for node and graph embeddings. As shown in <ref type="figure" target="#fig_3">Figure 4</ref>, we summarize graph neural network architectures for learning graph representations into three frameworks:</p><p>• Framework 1: Supervised/Unsupervised framework for nodelevel tasks, e.g. node classification, link prediction, etc.</p><p>• Framework 2: Supervised end-to-end neural networks for graph-level tasks, typically graph classification.</p><p>• Framework 3: UGRAPHEMB, unsupervised framework for multiple graph-level tasks with the key novelty in using graphgraph proximity.</p><p>The rest of this section describes the first two frameworks in detail, and compare UGRAPHEMB with various other related methods when appropriate. This section also serves as a more thorough survey of graph embedding methods, proving more background knowledge in the area of node and graph representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Framework 1: Node Embedding (Supervised and Unsupervised)</head><p>Since the goal is to perform node-level tasks, the key is the "Node Embedding Model" which produces one embedding per node for the input graph. As described in the main paper, there are many methods to obtain such node embeddings, such as:</p><p>•  <ref type="bibr">., 2017]</ref>. For WORD2VEC though, it is typically not a concern since out-of-vocabulary words tend to be rare in a large text corpus. This also reveals a fundamental difference between the text domain and graph domain -words have their specific semantic meaning making them identifiable across different documents, yet nodes in a graph or graphs usually lack such identity. This calls for the need of inductive representation learning, which is addressed more recently by the next type of node embedding model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Graph Convolution (Neighbor Aggregation):</head><p>As discussed in the main paper, Graph Convolutional Network (GCN) <ref type="bibr">[Defferrard et al., 2016]</ref> boils down to the aggregation operation that is applied to every node in a graph. This essentially allows the neural network model to learn a function that maps input graph to output node embeddings:</p><formula xml:id="formula_7">φ(G) = φ(AG, FG) = UG.<label>(9)</label></formula><p>where AG and FG denote the adjacency matrix (link structure) and the node and/or edge features/attributes, and U G ∈ R N ×D is the node embedding matrix.</p><p>The importance of such function φ is evident -for any new node i outside the training set of nodes, the neighbor aggregation models can simply apply the learned φ to obtain u i ; for any new graph outside the training set of graphs, the same procedure also works, achieving inductivity. Permutation-invariance can also be achieved as discussed in the main paper. So far we have discussed about the node embedding generation step. In order to make the node embeddings high-quality, additional components are usually necessary as auxiliaries/guidance in the architectures of methods belong to Framework 1, including:</p><p>• Predict Node Context:</p><p>The goal is to use the node embeddings to reconstruct certain "node local context" -in other words, to force the node embeddings to preserver certain local structure. We highlight three popular types of definitions of such context:</p><p>-1st order neighbor:</p><p>The model encourages directly connected nodes to have similar embeddings. In LINE-1ST, the loss function is similar to the Skip-gram objective proposed in WORD2VEC. In <ref type="bibr">SDNE [Wang et al., 2016]</ref>, an autoencoder framework, the loss function is phrased as the reconstruction loss, the typical name in auto-encoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>-Higher order context:</head><p>An example is LINE-2ND, which assumes that nodes sharing many connections to other nodes are similar to each other. In practice, such incorporation of higher order neighbors typically gives better performance in node-level tasks. -Random walk context:</p><p>"Context nodes" are defined as the nodes that co-occur on random walks on a graph. By this definition, for a given node, its context can include both its close-by neighbors and distant node. Equipped with various techniques of tuning and improving upon random walks, this type of methods seems promising. Example methods include DEEPWALK, NODE2VEC <ref type="bibr">[Grover and Leskovec, 2016]</ref>, GRAPH-SAGE, etc. Notice that the former two use direct encoding as its node embedding model as described previously, while GRAPHSAGE uses neighbor aggregation. From this, we can also see that Framework 1 indeed includes a vast amount of models and architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Predict Node Label:</head><p>So far all the methods we have discussed about are unsupervised node embedding methods. As said in the main paper, to evaluate these unsupervised node embeddings, a second stage is needed, which can be viewed as a series of downstream tasks as listed in <ref type="figure" target="#fig_3">Figure 4</ref>.  Before finishing presenting Framework 1, we highlight important distinctions between the proposed framework and the following baseline methods:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UGRAPHEMB vs NETMF</head><p>NETMF is among the state-of-the-art matrix factorization based methods for node embeddings. It performs eigen-decomposition, one-side bounding, and rank-d approximation by Singular Value Decomposition, etc. for a graph, and is transductive. UGRAPHEMB is graph-level and inductive. Section E.1 gives more details on how we obtain graph-level embeddings out of node-level embeddings for NETMF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UGRAPHEMB vs GRAPHSAGE</head><p>GRAPHSAGE belongs to neighbor aggregation based methods. Although being unsupervised and inductive, by design GRAPHSAGE performs node-level embeddings via an unsupervised loss based on context nodes on random walks (denoted as "Random walk context" as in <ref type="figure" target="#fig_3">Figure 4</ref>), while UGRAPHEMB performs graph-level embeddings via the MSNA mechanism, capturing structural difference at multiple scales and adaptive to a given graph similarity/distance metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Framework 2: Supervised Graph Embedding</head><p>The second framework we identify as supervised graph embedding. So far graph classification is the dominating and perhaps the only important task for Frmaework 2.</p><p>Here we highlight some existing works to demonstrate its popularity, including <ref type="bibr">PATCHYSAN [Niepert et al., 2016]</ref> Notice that most of these models adopt the neighbor aggregation based node embedding methods described previously, which are inductive so that for new graphs outside the training set of graphs, their graph-level embeddings can be generated, ans graph classification can be performed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UGRAPHEMB vs GRAPH KERNELS</head><p>Although GRAPH KERNELS <ref type="bibr">[Yanardag and Vishwanathan, 2015]</ref> are not supervised methods, we still make a comparison here, because GRAPH KERNELS are a family of methods designed for graph classification, the same task as Framework 2.</p><p>Different graph kernels extract different types of substructures in a graph, e.g. graphlets <ref type="bibr">[Shervashidze et al., 2009]</ref>, subtree patterns <ref type="bibr">[Shervashidze and Borgwardt, 2009]</ref>, etc., and the resulting vector representation for each graph is typically called "feature vector" <ref type="bibr">[Yanardag and Vishwanathan, 2015]</ref>, encoding the count/frequency of substructures. These feature vectors are analogous to graph-level embeddings, but the end goal is to create a kernel matrix encoding the similarity between all the graph pairs in the dataset fed into a kerkenl SVM classifier for graph classification.</p><p>Compared with graph kernels, UGRAPHEMB learns a function that preserves a general graph similarity/distance metric such as Graph Edit Distance (GED), and as a result, yields a graph-level embedding for each graph that can be used to facilitate a series of downstream tasks. It is inductive, i.e. handles unseen graphs due to the learned function. In contrast, although GRAPH KERNELS can be considered as inductive <ref type="bibr">[Shervashidze et al., 2011]</ref>, graph kernels have to perform the subgraph extraction for every graph, which can be slow and cannot adapt to different graph proximity metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UGRAPHEMB vs GRAPH2VEC</head><p>Similar to DEEPWALK, GRAPH2VEC is also inspired by the classic WORD2VEC paper, but instead of generating node embeddings, it is designed to generate graph-level embeddings, by treating each graph as a bag of rooted subgraphs, and adopting DOC2VEC <ref type="bibr">[Mikolov et al., 2013]</ref> instead of WORD2VEC. The difference between GRAPH2VEC and UGRAPHEMB is that, GRAPH2VEC is transduc-tive (similar to GRAPH KERNELS), as explained in Section A.1, while UGRAPHEMB is inductive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Framework 3: UGRAPHEMB</head><p>This is our proposed framework, which is the key novelty of the paper. Now since we have introduced Framework 1 and Framework 2, it can be clearly seen that the use of graph-graph proximity is a very different and new perspective of performing graph-level embeddings. UGRAPHEMB satifies all the following properties: graphlevel, unsupervised, and inductive.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Graph Edit Distance (GED)</head><p>The edit distance between two graphs <ref type="bibr">[Bunke, 1983]</ref> G1 and G2 is the number of edit operations in the optimal alignments that transform G1 into G2, where an edit operation on a graph G is an insertion or deletion of a node/edge or relabelling of a node. Note that other variants of GED definitions exist <ref type="bibr">[Riesen et al., 2013]</ref>, and we adopt the most basic version in this work. <ref type="figure">Fig. 5</ref> shows an example of GED between two simple graphs. Notice that although UGRAPHEMB currently does not handle edge types, UGRAPHEMB is a general framework and can be extended to handle edge types, e.g. adapting the graph neural network described in <ref type="bibr">[Kipf et al., 2018]</ref>. <ref type="figure">Figure 5</ref>: The GED between the graph to the left and the graph to the right is 4, involving the following edit operation sequence: A node addition, an edge addition, a node label substitution, and an edge deletion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Datasets C.1 Detailed Description of Datasets</head><p>Five real graph datasets are used for the experiments. A concise summary can be found in <ref type="table">Table 4</ref>.</p><p>• PTC [Shrivastava and Li, 2014] is a collection of 344 chemical compounds which report the carcinogenicity for rats. There are 19 node labels for each node.</p><p>• IMDBM [Yanardag and Vishwanathan, 2015] consists of 1500 ego-networks of movie actors/actresses with unlabeled nodes representing the people and edges representing the collaboration relationship. The nodes are unlabeled, but there could be 3 graph labels for each graph.</p><p>• WEB [Riesen and Bunke, 2008] is a collection of 2340 documents from 20 categories. Each node is a word, and there is an edge between two words if one word precedes the other. Since one word can appear in multiple sentences, the entire document is represented as a graph. Only the most frequent words are used to construct the graph, and there are 15507 words in total, thus 15507 node types associated with the dataset.</p><p>• NCI109 <ref type="bibr">[Wale et al., 2008]</ref> is another bioinformatics dataset.</p><p>It contains 4127 chemical compounds tested for their ability to suppress or inhibit human tumor cell growth.</p><p>• REDDIT12K [Yanardag and Vishwanathan, 2015] contains 11929 graphs each corresponding to an online discussion thread where nodes represent users, and an edge represents the fact that one of the two users responded to the comment of the other user. There is 1 of 11 graph labels associated with each of these 11929 discussion graphs, representing the category of the community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Additional Notes on WEB</head><p>Since each graph node in WEB represents a word, it is natural to consider incorporating the semantic similarity between two words, e.g. using WORD2VEC, into the GED definition, and even the broader topic of text matching and retrieval. In fact, the definition of GED does not specify that node labels must be discrete. There exists some variant of GED definition that can define node label difference in a more complicated way <ref type="bibr">[Riesen et al., 2013]</ref>, which is a promising direction to explore in future. It is also promising to explore document embedding based on graph representation of text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Data Preprocessing</head><p>For each dataset, we randomly split 60%, 20%, and 20% of all the graphs as training set, validation set, and testing set, respectively. For each graph in the testing set, we treat it as a query graph, and let the model compute the distance between the query graph and every graph in the training and validation sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Ground-Truth GED Computation</head><p>To compute ground-truth GED for training pair generation as well as similarity ranking evaluation, we have the following candidate GED computation algorithms:</p><p>• A* <ref type="bibr">[Hart et al., 1968]</ref>:</p><p>It is an exact GED solver, but due to the NP-hard nature of GED, it runs in exponential time complexity. What is worse, a recent study shows that no currently available algorithm can reliably compute GED within reasonable time between graphs with more than 16 nodes <ref type="bibr" target="#b0">[Blumenthal and Gamper, 2018]</ref>.  <ref type="table">Table 4</ref>: Statistics of datasets. "Min", "Max", "Mean", and "Std" refer to the minimum, maximum, mean, and standard deviation of the graph sizes (number of nodes), respectively.</p><p>We take the minimum distance computed by <ref type="bibr">BEAM [Neuhaus et al., 2006]</ref>, <ref type="bibr">HUNGARIAN [Riesen and</ref><ref type="bibr">Bunke, 2009], and</ref><ref type="bibr">VJ [Fankhauser et al., 2011]</ref>. The minimum is taken because their returned GEDs are guaranteed to be upper bounds of the true GEDs. In fact, the ICPR 2016 Graph Distance Contest 1 also adopts this approach to handle large graphs.</p><p>We normalize the GEDs according to the following formula: nGED(G1, G2) = GED(G 1 ,G 2 ) (|G 1 |+|G 2 |)/2 , where |Gi| denotes the number of nodes of <ref type="bibr">Gi [Qureshi et al., 2007]</ref>.</p><p>For the smaller datasets PTC, IMDBM, and WEB, we compute the ground-truth GEDs for all the pairs in the training set. For the larger datasets NCI109 and REDDIT12K, we do not compute all the pairs, and instead cap the computation at around 10 hours.</p><p>We run the ground-truth GED solvers on a CPU server with 32 cores, and utilize at most 20 cores at the same time, using code from <ref type="bibr">[Riesen et al., 2013]</ref>. The details are shown in <ref type="table" target="#tab_13">Table 5</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 "Hyper-Level" Graph</head><p>At this point, it is worth mentioning that the training procedure of UGRAPHEMB is stochastic, i.e. UGRAPHEMB is trained on a subset of graph pairs in each iteration. Moreover, UGRAPHEMB does not require the computation all the graph pairs in the training set, so the notion of "hyper-level" graph as mentioned in the main paper does not imply that UGRAPHEMB constructs a fully connected graph where each node is a graph in the dataset. In future, it would be promising to explore other techniques to construct such "hyper-level" graph beyond the current way of random selection of graph pairs in the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Node Label Encoding</head><p>For PTC, WEB, and NCI109, the original node representations are one-hot encoded according to the node labels. For graphs with unlabeled nodes, i.e., IMDBM and REDDIT12K, we treat every node to have the same label, resulting in the same constant number as the initialize representation.</p><p>In future, it would be interesting to consider more sophisticated ways to encode these node labels, because node labels help identifying different nodes across different graph datasets, which is an 1 https://gdc2016.greyc.fr/ important component for a successful pre-training method for graphs. Consider that we want to combine multiple different graph datasets of different domains for large-scale pre-training of graph neural networks. Then how to handle different node labels in different datasets and domains becomes an important issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Parameter Settings and Experimental Details</head><p>For the proposed model, to make a fair comparison with baselines, we use a single network architecture on all the datasets, and run the model using exactly the same test graphs as used in the baselines. We set the number of GIN layers to 3, and use ReLU as the activation function. The output dimensions for the 1st, 2nd, and 3rd layers of GIN are 256, 128, and 64, respectively. Following the original paper of <ref type="bibr">GIN [Xu et al., 2019]</ref>, we fix to 0.</p><p>Then we transform the concreted embeddings into graph-level embeddings of dimension 256 by using two fully connected (dense) layers, which are denoted as MLP in the main paper.</p><p>The model is written in TensorFlow <ref type="bibr">[Girija, 2016]</ref>. We conduct all the experiments on a single machine with an Intel i7-6800K CPU and one Nvidia Titan GPU. As for training, we set the batch size to 256, i.e. 256 graph pairs (512 graphs) per mini-batch, use the Adam algorithm for optimization <ref type="bibr">[Kingma and Ba, 2015]</ref>, and fix the initial learning rate to 0.001. We set the number of iterations to 20000, and select the best model based on the lowest validation loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Task 1: Graph Classification Evaluation Procedure</head><p>Since UGRAPHEMB is unsupervised, we evaluate all the methods following the standard strategy for evluating unsupervised node embeddings <ref type="bibr">[Tang et al., 2015;</ref><ref type="bibr">Wang et al., 2016]</ref>. It has three stages:</p><p>(1) Train a model using the training set with validation set for parameter tuning; (2) Train a standard logistic regression classifier using the embeddings as features as well as their ground-truth graph labels;</p><p>(3) Run the model on the graphs in the testing set and feed their embeddings into the classifier for label prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline Setup</head><p>By default, we use the results reported in the original work for baseline comparison. However, in cases where the results are not available, we use the code released by the original authors, performing a hyperparameter search based on the original author's guidelines. Notice that our baselines include a variety of methods of different flavors:</p><p>• GRAPH KERNELS:</p><p>For the GRAPH KERNELS baselines, there are two schemes to evaluate: (1) Treat the features extracted by each kernel method as the graph-level embeddings, and perform the second and third stages described previously;</p><p>(2) Feed the SVM kernels generated by each method into a kernel SVM classifier as in <ref type="bibr">[Yanardag and Vishwanathan, 2015]</ref>. The second scheme typically yields better accuracy and is more typical. We perform both schemes and report the better of the two accuracy scores for each baseline kernel. All the six versions of the GRAPH KER-NELS are described in detail in <ref type="bibr">[Yanardag and Vishwanathan, 2015]</ref>.</p><p>• GRAPH2VEC: Similar to GRAPH KERNELS, GRAPH2VEC is also transductive, meaning it has to see all the graphs in both the training set and the testing set, and generates a graph-level embedding for each.</p><p>• NETMF and GRAPHSAGE: Since they generate node-level embeddings, we take the average of node embeddings as the graph-level embedding. We also try various types of averaging schemes, including weighted by the node degree, weighted by the inverse of node degree, as well as summation. We report the best accuracy achieved by these schemes.</p><p>There is no training needed to be done for NETMF, since it is based on matrix factorization. For GRAPHSAGE, we combine all the graphs in the training set, resulting in one single graph to train GRAPHSAGE, which is consistent with its original design for inductive node-level embeddings. After training, we use the trained GRAPHSAGE model to generate graph-level embeddings for each individual graph in the test set, consistent with how UGRAPHEMB handles graphs in the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Embedding Dimension</head><p>For all the baseline methods, we ensure that the dimension of the graph-level embeddings is the same as our UGRAPHEMB by setting hyperparameters for each properly. For GRAPH KERNELS, however, they extract and count subgraphs, and for a given dataset, the number of unique subgraphs extracted depend on the dataset, which determines the dimension of the feature vector for each graph in the dataset. Thus, we do not limit the number of subgraphs they extract, giving them advantage, and follow the guidelines in their original papers for hyperparameter tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fine-Tuning</head><p>To incorporate the supervised loss function (cross-entropy loss) into our model, we use multiple fully connected layers to reduce the dimension of graph-level embeddings to the number of graph labels. When the fine-tuning procedure starts, we switch to using the supervised loss function to train the model with the same learning rate and batch size as before. After fine-tuning, the graph label information is integrated into the graph-level embeddings. We still feed the embeddings into the logistic regression classifier for evaluation to ensure it is consistent for all the configurations of all the models. The accuracy based on the prediction of the model is typically much higher because it utilizes supervised information for graph label prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Task 2: Similarity Ranking Evaluation Procedure</head><p>For all the methods, we adopt the procedure outlined in Section E.1 to obtain graph-level embeddings. For each graph in the test set, we treat it as a graph query, compute the similarity/distance score between the query graph and every graph in the training set, and rank the results, compared with the ground-truth ranking results by the ground-truth GED solvers.</p><p>We compute both the similarity score (inner product of two graphlevel embeddings) and distance score (squared L-2 distance between two graph-level embeddings) for every graph pair when doing the query, and report the better of the two in the paper. To verify that the actual ranking of the graphs makes sense, we perform several case studies. As shown in <ref type="figure">Figure 6</ref>, UGRAPHEMB computes the distance score between the query and every graph in the training set. Although the exact distance score is not exactly the same as the ground-truth normalized GEDs, the relatively position and ranking are quite reasonable.</p><p>Notice that for GRAPH KERNELS, the three deep versions, i.e. DGK, DSP, and WDL, generate the same graph-level embeddings as the non-deep versions, i.e. GK, SP, and DL, but use the idea of WORD2VEC to model the relation between subgraphs <ref type="bibr">[Yanardag and Vishwanathan, 2015]</ref>. Consequently, the non-deep versions simply compute the dot products between embeddings to generate the kernel matrices, but the deep versions further modify the kernel matrices, resulting in different graph-graph similarity scores. We thus evaluate the deep versions using their modified kernel matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Task 3: Embedding Visualization Evaluation Procedure</head><p>As outlined in the main paper, we feed the graph-level embeddings into the t-SNE <ref type="bibr">[Maaten and Hinton, 2008]</ref> tool to project them into a 2-D space. We then do the following linear interpolation: (1) Select two points in the 2-D space; (2) Form a line segment between the two selected points; (3) Split the line into 11 equal-length line segments, resulting in 12 points on the line segment in total; (4) Go through these 12 points: For each point, find an embedding point in the 2-D space that is closest to it; (5) Label the 12 embedding points on the embedding plot and draw the actual graph to the right of the embedding plot. This yields the visualization of the IMDBM dataset in the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Analysis and Discussion of Experimental Results</head><p>On graph classification, UGRAPHEMB does not achieve top 2 on WEB and NCI109, which can be attributed to the fact that there are many node labels associated with the two datasets, as shown in <ref type="table">Table 4</ref>. Combined with the fact that we use one-hot encoding for the initial node representations as described in Section D.3, UGRAPHEMB has limited capacity to capture the wide variety of node labels. In future, it is promising to explore other node encoding techniques, such as encoding based on node degrees and clustering coefficients <ref type="bibr">[Ying et al., 2018]</ref>. Another possible reason is that the current definition of GED cannot capture the subtle difference between different node labels. For example, a Carbon atom may be chemically more similar to a Nitrogen atom than a Hydrogen atom, which should be reflected in the graph proximity metric. As mentioned in Section C.2, there are other definitions of GED that can handle such cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Impact of Graph Proximity Metrics on UGRAPHEMB</head><p>We compare the performance of our framework trained under different graph-graph similarity metrics, with results in <ref type="table">Table 6</ref>. It turns out that accuracy scores on GED and MCS are comparable to each other on IMDBM, which is reasonable because both GED and MCS are NP-hard metrics that take node and edge correspondence into account, and it has been shown that GED and MCS are equivalent under certain conditions <ref type="bibr">[Bunke, 1997]</ref>. GRAPHLET KERNEL (GK) <ref type="bibr">[Shervashidze et al., 2009]</ref> measures the similarity between two graphs by their shared graphlets, e.g. triangles, rings, etc. When trained using this heuristic-based graph kernel, UGRAPHEMB performs worse than when trained using GED and MCS. Note that our model learns from GK, and thus in theory should achieve an accuracy very similar to GK, which is indeed verified by comparing UGRAPHEMB-GK with GK.  <ref type="table">Table 6</ref>: Graph classification accuracy of UGRAPHEMB trained under different graph proximity metrics (GED, MCS, and GK) as well as GK alone which is a graph kernel method.</p><p>Note that GED is not a graph kernel, and the high computational cost prevents us from directly using GED to conduct classification for large-scale graph databases.</p><p>H Analysis of The Multi-Scale Node Attention (MSNA) Mechanism <ref type="table">Table 7</ref> shows how much performance gain our proposed Multi-Scale Node Attention (MSNA) mechanism brings to our model, UGRAPHEMB. As can be seen in the table, a simple averaging scheme to generate graph-level embeddings cannot yields much worse performance, compared with the other three mechanisms. The supersource approach is not very bad, but still worse than the attention mechanism which brings learnable components into the model. The MSNA mechanism combines the node embeddings from different GCN layers, capturing structural difference at different scales and yielding the best performance. Please note that under all the four settings, the node embeddings layers are exactly the same, i.e. three sequential GIN layers. From UGRAPHEMB-AVG, we can see that the learnable components in the node embedding model are not enough for good graph-level embeddings.</p><p>It is also worth mentioning that the supersource idea works reasonably well, which can be attributed to the fact that the supersource node is connected to every other node in the graph, so that every node passes information to the supersource node, contributing to a relatively informative graph-level embedding. Compared with the averaging scheme, there is additional MLP transformation on the node embedding of the supersource node after the aggregation of other node embeddings, as indicated by the Equation for GIN in the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>acc τ p@10 UGRAPHEMB-AVG 34.73 0.243 0.647 UGRAPHEMB-SSRC 46.02 0.810 0.796 UGRAPHEMB-NA 49.51 0.851 0.810 UGRAPHEMB-MSNA 50.06 0.853 0.816 <ref type="table">Table 7</ref>: UGRAPHEMB-AVG uses the average of node embeddings to generate graph-level embeddings. UGRAPHEMB-SSRC uses supersource machanism as described in Section A.3. UGRAPHEMB-NA uses only the node embeddings of the last GCN layer to generate graph-level embeddings, but still uses the node attention mechanism. UGRAPHEMB-MSNA is our full model using three layers of GCN to generate graph-level embeddings. The results are on the IMDBM dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>scale</head><p># GIN layers acc τ p@10 1 3 47.33 0.775 0.789 2 3 47.33 0.838 0.797 3 3 49.51 0.851 0.810 1,2,3 3 50.06 0.853 0.816 1,2,3,4 4 50.01 0.851 0.810 1,2,3,4,5 5 50.67 0.854 0.815 <ref type="table">Table 8</ref>: "scale" denotes the indices of the GIN layers that are used for generating graph-level embeddings. "# GIN layers" indicates the total number of GIN layers stacked sequentially. The results are on the IMDBM dataset.</p><p>We also conduct experiments which demonstrates that the performance is much worse when only single scales are used as shown in <ref type="table">Table 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Extra Visualizations</head><p>A few more visualizations are included in <ref type="figure" target="#fig_4">Fig. 7, 2, 8, 9</ref>, and 10, for the PTC, WEB, NCI109, and REDDIT12K datasets used in the main paper.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(1) GRAPH KERNELS (GRAPHLET (GK), DEEP GRAPHLET (DGK), SHORTEST PATH (SP), DEEP SHORTEST PATH (DSP), WEISFEILER-LEHMAN (WL), and DEEP WEISFEILER-LEHMAN (DWL)) ; (2) GRAPH2VEC [Narayanan et al., 2017]; (3) NETMF [Qiu et al., 2018]; (4) GRAPH-SAGE [Hamilton et al., 2017].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Classification accuracy on the IMDBM dataset w.r.t. the dimension of graph-level embeddings and the percentage of graph pairs used for training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Methods including GRAPHSAGE [Hamilton et al., 2017], GAT [Velickovic et al., 2018], GIN [Xu et al., 2019], etc. are all under this category, with different aggregators proposed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Architecture 1 and 2 are typical neural network architectures for graph representation learning. Architecture 3 is our proposed UGRAPHEMB. However, a large amount of existing works incorporate a supervised loss function into the model, making the entire model trainable end-to-end. Examples include GCN as in [Kipf and Welling, 2016b], as well as a series of improvements upon GCN, such as GRAPH-SAGE [Hamilton et al., 2017], GAT [Velickovic et al., 2018], GIN [Xu et al., 2019], etc. as mentioned in the main paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Visualization of the PTC dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Visualization of the WEB dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Visualization of the NCI109 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Visualization of the REDDIT12K dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>0.00 0.29 0.47 0.52 0.79 ... 5.14 0.29 0.00 0.65 0.81 1.08 ... 9.15 0.47 0.65 0.00 0.12 0.55 ... 8.27 0.52 0.81 0.12 0.00 0.95 ... 9.08 0.79 1.08 0.55 0.95 0.00 ... 4.10 ... ... ... ... ... ... ... 5.14 9.15 8.27 9.08 4.10 ... 0.00</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>d 12 =0.71</cell><cell>d 23 =0.65</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>...</cell></row><row><cell cols="2">(a) Graph Dataset</cell><cell cols="2">(b) True Distance Matrix</cell><cell cols="2">(c) "Hyper-Level" Graph</cell><cell>(d) Similarity Ranking</cell></row><row><cell>Graph i Graph j</cell><cell>Node Embedding: GIN Layers</cell><cell>...</cell><cell cols="2">Graph Embedding: Multi-Scale Node Attention (MSNA)</cell><cell>Predict Graph Proximity</cell></row><row><cell></cell><cell></cell><cell>…</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Node Embeddings</cell><cell></cell><cell cols="2">One Embedding per Graph</cell><cell>(e) Graph Classification</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>HUNGARIAN 0.755 * 0.465 * 0.872 * 0.825 * 0.706 * 0.160 * 0.667 * 0.164 * 0.512 * 0.808</figDesc><table><row><cell></cell><cell>291</cell><cell>0.135</cell><cell>0.329</cell><cell>0.421</cell><cell>0.147</cell><cell>0.101</cell><cell>0.445</cell><cell>0.012</cell><cell>0.007</cell><cell>0.009</cell></row><row><cell>DGK</cell><cell>0.222</cell><cell>0.103</cell><cell>0.304</cell><cell>0.410</cell><cell>0.126</cell><cell>0.009</cell><cell>0.441</cell><cell>0.010</cell><cell>0.011</cell><cell>0.012</cell></row><row><cell>SP</cell><cell>0.335</cell><cell>0.129</cell><cell>0.009</cell><cell>0.011</cell><cell>0.008</cell><cell>0.065</cell><cell>0.238</cell><cell>0.012</cell><cell>−</cell><cell>−</cell></row><row><cell>DSP</cell><cell>0.344</cell><cell>0.130</cell><cell>0.007</cell><cell>0.010</cell><cell>0.011</cell><cell>0.072</cell><cell>0.256</cell><cell>0.019</cell><cell>−</cell><cell>−</cell></row><row><cell>WL</cell><cell>0.129</cell><cell>0.074</cell><cell>0.034</cell><cell>0.038</cell><cell>0.014</cell><cell>0.246</cell><cell>0.042</cell><cell>0.006</cell><cell>0.089</cell><cell>0.017</cell></row><row><cell>DWL</cell><cell>0.131</cell><cell>0.072</cell><cell>0.039</cell><cell>0.041</cell><cell>0.017</cell><cell>0.262</cell><cell>0.049</cell><cell>0.009</cell><cell>0.095</cell><cell>0.023</cell></row><row><cell>GRAPH2VEC</cell><cell>0.128</cell><cell>0.188</cell><cell>0.697</cell><cell>0.624</cell><cell>0.014</cell><cell>0.068</cell><cell>0.033</cell><cell>0.127</cell><cell>0.008</cell><cell>0.017</cell></row><row><cell>NETMF</cell><cell>0.004</cell><cell>0.012</cell><cell>0.003</cell><cell>0.143</cell><cell>0.002</cell><cell>0.010</cell><cell>0.001</cell><cell>0.008</cell><cell>0.009</cell><cell>0.042</cell></row><row><cell cols="2">GRAPHSAGE 0.011</cell><cell>0.033</cell><cell>0.042</cell><cell>0.059</cell><cell>0.009</cell><cell>0.010</cell><cell>0.018</cell><cell>0.040</cell><cell>0.089</cell><cell>0.017</cell></row><row><cell cols="2">BEAM 0.992  HED 0.788</cell><cell>0.433</cell><cell>0.627</cell><cell>0.801</cell><cell>0.667</cell><cell>0.291</cell><cell>0.199</cell><cell>0.174</cell><cell>0.199</cell><cell>0.237</cell></row><row><cell cols="2">UGRAPHEMB 0.840</cell><cell>0.457</cell><cell>0.853</cell><cell>0.816</cell><cell>0.618</cell><cell>0.303</cell><cell>0.476</cell><cell>0.189</cell><cell>0.572</cell><cell>0.365</cell></row></table><note>* 0.983* 0.892* 0.968* 0.963* 0.957* 0.615* 0.997* 0.657* 1.000** VJ 0.749* 0.403* 0.874* 0.815* 0.704* 0.151* 0.673* 0.097* 0.502* 0.867*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>level embeddings is proposed. Experiments show that the produced graph-level embeddings achieve competitive performance on three downstream tasks: graph classification, similarity ranking, and graph visualization. Scalable feature learning for networks. In SIGKDD, pages 855-864.ACM, 2016.   [Hamilton et al., 2017 Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In NIPS, pages 1024-1034, 2017. [Hart et al., 1968] Peter E Hart, Nils J Nilsson, and Bertram Raphael. A formal basis for the heuristic determination of minimum cost paths. IEEE transactions on Systems Science and Cybernetics, Biometrika, 30(1/2):81-93, 1938. [Kingma and Ba, 2015] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. ICLR, 2015. [Kipf and Welling, 2016a] Thomas N Kipf and Max Welling. Semisupervised classification with graph convolutional networks. ICLR, 2016. Kuansan Wang, and Jie Tang. Network embedding as matrix factorization: Unifyingdeepwalk, line, pte, and node2vec. WSDM, 2018. Iam graph database repository for graph based pattern recognition and machine learning. In Joint IAPR International Workshops on Statistical Techniques in Pattern Recognition (SPR) and Structural and Syntactic Pattern Recognition (SSPR), pages 287-297. Springer, 2008. [Riesen and Bunke, 2009] Kaspar Riesen and Horst Bunke. Approximate graph edit distance computation by means of bipartite graph matching. Image and Vision computing, 27(7):950-959, 2009. [Riesen et al., 2013] Kaspar Riesen, Sandro Emmenegger, and Horst Bunke. A novel software toolkit for graph edit distance computation. In GbRPR, pages 142-151. Springer, 2013. Yu, and Jiawei Han. Substructure similarity search in graph databases. In SIGMOD, pages 766-777. ACM, 2005. [Yanardag and Vishwanathan, 2015] Pinar Yanardag and SVN Vishwanathan. Deep graph kernels. In SIGKDD, pages 1365-1374. ACM, 2015. [Ying et al., 2018] Rex Ying, Jiaxuan You, Christopher Morris, Xiang Ren, William L Hamilton, and Jure Leskovec. Hierarchical graph representation learning with differentiable pooling.</figDesc><table><row><cell></cell><cell>[Kipf and Welling, 2016b] Thomas N Kipf and Max Welling. Vari-</cell></row><row><cell>[Bunke and Shearer, 1998] Horst Bunke and Kim Shearer. A graph distance metric based on the maximal common subgraph. Pattern recognition letters, 19(3-4):255-259, 1998. [Bunke, 1983] H Bunke. What is the distance between graphs. Bul-letin of the EATCS, 20:35-39, 1983. [Bunke, 1997] Horst Bunke. On a relation between graph edit dis-tance and maximum common subgraph. Pattern Recognition Letters, 18(8):689-694, 1997. [Shervashidze and Borgwardt, 2009] Nino Shervashidze and Karsten Borgwardt. Fast subtree kernels on graphs. In NIPS, pages 1660-1668, 2009. [Shervashidze et al., 2009] Nino Shervashidze, SVN Vishwanathan, Tobias Petri, Kurt Mehlhorn, and Karsten Borgwardt. Efficient graphlet kernels for large graph comparison. In Artificial Intelli-gence and Statistics, pages 488-495, 2009.</cell><cell>ational graph auto-encoders. NIPS Workshop on Bayesian Deep NeurIPS, 2018. Learning, 2016. [Zhang and Chen, 2018] Muhan Zhang and Yixin Chen. Link pre-[Kipf et al., 2018] Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, diction based on graph neural networks. In NeurIPS, pages 5171-Max Welling, and Richard Zemel. Neural relational inference for 5181, 2018. interacting systems. ICML, 2018. [Zhang and Chen, 2019] Xinyi Zhang and Lihui Chen. Capsule [Ktena et al., 2017] Sofia Ira Ktena, Sarah Parisot, Enzo Ferrante, graph neural network. ICLR, 2019. Martin Rajchl, Matthew Lee, Ben Glocker, and Daniel Rueckert. [Zhang et al., 2018] Muhan Zhang, Zhicheng Cui, Marion Neu-Distance metric learning using graph convolutional networks: Ap-mann, and Yixin Chen. An end-to-end deep learning architecture plication to functional brain networks. In MICCAI, pages 469-477. for graph classification. In AAAI, 2018. Springer, 2017. [Zhao et al., 2018] Xiaohan Zhao, Bo Zong, Ziyu Guan, Kai Zhang,</cell></row><row><cell>[Defferrard et al., 2016] Michaël Defferrard, Xavier Bresson, and</cell><cell>and Wei Zhao. Substructure assembling network for graph classi-</cell></row><row><cell>Pierre Vandergheynst. Convolutional neural networks on graphs</cell><cell>fication. AAAI, 2018.</cell></row><row><cell>with fast localized spectral filtering. In NIPS, pages 3844-3852,</cell><cell></cell></row><row><cell>2016.</cell><cell></cell></row><row><cell>pages 2086-2092, 2018.</cell><cell></cell></row><row><cell>[Fankhauser et al., 2011] Stefan Fankhauser, Kaspar Riesen, and</cell><cell></cell></row><row><cell>Horst Bunke. Speeding up graph edit distance computation</cell><cell></cell></row><row><cell>through fast bipartite matching. In GbRPR, pages 102-111.</cell><cell></cell></row><row><cell>Springer, 2011.</cell><cell></cell></row><row><cell>[Fischer et al., 2015] Andreas Fischer, Ching Y Suen, Volkmar</cell><cell></cell></row><row><cell>Frinken, Kaspar Riesen, and Horst Bunke. Approximation of</cell><cell></cell></row><row><cell>graph edit distance based on hausdorff matching. Pattern Recog-</cell><cell></cell></row><row><cell>nition, 48(2):331-343, 2015.</cell><cell></cell></row><row><cell>[Gilmer et al., 2017] Justin Gilmer, Samuel S Schoenholz, Patrick F</cell><cell></cell></row><row><cell>Riley, Oriol Vinyals, and George E Dahl. Neural message passing</cell><cell></cell></row><row><cell>for quantum chemistry. In ICML, pages 1263-1272. JMLR. org,</cell><cell></cell></row><row><cell>2017.</cell><cell></cell></row><row><cell>[Girija, 2016] Sanjay Surendranath Girija. Tensorflow: Large-scale</cell><cell></cell></row><row><cell>machine learning on heterogeneous distributed systems. 2016.</cell><cell></cell></row><row><cell>[Grover and Leskovec, 2016] Aditya Grover and Jure Leskovec.</cell><cell></cell></row><row><cell>203-209, 2017. [Williams, 2001] Christopher KI Williams. On a connection be-tween kernel pca and metric multidimensional scaling. In Ad-vances in neural information processing systems, pages 675-681, 2001. [Wu et al., 2019] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S Yu. A com-prehensive survey on graph neural networks. arXiv preprint node2vec: 4(2):100-107, 1968. arXiv:1901.00596, 2019.</cell><cell></cell></row></table><note>[Devlin et al., 2018] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec- tional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.[Du et al., 2018] Lun Du, Yun Wang, Guojie Song, Zhicong Lu, and Junshan Wang. Dynamic network embedding: An extended approach for skip-gram based network embedding. In IJCAI,[Heimann and Koutra, 2017] Mark Heimann and Danai Koutra. On generalizing neural node embedding methods to multi-network problems. In KDD MLG Workshop, 2017.[Kendall, 1938] Maurice G Kendall. A new measure of rank corre- lation.[Liang and Zhao, 2017] Yongjiang Liang and Peixiang Zhao. Sim- ilarity search in graph databases: A multi-layered indexing ap- proach. In ICDE, pages 783-794. IEEE, 2017.[Ma et al., 2018] Tengfei Ma, Cao Xiao, Jiayu Zhou, and Fei Wang. Drug similarity integration through attentive multi-view graph auto-encoders. IJCAI, 2018.[Maaten and Hinton, 2008] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(Nov):2579-2605, 2008.[Mikolov et al., 2013] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In NIPS, pages 3111-3119, 2013.[Narayanan et al., 2017] Annamalai Narayanan, Mahinthan Chan- dramohan, Rajasekar Venkatesan, Lihui Chen, Yang Liu, and Shantanu Jaiswal. graph2vec: Learning distributed representa- tions of graphs. KDD MLG Workshop, 2017.[Neuhaus et al., 2006] Michel Neuhaus, Kaspar Riesen, and Horst Bunke. Fast suboptimal algorithms for the computation of graph edit distance. In S+SSPR, pages 163-172. Springer, 2006.[Niepert et al., 2016] Mathias Niepert, Mohamed Ahmed, and Kon- stantin Kutzkov. Learning convolutional neural networks for graphs. In ICML, pages 2014-2023, 2016.[Perozzi et al., 2014] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representations. In SIGKDD, pages 701-710. ACM, 2014.[Peters et al., 2018] Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettle- moyer. Deep contextualized word representations. NAACL, 2018.[Qiu et al., 2018] Jiezhong Qiu, Yuxiao Dong, Hao Ma, Jian Li,[Qureshi et al., 2007] Rashid Jalal Qureshi, Jean-Yves Ramel, and Hubert Cardot. Graph based shapes representation and recognition. In GbRPR, pages 49-60. Springer, 2007.[Radford et al., 2018] Alec Radford, Karthik Narasimhan, Tim Sali- mans, and Ilya Sutskever. Improving language understanding by generative pre-training. 2018.[Riesen and Bunke, 2008] Kaspar Riesen and Horst Bunke.[Shervashidze et al., 2011] Nino Shervashidze, Pascal Schweitzer, Erik Jan van Leeuwen, Kurt Mehlhorn, and Karsten M Borgwardt. Weisfeiler-lehman graph kernels. Journal of Machine Learning Research, 12(Sep):2539-2561, 2011.[Shrivastava and Li, 2014] Anshumali Shrivastava and Ping Li. A new space for comparing graphs. In Proceedings of the 2014 IEEE/ACM International Conference on Advances in Social Net- works Analysis and Mining, pages 62-71. IEEE Press, 2014.[Simonovsky and Komodakis, 2017] Martin Simonovsky and Nikos Komodakis. Dynamic edgeconditioned filters in convolutional neural networks on graphs. In CVPR, 2017.[Tang et al., 2015] Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. Line: Large-scale information network embedding. In WWW, pages 1067-1077. International World Wide Web Conferences Steering Committee, 2015.[Velickovic et al., 2018] Petar Velickovic, Guillem Cucurull, Aran- txa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. ICLR, 2018.[Wale et al., 2008] Nikil Wale, Ian A Watson, and George Karypis. Comparison of descriptor spaces for chemical compound re- trieval and classification. Knowledge and Information Systems, 14(3):347-375, 2008.[Wang et al., 2016] Daixin Wang, Peng Cui, and Wenwu Zhu. Struc- tural deep network embedding. In SIGKDD, pages 1225-1234. ACM, 2016.[Wang et al., 2017] Xiao Wang, Peng Cui, Jing Wang, Jian Pei, Wenwu Zhu, and Shiqiang Yang. Community preserving net- work embedding. In AAAI, pages[Xu et al., 2019] Keyulu Xu, Weihua Hu, Jure Leskovec, and Ste- fanie Jegelka. How powerful are graph neural networks? ICLR, 2019.[Yan et al., 2005] Xifeng Yan, Philip S</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>, ECC [Simonovsky and Komodakis, 2017], SET2SET [Gilmer et al., 2017], GRAPH-SAGE, DGCNN/SORTPOOL [Zhang et al., 2018], SAN [Zhao et al., 2018], DIFFPOOL [Ying et al., 2018], CAPSGNN [Zhang and Chen, 2019], etc.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">shows a summary of</cell></row><row><cell>the methods.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Citation</cell><cell>G U</cell><cell>I</cell></row><row><cell>LLE</cell><cell cols="2">[Belkin and Niyogi, 2003] ×</cell><cell>×</cell></row><row><cell>GCN</cell><cell cols="2">[Kipf and Welling, 2016a] × ×</cell><cell></cell></row><row><cell>GIN</cell><cell>[Xu et al., 2019]</cell><cell>× ×</cell><cell></cell></row><row><cell>DIFFPOOL</cell><cell>[Ying et al., 2018]</cell><cell>×</cell><cell></cell></row><row><cell>GRAPH KERNELS</cell><cell>-</cell><cell></cell><cell></cell></row><row><cell>GRAPH2VEC</cell><cell>[Narayanan et al., 2017]</cell><cell></cell><cell>×</cell></row><row><cell>NETMF</cell><cell>[Qiu et al., 2018]</cell><cell>×</cell><cell>×</cell></row><row><cell>GRAPHSAGE</cell><cell>[Hamilton et al., 2017]</cell><cell>×</cell><cell></cell></row><row><cell>UGRAPHEMB</cell><cell>this paper</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: A brief comparison of methods on node and graph represen-</cell></row><row><cell>tation learning. "G": Designed for graph-level embeddings ( ) or not</cell></row><row><cell>(×). "U": Unsupervised or supervised. "I": Inductive or transductive.</cell></row><row><cell>For GRAPH KERNELS, there are multiple citations.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 5 :</head><label>5</label><figDesc>Number of graph pairs used to train UGRAPHEMB on each dataset, along with the total wall time to compute the ground-truth GEDs for these pairs.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>Visualization of the ranking results on a query in IMDBM. The top row depicts the ground-truth ranking, labeled with normalized ground-truth GEDs, and the bottom row depicts ranking by UGRAPHEMB, depicts the distance score computed by UGRAPHEMB.</figDesc><table><row><cell>nGED by Beam-Hungarian-VJ</cell><cell>0.00</cell><cell>0.30</cell><cell>0.34</cell><cell>0.34</cell><cell>0.34</cell><cell>0.36</cell><cell>... 3.19 ...</cell><cell>28.04</cell></row><row><cell>Dist by Our Model</cell><cell>1 0.00</cell><cell>2 0.00</cell><cell>3 0.00</cell><cell>4 0.01</cell><cell>5 0.01</cell><cell>6 0.01</cell><cell>... 600 ... 3.60</cell><cell>1200 28.83</cell></row><row><cell>Figure 6: Method</cell><cell></cell><cell cols="2">IMDBM NCI109</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">UGRAPHEMB-GED</cell><cell>50.06</cell><cell>69.17</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">UGRAPHEMB-MCS</cell><cell>50.01</cell><cell>69.85</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">UGRAPHEMB-GK</cell><cell>43.17</cell><cell>60.05</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GK</cell><cell></cell><cell>43.89</cell><cell>62.06</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">ConclusionWe present UGRAPHEMB, an end-to-end neural network based framework aiming to embed an entire graph into an embedding preserving the proximity between graphs in the dataset under a graph proximity metric, such as Graph Edit Distance (GED). A novel mechanism for generating graph-</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is partially supported by NIH R01GM115833 and U01HG008488, NSF DBI-1565137, DGE-1829071, NSF III-1705169, NSF CAREER Award 1741634, and Amazon Research Award.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Simgnn: A neural network approach to fast graph similarity computation. WSDM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1373" to="1396" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>Neural computation</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
