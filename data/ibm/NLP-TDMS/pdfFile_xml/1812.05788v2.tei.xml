<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AU R-CNN: Encoding Expert Prior Knowledge into R-CNN for Action Unit Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Software</orgName>
								<orgName type="department" key="dep2">Beijing National Research Center for Information Science and Technology (BNRist)</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Software</orgName>
								<orgName type="department" key="dep2">Beijing National Research Center for Information Science and Technology (BNRist)</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhai</forename><surname>Yong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Software</orgName>
								<orgName type="department" key="dep2">Beijing National Research Center for Information Science and Technology (BNRist)</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">AU R-CNN: Encoding Expert Prior Knowledge into R-CNN for Action Unit Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Action unit detection</term>
					<term>Expert prior knowledge</term>
					<term>R-CNN</term>
					<term>Facial Action Coding System</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Detecting action units (AUs) on human faces is challenging because various AUs make subtle facial appearance change over various regions at different scales. Current works have attempted to recognize AUs by emphasizing important regions. However, the incorporation of expert prior knowledge into region definition remains under-exploited, and current AU detection approaches do not use regional convolutional neural networks (R-CNN) with expert prior knowledge to directly focus on AU-related regions adaptively. By incorporating expert prior knowledge, we propose a novel R-CNN based model named AU R-CNN. The proposed solution offers two main contributions: (1) AU R-CNN directly observes different facial regions, where various AUs are located. Specifically, we define an AU partition rule which encodes the expert prior knowledge into the region definition and RoI-level label definition. This design produces considerably better detection performance than existing approaches. (2) We integrate various dynamic models (including convolutional long short-term memory, two stream network, conditional random field, and temporal action localization network) into AU R-CNN and then investigate and analyze the reason behind the performance of dynamic models. Experiment results demonstrate that only static RGB image information and no optical flow-based AU R-CNN surpasses the one fused with dynamic models. AU R-CNN is also superior to traditional CNNs that use the same backbone on varying image resolutions. State-of-the-art recognition performance of AU detection is achieved. The complete network is end-to-end trainable. Experiments on BP4D and DISFA datasets show the effectiveness of our approach. The implementation code is available in https://github.com/sharpstill/AU_R-CNN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Facial expressions reveal people's emotions and intentions. Facial Action Coding System (FACS) <ref type="bibr" target="#b0">[1]</ref> has defined 44 action units (AUs) related to the movement of specific facial muscles; these units can anatomically represent all possible facial expressions, considering the crucial importance of facial expression analysis. AU detection has been studied for decades and its goal is to recognize and predict AU labels on each frame of the facial expression video. Automatic detection of AUs has a wide range of applications, such as human-machine interfaces, affective computing, and car-driving monitoring.</p><p>Since the human face may present complex facial expression, and AUs appear in the form of subtle appearance changes on the local regions of face, that current classifiers cannot easily recognize. This problem is the main obstacle of current AU detection systems. Various approaches focus on fusing with extra information in convolutional neural networks (CNNs), e.g. , the optical flow information <ref type="bibr" target="#b1">[2]</ref> or landmark information <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, to help AU detection systems capture such subtle facial expressions. However, these approaches have high detection error rates, due to the lack of using prior knowledge. Human can easily recognize micro facial expression by their long accumulated experience. Hence, integrating the expert prior knowledge of FACS <ref type="bibr" target="#b0">[1]</ref> to AU detection system is promising. With fusing of this prior knowledge, our proposed approach addresses the AU detection problem by partitioning the face to easily recognizable AU-related regions, then the prediction of each region is merged to obtain the image-level prediction. <ref type="figure" target="#fig_0">Fig. 1</ref> shows our approach's framework, we design an "AU partition rule" to encode the expert prior knowledge. This AU partition rule decomposes the image into a bunch of AU-related bounding boxes. Then AU R-CNN head module focuses on recognizing each bounding box. This design can well address the three problems of existing approaches.</p><p>First, existing approaches <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref> have been proposed to extract features near landmarks (namely, "AU center"), which is trivially defined and leading to emphasize on inaccurate places. AUs occur in regions around specific facial muscles that may be inaccurately located on a landmark or an AU center due to the limitation of the facial muscle's activity place. Thus, most AUs limit their activities in specific irregular regions of a face, and we call this limitation the "space constraint". Our approach reviews the FACS and designs the "AU partition rule" to represent this space constraint accurately. This well-designed "AU partition rule" is called the "expert prior knowledge" in our approach which is built on the basis of the space-constraint for regional recognition, so it reduces the detection error rate caused by in- Second, existing approaches still use CNNs to recognize a full face image <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref> and do not learn to recognize individual region's labels, which may not use the correct image context to detect. For example, a CNN may use an unreliable context, such as mouth area features, to recognize eye-area-related AUs (e.g. AU 1, AU 2). Recent success in the object detection model of Fast/Faster R-CNN <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> has inspired us to utilize the power of R-CNN based models to learn the accurate regional features of AUs under space constraints. We propose AU R-CNN to detect AUs only from AU-related regions by limiting its vision inside space-constrained areas. In this process, unrelated areas can be excluded to avoid interference, which is key to improve detection accuracy.</p><p>Third, the multi-label learning problem in AU detection can be addressed at a fine-grained level under AU-related RoI space constraint. Previous approaches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b13">14]</ref> adopt the sigmoid cross-entropy cost function to learn the image-level multi-label and emphasize the important regions, but such a solution is not sufficiently fine-grained. The multi-label relationship can be captured more accurately in the RoI-level supervised information constraint. Most facial muscles can show diverse expressions that lead to RoI-level multi-label learning. For example, AU 12 (lip corner puller) is often present in a smile, which may also occur together with AU 10 (upper lip raiser), and deepen the nasolabial fold, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. Therefore, in the definition of the AU partition rule, AUs are grouped by the definition of FACS and related facial muscles. Each AU group shares the same region, and such AU group can be represented by a binary vector with element of 1 if the corresponding AU occurs in the ground truth and 0 otherwise. The sigmoid cross-entropy cost function is adopted in the RoI-level learning. In our experiments, we determine that using RoI-level labels to train and predict and then merging the RoI-level prediction result to that of the image level surpasses the previous approaches.</p><p>Furthermore, we analyze the effects of fusing temporal features into AU R-CNN (dynamic model extension). We conduct complete comparison experiments to investigate the effects of integrating dynamic models, including convolutional long short-term memory (ConvLSTM) <ref type="bibr" target="#b17">[18]</ref>, two-stream network <ref type="bibr" target="#b18">[19]</ref>, general graph conditional random field (CRF) model, and TAL-Net <ref type="bibr" target="#b19">[20]</ref>, into AU R-CNN. We analyze the reason behind such effects and the cases under which the dynamic models are effective. Our AU R-CNN with only static RGB images and no optical flow achieves 63% average F1 score on BP4D, and outperforms all dynamic models. The main contributions of our study are as follows.</p><p>(1) AU R-CNN is proposed to learn regional features adaptively by using RoI-level multi-label supervised information. Specifically, we encode the expert prior knowledge by defining the AU partition rule, including the AU groups and related regions, according to FACS <ref type="bibr" target="#b0">[1]</ref>.</p><p>(2) We investigate the effects of integrating various dynamic models, including two-stream network, ConvLSTM, CRF model and TAL-Net, in the experiments of BP4D <ref type="bibr" target="#b20">[21]</ref> and DISFA <ref type="bibr" target="#b21">[22]</ref> databases. The reasons behind such experiment effects and the effective cases are analyzed. The experiment results show that our static RGB image-based AU R-CNN achieves the best average F1 score in BP4D and is close to the performance of the best dynamic model in DISFA. Our approach achieves state-of-the-art performance in AU detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Extensive works on AU detection have been proposed to extract effective facial features. The facial features in AU detection can be grouped into appearance and geometric features. Appearance features portray the local or global changes in facial components. Most popular approaches in this category adopt Haar feature <ref type="bibr" target="#b22">[23]</ref>, local binary pattern <ref type="bibr" target="#b23">[24]</ref>, Garbor wavelets <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>, and canonical appearance feature <ref type="bibr" target="#b26">[27]</ref>. Geometric features represent the salient facial point or skin changing direction or distance. Geometric changes can be measured by optical flows <ref type="bibr" target="#b27">[28]</ref> or displacement of facial landmark points <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b28">29]</ref>. Landmark plays an important role in geometry approaches, and many methods have been proposed to extract features near landmark points <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b29">30]</ref>. Fabian et al. <ref type="bibr" target="#b30">[31]</ref> proposed a method that combines geometric changes and local texture information. Wu and Ji <ref type="bibr" target="#b31">[32]</ref> investigated the combination of facial AU recognition and facial landmark detection. Zhao et al. <ref type="bibr" target="#b12">[13]</ref> proposed joint patch and multi-label learning (JPML) for AU detection with a scale-invariant feature transform descriptor near landmarks. These traditional approaches focus on extracting handcraft features near landmark points. With the recent success of deep learning, CNN has been widely adopted to extract AU features <ref type="bibr" target="#b14">[15]</ref>. Zhao et al. <ref type="bibr" target="#b13">[14]</ref> proposed a deep region and multi-label learning (DRML) network to divide the face images into 8 × 8 blocks and used individual convolutional kernels to convolve each block. Although this approach treats each face as a group of individual parts, it divides blocks uniformly and does not consider the FACS knowledge, thereby leading to the poor performance. Li et al. <ref type="bibr" target="#b3">[4]</ref> proposed Enhancing and Cropping Net (EAC-Net), which intends to give significant attention to individual AU centers. However, this approach defines the AU center trivially and it uses image-level context to learn. Its CNN backbone may use incorrect context to classify and the lack of RoI-level supervised information can only give coarse guidance. Song et al. <ref type="bibr" target="#b32">[33]</ref> studied the sparsity and co-occurrence of AUs. Han et al. <ref type="bibr" target="#b14">[15]</ref> proposed an Optimized Filter Size CNN (OFS-CNN) to simultaneously learn the filter sizes and weights of all conv-layer. Other related problems, including the effects of dataset size <ref type="bibr" target="#b33">[34]</ref>, the action detection in videos <ref type="bibr" target="#b34">[35]</ref>, the pose-based feature of action recognition <ref type="bibr" target="#b35">[36]</ref>, and generalized multimodal factorized high-order pooling for visual question answering <ref type="bibr" target="#b36">[37]</ref> have also been studied. Previous works have mainly focused on landmark-based regions or learning multiple regions with convolutional kernels separately. Detection with the expert prior knowledge and utilizing RoI-level labels are important but have been undervalued in previous methods.</p><p>Researchers have utilized temporal dependencies in video sequences over the last few years. Romero et al. <ref type="bibr" target="#b1">[2]</ref> advocated a two-stream CNN model that combines optical flow and RGB information, and their result was promising. However, they used one binary classification model for each AU, which caused their approach to be time consuming to train and yield numerous model parameters. The CNN and LSTM hybrid network architectures are studied in Chu et al. <ref type="bibr" target="#b37">[38]</ref>, Li et al. <ref type="bibr" target="#b2">[3]</ref> and He et al. <ref type="bibr" target="#b38">[39]</ref>, which feed the CNN-produced features to LSTM to improve performance by capturing the temporal relationship across frames. However, their solutions are inefficient because they are not an end-to-end networks. In our experiments, we also investigate the effects of using temporal feature relationships in the time axis of videos. We use various dynamic models (including two-stream network, ConvLSTM etc.) that are Unlike existing approaches, AU R-CNN is a unified end-toend learning model that encodes expert prior knowledge and outperforms state-of-the-art approaches. Thus, it is a simple and practical model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>AU detection can be considered a multi-label classification problem. The most popular image classification approach is the CNN, and the basic assumption for a standard CNN is the shared convolutional kernels for an entire image. For a highly structural image, such as a human face, a standard CNN will fail to capture subtle appearance changes. To address this issue, we propose AU R-CNN, in which expert prior knowledge is encoded. We review FACS <ref type="bibr" target="#b0">[1]</ref> and define a rule ("AU partition rule") for partitioning a face on the basis of FACS knowledge using landmarks. With this rule, we can treat each face image as a group of separate regions and AU R-CNN is proposed to recognize each region. The overall procedure is composed of two steps. First, the face image's landmark points are obtained, and then the face is partitioned into regions on the basis of the AU partition rule and the landmark coordinates. The "AU masks" are generated in this step, and the expert prior knowledge is encoded into the AU masks. Second, the face images are input into the AU R-CNN's backbone, the produced feature map and the minimum bounding boxes of the AU mask are then fed into AU R-CNN's RoI pooling layer together. The final fully-connected (fc) layer's output can be treated as classification probabilities. The image-level ground truth label is also partitioned to RoI-level in the learning. After AU R-CNN is trained over, the prediction is performed on the RoI-level. Then, we use a "bit-wise OR" operator to merge RoI-level prediction labels to image-level ones. In this section, we introduce the AU partition rule and then AU R-CNN. We also introduce a dynamic model extension of AU R-CNN in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">AU partition rule</head><p>AUs appear in specific regions of a face but are not limited to facial landmark points; previous AU feature extraction approaches directly use facial landmarks or offsets of the landmarks as AU centers <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b12">13]</ref>, but the actual places where activities occur may be missed, and sensitivity of the system may be increased. Instead of identifying the AU center, we adopt the domain-related expertise to guide the partition of AU-related RoIs. The first step is to utilize the dlib <ref type="bibr" target="#b39">[40]</ref> toolkit to obtain 68 landmark points. The landmark points provide rich information about the face, and the landmark points help us focus on areas where AUs may occur. <ref type="figure" target="#fig_1">Fig. 2</ref> shows the region partition of a face, and several extra points are calculated using 68 landmarks. A typical example is shown in <ref type="figure" target="#fig_1">Fig. 2</ref> right. The face image is partitioned into 43 basic RoIs using landmarks. Then, on the basis of FACS definition 1 <ref type="table" target="#tab_0">(Table 1</ref>) and the anatomy of facial muscle structure 2 , the AU partition rule and the AU mask can be defined for representing the expert prior knowledge. For this purpose, we classify AUs into four cases.</p><p>(1) The RoIs defined in <ref type="figure" target="#fig_1">Fig. 2</ref> are the basic building blocks, named basic RoIs. One AU contains multiple basic RoIs; hence, multiple basic RoIs are selected to be grouped and assigned to AUs by RoI numbers ( <ref type="table" target="#tab_1">Table 2</ref>). The principle of such RoI assignment is the FACS muscle definition ( <ref type="table" target="#tab_0">Table 1</ref>). The region of the grouped RoIs is called the "AU mask".</p><p>(2) Most muscles can present multiple AUs-in other words, some AUs can co-occur in the same place. For example, AU 12 (lip corner puller) and AU 10 (upper lip raiser) are often present together in a smile, which requires lifting of the muscle and may also deepen the nasolabial fold, as shown in <ref type="figure">Fig. 4</ref>(e). Therefore, we group AUs into 8 "AU groups" on the basis of AU-related muscles defined in FACS <ref type="table" target="#tab_0">(Table 1</ref>) and the AU cooccurrence statistics of the database. Each AU group has its own mask, whose region is shared by the AUs. One AU group contains multiple basic RoIs, which are defined in <ref type="figure" target="#fig_1">Fig. 2</ref>, to form an AU mask ( <ref type="figure">Fig. 4</ref>).</p><p>(3) Some AU groups are defined in a hierarchical structure, that is, these AU group masks have a broad area, which may contain other AU groups' small areas. For example, AU group # 6 contains AU group # 7 <ref type="figure">(Fig. 4</ref>). The reason behind such a design is that AU group # 7 (AU 17) is caused by the movement of the mentalis <ref type="table" target="#tab_0">(Table 1)</ref>, which is in the chin. The bone structure of the chin makes it a relatively stable area, which limits the possible occurrence of AU 17. Therefore, we can define a detailed area in AU group # 7 ( <ref type="figure">Fig.4(g)</ref>). However, AU group # 6 consists of AU 16, AU 20, AU 25 and AU 26, and it is located in the mouth area. The mouth area contains several possible movement locations (mouth open, mouth close, smell, laugh, etc.), and the chin area follows mouth opening and closing. Therefore we define AU group # 6 to contain the area of AU group # 7 ( <ref type="figure">Fig. 4(f)</ref>). The partition of the face image naturally leads to the RoI-level label assignment. In this case, the AU group # 6 must contain RoI-level labels of AU group # 7. 1 https://www.cs.cmu.edu/˜face/facs.htm 2 https://en.wikipedia.org/wiki/Facial_muscles We define operator "label fetch" #7∈#6 to enable AU group # 6 to fetch labels from AU group #7 <ref type="table" target="#tab_1">(Table 2</ref>).</p><p>(4) Some AU groups have overlapping areas with other AU groups' areas. For example, AU group # 3's mask, which is across the nose area ( <ref type="figure">Fig. 4(c)</ref>), will also contain labels of AU group # 4 ( <ref type="figure">Fig. 4(d)</ref>); thus, we also use the operator "label fetch" #4∈#3 to fetch labels from AU group # 4 in this case. <ref type="table" target="#tab_1">(Table 2</ref>).</p><p>In summary, <ref type="table" target="#tab_1">Table 2</ref> and <ref type="figure">Fig. 4</ref> show the AU partition rule and the AU mask. The AU group definition is related not only to the RoI partition of the face, but also to the RoI-level label assignment. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">AU R-CNN</head><p>AU R-CNN is composed of two modules, namely, feature extraction and head modules. This model can use ResNet <ref type="bibr" target="#b40">[41]</ref> or VGG <ref type="bibr" target="#b41">[42]</ref> as its backbone. Here, we use ResNet-101 to illustrate ( <ref type="figure" target="#fig_2">Fig. 3</ref>). The feature extraction module comprises convlayers that produce the feature maps (ResNet-101's conv1, bn1, res2, res3, res4 layers), and the head module includes an RoI pooling layer and the subsequent top layers (res5, avg-pool, and fc layers). After AU masks are obtained, unrelated areas can be excluded. However, each AU mask is an irregular polygon area, which means it cannot be directly fed into the fc layer. Therefore, we introduce the RoI pooling layer originally from Fast R-CNN <ref type="bibr" target="#b15">[16]</ref>. The RoI pooling layer is designed to convert the features inside any rectangle RoI (or bounding box) into a small feature map with a fixed spatial extent of H × W . To utilize the RoI pooling layer, each AU mask is converted into a minimum bounding box (named "AU bounding box") around the mask to input 3 <ref type="figure" target="#fig_4">(Fig. 6</ref>). The RoI pooling layer needs a parameter named "RoI size", indicates the RoI's height and width after pooling. In our experiment, we set RoI size to 14 × 14 in ResNet101 backbone and 7 × 7 in VGG-16 and VGG-19 backbone.  Object detection networks, such as Fast R-CNN, aim to identify and localize the object. Benefiting from the design of the AU mask, we have strong confidence in where the AUs should occur; thus, we can concentrate on what the AUs are. <ref type="figure" target="#fig_2">Fig. 3</ref> depicts the AU R-CNN's forward process. In the RoI pooling layer, we input the AU bounding box and feature map (The bounding box coordinates and feature map are usually 16× smaller than the input image resolution). We treat the last fully connected layer's output vector as predicted label probabilities. The total AU category number we wish to discriminate is set</p><formula xml:id="formula_0">(a) AU group # 1 (b) AU group # 2 (c) AU group # 3 (d) AU group # 4</formula><p>(e) AU group # 5 (f) AU group # 6 (g) AU group # 7 (h) AU group # 8 <ref type="figure">Fig. 4</ref>. Action Unit masks for AU group #1 ∼ #8 (see <ref type="table" target="#tab_1">Table 2</ref>).</p><p>as L 4 ; the number of bounding boxes in each image is R 5 ; the ground truth y ∈ {0, 1} R×L , y i,j indicates the (i, j)-th element of y, where y i,j = 0 denotes AU j is inactive in bounding box i, and AU j is active if y i,j = 1. The ground truth y must satisfy the AU partition rule's space constraint: y i,j = 0 if AU j does not belong to bounding box i's corresponding AU group ( <ref type="figure" target="#fig_4">Fig. 6</ref> and <ref type="table" target="#tab_1">Table 2</ref>). The RoI-level prediction probability iŝ y ∈ R R×L . Given multiple labels inside each RoI (e.g. AU 10 and AU 12 often occur together in the mouth area), we adopt the multi-label sigmoid cross-entropy loss function, namely,</p><formula xml:id="formula_1">L(y,ŷ) = − 1 R R r=1 L l=1 {y r,l log(ŷ r,l )}<label>(1)</label></formula><p>Unlike ROI-Nets <ref type="bibr" target="#b2">[3]</ref> and EAC-Net <ref type="bibr" target="#b3">[4]</ref>, AU R-CNN has considerably fewer parameters due to the sharing of conv-layer in  the feature extraction module, which leads to space and time saving. The RoI pooling layer and RoI-level label also help improve classifier performance through the space constraint and supervised information of the RoIs.</p><p>In the inference stage, the last fc layer's output is converted to a binary integer prediction vector using the threshold of zero (the elements that greater than 0 set to 1, others set to 0). Mul-tiple RoIs' prediction results are merged via a "bit-wise OR" operator to obtain the image-level label. We report F1 scores of this merged image-level prediction results in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Dynamic model extension of AU R-CNN</head><p>AU R-CNN can use only static RGB images to learn. A natural extension is to use the RoI feature map extracted from AU R-CNN to model the temporal dependency of RoIs across frames. In this extension, we can adopt various dynamic models to observe RoI-level appearance changes (Experiments are shown in Section 4.5). In this section, we introduce one extension that integrates ConvLSTM <ref type="bibr" target="#b17">[18]</ref> into the AU R-CNN architecture. <ref type="figure" target="#fig_3">Fig. 5</ref> shows the AU R-CNN integrated with ConvLSTM architecture. In each image, we first extract nine AU group RoI features (7 × 7 × 2048) corresponding to nine AU bounding boxes of <ref type="figure" target="#fig_4">Fig. 6</ref> from the last conv-layer. To represent the evolvement of facial local regions, we construct an RoI parallel line stream with nine timelines. The timeline is constructed by skipping four frames per time-step in the video to eliminate the similar frames. In total, we set 10 time-steps for each iteration. In each timeline, we connect the RoI at the current frame to the corresponding RoI at the adjacent frames, e.g. the mouth area has only temporal correlation to the next/previous frame's mouth area. Therefore, each timeline corresponds to an AU bounding box's evolution across time. Nine ConvL-STM kernels are used to process on the nine timelines. The output of each ConvLSTM kernel are fed into two fc layers to produce the prediction probability. More specifically, Let's denote the mini-batch size as N . the time-steps as T , the channel, height and width of RoI feature as C, H and W respectively. The concatenation of ConvLSTM's all time-step's output is a five-dimensional tensor of shape [N, T, C, H, W ]. We reshape this tensor to a two-dimensional tensor of shape [N × T, C × H × W ], the first dimension is treated as the mini-batch of shape [N × T ]. This reshaped tensor is input to two fc layers to get a prediction probability vector of shape [N ×T, Class], where Class denotes AU category number. We adopt the sigmoid cross-entropy loss function to minimize difference between the prediction probability vector and ground truth, which is the same as Eq. 1. In the inference stage, we use the last frame's prediction result of the 10-frame video clip to evaluate. This model, named "AR ConvLST M ", is trained together with AU R-CNN in an end-to-end form.</p><p>The introduction of the dynamic model extension brings new issues, as shown in our experiments (Section 4.5), the dynamic model cannot always improve overall performance as expected. We use database statistics and a data visualization technique to identify the effective cases. Various statistics of BP4D and DISFA databases are collected, including the AU duration of each database and the AU group bounding box areas. Liet al. <ref type="bibr" target="#b3">[4]</ref> found that the occurrence of AUs in the database has the influence of static-image-based AU detection classifiers. However, in the ConvLSTM extension model, the average AU activity duration of videos and AR ConvLST M classification performance are correlated. <ref type="figure">Fig. 9</ref> provides an intuitive figure of such correlation, when the AU duration increases at high peak, the performance of AR ConvLST M can be always improved. Therefore, in situations such as long-duration activities, AR ConvLST M can be adopted to improve the performance. Other dynamic models can also be integrated into AU R-CNN, including the two-stream network, TAL-Net, and the general graph CRF model. In Section 4.5, we collect the experiment results and analyze various dynamic models in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">Dataset description</head><p>We evaluate our method on two datasets, namely, BP4D dataset <ref type="bibr" target="#b20">[21]</ref> and DISFA dataset <ref type="bibr" target="#b21">[22]</ref>. For both datasets, we adopt a 3-fold partition to ensure that the subjects are mutually exclusive in the train/test split sets. AUs that present more than 5% base rate are included for evaluation. In total, we select 12 AUs on BP4D and 8 AUs on DISFA to report the experiment results.</p><p>(1) BP4D <ref type="bibr" target="#b20">[21]</ref> contains 41 young adults of different races and genders (23 females and 18 males). We use 328 videos (41 participants ×8 videos) captured in total, which result in ∼ 140,000 valid face images. We select positive samples as those with AU intensities equal to or higher than A-level, and the rest are negative samples. We use 3-fold splits exactly the same as <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> partition to ensure that the training and testing subjects are mutually exclusive. The average AU activity duration of all videos in BP4D and the total activity segment count are shown in <ref type="table" target="#tab_0">Table 14</ref>. The average AU mask bounding box area is provided in <ref type="table" target="#tab_8">Table 9</ref>.</p><p>(2) DISFA <ref type="bibr" target="#b21">[22]</ref> contains 27 subjects. We use ∼ 260,000 valid face images and 54 videos (27 videos captured by left camera and 27 videos captured by right camera). We also use the 3-fold split partition protocol in the DISFA experiment. The average AU activity duration of all videos in DISFA and the total activity segment count are shown in <ref type="table" target="#tab_0">Table 15</ref>. The average AU mask bounding box area is given in <ref type="table" target="#tab_0">Table 10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Evaluation metric</head><p>Our task is to detect whether the AUs are active, which is a multi-label binary classification problem. Since our approach focuses on RoI prediction for each bounding box <ref type="figure" target="#fig_4">(Fig. 6)</ref>, the RoI-level prediction is a binary vector with L elements, where L denotes the total AU category number we wish to discriminate. We use the image-level prediction to evaluate, which is obtained by using a "bit-wise OR" operator for merging an image's RoI-level predictions. After obtaining the image-level prediction, we directly use the database provided image-level ground truth labels to evaluate, which are binary vectors with elements equal 1 for active AUs and equal 0 for inactive AUs. The F1 score can be used as an indicator of the performances of the algorithms on each AU and is widely employed in AU detection. In our evaluation, we compute frame-based F1 score <ref type="bibr" target="#b8">[9]</ref> for 12 AUs in BP4D and 8 AUs in DISFA on image-level prediction. The overall performance of the algorithm is described by the average F1 score(denoted as Avg.).  We collect the F1 scores of the most popular state-of-theart approaches that used the same 3-fold protocol in <ref type="table" target="#tab_4">Table  4</ref> and <ref type="table" target="#tab_6">Table 7</ref> to compare our approaches with other methods. These techniques include a linear support vector machine (LSVM), active patch learning (APL <ref type="bibr" target="#b42">[43]</ref>), JPML <ref type="bibr" target="#b12">[13]</ref>, a confidence-preserving machine (CPM <ref type="bibr" target="#b9">[10]</ref>), a block-based region learning CNN (DRML <ref type="bibr" target="#b13">[14]</ref>), an enhancing and cropping nets (EAC-net <ref type="bibr" target="#b3">[4]</ref>), an ROI adaption net (ROI-Nets <ref type="bibr" target="#b2">[3]</ref>), and LSTM fused with a simple CNN (CNN+LSTM <ref type="bibr" target="#b37">[38]</ref>), an optimized filter size CNN (OFS-CNN <ref type="bibr" target="#b14">[15]</ref>). We also conduct complete control experiments of AU R-CNN in <ref type="table">Table 5</ref> and <ref type="table" target="#tab_7">Table 8,</ref>   <ref type="table" target="#tab_2">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3.">Compared methods</head><formula xml:id="formula_2">CNNres × × ARvgg16 × ARvgg19 × ARres × AR mean box × × AR F P N × AR ConvLST M AR2stream × AR CRF × × AR T AL × × ×</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4.">Implementation details</head><p>We resize the face images to 512 × 512 after cropping the face areas. Each image and bounding boxes are horizontally mirrored randomly before being sent to AU R-CNN for data augmentation. We subtract the mean pixel value from all the images in the dataset before sending to AU R-CNN. We use dlib <ref type="bibr" target="#b39">[40]</ref> to landmark faces, and the landmark operator is consequently time consuming. We cache the mask in the memcached database to accelerate speed in later epochs. The VGG and ResNet-101 backbones of AU R-CNN use pre-trained Ima-geNet ILSVRC dataset <ref type="bibr" target="#b44">[45]</ref> weights to initialize. AU R-CNN is initialized with a learning rate of 0.001 and further reduced by a factor of 0.1 after every 10 epochs. In all experiments, we select momentum stochastic gradient descent to train AU R-CNN for 25 epochs and set momentum to 0.9 and weight decay to 0.0005. The mini-batch size is set to 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Conventional CNN versus AU R-CNN</head><p>AU R-CNN is proposed for adaptive regional learning in Section 3.3. Thus, our first experiment aims to determine whether it can perform better than the baseline conventional CNN, which uses entire face images to learn. We suppose that by learning the adaptive RoIs separately, recognition capability can be improved. We train CNN res and AR res on the BP4D and DISFA datasets using the same ResNet-101 backbone for comparison. Twelve AUs in BP4D and eight AUs in DISFA are used; therefore, AR res and CNN res use the sigmoid cross-entropy loss function, as shown in Eq. 1. Both models are based on static images. During each iteration, we randomly select five images to comprise one mini-batch to train and initialize the learning rate to 0.001. <ref type="figure">Fig. 7</ref> demonstrates the example detection results of our approach. <ref type="table">Table 5</ref> and <ref type="table" target="#tab_7">Table 8</ref> show the BP4D and DISFA results, in which the margin is larger in DISFA (3.69%) than in BP4D (2.1%). These results can be attributed to the relatively lower resolution images in DISFA, which cause AR res to benefit more. We also show that AU R-CNN performs efficiently with varying image resolutions. Experiments have been conducted to compare the proposed AU R-CNN and baseline CNN with the same ResNet-101 backbone on the BP4D database with different resolutions of the input image. <ref type="table" target="#tab_5">Table 6</ref> shows the result, and the resolutions of images are set to 256 × 256, 416 × 416, 512 × 512, and 608 × 608. Most AU results prefer AU R-CNN model by observing subtle cues of facial appearance changes. In 256 × 256, although the resolution is nearly half of that in 512 × 512, the performance is close to that in 512 × 512. This similarity leads to efficient detection when using 256 × 256. But in the highest resolution 608 × 608, the F1 score is lower than that of 512 × 512, we believe this performance drop can be attribute to two possible reasons. (1) As pointed out by Han et al. <ref type="bibr" target="#b14">[15]</ref>, when the image resolution increases to 608 × 608, the receptive field covers a smaller actual area of the entire face when using the same convolution filter size. The smaller receptive field deteriorates the vision. (2) Larger images produce larger feature maps before RoI pooling layer in AR res , or larger feature maps before the final avg pooling layer in CNN res . The increase of feature map size also increases each pooling grid cell's covered size dramatically in RoI pooling/avg pooling layer, which has negative impact on high level features. Regardless of the overall improvement of AU R-CNN across the 12 AUs. In AU 10 and AU 12, CNN and AU R-CNN obtain similar results. One explanation is that AU 10 and AU 12 have relatively sufficient training samples compared with other AUs.</p><p>In the DISFA dataset ( <ref type="table" target="#tab_7">Table 8</ref>, <ref type="table" target="#tab_6">Table 7</ref>), AR res outperforms CNN res in six out of eight AUs. The two remaining AUs are AU 12 and AU 25. As shown in <ref type="table" target="#tab_0">Table 10</ref>, AU 12 and AU 25 have the largest area proportions (29.8 % and 26.6 %) on the face images. In BP4D and DISFA, AU 1 (inner brow raiser) has a significant improvement in AR res because of the relatively small area on the face.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">ROI-Nets versus AU R-CNN</head><p>Our proposed AU R-CNN in Section 3.3 is designed to recognize local regional AUs in static images under AU mask. Previous state-of-the-art static image AU detection approach ROI-Nets <ref type="bibr" target="#b2">[3]</ref> also focuses on regional learning. It attempts to learn regional features by using individual conv-layers over regions centered on AU center <ref type="figure" target="#fig_6">(Fig. 8)</ref>. The two models are based on static images, whereas our AU R-CNN uses the shared convlayer in feature extraction module and RoI-level supervised information. This choice saves space and time, and provides accurate guidance. Instead of using the concept of the AU center area, we introduce the AU mask. We believe that AU mask can preserve more context information than cropping the bounding box from AU center. ROI-Nets adopts VGG-19 as backbone. For fair comparison, we adopt VGG-19 based AU R-CNN (denoted as AR vgg19 ) to compare. AR vgg19 outperforms ROI-Nets in 8 out of 12 AUs in BP4D ( <ref type="table" target="#tab_4">Table 4</ref>).</p><p>The interesting part lies in AU 23 (lip pressor) and AU 24 (lip tighter), in which AR vgg19 significantly outperforms ROI-Nets by 7.8% and 10.9%, respectively. This superiority is because the lip area is a relatively small area on face; AU R-CNN uses AU mask and RoI-level label so that it can concentrate on this  <ref type="table">Table 5</ref>. Control experiments for BP4D. Results are reported using F1 score on 3-fold protocol.</p><p>AU CNNres ARvgg16 ARvgg19 ARres AR mean box AR F P N AR ConvLST M AR2stream AR CRF AR T AL   area. This fact can be verified from <ref type="table" target="#tab_8">Table 9</ref> that the AU 23 and AU 24 bounding box only occupies 14.7% area of the face image. Other typical cases are AU 1, AU 2, and AU 4, which are located in the areas around eyebrows and eyes; AR vgg19 outperforms ROI-Nets by 8.5%, 11.9%, and 8.8%, respectively. In AU 6 (cheek raiser, see <ref type="figure">Fig. 4(c)</ref>), AU 10, AU 12, AU 14, and AU 15 results, ROI-Nets and AU R-CNN achieve close results. These areas occupy relatively large proportions in the image <ref type="table" target="#tab_8">(Table 9)</ref>, and ROI-Nets focuses on the central large area of the image. The experiment in DISFA dataset <ref type="table" target="#tab_6">(Table 7)</ref> demonstrates the similar result. The above comparisons prove that, AU R-CNN better expresses the classification information of local regions than ROI-Nets. We also found that the ResNetbased AU R-CNN (AR res ) outperforms AR vgg19 in the BP4D and DISFA datasets, and achieves the best performance over all static-image-based approaches. For better representation of AU features, we conduct our remaining experiments on the basis of AR res features. AU CNNres ARvgg16 ARvgg19 ARres AR mean box AR F P N AR ConvLST M AR2stream AR CRF We further evaluate the inference time of our approach, LCN (CNN with locally connected layer <ref type="bibr" target="#b46">[47]</ref>) and ROI-Nets on a Nvidia Geforce GTX 1080Ti GPU. We run each network for 20 trails over 1000 iterations with the mini-batch size sets to 1; then we evaluate the running time for each iteration, and finally compute the mean and standard deviation over the 20 trials. The inference time is showed in <ref type="table" target="#tab_0">Table 11</ref>, we can see our approach benefits from the RoI pooling layer's parallel computing over multiple bounding boxes, its inference time is lower than LCN and ROI-Nets. The RoI-Nets adopt 20 individual conv-layers for 20 bounding boxes, thus it results worst performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">AU R-CNN + Mean Box</head><p>The computation of each image's precise landmark point location is time consuming. We believe it is enough to use the "mean" AU bounding box coordinates to represent all images' bounding boxes. In this section, we collect the average coordinates of all images of nine AU group bounding boxes in each database to form a unified "mean box" across all images ( <ref type="table" target="#tab_0">Table 12 and Table 13</ref>). We use this "mean box" coordinates to replace the real bounding box coordinates calculated from the landmark in each image to evaluate. The experiment results are shown in <ref type="table">Table 5</ref> and <ref type="table" target="#tab_7">Table 8</ref>, denoted as AR mean box . Although most images of BP4D and DISFA dataset are the frontal face, the deviation of mean bounding box coordinates from real box location exists. However, the F1 score is remarkably close to AR res , because the RoI pooling layer in AU R-CNN performs a coarse spatial quantization. This performance similar-  ity demonstrates that AU R-CNN is robust to small landmark location error, and the computation consumption of each image's landmark can be saved via using "mean box".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">AU R-CNN + Feature Pyramid Network</head><p>In the previous sections, we use the single scale (16× smaller scale) RoI feature to detect. Feature Pyramid Network (FPN) <ref type="bibr" target="#b43">[44]</ref> is a popular architecture for leveraging a CNN's pyramidal features in the object detection field, which has semantics from low to high levels. In this experiment, FPN is integrated into AU R-CNN's backbone as feature extractor that extracts RoI features from the feature pyramid. The assignment of an RoI of width w and height h to the level k of FPN is as follows <ref type="bibr" target="#b43">[44]</ref>:</p><formula xml:id="formula_3">k = k 0 + log 2 ( √ wh/224)<label>(2)</label></formula><p>The experiment results (denoted as AR F P N ) are shown in <ref type="table">Table 5</ref> and <ref type="table" target="#tab_7">Table 8</ref>. The AR F P N performs worse than the single-scale RoI feature counterpart AR res . This is because AU R-CNN needs high-level RoI features to classify AUs well and does not need to perform box coordinate regression. Furthermore, the bounding boxes in AU R-CNN are not too small to detect compared with those in the object detection scenario. Therefore, pyramidal features are not necessary in detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Static versus Dynamic</head><p>Can the previous state of facial expression action always improve AU detection? In this section, we conduct a series of experiments using the most popular dynamic models that are integrated into AU R-CNN, including AR ConvLST M , as described in Section 3.4, to determine the answer.  <ref type="figure">Fig. 9</ref>. Correlation between F1 score improvement of that in AR ConvLST M over that in AR res and AU activity duration, AU activity duration is rescaled presenting clarity.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1.">AU R-CNN + ConvLSTM</head><p>In this section, we conduct experiments on AR ConvLST M , whose architecture is described in Section 3.4. <ref type="table">Table 5</ref> and <ref type="table" target="#tab_7">Table 8</ref> present that AR ConvLST M has a slightly lower average F1 score than AR res . The main reason of the overall performance drop is that the action duration varies drastically in different AUs <ref type="table" target="#tab_0">(Table 14 and Table 15</ref>); if the temporal length of AU duration is short, ConvLSTM model does not have sufficient capability to observe such actions. The switch of action is so rapid that ConvLSTM cannot infer such label change when processing in the video. We draw a plot of F1 score improvement of AR ConvLST M over AR res and average AU duration (rescale to 1/60 scale) in <ref type="figure">Fig. 9</ref> to justify our hypothesis. Other factors also influence the performance of ConvLSTM, we can see the red line and the black line have strong correlation in most AUs except AU 1, 2, 4 and AU 15, 17, 23. The reason of high F1 score improvement in AU 17 is that AU 17 has much more segment count (1203) than AU 15 and AU 23 <ref type="table" target="#tab_0">(Table 14)</ref>, which results in sufficient training samples of AU 17. The AU 4 has lower F1 score improvement than that of AU 1, 2, because AU 4's bounding box (corresponding AU group #2) is double the size of the area of AU 1 and AU 2 <ref type="figure" target="#fig_4">(Fig. 6)</ref>, the larger bounding box leads to weaker recognition capability of capturing the subtle skin change between eyebrows. Most AUs do not have long activity duration; hence, AR res surpasses AR ConvLST M in average F1 score. Convolutional two-stream network <ref type="bibr" target="#b18">[19]</ref> achieves impressive results in video action recognition. In this experiment, we experiment a two-stream network integrated into the AU R-CNN architecture for comparison, denoted as "AR 2stream ". We use a 10-frame optical flow and a single corresponding RGB image, <ref type="bibr" target="#b5">6</ref> which are fed into two AU R-CNNs. Both AU R-CNN branches use the same bounding boxes, which is the corre- The performance of the two-stream network AR 2stream is remarkably close to that of RGB-image-based AR res , which is slightly worse in the BP4D database <ref type="table">(Table 5</ref>) and is better in the DISFA database <ref type="table" target="#tab_7">(Table 8</ref>). In BP4D, the score significantly increases in AU 17 and AU 24 in AR res . All these AUs are in the lip area. We attribute this result to the relative small area in the lip area causes the optical flow to be an obvious signal to classify. If we check the result in DISFA dataset in <ref type="table" target="#tab_7">Table  8</ref>, this reason can be verified -AU 1, AU 6, and AU 9 in the DISFA dataset have the smallest AU group areas <ref type="table" target="#tab_0">(Table 10)</ref>, and the F1 scores of these AUs increase. However, the performance of AR ConvLST M in these AUs cannot be improved compared with AR 2stream . This justifies that AU group bounding box area is not the reason of the performance improvement in AR ConvLST M but is the reason of performance improvement in AR 2stream . Although the average F1 score of AR 2stream is worse than that of AR res in the BP4D database, an interesting property exists in AR 2stream -the training convergence speed is faster than that in AR res (see loss curve comparison in <ref type="figure" target="#fig_0">Fig.  10</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2.">AU R-CNN + Two-Stream Network</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.3.">AU R-CNN + TAL-Net</head><p>TAL-Net <ref type="bibr" target="#b19">[20]</ref> follows the Faster R-CNN detection paradigm for temporal action location, and its goal is to detect 1D temporal segments in the time axis of videos. In this experiment, we regard the video sequence as separate segments, and each segment has one label with it. We use the same RoI parallel line stream with the AR ConvLST M because we want to detect each region's activity temporal segments. We reformulate the labels of segments in the AU video sequence as a label inside a start and end time interval. In TAL-Net, we use pre-computed AR 2stream features. We stack 10 1-D 3×3 kernel conv-layer on the 1-D feature map in the segment proposal network module to generate segment proposals, and we directly feed the precomputed 1-D feature map into the SoI pooling layer and subsequent fc layers. This network is denoted as "AR T AL ".</p><p>In our experiment, we determine that AR T AL cannot converge easily, and the loss can only decrease to approximately 1.3 at most (starting from approximately 2.7), which causes AR T AL to perform worse than AR ConvLST M ( <ref type="table">Table 5</ref>). We can attribute this result to two reasons. First, facial expression is more subtle than the obvious human body action, and the temporal action localization mechanism cannot work efficiently. Second, training 1-D conv-layer and fc layers requires millions of data samples, which cannot be satisfied when converting an entire video to a 1-D feature map. Therefore, this model has the worst performance among all dynamic models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.4.">AU R-CNN + General Graph CRF</head><p>CRF model is a classical model for graph inference. We experiment with an interesting idea that involves connecting all separate parts of faces in a video to construct a spatio-temporal graph and then using the general graph CRF to learn from such a graph. This model is denoted as "AR CRF ". We not only connect RoIs with the same AU group number in the adjacent frames of the time axis but also fully connect different RoIs inside each frame, thereby yielding a spatio-temporal graph. In this method, the entire facial expression video is converted to a spatio-temporal graph using pre-computed 2048-D features extracted by AR res (average pooling layer's output). This graph encodes not only the temporal dependencies of RoIs but also the spatial dependencies of each frame's RoIs. <ref type="table">Table 5</ref> and <ref type="table" target="#tab_7">Table 8</ref> present that AR CRF has a lower score than does AR res in BP4D and DISFA. We attribute this score decrease to the number of weight parameters. In AR CRF , we have only |F| × |Y| + |E| × |Y| 2 weight parameters in total (in BP4D, it is 45,540), where |F| denotes the feature dimension, |Y| denotes the class number, and |E| denotes the number of edge type. We extract 2048-D features from the average pooling layer. Two other fc layers exist on top of the average pooling layer in AR res , and their weight matrices are 2048 × 1000 and 1000 × 22, which result in 2,070,000 parameters that are much more than 45,540 in AR CRF . Therefore, classification performance is influenced not only by correlation but also by model capacity (including the number of learned parameters).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.5.">Dynamic models summary</head><p>After above discussion, the features and application of dynamic models extension can be summarized in <ref type="table" target="#tab_0">Table 16</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we present AU R-CNN for AU detection. It focuses on adaptive regional learning using expert prior knowledge, whose introduction provides accurate supervised information and fine-grained guidance for detection. Complete comparison experiments are conducted, and the results show that the presented model outperforms state-of-the-art approaches and the conventional CNN baseline model which uses the same backbone, proving the benefit of introducing the expert prior knowledge. We also investigate various dynamic architectures that are integrated into AU R-CNN, which demonstrate that the static-image-based AU R-CNN outperforms all the dynamic models. Experiments conducted on the BP4D and DISFA databases manifest the effectiveness of our approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The overall of AU R-CNN framework. It recognizes each RoI-level's label based on the AU partition rule, which uses the landmark point location information to encode the expert prior knowledge. This rule indicates the place where the AUs may occur. AU R-CNN head module focuses on recognizing each bounding box to improve performance. accurate landmark positioning (see experiment Section 4.3.1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Landmark and region partition of face. Yellow and white numbers indicate the RoI number and landmark number respectively. Left: Partition of 43 RoIs. Right: Position of blue point is the average position of landmark 13 and 29.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>AU R-CNN using ResNet-101 backbone architecture, where #class denotes the AU category number we wish to discriminate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>AU R-CNN integrated with ConvLSTM architecture, where N denotes mini-batch size; T denotes the frames to process in each iteration; R denotes AU bounding box number; C, H, and W denotes the ConvLSTM's output channel number, height and width respectively. #class denotes the AU category number we wish to discriminate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>AU bounding boxes, which are defined as the minimum bounding box around each AU mask. Since AU group #1 has two symmetrical regions, the bounding box number is 9.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>17 Fig. 7 .</head><label>177</label><figDesc>including ResNet-101 based traditional CNN that classifies the entire face images (CNN res ), ResNet-101 based AU R-CNN (AR res ), VGG-16 based AU R-CNN (AR vgg16 ), VGG-19 based AU R-CNN (AR vgg19 ), mean bounding boxes (a) AU 1, 4, 7, 10 (b) AU 6, 7, 10, 12 (c) AU 2, 14, 17 (d) AU 6, 7, 12 (e) AU 7, 14, Example figures of detection result. version AU R-CNN (AR mean box ), AU R-CNN incorporate with Feature Pyramid Network [44](AR F P N ), AU R-CNN integrated with ConvLSTM [18] (AR ConvLST M ), AU R-CNN with optical flow and RGB feature fusion two-stream network architecture [19](AR 2stream ), general graph CRF with features extracted by AU R-CNN(AR C RF ), and AU R-CNN with a temporal action localization in video network, TAL-Net [20](AR T AL ). We use ResNet-101 based CNN(CNN res ) as our baseline model. The details of the compared models are summarized in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>The AU centers of ROI-Nets, each AU center location is an offset of landmark point, and the 3 × 3 bounding boxes centered at AU centers from top layer's feature map are cropped.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 .</head><label>10</label><figDesc>AR res vs. AR 2stream train loss curve</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>FACS definition of AUs and related muscles<ref type="bibr" target="#b0">[1]</ref> </figDesc><table><row><cell>AU number</cell><cell>AU name</cell><cell>Muscle Basis</cell></row><row><cell>1</cell><cell>Inner brow raiser</cell><cell>Frontalis</cell></row><row><cell>2</cell><cell>Outer brow raiser</cell><cell>Frontalis</cell></row><row><cell>4</cell><cell>Brow lowerer</cell><cell>Corrugator supercilii</cell></row><row><cell>6</cell><cell>Cheek raiser</cell><cell>Orbicularis oculi</cell></row><row><cell>7</cell><cell>Lid tightener</cell><cell>Orbicularis oculi</cell></row><row><cell>10</cell><cell>Upper lip raiser</cell><cell>Levator labii superioris</cell></row><row><cell>12</cell><cell>Lip corner puller</cell><cell>Zygomaticus major</cell></row><row><cell>14</cell><cell>Dimpler</cell><cell>Buccinator</cell></row><row><cell>15</cell><cell cols="2">Lip corner depressor Depressor anguli oris</cell></row><row><cell>17</cell><cell>Chin raiser</cell><cell>Mentalis</cell></row><row><cell>23</cell><cell>Lip tightener</cell><cell>Orbicularis oris</cell></row><row><cell>24</cell><cell>Lip pressor</cell><cell>Orbicularis oris</cell></row><row><cell>25</cell><cell>Lips part</cell><cell>Depressor labii inferioris</cell></row><row><cell>26</cell><cell>Jaw drop</cell><cell>Masseter</cell></row><row><cell cols="3">incorporated into AU R-CNN. Such temporal dependency can-</cell></row><row><cell cols="3">not always improve performance in all cases (Section 4.5).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>AU partition rule Symbol * means the corresponding AU group have symmetrical regions. Symbol ∈ indicates the "label fetch".</figDesc><table><row><cell>AU group</cell><cell>AU NO</cell><cell>RoI NO</cell></row><row><cell># 1  *</cell><cell>AU 1 , AU 2 ,</cell><cell>1, 2, 5, 6, 8, 9, 12, 13,</cell></row><row><cell>(∈# 2)</cell><cell>AU 5 , AU 7</cell><cell>40, 41, 42, 43</cell></row><row><cell># 2</cell><cell>AU 4</cell><cell>1, 2, 3, 4, 5, 6, 8, 9, 12,</cell></row><row><cell></cell><cell></cell><cell>13, 40, 41</cell></row><row><cell># 3</cell><cell>AU 6</cell><cell>16, 17, 18, 19, 42, 43</cell></row><row><cell># 4 (∈ # 3)</cell><cell>AU 9</cell><cell>10, 11, 17, 18, 22, 23</cell></row><row><cell># 5 (∈ # 6)</cell><cell>AU 10 , AU 11 , AU 12 , AU 13 , AU 14 , AU 15</cell><cell>21, 22, 23, 24, 25, 26, 27, 28, 37</cell></row><row><cell># 6 (∈ # 5)</cell><cell>AU 16 , AU 20 , AU 25 , AU26 , AU 27</cell><cell>25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37</cell></row><row><cell># 7 (∈ # 6)</cell><cell>AU 17</cell><cell>29, 30, 31, 32, 33, 34, 35, 36</cell></row><row><cell># 8 (∈ # 5, # 6)</cell><cell>AU 18 , AU 22 , AU 28 AU 23 , AU 24 ,</cell><cell>26, 27, 29, 30, 31, 32, 37</cell></row><row><cell>Note:</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Compared models details</figDesc><table><row><cell>Model</cell><cell>E2E ML RGB LANDMARK CONVERGE VIDEO</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>F1 score result comparison with state-of-the-art methods on BP4D dataset. Bracketed bold numbers indicate the best score; bold numbers indicate the second best. AU LSVM JPML [13] DRML [14] CPM [10] CNN+LSTM [38] EAC-Net [4] OFS-CNN [15] ROI-Nets [3] FERA [46] ARvgg16 ARvgg19 ARres</figDesc><table><row><cell>1</cell><cell>23.2</cell><cell>32.6</cell><cell>36.4</cell><cell>43.4</cell><cell>31.4</cell><cell>39</cell><cell>41.6</cell><cell>36.2</cell><cell>28</cell><cell>47.5</cell><cell>44.8</cell><cell>[50.2]</cell></row><row><cell>2</cell><cell>22.8</cell><cell>25.6</cell><cell>41.8</cell><cell>40.7</cell><cell>31.1</cell><cell>35.2</cell><cell>30.5</cell><cell>31.6</cell><cell>28</cell><cell>40.5</cell><cell>43.5</cell><cell>[43.7]</cell></row><row><cell>4</cell><cell>23.1</cell><cell>37.4</cell><cell>43</cell><cell>43.4</cell><cell>[71.4]</cell><cell>48.6</cell><cell>39.1</cell><cell>43.4</cell><cell>34</cell><cell>55.1</cell><cell>52.2</cell><cell>57</cell></row><row><cell>6</cell><cell>27.2</cell><cell>42.3</cell><cell>55</cell><cell>59.2</cell><cell>63.3</cell><cell>76.1</cell><cell>74.5</cell><cell>77.1</cell><cell>70</cell><cell>73.8</cell><cell>75.7</cell><cell>[78.5]</cell></row><row><cell>7</cell><cell>47.1</cell><cell>50.5</cell><cell>67</cell><cell>61.3</cell><cell>77.1</cell><cell>72.9</cell><cell>62.8</cell><cell>73.7</cell><cell>78</cell><cell>76.6</cell><cell>75.2</cell><cell>[78.5]</cell></row><row><cell>10</cell><cell>77.2</cell><cell>72.2</cell><cell>66.3</cell><cell>62.1</cell><cell>45</cell><cell>81.9</cell><cell>74.3</cell><cell>[85]</cell><cell>81</cell><cell>82</cell><cell>82.7</cell><cell>82.6</cell></row><row><cell>12</cell><cell>63.7</cell><cell>74.1</cell><cell>65.8</cell><cell>68.5</cell><cell>82.6</cell><cell>86.2</cell><cell>81.2</cell><cell>[87]</cell><cell>78</cell><cell>85.2</cell><cell>85.9</cell><cell>[87]</cell></row><row><cell>14</cell><cell>64.3</cell><cell>65.7</cell><cell>54.1</cell><cell>52.5</cell><cell>72.9</cell><cell>58.8</cell><cell>55.5</cell><cell>62.6</cell><cell>[75]</cell><cell>64.9</cell><cell>63.4</cell><cell>67.7</cell></row><row><cell>15</cell><cell>18.4</cell><cell>38.1</cell><cell>36.7</cell><cell>34</cell><cell>34</cell><cell>37.5</cell><cell>32.6</cell><cell>45.7</cell><cell>20</cell><cell>48.8</cell><cell>45.3</cell><cell>[49.1]</cell></row><row><cell>17</cell><cell>33</cell><cell>40</cell><cell>48</cell><cell>54.3</cell><cell>53.9</cell><cell>59.1</cell><cell>56.8</cell><cell>58</cell><cell>36</cell><cell>60.6</cell><cell>60</cell><cell>[62.4]</cell></row><row><cell>23</cell><cell>19.4</cell><cell>30.4</cell><cell>31.7</cell><cell>39.5</cell><cell>38.6</cell><cell>35.9</cell><cell>41.3</cell><cell>38.3</cell><cell>41</cell><cell>43.9</cell><cell>46.1</cell><cell>[50.4]</cell></row><row><cell>24</cell><cell>20.7</cell><cell>42.3</cell><cell>30</cell><cell>37.8</cell><cell>37</cell><cell>35.8</cell><cell>-</cell><cell>37.4</cell><cell>-</cell><cell>[49.3]</cell><cell>48.3</cell><cell>[49.3]</cell></row><row><cell cols="2">Avg 35.3</cell><cell>45.9</cell><cell>48.3</cell><cell>50</cell><cell>53.2</cell><cell>55.9</cell><cell>53.7</cell><cell>56.4</cell><cell>51.7</cell><cell>60.7</cell><cell>60.3</cell><cell>[63]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>F1 score of varying resolutions comparison result on BP4D dataset. The bold highlights the best performance in each resolution experiment.</figDesc><table><row><cell>resolution</cell><cell cols="2">256 × 256</cell><cell cols="2">416 × 416</cell><cell cols="2">512 × 512</cell><cell cols="2">608 × 608</cell></row><row><cell>AU</cell><cell cols="8">CNNres ARres CNNres ARres CNNres ARres CNNres ARres</cell></row><row><cell>1</cell><cell>45.6</cell><cell>50.1</cell><cell>47.4</cell><cell>49.3</cell><cell>45.8</cell><cell>50.2</cell><cell>44.3</cell><cell>47.5</cell></row><row><cell>2</cell><cell>43.6</cell><cell>46.5</cell><cell>38.3</cell><cell>42.1</cell><cell>43.2</cell><cell>43.7</cell><cell>40.1</cell><cell>39.2</cell></row><row><cell>4</cell><cell>52.2</cell><cell>54.6</cell><cell>53.3</cell><cell>50.0</cell><cell>54.3</cell><cell>57.0</cell><cell>49.5</cell><cell>53.5</cell></row><row><cell>6</cell><cell>74.9</cell><cell>77.7</cell><cell>75.7</cell><cell>75.2</cell><cell>77.4</cell><cell>78.5</cell><cell>76.3</cell><cell>76.9</cell></row><row><cell>7</cell><cell>76.3</cell><cell>78.3</cell><cell>75.7</cell><cell>78.7</cell><cell>77.9</cell><cell>78.5</cell><cell>76.4</cell><cell>78.6</cell></row><row><cell>10</cell><cell>82.5</cell><cell>81.7</cell><cell>82.4</cell><cell>82.3</cell><cell>81.8</cell><cell>82.6</cell><cell>81.5</cell><cell>82.7</cell></row><row><cell>12</cell><cell>86.5</cell><cell>87.5</cell><cell>87.2</cell><cell>86.5</cell><cell>85.8</cell><cell>87.0</cell><cell>87.5</cell><cell>85.5</cell></row><row><cell>14</cell><cell>55.4</cell><cell>62.1</cell><cell>59.5</cell><cell>61.9</cell><cell>60.8</cell><cell>67.7</cell><cell>59.5</cell><cell>62.0</cell></row><row><cell>15</cell><cell>48.0</cell><cell>51.2</cell><cell>44.1</cell><cell>49.2</cell><cell>50.0</cell><cell>49.1</cell><cell>44.9</cell><cell>49.6</cell></row><row><cell>17</cell><cell>59.9</cell><cell>61.8</cell><cell>57.5</cell><cell>61.4</cell><cell>58.3</cell><cell>62.4</cell><cell>57.4</cell><cell>61.3</cell></row><row><cell>23</cell><cell>44.7</cell><cell>46.2</cell><cell>41.2</cell><cell>44.9</cell><cell>47.6</cell><cell>50.4</cell><cell>45.6</cell><cell>45.1</cell></row><row><cell>24</cell><cell>46.9</cell><cell>52.3</cell><cell>44.5</cell><cell>47.7</cell><cell>48.4</cell><cell>49.3</cell><cell>48.2</cell><cell>51.1</cell></row><row><cell>Avg</cell><cell>59.7</cell><cell>62.5</cell><cell>58.9</cell><cell>60.8</cell><cell>60.9</cell><cell>63.0</cell><cell>59.3</cell><cell>61.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>F1 score result comparison with state-of-the-art methods on DISFA dataset. Bracketed bold numbers indicate the best score; bold numbers indicate the second best.</figDesc><table><row><cell cols="9">AU LSVM APL [43] DRML [14] ROI-Nets [3] CNNres ARvgg16 ARvgg19 ARres</cell></row><row><cell cols="2">1 10.8</cell><cell>11.4</cell><cell>17.3</cell><cell>[41.5]</cell><cell>26.3</cell><cell>24.9</cell><cell>26.9</cell><cell>32.1</cell></row><row><cell>2</cell><cell>10</cell><cell>12</cell><cell>17.7</cell><cell>[26.4]</cell><cell>23.4</cell><cell>23.5</cell><cell>21</cell><cell>25.9</cell></row><row><cell cols="2">4 21.8</cell><cell>30.1</cell><cell>37.4</cell><cell>[66.4]</cell><cell>51.2</cell><cell>55.5</cell><cell>59.6</cell><cell>59.8</cell></row><row><cell cols="2">6 15.7</cell><cell>12.4</cell><cell>29</cell><cell>50.7</cell><cell>48.1</cell><cell>51</cell><cell>[56.5]</cell><cell>55.3</cell></row><row><cell cols="2">9 11.5</cell><cell>10.1</cell><cell>10.7</cell><cell>8.5</cell><cell>29.9</cell><cell>41.8</cell><cell>[46]</cell><cell>39.8</cell></row><row><cell cols="2">12 70.4</cell><cell>65.9</cell><cell>37.7</cell><cell>[89.3]</cell><cell>69.4</cell><cell>68</cell><cell>67.7</cell><cell>67.7</cell></row><row><cell>25</cell><cell>12</cell><cell>21.4</cell><cell>38.5</cell><cell>[88.9]</cell><cell>80.1</cell><cell>74.9</cell><cell>79.8</cell><cell>77.4</cell></row><row><cell cols="2">26 22.1</cell><cell>26.9</cell><cell>20.1</cell><cell>15.6</cell><cell>52.4</cell><cell>49.4</cell><cell>47.6</cell><cell>[52.6]</cell></row><row><cell cols="2">Avg 21.8</cell><cell>23.8</cell><cell>26.7</cell><cell>48.5</cell><cell>47.6</cell><cell>48.6</cell><cell>50.7</cell><cell>[51.3]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>Control experiments for DISFA. Results are reported using F1 score on 3-fold protocol.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 .</head><label>9</label><figDesc>Average bounding box area in BP4D</figDesc><table><row><cell>AU group</cell><cell># 1</cell><cell># 2</cell><cell># 3</cell><cell># 5</cell><cell># 7</cell><cell># 8</cell></row><row><cell>AU index</cell><cell>1,2,7</cell><cell>4</cell><cell>6</cell><cell>10,12,</cell><cell>17</cell><cell>23,24</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>14,15</cell><cell></cell><cell></cell></row><row><cell cols="2">Avg box area (pixels) 17785</cell><cell>46101</cell><cell>54832</cell><cell cols="2">103875 42388</cell><cell>38470</cell></row><row><cell>Area proportion</cell><cell>6.8%</cell><cell>17.6%</cell><cell>20.9%</cell><cell cols="3">39.6 % 16.2 % 14.7 %</cell></row><row><cell cols="6">Table 10. Average bounding box area in DISFA</cell><cell></cell></row><row><cell>AU group</cell><cell># 1</cell><cell># 2</cell><cell># 3</cell><cell># 4</cell><cell># 5</cell><cell># 6</cell></row><row><cell>AU index</cell><cell>1,2</cell><cell>4</cell><cell>6</cell><cell>9</cell><cell>12</cell><cell>25,26</cell></row><row><cell cols="2">Avg box area (pixels) 17545</cell><cell>45046</cell><cell>46317</cell><cell>48393</cell><cell>78131</cell><cell>69624</cell></row><row><cell>Area proportion</cell><cell>6.7%</cell><cell>17%</cell><cell>17.7%</cell><cell cols="3">18.5 % 29.8 % 26.6 %</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 11 .</head><label>11</label><figDesc>Inference time(ms) of VGG-19 on 512 × 512 images</figDesc><table><row><cell>Ours</cell><cell>ROI-Nets [3]</cell><cell>LCN [47]</cell></row><row><cell cols="3">27.4 ± 0.0005 67.7 ± 0.0004 34.7 ± 0.008</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 12 .</head><label>12</label><figDesc>Mean box coordinates of 512 × 512 images in BP4D.</figDesc><table><row><cell cols="3">AU group AU index Mean boxes coordinates (ymin, xmin, ymax, xmax format)</cell></row><row><cell># 1</cell><cell>1,2,7</cell><cell>(30.4, 58.1, 140.3, 222.5), (30.1, 297.2, 140.9, 456.5)</cell></row><row><cell># 2</cell><cell>4</cell><cell>(23.9, 57.8, 139, 455.9)</cell></row><row><cell># 3</cell><cell>6</cell><cell>(109.4, 79.8, 264.5, 431.8)</cell></row><row><cell># 5</cell><cell cols="2">10,12,14,15 (198.9, 35.2, 437.0, 472.6)</cell></row><row><cell># 7</cell><cell>17</cell><cell>(378.7, 94.5, 510.9, 416.6)</cell></row><row><cell># 8</cell><cell>23,24</cell><cell>(282.7,145.5,455.0,368.3)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 13 .</head><label>13</label><figDesc>Mean box coordinates of 512×512 images in DISFA. AU group AU index Mean boxes coordinates (ymin, xmin, ymax, xmax format)</figDesc><table><row><cell># 1</cell><cell>1,2</cell><cell>(55.5, 71.3, 168.6, 220.0), (53.5, 277.6, 167.6, 431.4)</cell></row><row><cell># 2</cell><cell>4</cell><cell>(48.5, 58.7, 165.1, 444.0)</cell></row><row><cell># 3</cell><cell>6</cell><cell>(141.4, 86.7, 281.5, 418.9)</cell></row><row><cell># 4</cell><cell>9</cell><cell>(107.8, 152.2, 348.8, 352.8)</cell></row><row><cell># 5</cell><cell>12</cell><cell>(236.9, 53.5, 433.3, 454.4)</cell></row><row><cell># 6</cell><cell>25,26</cell><cell>(316.4, 73.8, 511.0, 433.4)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 14 .</head><label>14</label><figDesc>AU average duration &amp; segments count in BP4D</figDesc><table><row><cell>AU</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>7</cell><cell>10 12 14 15 17 23 24</cell></row><row><cell cols="7">Avg duration 65 66 73 125 142 148 184 120 38 42 30 49</cell></row><row><cell cols="7">Seg count 474 380 408 540 569 591 448 571 647 1203 806 458</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 15 .</head><label>15</label><figDesc>AU average duration &amp; segments count in DISFA</figDesc><table><row><cell>AU</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>9</cell><cell>12</cell><cell>25</cell><cell>26</cell></row><row><cell cols="2">Avg duration 55</cell><cell>68</cell><cell cols="3">112 115 96</cell><cell cols="3">133 154 82</cell></row><row><cell cols="9">Seg count 320 218 438 340 148 464 600 606</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 16 .</head><label>16</label><figDesc>The features and applications of dynamic models extension Inappropriate MediumLow accuracy and has more layers than ARres AR ConvLST M Suitable for long AU activity duration case Slow Accuracy can be improved in long duration activities AR2streamSuitable for AUs in small sub-regions especially eye or mouth area Fast but need pre-compute optical flow Need pre-compute optical flow first AR CRF Application in the case of CPU only Medium and need pre-computed features Small model parameter size and no need to use GPU AR T AL Inappropriate FastThe training cannot fully converged sponding bounding boxes of RGB image branch, for classification. Two produced 7 × 7 × 2048 RoI features are concatenated along the channel dimension. The channel size of 4096 feature map is yielded, which will be reduced to 2048 channels using one kernel size of 1 conv-layer. The features are sent to two fc layers to obtain the classification scores. The ground truth label involved in calculating the loss function adopts the single RGB image's labels.</figDesc><table><row><cell>Model</cell><cell>Application</cell><cell>Training speed</cell><cell>Feature</cell></row><row><cell>ARres</cell><cell>Most cases, no need for the video context</cell><cell>Fast</cell><cell>High accuracy and universal application</cell></row><row><cell>AR F P N</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">AU group #1 contains two separate symmetrical regions, thus it contains two bounding boxes, which results in total 9 AU bounding boxes, one more than AU group number.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">L = 22 in BP4D database and L = 12 in DISFA database. 5 R = 9 in BP4D database (Fig. 6) and R = 7 in DISFA database (since DISFA doesn't contain AU group# 7 and AU group # 8).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">This corresponding RGB image is in the corresponding location that centers on 10 flow frames</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Facial action coding system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">V</forename><surname>Friesen</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Multi-view dynamic facial action unit detection, Image and Vision Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>León</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Action unit detection with region adaptation, multi-labeling learning and optimal temporal fusing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Abtahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1841" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Eac-net: A region-based deep enhancing and cropping approach for facial action unit detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Abtahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th IEEE International Conference on Automatic Face &amp; Gesture Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="103" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-conditional latent variable model for joint facial action unit detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Eleftheriadis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rudovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3792" to="3800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A dynamic texture-based approach to recognition of facial actions and their temporal models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Koelstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Patras</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1940" to="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Capturing global semantic relationships for facial action unit recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3304" to="3311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Selective transfer machine for personalized facial action unit detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3515" to="3522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Facial action unit event detection by cascade of tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2400" to="2407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Confidence preserving machine for facial action unit detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3622" to="3630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Au-aware deep networks for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th IEEE International Conference and Workshops on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
	<note>Automatic Face and Gesture Recognition (FG)</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fera 2015 -second facial expression recognition and analysis challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Almaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mehu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<idno type="DOI">10.1109/FG.2015.7284874</idno>
	</analytic>
	<monogr>
		<title level="m">11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">06</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Joint patch and multi-label learning for facial action unit and holistic expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3931" to="3946" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep region and multi-label learning for facial action unit detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3391" to="3399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Optimizing filter size in convolutional neural networks for facial action unit recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>O&amp;apos;reilly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5070" to="5078" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xingjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rethinking the faster r-cnn architecture for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1130" to="1139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bp4d-spontaneous: a high-resolution spontaneous 3d dynamic facial expression database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Canavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Girard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="692" to="706" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Disfa: A spontaneous facial action intensity database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mavadati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="151" to="160" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Haar features for facs au recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Whitehill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Omlin</surname></persName>
		</author>
		<idno type="DOI">10.1109/FGR.2006.61</idno>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Automatic Face and Gesture Recognition (FGR06)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="5" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Action unit detection using sparse appearance descriptors in space-time video volumes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<idno type="DOI">10.1109/FG.2011.5771416</idno>
	</analytic>
	<monogr>
		<title level="m">Face and Gesture</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="314" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recognizing facial actions using gabor wavelets with neutral face average difference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Bazzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V</forename><surname>Lamar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face and Gesture Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="505" to="510" />
		</imprint>
	</monogr>
	<note>Sixth IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fully automatic facial action unit detection and temporal analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPRW.2006.85</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition Workshop (CVPRW&apos;06</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="149" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ambadar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPRW.2010.5543262</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition -Workshops</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="94" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Detection, tracking, and classification of action units in facial expression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="131" to="146" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fully automatic recognition of the temporal phases of facial actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="28" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rethinking diversified and discriminative proposal generation for visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Benitez-Quiroz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5562" to="5570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Constrained joint cascade regression framework for simultaneous facial action unit recognition and facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3400" to="3408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Exploiting sparsity and cooccurrence structure for action unit recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcduff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vasisht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<idno type="DOI">10.1109/FG.2015.7163081</idno>
	</analytic>
	<monogr>
		<title level="m">11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">How much training data for facial action unit detection?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Jeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Tube convolutional neural network (t-cnn) for action detection in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5822" to="5831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pose-based cnn features for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chéron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>-Cnn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3218" to="3226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Generalized multimodal factorized high-order pooling for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beyond</forename><surname>Bilinear</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2018.2817340</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5947" to="5959" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning spatial and temporal cues for multi-label facial action unit detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<idno type="DOI">10.1109/FG.2017.13</idno>
	</analytic>
	<monogr>
		<title level="m">12th IEEE International Conference on Automatic Face Gesture Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multi view facial action unit detection based on cnn and blstm-rnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1109/FG.2017.108</idno>
	</analytic>
	<monogr>
		<title level="m">12th IEEE International Conference on Automatic Face Gesture Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="848" to="853" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dlib-ml: A machine learning toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1755" to="1758" />
			<date type="published" when="2009-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for largescale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning multiscale active facial patches for expression analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCYB.2014.2354351</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1499" to="1510" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2009.5206848</idno>
		<idno>doi:10.1109/ CVPR.2009.5206848</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep learning the dynamic appearance and shape of facial action units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jaiswal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of Computer Vision (WACV), 2016 IEEE Winter Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">His research interests include facial expression analysis, action unit detection, and deep learning interpretability</title>
	</analytic>
	<monogr>
		<title level="m">Chen Ma is currently pursuing the Ph.D. degree with the School of Software Department</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>Tsinghua University</orgName>
		</respStmt>
	</monogr>
	<note>Beijing, China. He received the Master degree from Beijing University of Posts and Telecommunications</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">She is currently an Associate Professor with the Institute of Computer Graphics and Computer Aided Design, School of Software</title>
		<imprint>
			<date type="published" when="1996" />
			<pubPlace>Hangzhou, China; Beijing, China</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Li Chen received the Ph.D. degree in visualization from Zhejiang University ; Tsinghua University</orgName>
		</respStmt>
	</monogr>
	<note>Her research interests include data visualization, mesh generation. and parallel algorithm</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">He received several awards, such as the National Excellent Doctoral Dissertation Award, the National Science Fund for Distinguished Young Scholars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jun-Hai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the Best Paper Award of the ACM SIGGRAPH/Eurographics Symposium on Computer Animation, the Outstanding Service Award as an Associate Editor of the Computers and Graphics</title>
		<meeting><address><addrLine>Beijing, China; Hong Kong; Lexington, KY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1996" />
		</imprint>
		<respStmt>
			<orgName>Yong is currently a Professor with the School of Software, Tsinghua University ; Hong Kong University of Science and Technology ; University of Kentucky</orgName>
		</respStmt>
	</monogr>
	<note>) journal, and several National Excellent Textbook Awards. His main research interests include computer-aided design and computer graphics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
