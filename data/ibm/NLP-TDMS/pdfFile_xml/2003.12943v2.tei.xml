<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adaptive Object Detection with Dual Multi-Label Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DiDi Chuxing</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DiDi Chuxing</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Carleton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DiDi Chuxing</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieping</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DiDi Chuxing</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Adaptive Object Detection with Dual Multi-Label Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>cross-domain object detection, auxiliary task</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a novel end-to-end unsupervised deep domain adaptation model for adaptive object detection by exploiting multi-label object recognition as a dual auxiliary task. The model exploits multi-label prediction to reveal the object category information in each image and then uses the prediction results to perform conditional adversarial global feature alignment, such that the multimodal structure of image features can be tackled to bridge the domain divergence at the global feature level while preserving the discriminability of the features. Moreover, we introduce a prediction consistency regularization mechanism to assist object detection, which uses the multi-label prediction results as an auxiliary regularization information to ensure consistent object category discoveries between the object recognition task and the object detection task. Experiments are conducted on a few benchmark datasets and the results show the proposed model outperforms the stateof-the-art comparison methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The success of deep learning models has led to great advances for many computer vision tasks, including image classification <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b15">16]</ref>, image segmentation <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b42">43]</ref> and object detection <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28]</ref>. The smooth deployment of the deep models typically assumes a standard supervised learning setting, where a sufficient amount of labeled data is available for model training and the training and test images come from the same data source and distribution. However, in practical applications, the training and test images can come from different domains that exhibit obvious deviations. For example, <ref type="figure" target="#fig_0">Figure 1</ref> demonstrates images from domains with different image styles, which obviously present different visual appearances and data distributions. The violation of the i.i.d sampling principle across training and test data prevents effective deployment of supervised learning techniques, while acquiring new labeled data in each test domain is costly and impractical. To address this problem, unsupervised domain adaptation has recently received increasing attention <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>Unsupervised domain adaptation aims to adapt information from a labelrich source domain to learn prediction models in a target domain that only has unlabeled instances. Although many unsupervised domain adaptation methods have been developed for simpler image classification and segmentation tasks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref>, much fewer domain adaptation works have been done on the more complex object detection task, which requires recognizing both the objects and their specific locations. The authors of <ref type="bibr" target="#b1">[2]</ref> propose a domain adaptive faster R-CNN model for cross-domain object detection, which employs the adversarial domain adaptation technique <ref type="bibr" target="#b9">[10]</ref> to align cross-domain features at both the image-level and instance-level to bridge data distribution gaps. This adaptive faster R-CNN method presents some promising good results. However, due to the typical presence of multiple objects in each image, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>, both the image-level and instance-level feature alignments can be problematic without considering the specific objects contained. The more recent work <ref type="bibr" target="#b30">[31]</ref> proposes to address the problem of global (image-level) feature alignment by incorporating an additional local feature alignment under a strong-weak alignment framework for cross-domain object detection, which effectively improved the performance of the domain adaptive faster R-CNN. Nevertheless, this work still fails to take the latent object category information into account for crossdomain feature alignment. With noisy background and various objects, a whole image can contain very complex information and the overall features of an image can have complex multimodal structures. Aiming to learn an accurate object detector in the target domain, it is important to induce feature representations that minimize the cross-domain feature distribution gaps, while preserving the cross-category feature distribution gaps.</p><p>In light of the problem analysis above, in this paper we propose a novel endto-end unsupervised deep domain adaptation model, Multi-label Conditional distribution Alignment and detection Regularization model (MCAR), for multiobject detection, where the images in the target domain are entirely unannotated. The model exploits multi-label prediction as an auxiliary dual task to reveal the object category information in each image and then uses this information as an additional input to perform conditional adversarial cross-domain feature alignment. Such a conditional feature alignment is expected to improve the discriminability of the induced features while bridging the cross-domain representation gaps to increase the transferability and domain invariance of features. Moreover, as object recognition is typically easier to solve and can yield higher accuracy than the more complex object detection task, we introduce a consistency regularization mechanism to assist object detection, which uses the multi-label prediction results as auxiliary regularization information for the object detection part to ensure consistent object category discoveries between the object recognition task and the object detection task.</p><p>The contribution of this work can be summarized as follows: (1) This is the first work that exploits multi-label prediction as an auxiliary dual task for the multi-object detection task. <ref type="bibr" target="#b1">(2)</ref> We deploy a novel multi-label conditional adversarial cross-domain feature alignment methodology to bridge domain divergence while preserving the discriminability of the features. (3) We introduce a novel prediction consistency regularization mechanism to improve the detection accuracy. (4) We conduct extensive experiments on multiple adaptive multi-object detection tasks by comparing the proposed model with existing methods, and demonstrate effective empirical results for the proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Object Detection. Detection models have benefited from using advanced convolutional neural networks as feature extractors. Many widely used detection methods are two-stage methods based on the region of interest (ROI) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b28">29]</ref>. The RCNN in <ref type="bibr" target="#b11">[12]</ref> is the first detection model that deploys the ROI for object detection. It extracts features independently from each region of interest in the image, instead of using the sliding window and manual feature design in traditional object detection methods. Later, the author of <ref type="bibr" target="#b10">[11]</ref> proposed a Fast-RCNN detection model, which adopts a ROI pooling operation to share the convolution layers between all ROIs and improve the detection speed and accuracy. The work in <ref type="bibr" target="#b28">[29]</ref> made further improvements and proposed the Faster-RCNN, which combines Region Proposal Network (RPN) with Fast-RCNN to replace selective search and further improve detection performance. Faster-RCNN provides a foundation for many subsequent research studies <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b27">28]</ref>. In this work and many related unsupervised domain adaptation methods, the widely used two-stage method, Faster-RCNN, is adopted as the backbone detection model.</p><p>Unsupervised Domain Adaptation. Unsupervised domain adaptation has attracted a lot of attention in computer vision research community and made great progress <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b32">33]</ref>. The main idea employed in these works is to learn feature representations that align distributions across domains. For example, the work in <ref type="bibr" target="#b9">[10]</ref> adopts the principle of generative adversarial networks (GANs) <ref type="bibr" target="#b13">[14]</ref> through a gradient reversal layer (GRL) <ref type="bibr" target="#b8">[9]</ref> to achieve cross-domain feature alignment. The work in <ref type="bibr" target="#b24">[25]</ref> further extends adversarial adaptation into conditional adversarial domain adaptation by taking the classifier's prediction into account. The works in <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b2">3]</ref> use image generation to realize cross-domain feature transformation and align the source and target domains. Moreover, some other works adopt distance metric learning methods, such as asymmetric metric learning <ref type="bibr" target="#b19">[20]</ref>, maximum mean discrepancy (MMD) minimization <ref type="bibr" target="#b6">[7]</ref> and Wasserstein distance minimization <ref type="bibr" target="#b32">[33]</ref>, to achieve domain alignment. Nevertheless, these studies focus on the simpler image classification and segmentation tasks.</p><p>Adaptive Object Detection. Recently domain adaptation for object detection has started drawing attention. The work in <ref type="bibr" target="#b1">[2]</ref> proposes an adaptive Faster-RCNN method that uses adversarial gradient reversal to achieve image-level and instance-level feature alignment for adaptive cross-domain object detection. <ref type="bibr" target="#b17">[18]</ref> adopts image transformation and exploits pseudo labels to realize a weakly supervised cross-domain detection. The work in <ref type="bibr" target="#b18">[19]</ref> leverages multi-style image generation between multiple domains to achieve cross-domain object detection. The authors of <ref type="bibr" target="#b30">[31]</ref> propose a strong and weak alignment of local and global features to improve cross-domain object detection performance. <ref type="bibr" target="#b43">[44]</ref> focuses on relevant areas for selective cross-domain alignment. <ref type="bibr" target="#b16">[17]</ref> adopts hierarchical domain feature alignment while adding a scale reduction module and a weighted gradient reversal layer to achieve domain invariance. <ref type="bibr" target="#b0">[1]</ref> advances the Mean Teacher paradigm with object relations for cross-domain detection. <ref type="bibr" target="#b33">[34]</ref> uses a gradient detach based multi-level feature alignment strategy for cross-domain detection. <ref type="bibr" target="#b39">[40]</ref> adopts multi-level feature adversary to achieve domain adaptation. Nevertheless, these methods are limited to cross-domain feature alignment, while failing to take the latent object category information into account when performing feature alignment. Our proposed model employs multi-label object recognition as an auxiliary task and uses it to achieve conditional feature alignment and detection regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we present the proposed Multi-label Conditional distribution Alignment and detection Regularization model (MCAR) for cross-domain adaptive object detection. We assume there are two domains from different sources and with different distributions. The source domain is fully annotated for object detection and the target domain is entirely unannotated. Let</p><formula xml:id="formula_0">X s = {(x s i , b s i , c s i )} ns i=1</formula><p>denote the annotated images from the source domain, where x s i denotes the i-th image, b s i and c s i denote the bounding boxes' coordinates and the category labels of the corresponding objects contained in the image respectively. Let X t = {x t i } nt i=1 denote the unannotated images from the target domain. We assume in total K classes of objects are presented in images of both the source and target domains. We aim to train an object detection model by exploiting the available data from both domains such that the model can have good detection performance in the target domain. The structure of the proposed MCAR model. Conditional adversarial global feature alignment is conducted through a domain discriminator by using multi-label prediction results as object category input. Meanwhile, multi-label prediction results are also used to provide a prediction consistency regularization mechanism on object detection after the RPN.</p><p>The main idea of the proposed MCAR model is to exploit multi-label prediction (for multi-object recognition) as an auxiliary task and use it to perform both conditional adversarial cross-domain feature alignment and prediction consistency regularization for the target object detection task. This end-to-end deep learning model adopts the widely used Faster-RCNN as the backbone detection network. Its structure is presented in <ref type="figure">Figure 2</ref>. Following this structure, we present the model in detail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multi-Label Prediction</head><p>The major difference between object recognition and object detection lies in that the former task only needs to recognize the presence of any object category in the given image, while the latter task needs to identify each specific object and its location in the image. The cross-domain divergence in image features that impacts the object recognition task can also consequently degrade the detection performance, since it will affect the region proposal network and the regional local object classification. Therefore we propose to deploy a simpler task of object recognition to help extract suitable image-level features that can bridge the distribution gap between the source and target domains, while being discriminative for recognizing objects.</p><p>In particular, we treat the object recognition task as a multi-label prediction problem <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b12">13]</ref>. It takes the global image-level features produced by the feature extraction network F of the Faster-RCNN model as input, and predicts the presence of K object category using K binary classifier networks, M 1 , · · · , M K .</p><p>These classifiers can be learned on the annotated images in the source domain, where the global object category label indicator vector y s i ∈ {0, 1} K for the i-th image can be gathered from its bounding boxes' labels c s i through a fixed transformation operation function ϕ : c s i → y s i , which simply finds all the existing object categories in c s i and represents their presence using y s i . The multi-label classifiers can then be learned by minimizing the following cross-entropy loss:</p><formula xml:id="formula_1">L multi = − 1 n s ns i=1 y s i log(p s i ) + (1−y s i ) log(1−p s i )<label>(1)</label></formula><p>where each k-th entry of the prediction output vector p s i is produced from the k-th binary classifier:</p><formula xml:id="formula_2">p s ik = M k (F (x s i ))<label>(2)</label></formula><p>which indicates the probability of the presence of objects from the k-th class.</p><p>The multi-label classifiers work on the global features extracted before the RPN of the Faster-RCNN. For Faster-RCNN based object detection, these global features will be used through RPNs to extract region proposals and then perform object classification and bounding box regression on the proposed regions. In the source domain, supervision information such as bounding boxes and the object labels are provided for training the detector, while in the target domain, the detection is purely based on the global features extracted and the detection model parameters (for RPN, region classifiers and regressors) obtained in the source domain. Hence it is very important to bridge the domain gap at the global feature level. Moreover, image features that led to good global object recognition performance are also expected to be informative for the local object classification on proposed regions. Therefore we will exploit multi-label prediction for global feature alignment and regional object prediction regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Conditional Adversarial Feature Alignment</head><p>The popular generative adversarial network (GAN) <ref type="bibr" target="#b13">[14]</ref> has shown that two distributions can be aligned by using a discriminator as an adversary to play a minimax two-player game. Following the same principle, conditional adversary is designed to take label category information into account. It has been suggested in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b26">27]</ref> that the cross-covariance of the predicted category information and the global image features can be helpful for avoiding partial alignment and achieving multimodal feature distribution alignment. We propose to integrate the multilabel prediction results together with the global image features extracted by F to perform conditional adversarial feature alignment at the global image level.</p><p>The key component network introduced is the domain discriminator D, which predicts the domain of the input image instance, with label 1 indicating the source domain and 0 indicating the target domain. As shown in <ref type="figure">Figure 2</ref>, the discriminator consists of a convolution filter layer f , which reduces the dimension of the input features, and a fully connected layer F C, which integrates the inputs to perform classification. It takes features F (x i ) and the multi-label prediction probability vector p i as input, and uses softmax activation function to produce probabilistic prediction output. For the conditional adversarial training, we adopted a focal loss <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b30">31]</ref>, which uses the prediction confidence deficiency score to weight each instance in order to give more weights to hard-to-classify examples. The loss of conditional adversarial training, L adv , is as below:</p><formula xml:id="formula_3">min F max D L adv = − 1 2 (L s adv + L t adv )<label>(3)</label></formula><formula xml:id="formula_4">L s adv = − 1 n s ns i=1 (1−D(F (x s i ), p s i )) γ log(D(F (x s i ), p s i )) L t adv = − 1 n t nt i=1 D(F (x t i ), p t i ) γ log(1−D(F (x t i ), p t i ))</formula><p>where γ is a modulation factor that controls how much to focus on the hardto-classify example; the global features F (x i ) and the multi-label prediction probability vector p i are integrated through a multi-linear mapping function such that</p><formula xml:id="formula_5">D(F (x i ), p i ) = F C(f (F (x i )) ⊗ p i ).</formula><p>With this adversary loss, the feature extractor F will be adjusted to try to confuse the domain discriminator D, while D aims to maximumly separate the two domains. This multi-label prediction conditioned adversarial feature alignment is expected to bridge the domain distribution gaps while preserving the discriminability for object recognition, which will improve the adaptation of the consequent region proposal, object classification on each proposed region and its location identification in the target domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Category Prediction based Regularization</head><p>The detection task involves recognizing both the objects and their locations, which is relatively more difficult than object recognition <ref type="bibr" target="#b7">[8]</ref>. The multi-label classifiers we applied can produce more accurate recognition results as the region proposal mistakes can be accumulated to objection classification on the proposed regions in the detection task. Based on such an observation, we propose a novel category prediction consistency regularization mechanism for object detection by exploiting multi-label prediction results.</p><p>Assume N region proposals are generated through the region proposal network (RPN) for an input image x. Each proposal will be classified into one of the K object classes using an object classifier C, while its location coordinates will be produced using a regressor R. The multi-class object classifier produces a length K prediction vectorq on each proposal that indicates the probability of the proposed region belonging to one of the K object classes. The object prediction on the total N proposals can form a prediction matrix Q ∈ [0, 1] K×N . We can then compute an overall multi-object prediction probability vector q by taking the row-wise maximum over Q, such that q k = max(Q(k, :)), and use q k as the prediction probability of the image x containing the k-th object category.</p><p>To enforce consistency between the prediction produced by the detector and the prediction produced by the multi-label object recognition, we propose to minimize the KL divergence between their prediction probability vectors p and q after renormalizing each vector with softmax function. As KL divergence is an asymmetric measure, we define the consistency regularization loss as:</p><formula xml:id="formula_6">L kl = L s kl + L t kl (4) L s kl = 1 2n s ns i=1 (KL(p s i , q s i ) + KL(q s i , p s i )) (5) L t kl = 1 2n t nt i=1 (KL(p t i , q t i ) + KL(q t i , p t i ))<label>(6)</label></formula><p>With this regularization loss, we expect the multi-label prediction results can assist object detection through unified mutual learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Overall End-to-End Learning</head><p>The detection loss of the base Faster-RCNN model, denoted as L det , is computed on the annotated source domain data under supervised classification and regression. It has two components, the proposal classification loss and the bounding box regression loss. We combine the detection loss, the multi-label prediction loss, the conditional adversarial feature alignment loss, and the prediction consistency regularization loss together for end-to-end deep learning. The total loss can be written as:</p><formula xml:id="formula_7">   L all = L det + λL adv + µL multi + εL kl min F max D L all<label>(7)</label></formula><p>where λ, µ, and ε are trade-off parameters that balance the multiple loss terms. We use SGD optimization algorithm to perform training, while GRL [9] is adopted to implement the gradient sign flip for the domain discriminator part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conducted experiments with multiple cross-domain multi-object detection tasks under different adaptation scenarios: (1) Domain adaptation from real to virtual image scenarios, where we used cross-domain detection tasks from PAS-CAL VOC <ref type="bibr" target="#b7">[8]</ref> to Watercolor2K <ref type="bibr" target="#b17">[18]</ref> and Comic2K <ref type="bibr" target="#b17">[18]</ref> respectively. (2) Domain adaption from normal/clear images to foggy image scenarios, where we used object detection tasks that adapt from Cityscapes <ref type="bibr" target="#b4">[5]</ref> to Foggy Cityscapes <ref type="bibr" target="#b31">[32]</ref>. In each adaptive object detection task, the images in the source domain are fully annotated and the images in the target domain are entirely unannotated. We present our experimental results and discussions in this section. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>In the experiments, we followed the setting of <ref type="bibr" target="#b30">[31]</ref> by using the Faster-RCNN as the backbone detection network, pretraining the model weights on the ImageNet, and using the same 600 pixels of images' shortest side. We set the training epoch as 25, and set λ, µ, ε, and γ as 0.5, 0.01, 0.1, and 5 respectively. The momentum is set as 0.9 and weight decay as 0.0005. For all experiments, we evaluated different methods using mean average precision (mAP) with a threshold of 0.5. By default, in the multi-label learning, all the convolutional layers have 3x3 convolution kernels and 512 channels. The convolution layer in conditional adversary learning also has 3x3 convolution kernel and 512 channels. These convolution parameters can be adjusted to suit different tasks, but our experiments all adopt the default setting, which yield good results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Domain Adaptation from Real to Virtual Scenes</head><p>In this set of experiments, we used the PASCAL VOC <ref type="bibr" target="#b7">[8]</ref> dataset as the source domain, and used the Watercolor2k and Comic2k <ref type="bibr" target="#b17">[18]</ref> as the target domains.  <ref type="table">Table 1</ref>. Our proposed MCAR model is compared with the source-only baseline and the state-of-theart adaptive object detection methods, including BDC-Faster <ref type="bibr" target="#b30">[31]</ref>, DA-Faster <ref type="bibr" target="#b1">[2]</ref>, SW-DA <ref type="bibr" target="#b30">[31]</ref>, and SCL <ref type="bibr" target="#b33">[34]</ref>. The Train-on-Target results, obtained by training on labeled data in the target domain, are provided as upperbound reference values. We can see under the same experimental conditions, our proposed method achieves the best overall result, while only underpeforming the Train-on-Target by 2.6%. Comparing to source only, our method achieves a remarkable overall performance improvement of 9.8%. Although SW-DA <ref type="bibr" target="#b30">[31]</ref> confirmed the validity of local and global feature alignment and showed a significant performance improvement over other methods, our method surpasses SW-DA by 2.7%. Mean-while, our method also outperforms SCL <ref type="bibr" target="#b33">[34]</ref> which relies on stacked multi-level feature alignment. The results suggest the proposed multi-label learning based feature alignment and prediction regularization are effective.</p><p>PASCAL VOC to Comic. The results of adaptation from PASCAL VOC to Comic are reported in <ref type="table">Table 2</ref>. Again, the proposed MCAR method achieved the best adaptive detection result. It outperforms the baseline, source-only (trained on source domain data without any adaptation), by 13.8%, and outperforms the best comparison method, SW-DA, by 4.1%, These results again show that our model is very suitable for adaptive multi-object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Adaptation from Clear to Foggy Scenes.</head><p>In this experiment, we perform adaptive object detection from normal clear images to foggy images. We use the Cityscapes dataset as the source domain. Its images came from 27 different urban scenes, where the annotated bounding boxes are generated by the original pixel annotations. We use the Foggy Cityscapes dataset as the target domain. Its images have been rendered by Cityscapes, which can simulate fog in real road conditions with deep rendering. They contain 8 categories: 'person', 'rider', 'car', 'truck', 'bus', 'train', 'motorcycle' and 'bicycle'. In this experiment, we used vgg16 <ref type="bibr" target="#b34">[35]</ref> as the backbone of the detection model. We recorded the test results on the validation set of Foggy Cityscapes.</p><p>The results are reported in the <ref type="table" target="#tab_1">Table 3</ref>. We can see the proposed MCAR method achieved the best adaptive detection result. It outperforms source-only by 15.4%, and outperforms the two best comparison methods, Dense-DA <ref type="bibr" target="#b39">[40]</ref> and SCL <ref type="bibr" target="#b33">[34]</ref>, by 2.8% and 0.9%. Moreover, it is worth noting that the performance of the proposed approach is very close to the Train-on-Target; the result of the Train-on-Target is only 1.5% higher than ours. Due to the very complex road conditions in this task, although the multi-label classifier is more capable of category judgment than the detection model, its accuracy is not much higher. Hence in this experiment, we used the combination of the multi-label category prediction and the object detection level category prediction. That is, we used sof tmax(p + q) as the label category information for the conditional adversarial feature alignment. This experiment presents and validates a natural variant of the proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>The proposed MCAR model has two major mechanisms, Multilabel-conditional adversary (MC) and Prediction based Regularization (PR), which are incorporated into the learning process through the three auxiliary loss terms in Eq. <ref type="formula" target="#formula_7">(7)</ref>: the conditional adversary loss L adv , the multi-label prediction loss L multi , and the prediction regularization loss L kl . The conditional adversary loss uses the multi-label prediction outputs as its conditions, and hence the two loss terms, L adv and L multi , together form the multilabel-conditional adversary (MC), while <ref type="table">Table 4</ref>. The ablation study results in terms of mAP(%) on the adaptive detection task of Cityscapes → Foggy Cityscapes. "w/o-adv" indicates dropping the conditional adversary loss; "uadv" indicates replacing the conditional adversary loss with an unconditional adversary loss; "w/o-PR" indicates dropping the prediction regularization loss; and "w/o-MP-PR" indicates dropping both the multilabel prediction loss and the prediction regularization loss. the prediction regularization (PR) is also built on the multi-label prediction outputs through the regularization loss L kl . To investigate the impact of these loss components, we conducted a more comprehensive ablation study on the adaptive detection task from Cityscapes to Foggy Cityscapes by comparing MCAR with its multiple variants. The variant methods and results are reported in <ref type="table">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>We can see that dropping the conditional adversary loss (MCAR-w/o-adv) leads to large performance degradation. This makes sense since the adversarial loss is the foundation for cross-domain feature alignment. By replacing the conditional adversary loss with an unconditional adversary loss, MCAR-uadv loses the multilabel-conditional adversary (MC) component, which leads to remarkable performance degradation and verifies the usefulness of the multi-label prediction based cross-domain multi-modal feature alignment. Dropping the prediction regularization loss from either MCAR, which leads to MCAR-w/o-PR, or MCAR-uadv, which leads to MCAR-uadv-w/o-PR, induces additional performance degradation. This verifies the effectiveness of the prediction regularization strategy, which is built on the multi-label prediction outputs as well. Moreover, by further dropping the multi-label prediction loss from MCAR-uadv-w/o-PR, the variant MCAR-uadv-w/o-MP-PR's performance also drops slightly. Overall these results validated the effectiveness of the proposed MC and PR mechanisms, as well as the multiple auxiliary loss terms in the proposed learning objective.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Further Analysis</head><p>Feature visualization. On the task of adaptation from Cityscapes to Foggy Cityscapes, we used t-SNE <ref type="bibr" target="#b25">[26]</ref> to compare the distribution of induced features between our model and the Source-only model (clear to fogg scenes). The results are shown in <ref type="figure" target="#fig_3">Figure 3</ref>. We can see that with the feature distribution obtained by source-only <ref type="figure" target="#fig_3">(Figure 3(a)</ref>), the source domain and target domain are obviously separated, which shows the existence of domain divergence. By contrast, our proposed method produced features that can well confuse the domain discriminators. This suggests that our proposed model has the capacity to bridge the domain distribution divergence and induce domain invariant features.</p><p>Parameters sensitivity analysis. We conducted sensitivity analysis on the two hyperparameters, λ and γ using the adaption task from PASCAL VOC to Watercolor. λ controls the weight of adversarial feature alignment, while γ controls the degree of focusing on hard-to-classify examples. Other hyperparameters are set to their default values. We conducted the experiment by fixing the value of γ to adjust λ, and then fixing λ to adjust γ. <ref type="table">Table 5</ref> presents the results. We can see with the decrease of parameter γ from its default value 5, the test performance degrades as the influence of domain classifier on difficult samples is weakened and the contribution of easy samples is increased. When γ = 1, it leads to the same result as the basic model, suggesting the domain regulation ability basically fails to play its role. On the other hand, a very large γ value is not good either, as the most difficult samples will dominate. For λ, we find that λ = 0.5 leads to the best performance. As detection is still the main task, it makes sense to have the λ &lt; 1. When λ = 0, it degrades to a basic model without feature alignment. Therefore, some value in the middle would be a proper choice.</p><p>Qualitative results. Object detection results are suitable to be qualitatively judged through visualization. Hence we present some qualitative adaptive detection results in the target domain in <ref type="figure" target="#fig_4">Figure 4</ref>. The top row of <ref type="figure" target="#fig_4">Figure 4</ref>  the qualitative detection result of three state-of-the-art adaptive detection methods, DA-Faster, SW-DA, and MCAR (ours), and the ground-truth on an image from Watercolor. We can see both 'DA-Faster' and 'SW-DA' have some false positives, while failing to detect the object of 'dog'. Our model correctly detected both the 'person' and the 'dog'. The bottom row of <ref type="figure" target="#fig_4">Figure 4</ref> presents the detection results of the DA methods and the ground-truth on an image from Foggy Cityscapes. We can see it is obvious that the cars in the distance are very blurred and difficult to detect due to the fog. The DA-Faster and SW-DA fail to find these cars, while our model successfully detected them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose an unsupervised multi-object cross-domain detection method. We exploit multi-label object recognition as a dual auxiliary task to reveal the category information of images from the global features. The crossdomain feature alignment is conducted by performing conditional adversarial distribution alignment with the combination input of global features and multilabel prediction outputs. We also use the idea of mutual learning to improve the detection performance by enforcing consistent object category predictions between the multi-label prediction over global features and the object classification over detection region proposals. We conducted experiments on multiple cross-domain multi-objective detection datasets. The results show the proposed model achieved the state-of-the-art performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>(a) and (b) are images from real scenes and virtual scenes respectively. It is obvious that the visual appearances of the images from different domains are very different, even if they contain the same categories of objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Fig. 2. The structure of the proposed MCAR model. Conditional adversarial global feature alignment is conducted through a domain discriminator by using multi-label prediction results as object category input. Meanwhile, multi-label prediction results are also used to provide a prediction consistency regularization mechanism on object detection after the RPN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Feature visualization results. (a) and (b) respectively represent the feature distribution results of the Source-only model and our model in the clear (Cityscapes) and foggy (Foggy Cityscapes) scenes. Red indicates from the source domain and blue indicates from the target domain</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Qualitative results on adaptive detection. The top row presents examples of domain adaptive detection from PASCAL VOC to Watercolor. The bottom row shows examples of adaptive detection from Cityscapes to Foggy Cityscapes. The green box represents the results obtained by the detection models, and the blue box represents the ground-truth annotation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Test results of domain adaptation for object detection from PASCAL VOC to Watercolor in terms of mean average precision (%). MC and PR indicate Multilabel-Conditional adversary and Prediction based Regularization, respectively. Test results of domain adaptation for object detection from PASCAL VOC to Comic, The definition of MC and PR is same as inTable 1.</figDesc><table><row><cell>Method</cell><cell>MC PR bike bird car</cell><cell>cat</cell><cell cols="2">dog person mAP</cell></row><row><cell>Source-only</cell><cell cols="3">68.8 46.8 37.2 32.7 21.3</cell><cell>60.7</cell><cell>44.6</cell></row><row><cell>BDC-Faster [31]</cell><cell cols="3">68.6 48.3 47.2 26.5 21.7</cell><cell>60.5</cell><cell>45.5</cell></row><row><cell>DA-Faster [2]</cell><cell cols="3">75.2 40.6 48.0 31.5 20.6</cell><cell>60.0</cell><cell>46.0</cell></row><row><cell>SW-DA [31]</cell><cell cols="3">82.3 55.9 46.5 32.7 35.5</cell><cell>66.7</cell><cell>53.3</cell></row><row><cell>SCL [34]</cell><cell cols="4">82.2 55.1 51.8 39.6 38.4 64.0</cell><cell>55.2</cell></row><row><cell>MCAR (Ours)</cell><cell cols="4">92.5 52.2 43.9 46.5 28.8 87.9 52.1 51.8 41.6 33.8 68.8 56.0 62.5 54.4</cell></row><row><cell>Train-on-Target</cell><cell cols="3">83.6 59.4 50.7 43.7 39.5</cell><cell>74.5</cell><cell>58.6</cell></row><row><cell>Method</cell><cell>MC PR bike bird car</cell><cell>cat</cell><cell cols="2">dog person mAP</cell></row><row><cell>Source-only</cell><cell cols="3">32.5 12.0 21.1 10.4 12.4</cell><cell>29.9</cell><cell>19.7</cell></row><row><cell>DA-Faster</cell><cell cols="3">31.1 10.3 15.5 12.4 19.3</cell><cell>39.0</cell><cell>21.2</cell></row><row><cell>SW-DA</cell><cell cols="3">36.4 21.8 29.8 15.1 23.5</cell><cell>49.6</cell><cell>29.4</cell></row><row><cell>MCAR (Ours)</cell><cell cols="4">40.9 22.5 30.3 23.7 24.7 53.6 47.9 20.5 37.4 20.6 24.5 50.2</cell><cell>32.6 33.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Test results of domain adaptation for object detection from Cityscapes to Foggy Cityscapes in terms of mAP (%). MC and PR are same as inTable 1. training and test sets. These 6 categories are included in the 20 categories of PASCAL VOC. We used the 1K training set in each target domain for training the domain adaptation model, while evaluating the model and report results with the 1K test set. In this experiment, we used resnet101<ref type="bibr" target="#b15">[16]</ref> as the backbone network of the detection model.</figDesc><table><row><cell>Method</cell><cell cols="3">MC PR person rider car truck bus train motorbike bicycle mAP</cell></row><row><cell>Source-only</cell><cell>25.1 32.7 31.0 12.5 23.9 9.1</cell><cell>23.7</cell><cell>29.1 23.4</cell></row><row><cell>BDC-Faster [31]</cell><cell>26.4 37.2 42.4 21.2 29.2 12.3</cell><cell>22.6</cell><cell>28.9 27.5</cell></row><row><cell>DA-Faster [2]</cell><cell>25.0 31.0 40.5 22.1 35.3 20.2</cell><cell>20.0</cell><cell>27.1 27.6</cell></row><row><cell>SC-DA [44]</cell><cell>33.5 38.0 48.5 26.5 39.0 23.3</cell><cell>28.0</cell><cell>33.6 33.8</cell></row><row><cell>MAF [17]</cell><cell>28.2 39.5 43.9 23.8 39.9 33.3</cell><cell>29.2</cell><cell>33.9 34.0</cell></row><row><cell>SW-DA [31]</cell><cell>36.2 35.3 43.5 30.0 29.9 42.3</cell><cell>32.6</cell><cell>24.5 34.3</cell></row><row><cell>DD-MRL [19]</cell><cell>30.8 40.5 44.3 27.2 38.4 34.5</cell><cell>28.4</cell><cell>32.2 34.6</cell></row><row><cell>MTOR [1]</cell><cell>30.6 41.4 44.0 21.9 38.6 40.6</cell><cell>28.3</cell><cell>35.6 35.1</cell></row><row><cell>Dense-DA [40]</cell><cell>33.2 44.2 44.8 28.2 41.8 28.7</cell><cell>30.5</cell><cell>36.5 36.0</cell></row><row><cell>SCL [34]</cell><cell>31.6 44.0 44.8 30.4 41.8 40.7</cell><cell>33.6</cell><cell>36.2 37.9</cell></row><row><cell>MCAR (Ours)</cell><cell>31.2 42.5 43.8 32.3 41.1 33.0 32.0 42.1 43.9 31.3 44.1 43.4</cell><cell>32.4 37.4</cell><cell>36.5 36.6 36.6 38.8</cell></row><row><cell>Train-on-Target</cell><cell>50.0 36.2 49.7 34.7 33.2 45.9</cell><cell>37.4</cell><cell>35.6 40.3</cell></row><row><cell cols="4">PASCAL VOC contains realistic images, while Watercolor2k and Comic2k con-</cell></row><row><cell cols="4">tain virtual scene images. There are significant differences between the source</cell></row><row><cell cols="4">and target domains. The training set of PASCAL VOC (Trainval of PASCAL</cell></row><row><cell cols="4">VOC 2007 and PASCAL VOC 2012) includes 20 different object labels and a</cell></row><row><cell cols="4">total of 16,551 images. Watercolor2k and Comic2k contain 6 different classes</cell></row><row><cell cols="4">('bicycle', 'bird', 'car', 'cat', 'Dog', 'person'), each providing 2K images, and</cell></row><row><cell cols="2">splitting equally into</cell><cell></cell><cell></cell></row></table><note>PASCAL VOC to Watercolor. The test detection results yield by adapta- tion from PASCAL VOC to Watercolor are reported in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>54.4 49.1 44.8 mAP 49.1 50.2 54.4 50.1 49.3</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="7">person rider car truck bus train motorbike bicycle mAP</cell></row><row><cell>MCAR</cell><cell></cell><cell></cell><cell></cell><cell cols="4">32.0 42.1 43.9 31.3 44.1 43.4</cell><cell>37.4</cell><cell>36.6 38.8</cell></row><row><cell cols="2">MCAR-w/o-PR</cell><cell></cell><cell></cell><cell cols="4">31.2 42.5 43.8 32.3 41.1 33.0</cell><cell>32.4</cell><cell>36.5 36.6</cell></row><row><cell cols="2">MCAR-uadv</cell><cell></cell><cell></cell><cell cols="4">31.7 42.0 45.7 30.4 39.7 14.9</cell><cell>28.6</cell><cell>36.5 33.7</cell></row><row><cell cols="3">MCAR-uadv-w/o-PR</cell><cell></cell><cell cols="4">32.8 40.1 43.8 23.0 30.9 14.3</cell><cell>30.3</cell><cell>33.1 31.0</cell></row><row><cell cols="8">MCAR-uadv-w/o-MP-PR 30.5 43.2 41.4 21.7 31.4 13.7</cell><cell>29.8</cell><cell>32.6 30.5</cell></row><row><cell cols="2">MCAR-w/o-adv</cell><cell></cell><cell></cell><cell cols="4">25.0 34.9 34.2 13.9 29.9 10.0</cell><cell>22.5</cell><cell>30.2 25.1</cell></row><row><cell cols="10">Table 5. Parameter sensitivity analysis on the adaptation task from PASCAL VOC</cell></row><row><cell>to watercolor.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>λ</cell><cell></cell><cell></cell><cell>0.5</cell><cell></cell><cell></cell><cell>γ</cell><cell></cell><cell>5</cell></row><row><cell>γ</cell><cell>1</cell><cell>3</cell><cell>5</cell><cell>7</cell><cell>9</cell><cell>λ</cell><cell cols="3">0.1 0.25 0.5 0.75</cell><cell>1</cell></row><row><cell cols="3">mAP 44.0 46.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Exploring object relation in mean teacher for cross-domain detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Domain adaptive faster R-CNN for object detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Self-ensembling with gan-based data augmentation for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation via regularized conditional alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10885</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Training generative neural networks via maximum mean discrepancy optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Dziugaite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>UAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<title level="m">Domain-adversarial training of neural networks</title>
		<imprint>
			<publisher>JMLR</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<title level="m">Fast R-CNN. In: ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<title level="m">Deep convolutional ranking for multilabel image annotation. ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Generative adversarial nets</title>
		<imprint>
			<publisher>NIPS</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<editor>Mask R-CNN</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Multi-adversarial Faster-RCNN for unrestricted object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Cross-domain weakly-supervised object detection through progressive domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Inoue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Furuta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Diversify and match: A domain adaptive representation learning paradigm for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">What you saw is not what you get: Domain adaptation using asymmetric kernel transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Conditional adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">From source to target and back: symmetric bi-directional adaptive gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Russo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Strong-weak distribution alignment for adaptive object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Semantic foggy scene understanding with synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Wasserstein distance guided representation learning for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">SCL: Towards accurate domain adaptive object detection via gradient detach based stacked complementary losses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maheshwari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.02559</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Domain adaptation for structured output via discriminative representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Multi-level domain adaptive learning for cross-domain detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
		<title level="m">Multilabel neural networks with applications to functional genomics and text categorization. TKDE</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Curriculum domain adaptation for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Adapting object detectors via selective cross-domain alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
