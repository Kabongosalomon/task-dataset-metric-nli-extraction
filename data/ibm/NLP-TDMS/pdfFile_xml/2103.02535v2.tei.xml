<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Style-based Point Generator with Adversarial Rendering for Point Cloud Completion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chulin</forename><surname>Xie</surname></persName>
							<email>chulinx2@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuxin</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
							<email>fangwen@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Style-based Point Generator with Adversarial Rendering for Point Cloud Completion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we proposed a novel Style-based Point Generator with Adversarial Rendering (SpareNet) for point cloud completion. Firstly, we present the channel-attentive EdgeConv to fully exploit the local structures as well as the global shape in point features. Secondly, we observe that the concatenation manner used by vanilla foldings limits its potential of generating a complex and faithful shape. Enlightened by the success of StyleGAN, we regard the shape feature as style code that modulates the normalization layers during the folding, which considerably enhances its capability. Thirdly, we realize that existing point supervisions, e.g., Chamfer Distance or Earth Mover's Distance, cannot faithfully reflect the perceptual quality of the reconstructed points. To address this, we propose to project the completed points to depth maps with a differentiable renderer and apply adversarial training to advocate the perceptual realism under different viewpoints. Comprehensive experiments on ShapeNet and KITTI prove the effectiveness of our method, which achieves state-of-the-art quantitative performance while offering superior visual quality. Code is available at https://github.com/microsoft/SpareNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>As the 3D scanning devices such as depth camera and LiDAR become ubiquitous, point clouds get easier to acquire and have recently attracted a surge of research interest in the vision and robotics community. However, raw points directly captured by those devices are usually sparse and incomplete due to the limited sensor resolution and occlusions. Hence, it is essential to infer the complete shape from the partial observation so as to facilitate various downstream * Equal contribution. Authors did this work during the internship at Microsoft Research Asia.</p><p>tasks <ref type="bibr" target="#b11">[12]</ref> such as classification and shape manipulation as required in real-world applications.</p><p>Due to the irregularity and unorderedness of point clouds, one workaround is to leverage intermediate representations, e.g., depth map <ref type="bibr" target="#b14">[15]</ref> or voxels <ref type="bibr" target="#b34">[35]</ref>, that are more amenable to neural networks. However, the representation transform may result in information loss, so the detailed structures can not be well preserved. With the emergence of point-based networks <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b11">12]</ref>, predominant methods <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b40">41]</ref> nowadays digest the partial inputs directly and estimate the complete point clouds in an end-to-end manner. These methods typically follow the encoder-decoder paradigm and adopt permutation invariant losses <ref type="bibr" target="#b7">[8]</ref>, e.g., Chamfer Distance or Earth Mover's Distance, for regressing the groundtruth.</p><p>Ideally, the point completion network should simultaneously meet the following needs: 1) The output is desired to faithfully preserve the detailed structures of the partial input; 2) The network has a strong imaginative power to infer the global shape from the partial clue; 3) the local structure should be sharp, accurate, and free from the corruption by the noise points. Nonetheless, existing methods fail to achieve the above goals because of the neglecting of the global context during the feature extraction, the insufficient capability of modeling the structural details, and the lack of perceptual metrics for measuring the visual quality.</p><p>In this paper, we propose Style-based Point generator with Adversarial REndering, i.e., the SpareNet, to circumvent the above issues. We have made improvements from encoder, generator, and loss function, and proposed 3 new modules: channel-attentive EdgeConv, Style-based Point Generator, and Adversarial Point Rendering. Firstly, while previous works employ PointNet or PointNet++ to learn point-wise or local features, we propose channel-attentive EdgeConv (Section 3.1), which not only considers the local information within the k-nearest neighbors but also wisely leverages the global context by aggregating the global fea-D <ref type="figure">Figure 1</ref>: The architecture of SpareNet. An encoder E encodes the partial points X into a shape code g, leveraged by a stylebased generator G to synthesize a coarse completion Y c , which is recurrently improved with refiner R into the final result Y . Adversarial point rendering is applied to advocate the perceptual realism of completed points under different views.</p><p>tures and weighting the feature channel attention for each point accordingly. The fusion of local and global context enriches the learnt representation, so the network is more powerful to characterize fine structures of the input.</p><p>Further, we claim that the vanilla folding module <ref type="bibr" target="#b36">[37]</ref> in conventional methods, which outputs the 3D shapes by morphing the 2D surfaces through multilayer perceptrons (MLP), has limited modeling capability due to the improper usage of the features, i.e., the features are tiled and concatenated to each location of the 2D lattice. Drawn by the success of StyleGAN <ref type="bibr" target="#b18">[19]</ref> in image synthesis, we boost the folding capability by regarding the learnt features as style codes, which can be used to modulate the feature normalization within the folding MLPs. The resulting style-based generator, as elaborated in Section 3.2, shows considerably improved capability of modeling structural details.</p><p>Last but not least, in order to generate visually-pleasing results, we propose to project the generated point clouds to view images (Section 3.3), whose realism is further examined by adversarial discriminators. Since the renderer we use is differentiable, the gradient from the discriminators will guide the network to learn the completion with high perceptual quality when viewed at different angles. We conduct extensive experiments on ShapeNet <ref type="bibr" target="#b3">[4]</ref> and KITTI <ref type="bibr" target="#b8">[9]</ref> datasets, and our SpareNet performs favorably over stateof-the-art methods both quantitatively and qualitatively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Point cloud processing. Pioneer works <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b39">40]</ref> propose max-pooling to aggregate the features of individual points to ensure permutation invariance. Such aggregation, however, neglects the contextual relationships among different points, resulting in the representation incapable to characterize the fine structures. To amend this, <ref type="bibr" target="#b25">[26]</ref> hierarchically groups the points and extracts features for the local context. Inspired by the tremendous success of 2D convolution, a surge of conv-based methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32]</ref> has recently emerged, which generalizes the convolution to irregular coordinate space. Meanwhile, graph-based methods <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b29">30]</ref> regard the point clouds as graph structures where points are treated as nodes, and their local connectivity is denoted by edges. In <ref type="bibr" target="#b31">[32]</ref>, the EdgeConv is proposed to process the k-nearest neighbor graph and dynamically models the locality according to not only the coordinate distance but also the semantic affinity. While these networks are powerful to characterize the local structure, they fail to simultaneously consider the global shape, i.e., the local feature extraction is unaware of the global information.</p><p>Point cloud reconstruction. To hallucinate the complete 3D coordinates, previous works design point decoders in various forms: multilayer perceptrons (MLPs) <ref type="bibr" target="#b4">[5]</ref>, hierarchical structures like a tree <ref type="bibr" target="#b27">[28]</ref> or a multi-level pyramid <ref type="bibr" target="#b16">[17]</ref>, or through iterative refinement <ref type="bibr" target="#b30">[31]</ref>.</p><p>While some works resort to proxy representations other than points for shape completion, like volumetric grids <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b34">35]</ref> or depth maps <ref type="bibr" target="#b14">[15]</ref>, FoldingNet <ref type="bibr" target="#b36">[37]</ref> uses 2D manifolds to represent 3D point clouds, which also inspires others to model the target point clouds as non-linear foldings of 2D grids. PCN <ref type="bibr" target="#b38">[39]</ref> predicts the folding of a single 2D patch, whereas AtlasNet <ref type="bibr" target="#b10">[11]</ref> and MSN <ref type="bibr" target="#b20">[21]</ref> generate the output with multiple patches. Recently, SA-Net <ref type="bibr" target="#b32">[33]</ref> proposes a hierarchical folding that progressively hallucinates the detailed structures. Instead of improving the folding mechanism, SFA-Net <ref type="bibr" target="#b40">[41]</ref> addresses the information loss during the global feature extraction. Albeit effective, these folding-based methods feed the partial features to folding modules via concatenation, but we claim that such a concatenation manner would impair the capacity of the foldings. In comparison, our style-based point generator greatly enhances the capacity for modeling structural details. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The SpareNet</head><p>The overview architecture of SpareNet is exhibited in <ref type="figure">Figure 1</ref>. Given a partial and low-res point cloud X as input, SpareNet first completes X with a coarse point cloud Y c through an encoder and a generator: the encoder E embeds X into a shape code g, the style-based generator G exploits the shape code g and synthesizes the coarse output Y c . In addition, SpareNet adopts a refinement part that employs adversarial point rendering to further refine the coarse points Y c and outputs a final complete and high-res point cloud Y with improved visual quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Channel-Attentive EdgeConv</head><p>We devise the Channel-Attentive EdgeConv (CAE) to simultaneously integrate both local and global context from point features. It is inspired by the EdgeConv <ref type="bibr" target="#b31">[32]</ref> that captures a local context and the Squeeze-and-Excitation blocks <ref type="bibr" target="#b13">[14]</ref> for capturing a global context. Our point encoder E heavily relies on the CAE blocks.</p><p>Let P in be the input of a CAE block. Suppose it has M points with feature dimension C in . For each point p i ∈ P in , we first find its k-nearest neighbors in P in with respect to the Euclidean distance defined in the C in -dimensional feature space (k = 8 in experiments). Denote these neighbors as {q j i , 1 ≤ j ≤ k}, we have k directional edges on p i , with each edge represented as (p i , q j i − p i ). Then we use a multilayer perceptron (MLP) (denoted as F 1 ) to compute a new feature e j i = F 1 (p i , q j i − p i ) from each edge. In order to leverage a global context from the k-NN graph, we feed the global average of all edge features {e j i , 1≤i≤M, 1≤j≤k} into a second MLP (denoted as F 2 ) to calculate a gating vector η:</p><formula xml:id="formula_0">η = σ • F 2   1 kM × M,k i,j e j i   ,<label>(1)</label></formula><p>where σ represents sigmoid. We re-calibrate every edge feature e j i by multiplying it with η. Finally, for each point, we reduce its k edge features into a new point feature through maximum pooling and ReLU activation. We also add an additional linear layer F 3 that skip-connects the output with the input to make the block residual.</p><p>Unlike the T-net presented in <ref type="bibr" target="#b24">[25]</ref> that predicts an affine transformation to apply on point coordinates, we predict a</p><formula xml:id="formula_1">× 2 × 1 Style- based Linear × 3 × 2 Style- based × 3 Style- based × 2 × 1 Style- based Linear × 3 × 2 Style- based × 3 Style- based … … … … × 3</formula><p>Style-based Folding Layer</p><formula xml:id="formula_2">Linear BN CAE × × × Linear C ഥ 0,1 2 0,1 2</formula><p>Generator G … … <ref type="figure">Figure 3</ref>: The styled-based folding and the generator G.</p><p>gating vector that applies on edge features, in order to leverage both the local and global context for feature activation.</p><p>Our point encoder E is built with four sequential CAE blocks. The input X is fed into the first one and passes through all the rest, resulting in four point features with different dimensions and different receptive fields. We concatenate them together and compress the feature dimension through a final MLP. The output shape code g is derived as a concatenation of two global poolings of the above result: a maximum pooling and an average pooling. It has a dimension of C g , which is set to 4, 096 in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Style-based Point Generator</head><p>We present a style-based point generator G to generate a completed point cloud from the shape code g through novel style-based folding layers. In previous folding methods <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b20">21]</ref>, the shape code is tiled and concatenated with N 2D coordinates all sampled from a unit square [0, 1] 2 . They learn a mapping from such combination into the 3D space using MLP, to emulate the morphing of a 2D grid into a 3D surface. Under such foldings, the shape code determines the morphing through the input concatenations ahead but hardly affects all the layers behind in an effective way. Such concatenation-based folding induces bottleneck that limits its capacity to represent different 3D surfaces.</p><p>Enlightened by the success of StyleGAN <ref type="bibr" target="#b18">[19]</ref> in image generation, we propose style-based folding to circumvent these disadvantages in point cloud generation. <ref type="figure">Figure 3</ref> exhibits the structure of our generator G. Instead of combining the shape code g with grid coordinates as an input to vanilla folding MLPs, we directly inject g into the generator G's internal layers, to ensure a more extensive information aggregation in point cloud synthesis. We design novel stylebased folding layers to accomplish such injection.</p><p>A style-based folding layer transforms the input point features P in into new point features P out under the modulation of the shape code g. In specific, let h in ∈ R B×M ×C be a mini-batch of point activations that are linearly transformed from P in , with B being the batch size, M the number of points and C the dimension of point features. We first normalize h in to beh in ∈ R B×M ×C batch-wisely:</p><formula xml:id="formula_3">h in = h in − µ hin σ hin ,<label>(2)</label></formula><p>with µ hin , σ hin ∈ R 1×1×C being the means and standard deviations of h in 's channel-wise activations. In order to integrate the shape code, we compute new activation h out by denormalizing the normalizedh in according to the shape code g, with the formulation</p><formula xml:id="formula_4">h out = γ g ⊗h in + β g ,<label>(3)</label></formula><p>where γ g and β g are two modulation parameters both transformed from g through linear layers. Finally we append a CAE block to h out to compute the output P out . Same with <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b20">21]</ref>, our generator G employs K (32 in experiments) surface elements to form a complex shape, as depicted in <ref type="figure">Figure 3</ref>. For each surface element, the generator learns a mapping from a unit square [0, 1] 2 to a 3D surface through three sequential style-based folding layers and one linear layer. We sample n = N/K points for each surface element. Finally, the K 3D surfaces (each surface represented as n = N/K points) are directly merged together forming the coarse output point cloud Y c with N points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Adversarial Point Rendering</head><p>After generating a coarse point cloud Y c , we additionally refine it to acquire a final result Y with improved quality. But unlike some previous works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b30">31]</ref>, our refinement guarantees an advantageous visual quality of our final point cloud Y by employing a novel adversarial point rendering.</p><p>By point rendering, we mean a fully differentiable point renderer that enables end-to-end rendering from 3D point cloud to 2D depth maps. The renderer makes it possible that we can supervise the training not only in the point domain but also in the image domain. By adversarial, we mean to adversarially improve the point cloud quality with a discriminator D, which not directly discriminates the 3D point clouds, but their rendered 2D depth maps. Observing the success of image-based convolutional networks, we believe that an image-based convolutional discriminator, combined with our differentiable point renderer, can better capture geometric details in point clouds than the point-based discriminators used by <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b4">5]</ref>. Renderer Let P be a point cloud and v be a camera pose, the point renderer aims to generate a 2D depth map I v (P ) whose pixels reflect P 's geometry that is visible in v.</p><p>As shown in <ref type="figure" target="#fig_1">Figure 4</ref>, we start the rendering pipeline by projecting every 3D point p = (p x , p y , p z ) ∈ P onto a projection plane, retrieving a 2D pixel location (p v x ,p v y ) and a depthp v z (a.k.a. distance from the projection). We calculate such projection with a projective transformation T v , which is derived from camera pose v by combining both its extrinsic and intrinsic parameters.</p><formula xml:id="formula_5">1 2 3 Ƹ1 Ƹ2 Ƹ3 1 2 3 (a) (b) Ψ ⋅, ො 1 Ψ ⋅, ො 3 Ψ ⋅, ො 2 (c) × 1 × 2 × 3 (d) max 0 1 Ƹ1 Ƹ2 Ƹ3</formula><p>We then rasterize these 2D</p><formula xml:id="formula_6">pointsp v = (p v x ,p v y ) with depthsp v z to generate a rendered image.</formula><p>To guarantee differentiability, we regard each point not as a hard image pixel, but like a density function that spread smooth influence around its center. Let F p be the point feature used for rendering, the rasterization can be formulated as</p><formula xml:id="formula_7">I v x,y (P ) = max p∈P {Ψ ( (x, y),p v 2 ) × F p , 0} ,<label>(4)</label></formula><p>where Ψ(x) = exp(−x 2 /2ρ 2 ) is a Gaussian-shape kernel that models the density function of a point, with ρ being a hyper-parameter that controls the radius of its influence. In order to render I v (P ) as a depth map, we define</p><formula xml:id="formula_8">F p = 1 − (p v z −min p∈Pp v z )/(max p∈Pp v z −min p∈Pp v z )</formula><p>as a negative point depth normalized within [0, 1]. Choosing a different F for rendering is also supported, our implementation is able to render any point features without limits.</p><p>Comparing with previous differentiable point renderers <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b6">7]</ref>, ours is much simpler yet effective for rendering depth maps. Unlike <ref type="bibr" target="#b6">[7]</ref>, our renderer does not attach additional parameters for training. We don't need to perform z-buffer sorting like <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b17">18]</ref>, since the maximum reduction among negative depths can automatically locate the nearest point. More importantly, unlike <ref type="bibr" target="#b1">[2]</ref> where point coordinates are fixed, our renderer is fully differentiable: it supports gradients to be back-propagated not only to the feature F p , but also to the 2D coordinates (p v x ,p v y ). To reduce information loss in rendering, we further propose the multi-view point renderer π, which utilizes our differentiable point renderer to simultaneously render a point cloud P into eight depth maps, each observed from a different viewpoint. The resulting π(P ) with shape H × W × 8 is an ordered concatenation of all eight depth maps in the channel dimension. The size H × W is set to 256 × 256 in experiments; the eight viewpoints are set as the eight corners of a cube: [±1, ±1, ±1], to cover a wide angle of observation. Unlike the multi-view depth maps used by <ref type="bibr" target="#b14">[15]</ref>, our multi-view depth maps are rendered in a differentiable way: gradients can be back-propagated from depths maps on to the rendered points. Hence, our renderer enables end-toend training with both image and point supervisions, which considerably promotes the perceptual quality of results. Refiner Our refiner R shares similar structure with <ref type="bibr" target="#b20">[21]</ref>: they both consist of a minimum density sampling and a residual network that resembles PointNet <ref type="bibr" target="#b24">[25]</ref>. But different from <ref type="bibr" target="#b20">[21]</ref>, we add CAE blocks to the residual network for enhanced capability. Moreover, we recurrently deploy the refiner twice upon the coarse result Y c to get a first and a second refining result, Y 1 r and Y 2 r , with Y = Y 2 r being the final output, as illustrated by <ref type="figure">Figure 1</ref>.</p><p>Discriminator We render three point clouds during training: the partial input X, the groundtruth Y gt and the first refined result Y 1 r . Our discriminator D utilizes a cGAN <ref type="bibr" target="#b22">[23]</ref> strategy: the real sample is a concatenation of π(Y gt ) and π(X) in channel dimension; the fake sample is a concatenation of π(Y 1 r ) and π(X). This makes the adversarial update of Y 1 r to be conditioned on the input X. We implement D as a sequence of 2D convolution layers with spectral normalizations <ref type="bibr" target="#b23">[24]</ref> and LeakyReLU activations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training Losses</head><p>During training, the reconstruction loss L rec is required to match the output point cloud to the ground-truth. Even though the Chamfer Distance (CD) is very popular among existing works due to its efficiency in computation, we follow <ref type="bibr" target="#b20">[21]</ref> and implement L rec with the Earth Mover's Distance (EMD) instead, which is more faithful to visual quality as verified by <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b20">21]</ref>. The L rec supervises both the coarse output Y c and the final output Y with</p><formula xml:id="formula_9">L rec = d EMD (Y c , Y gt ) + d EMD (Y, Y gt ).<label>(5)</label></formula><p>The fidelity loss L f d is employed to preserve structures of the input X within the output Y as</p><formula xml:id="formula_10">L f d = 1 |X| p∈X min q∈Y p − q 2 2 .<label>(6)</label></formula><p>We also introduce losses in the image domain. We impose the depth map matching loss L depth as a L-1 distance on the multi-view depth maps. We also adopt the feature matching loss L f ea as a L-2 distance on the discriminator features. These two losses are formulated as    </p><formula xml:id="formula_11">L depth = 1 8HW π(Y 1 r ), π(Y gt ) 1 ,<label>(7)</label></formula><formula xml:id="formula_12">L f ea = 4 i α i H i W i D i D i [π(Y 1 r )], D i [π(Y gt )] 2 2 ,<label>(8)</label></formula><formula xml:id="formula_13">L = w rec L rec + w f d L f d + w depth L depth + (9) w f ea L f ea + w adv L adv + w exp L exp ,</formula><p>with L adv the adversarial loss from the discriminator, L exp an expansion loss also used by <ref type="bibr" target="#b20">[21]</ref>. We follow <ref type="bibr" target="#b21">[22]</ref> and implement L adv as mean square in GAN training. The loss weights are set as w rec = 200,w f d = 0.5,w adv = 0.1,w depth = w f ea = 1,w exp = 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>During training, we use the Adam optimizer with β 1 = 0 and β 2 = 0.9. Following TTUR <ref type="bibr" target="#b12">[13]</ref>, we set imbalanced initial learning rates, 1×10 −4 for the generator and 4×10 −4 for the discriminator. We train the network for 200 epochs, with the learning rates decayed by 0.1 at 100 and 150 epochs. With the batch size 32, it takes 5 days for training on the ShapeNet dataset with 4 Tesla V100 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We conduct experiments on two datasets commonly used for point cloud completion: ShapeNet <ref type="bibr" target="#b3">[4]</ref> and KITTI <ref type="bibr" target="#b8">[9]</ref>. * The PCN scores are calculated from a released PCN <ref type="bibr" target="#b38">[39]</ref> model that is trained on a fully sized ShapeNet, which is 8 times larger than the training set of all the other models reported in Tables 9, 10, 11. Even though, our model demonstrates superiority over it in terms of both EMD and FPD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>AtlasNet <ref type="bibr" target="#b10">[11]</ref> FCAE FoldingNet <ref type="bibr" target="#b36">[37]</ref> PCN <ref type="bibr" target="#b38">[39]</ref> MSN <ref type="bibr" target="#b20">[21]</ref> GRNet <ref type="bibr" target="#b34">[35]</ref> Ours Groundtruth Lamp Chair Sofa Vessel Airplane  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison on ShapeNet</head><p>Metrics. We employ three evaluation metrics to measure completion accuracy on ShapeNet: the Chamfer Distance (CD), the Earth Mover's Distance (EMD) and the Fréchet Point cloud Distance (FPD). Among these distances, CD is a widely used one due to its computation efficiency. EMD is more discriminative to the local details and density distribution. Previous works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b0">1]</ref> have claimed EMD to be a more reliable measure for visual quality than CD. The FPD <ref type="bibr" target="#b26">[27]</ref>, inspired by the FID <ref type="bibr" target="#b12">[13]</ref>, is a metric that computes the Fréchet distance between Gaussian fitted distributions. It evaluates not the accuracy of an individual point cloud, but the overall perceptual quality of all predicted point clouds.</p><p>Completion evaluation. We compare SpareNet with the following state-of-the-art approaches: 1) PointFCAE, a simple autoencoder adopting PointNet as encoder and MLPs as decoder; 2) AtalasNet <ref type="bibr" target="#b10">[11]</ref>, which generates points by sampling from the parametric surface elements; 3) Fold-ingNet <ref type="bibr" target="#b36">[37]</ref>, which proposes folding-based decoder to directly generate points; 4) PCN <ref type="bibr" target="#b38">[39]</ref> that uses stacked Point-Net for feature extraction; 5) MSN <ref type="bibr" target="#b20">[21]</ref>, the baseline of this work; 6) GRNet <ref type="bibr" target="#b34">[35]</ref>, a recent leading approach that oper-    ates on volumetric grids. For a fair comparison, we train all the models using the EMD loss with their released codes. All models utilize the same training set except PCN * . <ref type="table" target="#tab_1">Tables 9 and 11</ref> show that our method outperforms prior works in terms of both EMD and CD metric. In particular, our method is more advantageous in terms of the FPD score as shown in <ref type="table" target="#tab_1">Table 10</ref>, reducing the FPD from the secondbest 0.930 to 0.645. The qualitative results in <ref type="figure">Figure 5</ref> further corroborate the perceptual advantage of our method. In comparison, our method is able to generate fine structures with shapes whereas other methods are prone to give blurry results. Thin structures, such as the mast of the vessel, can also be faithfully generated. Overall, our completed result appears much less noisy and visually pleasing. Comparison via classification. To better quantify the completion quality, we propose two novel metrics for comparison. First, we employ a pretrained PointNet model and test its classification accuracy on the completed point clouds by different methods. A pretrained classifier should perform better when the input is more resemblance to the real point clouds. In <ref type="figure" target="#fig_3">Figure 6 (a)</ref>, the PointNet achieves the highest classification accuracy when using our completed results as input, showing that our method can well preserve the semantics of the partial input and is able to complete the points as the real ones. Besides, we compare the perceptual quality of various methods by examining their rendered view images. As we do not have the labeling of quality scores, we train an image classifier (ResNet-18) to differentiate the fake or real data based on the complete results of all seven methods, and then make use of this fake detector to test each method. Here we assume that different methods share similar types of artifacts and the fake detection  trained on one method is generalizable to other approaches. <ref type="figure" target="#fig_3">Figure 6</ref> (b) shows that our method can better mislead the fake detector by giving a lower detection accuracy, proving that our methods suffer from less noticeable artifacts when observed at different viewpoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison on KITTI</head><p>This dataset contains the LiDAR scans from auto-driving scenes, and the objects are much sparser than the ShapeNet dataset. For better transferred performance, we finetune all the models using the ShapeNetCars subset that only contains the cars from the ShapeNet so that the prior knowledge for this category can be better leveraged.</p><p>Since the groundtruth point clouds are not available in this real-world dataset, we cannot apply the full-reference metrics (CD, EMD and FPD) for evaluation. As such, we follow the practice in <ref type="bibr" target="#b38">[39]</ref>, and use the following metrics: 1) the Temporal Consistency that measures the Chamfer distance for the consecutive frames; 2) the Fidelity, a single-directional Chamfer distance that measures how the input structures are preserved in the output; 3) the Minimum Matching Distance (MMD), the CD between the output and the point cloud in ShapetNetCars that is closest to the output in terms of CD. This metric makes sure that the output resembles a car model in ShapeNet. <ref type="table" target="#tab_7">Table 4</ref> shows the quantitative results. Our method shows superior consistency and MMD, indicating that our method can complete the real point points with high stability and quality. Yet, MSN and GRNet outperform in fidelity, possibly because our adversarial rendering that enforces high perceptual quality brings a slight sacrifice of the fidelity relative to the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>We report the results of extensive ablation experiments in <ref type="table" target="#tab_9">Table 5</ref>, with each row corresponding to one ablation setting. We observe that the ablation settings all cause the drop of performance (i.e. raise of the three distance numbers) comparing to our full model shown at the bottom. It in turn exhibits the effectiveness of each ablated component we propose. For example, in [A], the structure that computes and applies the channel attention vector η (Equation 1) is removed from CAE blocks. The raise of metric numbers reflects the importance of our proposed channelattentive functionality. On the other hand, the advantage of where its CAE blocks abandon the EdgeConv structure but preserve the channel attention η on point features. We also ablate on the style-based generator in [C], where the generator G adopts the vanilla concatenation-based folding instead of our style-based folding. The large-margin performance drop indicates the criticalness of our style-based folding in point cloud completion. It is also verified by <ref type="figure">Figure 8</ref>, where the style-base folding leads to much faster convergence of training loss than the vanilla folding. In addition, we use a single-step refinement (with renderings applied on the final output) instead of the recurrent refinement and report results in [D], the performance drop indicates a necessity of recurrent refinement in our framework.</p><p>More experiments are conducted to study the adversar-  <ref type="table">Table 6</ref>: Ablation of adversarial training settings. By default we apply discriminator only on Y 1 r with w adv = 0.1.</p><p>ial training settings. We compare the accuracies with or without adversarial training in terms of both EMD and CD in <ref type="figure">Figure 7</ref>. It shows that 1) adversarial rendering brings significant improvement under all point resolutions, from 2,048 to 16,384; 2) a denser predicted point cloud leads to a larger improvement. We believe it is due to the rendering quality: a denser point cloud renders smoother depth maps with less point scattering effects, aiding the discriminator to better capture the intrinsic geometry of point clouds.</p><p>Let Y adv be the point clouds which we apply adversarial rendering on; let w adv be the weight of the adversarial loss. Different settings of Y adv and w adv are exploited in <ref type="table">Table  6</ref> showing that: 1) when adversarial rendering involves the final output, i.e. when Y 2 r ∈ Y adv , the completion accuracy is slightly impaired; 2) a large adversarial loss weight (w adv = 5) also hinders the completion accuracy. We deem this a conflict between the adversarial loss L adv and the reconstruction loss L rec (defined by Equation 5) when they are applied on the same output, or when their loss weights are comparable: L rec encourages the output to match the single corresponding groundtruth, whereas the L adv promotes an improved visual quality through the discriminator, which is learned from a larger distribution instead of a single groundtruth. This conflict is also visualized by <ref type="figure">Figure  9</ref>, where a larger adversarial loss weight w adv , though helps forge some structures (e.g. the airplane engine, the chair leg) reasonable for the category, nevertheless often distracts the resulting shape from its true groundtruth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper presents a novel framework, SpareNet, for point cloud completion. It comprises channel-attentative Edge-Convs for the fusion of local and global context in point feature extraction. It also performs style-based folding for an enhanced capability in point cloud synthesis. In addition, adversarial point rendering is adopted: by leveraging a fully differentiable point renderer, an image-based discriminator is utilized for capturing geometric details in point clouds. Extensive experiments on ShapeNet and KITTI verify the state-of-the-art performance of SpareNet, as well as the effectiveness of each proposed component.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Importance of the Image Domain Supervisions</head><p>We remove all the image-domain losses but instead implement the discriminator with a PointNet. We denote such setting as the Point-based, and report its comparisons with our proposed setting in Tables 9, 10, 11 and <ref type="figure">Figure 15</ref>. These results all verify the effectiveness of our proposed image-domain losses in point cloud completion.    <ref type="table" target="#tab_1">Table 11</ref>: Completion comparison on ShapeNet in terms of CD ×10 3 (lower is better).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. More Qualitative Results</head><p>We show more qualitative completion results in <ref type="bibr">Figures 11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13</ref>. We also demonstrate more qualitative comparisons in <ref type="figure" target="#fig_1">Figure 14</ref> with respect to the rendered depth maps. Moreover, in <ref type="figure">Figure 17</ref>, we illustrate the car completion results based on the real-world LiDAR scans from KITTI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Rendering with Different Point Cloud Resolutions</head><p>We also illustrate the multi-view depth maps of the same shape that are rendered with different point numbers in <ref type="figure" target="#fig_3">Figure 16</ref>. It demonstrates that a denser point cloud can alleviate the point scattering artifacts in rendered depth maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Robustness Study</head><p>We finally visualize point completion results of the same 3D shape, but from partial points that are sampled from different view angles ( <ref type="figure">Figure 18</ref>) or with different sampling densities ( <ref type="figure">Figure 19</ref>). These results verify that our method is robust to various acquisition conditions, such as different viewpoint and point density.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>AtlasNet <ref type="bibr" target="#b10">[11]</ref> FCAE FoldingNet <ref type="bibr" target="#b36">[37]</ref> PCN <ref type="bibr" target="#b38">[39]</ref> MSN <ref type="bibr" target="#b20">[21]</ref> GRNet <ref type="bibr" target="#b34">[35]</ref> Ours Input AtlasNet <ref type="bibr" target="#b10">[11]</ref> FCAE FoldingNet <ref type="bibr" target="#b36">[37]</ref> PCN <ref type="bibr" target="#b38">[39]</ref> MSN <ref type="bibr" target="#b20">[21]</ref> GRNet <ref type="bibr" target="#b34">[35]</ref> Ours Groundtruth Input AtlasNet <ref type="bibr" target="#b10">[11]</ref> FCAE FoldingNet <ref type="bibr" target="#b36">[37]</ref> PCN <ref type="bibr" target="#b38">[39]</ref> MSN <ref type="bibr" target="#b20">[21]</ref> GRNet <ref type="bibr" target="#b34">[35]</ref> Ours Groundtruth <ref type="table">Table   Table   Table   Table   Vessel</ref> Vessel Vessel Vessel <ref type="figure">Figure 13</ref>: Visualized completion comparison on ShapeNet. view1 view2 view3 view4 view5 view6 view7 view8</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chair</head><p>Input AtlasNet <ref type="bibr" target="#b10">[11]</ref> FCAE FoldingNet <ref type="bibr" target="#b36">[37]</ref> PCN <ref type="bibr" target="#b38">[39]</ref> MSN <ref type="bibr" target="#b20">[21]</ref> GRNet <ref type="bibr" target="#b34">[35]</ref> Ours Groundtruth <ref type="figure">Figure</ref>   <ref type="figure">Figure 19</ref>: Completing the same 3D shape from partial points of various densities (from 1500 points to 3000 points).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The structure of the Channel-Attentive EdgeConv.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Pipeline of our differentiable point rendering. (a) Given 3D points P and a camera view v, (b) the 3D points p i ∈ P are firstly projected as 2D pointsp v i with depths according to v. (c) We regard each 2D point as a smooth density function modeled by kernel Ψ. (d) A depth map I v (P ) is generated through a pixel-wise maximum reduction of negated point depths F weighted by point densities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>D i / 4 i</head><label>4</label><figDesc>D i a re-weighting factor of each feature. The final loss for the end-to-end SpareNet training combines all individual losses as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>(a) Comparing informativeness. Higher is better. (b) Comparing fake detection accuracies. Lower is better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :Figure 8 :Figure 9 :</head><label>789</label><figDesc>Ablation of adversarial rendering under different point resolutions. Our adversarial rendering demonstrates larger improvement when predicting denser point clouds. Loss curves of training two models: one uses the vanilla folding (red); the other applies style-based folding (blue). The latter converges much faster than the former. Input w adv = 0.0 w adv = 0.1 w adv = 5 Groundtruth airplane chair lamp Results with different adversarial loss weights. edge features over point features can be observed by comparing the full model with [B]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 10 :</head><label>10</label><figDesc>Discriminator D is trained using depth maps generated from the multi-view point renderer π.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 11 :</head><label>11</label><figDesc>Visualized completion comparison on ShapeNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 12 :</head><label>12</label><figDesc>Visualized completion comparison on ShapeNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>14 :Figure 15 :Figure 16 :Figure 17 :Figure 18 :</head><label>1415161718</label><figDesc>Completion comparison visualized in rendered depth maps. Visualized comparison of models with or without rendering supervisions. In comparison, the proposed rendered discriminator is more capable to examine the local details than the point-based discriminator. Multi-view depth maps rendered with different point numbers. A denser point cloud can alleviate the point scattering artifacts in its rendered depth maps.InputCompletion results Visualized car completion results based on real-world LiDAR scans from the KITTI dataset<ref type="bibr" target="#b8">[9]</ref>. The left frames show the input partial points in blue, the right frames show the completed point clouds of cars in red. Completing the same 3D shape from partial points that are sampled from four different view angles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>with D 1≤i≤4 being features extracted from intermediate layers of discriminator D, H i W i D i the feature shape, α i = Method plane cabinet car chair lamp sofa table vessel avg PointFCAE 1.554 2.631 2.132 2.954 4.067 2.997 2.899 2.619 2.732 FoldingNet 1.682 2.576 2.183 2.847 3.062 3.003 2.500 2.357 2.526 AtlasNet 1.324 2.582 2.085 2.442 2.718 2.829 2.160 2.114 2.282 PCN * 2.426 1.888 2.744 2.200 2.383 2.062 1.242 2.208 2.</figDesc><table><row><cell></cell><cell>144</cell></row><row><cell>MSN</cell><cell>1.334 2.251 2.062 2.346 2.449 2.712 1.977 2.001 2.142</cell></row><row><cell>GRNet</cell><cell>1.376 2.128 1.918 2.127 2.150 2.468 1.852 1.876 1.987</cell></row><row><cell>Ours</cell><cell>1.131 2.014 1.783 2.050 2.063 2.333 1.729 1.790 1.862</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Completion comparison on ShapeNet in terms of EMD ×10 3 (lower is better).</figDesc><table><row><cell>Method</cell><cell>plane cabinet car chair lamp sofa table vessel avg</cell></row><row><cell cols="2">PointFCAE 1.424 3.878 0.519 2.404 6.989 2.594 2.673 8.998 3.683</cell></row><row><cell cols="2">FoldingNet 1.593 5.918 0.649 1.355 4.344 2.400 2.243 5.508 3.001</cell></row><row><cell>AtlasNet</cell><cell>0.512 2.536 0.706 1.181 2.295 2.460 1.810 2.475 1.747</cell></row><row><cell>PCN *</cell><cell>0.484 1.221 0.200 1.417 0.947 1.680 0.566 0.926 0.930</cell></row><row><cell>MSN</cell><cell>0.324 1.858 0.412 0.968 1.652 2.409 0.917 0.744 1.161</cell></row><row><cell>GRNet</cell><cell>4.649 1.002 1.152 1.712 2.977 2.717 1.713 5.528 2.681</cell></row><row><cell>Ours</cell><cell>0.307 0.691 0.142 1.113 0.774 0.945 0.668 0.523 0.645</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Completion comparison on ShapeNet in terms of FPD ×0.1 (lower is better).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table Figure 5</head><label>Figure</label><figDesc>: Visualized completion comparison on ShapeNet.</figDesc><table><row><cell>Method</cell><cell>plane cabinet car chair lamp sofa table vessel avg</cell></row><row><cell cols="2">PointFCAE 0.340 1.170 0.480 1.240 2.294 1.312 1.433 0.893 1.145</cell></row><row><cell cols="2">FoldingNet 0.622 1.608 0.619 1.553 2.025 1.543 1.534 0.910 1.302</cell></row><row><cell>AtlasNet</cell><cell>0.301 0.967 0.440 0.858 1.126 1.174 0.813 0.639 0.790</cell></row><row><cell>PCN *</cell><cell>0.559 0.389 0.581 0.466 0.684 0.263 0.156 0.395 0.437</cell></row><row><cell>MSN</cell><cell>0.252 0.974 0.445 0.770 0.933 1.152 0.669 0.491 0.711</cell></row><row><cell>GRNet</cell><cell>0.293 0.560 0.363 0.583 0.690 0.935 0.532 0.389 0.543</cell></row><row><cell>Ours</cell><cell>0.176 0.664 0.362 0.616 0.631 0.789 0.498 0.384 0.515</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Completion comparison on ShapeNet in terms of CD ×10 3 (lower is better).</figDesc><table><row><cell>ShapeNet. The ShapeNet dataset derived from [39] con-sists of 30,974 3D models that belong to 8 categories: air-plane, cabinet, car, chair, lamp, sofa, table, and vessel. Each groundtruth point cloud comprises 16,384 points that are uniformly sampled from the corresponding 3D model. The partial point clouds are constructed by projecting the 2.5D depth maps of the model back into 3D. This leads to 8 dif-ferent partial point clouds for each groundtruth in the train-ing set. Following [35, 31], we randomly select only one partial point cloud from the 8 to build our training pairs. This results in a training set only 1/8 the size of the one</cell></row></table><note>used by PCN [39]. For a fair comparison, we use the same train/val/test splits as [35, 31]. KITTI. The KITTI dataset comprises a sequence of real Li- DAR scans. For each frame, the cars are extracted accord- ing to the labeled bounding box, resulting in 2,401 point partial inputs. Since no groundtruth exist in this dataset, we cannot rely on paired metrics for evaluation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Quantitative comparison on KITTI dataset in terms of consistency, fidelity and minimum matching distance (MMD). The best results are highlighted in bold.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>EdgeConv in CAE 1.972 0.542 0.692 [C] w/o Style-based Folding 3.274 1.779 3.111 [D] w/o Recurrent Refine 2.184 0.708 1.856</figDesc><table><row><cell></cell><cell>Ablation</cell><cell>EMD CD FPD</cell></row><row><cell>[A]</cell><cell>w/o η in CAE</cell><cell>2.060 0.606 1.012</cell></row><row><cell cols="2">[B] w/o Ours Full</cell><cell>1.862 0.515 0.645</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Ablation results of SpareNet.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Layer C in F 1</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>F 2</cell></row><row><cell>1</cell><cell>4</cell><cell>[64]</cell><cell>[4, 64]</cell></row><row><cell>2</cell><cell cols="3">64 [128] [8, 128]</cell></row><row><cell>3</cell><cell cols="3">128 [1024] [64, 1024]</cell></row><row><cell>4</cell><cell cols="3">1088 [512] [32, 512]</cell></row><row><cell>5</cell><cell cols="3">512 [256] [16, 256]</cell></row><row><cell>6</cell><cell cols="3">256 [128] [8, 128]</cell></row><row><cell>7</cell><cell cols="2">128 [3]</cell><cell>/</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Channels of the CAE blocks in the residual network of refiner.</figDesc><table><row><cell>1</cell><cell>× 3 × 3 × 3</cell><cell>× × 8 × × 8 × × 8</cell><cell>C C</cell><cell>× × 16 × × 16</cell><cell>D D</cell><cell>Real Fake</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>Completion comparison on ShapeNet in terms of EMD ×10 3 (lower is better).</figDesc><table><row><cell>Method</cell><cell>plane cabinet car chair lamp sofa table vessel avg</cell></row><row><cell cols="2">Proposed 0.307 0.691 0.142 1.113 0.774 0.945 0.668 0.523 0.645</cell></row><row><cell cols="2">Point-based 1.961 0.826 1.592 4.275 1.406 5.153 0.673 1.074 2.120</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 10 :</head><label>10</label><figDesc>Completion comparison on ShapeNet in terms of FPD ×0.1 (lower is better). plane cabinet car chair lamp sofa table vessel avg Proposed 0.176 0.664 0.362 0.616 0.631 0.789 0.498 0.384 0.515 Point-based 0.331 0.809 0.471 0.812 0.834 1.187 0.649 0.557 0.706</figDesc><table><row><cell>Method</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Network Architectures</head><p>We first elaborate on the network architectures of our SpareNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Encoder</head><p>The input for our point encoder E is a partial and low-res point cloud X with M points in 3D. As shown by <ref type="table">Table 7</ref>, the encoder E consists of four sequential Channel-Attentive EdgeConv (CAE) blocks with layer output sizes 256, 256, 512, 1024. The k of k-NN is set as 8. The slope of all LeakyReLU layers is set to 0.2. We concatenate the outputs of the four layers, and feed them into a final shared MLP-BN-ReLU layer with dimension 2048. The output shape code g with dimension 4096 is the concatenation of two global poolings of the above result: a maximum pooling and an average pooling.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Generator</head><p>Our style-based point generator G employs K (K=32) surface elements to form a complex shape. For each surface element, the generator maps a n × 2 unit square (n = N/K) into a n × 3 surface through three sequential style-based folding layers and one linear layer. The output sizes of the four layers are 4096, 2048, 1024 and 3.</p><p>We use two linear layers with {4096, 3059} neurons to transform the shape code g into modulation parameters γ g and β g for the three style-based folding layers, with 3059 being the total size of modulation parameters in the three style-based folding layers. We partition the modulation parameters into parts according to the size of activationh in in each style-based folding layer, and assign them to each layer respectively. The learned γ g and β g are shared among all the K surface elements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Renderer</head><p>The size of a rendered depth map H × W is set to 256 × 256 in experiments. The eight viewpoints for the multi-view rendering are set as the eight corners of a cube: (±1, ±1, ±1). We adopt the radius ρ = 3 in point rendering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Refiner</head><p>In each refiner R, instead of directly concatenating the previous output points (containing N points) and the partial input points (denoted by X, containing M points) into one point cloud, we attach a flag to each point before concatenation: a 0 is attached if the point comes from X while a 1 label is attached otherwise. This results in a point cloud with N + M 4-dimensional points, which is fed into a minimum density sampling <ref type="bibr" target="#b20">[21]</ref> that samples N points out of the N + M points. A residual network then learns point-wise residuals for the re-sampled N points to adjust their positions. The residual network consists of 7 CAE blocks as depicted in <ref type="table">Table 8</ref>. We concatenate the outputs of the first and the third CAE blocks, which is fed into the fourth CAE block and outputs residuals for the refined point coordinates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5. Discriminator</head><p>The real samples for training the discriminator D is the concatenation of the depth maps rendered from X and Y 1 r , while the fake samples come from concatenating the depth maps of X and Y gt , as illustrated in <ref type="figure">Figure 10</ref>. The discriminator consists of four Conv-LeakyReLu-Dropouts and one Linear, with spectral normalization <ref type="bibr" target="#b23">[24]</ref> applied on the Conv and Linear weights. The number of channels are 16, 32, 64, 128 for the four convolutional layers. Each Conv has a kernel size of 3, a stride of 2 and a padding size of 1. The slope of LeakyReLU is 0.2. The drop rate of all Dropouts is 0.75. A last Linear outputs a scalar for discrimination.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning representations and generative models for 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panos</forename><surname>Achlioptas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Diamanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kara-Ali</forename><surname>Aliev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Sevastopolsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Kolos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08240</idno>
		<title level="m">Neural point-based graphics</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Point convolutional neural networks by extension operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Atzmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haggai</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Lipman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.10091</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zimo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">An information-rich 3d model repository</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Unpaired point cloud completion on real scans using adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niloy J</forename><surname>Mitra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00069</idno>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Shape completion using 3d-encoder-predictor cnns and shape synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">Ruizhongtai</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5868" to="5877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural point cloud rendering via multi-plane projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuwen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7830" to="7839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A point set generation network for 3d object reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqiang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="605" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset. The International</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Flex-convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Groh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Wieschollek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendrik</forename><forename type="middle">Pa</forename><surname>Lensch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="105" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">AtlasNet: A Papier-Mâché Approach to Learning 3D Surface Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>Groueix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Aubry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deep learning for 3d point clouds: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Bennamoun</surname></persName>
		</author>
		<idno>2020. 1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Render4completion: Synthesizing multi-view depth maps for 3d shape completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Zwicker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pointwise convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Binh-Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Khoi</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai-Kit</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="984" to="993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pf-net: Point fractal network for 3d point cloud completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zitian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised learning of shape and pose with differentiable point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2802" to="2812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pointcnn: Convolution on x-transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingchao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhan</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="820" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Morphing and sampling network for dense point cloud completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">Paul</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smolley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2794" to="2802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05957</idno>
		<title level="m">Spectral normalization for generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sung Woo Park, and Junseok Kwon. 3d point cloud generative adversarial network based on tree structured graph convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong Wook</forename><surname>Shu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3859" to="3868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Topnet: Structural point cloud decoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineet</forename><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatriz</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6411" to="6420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Local spectral graph convolution for point set feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babak</forename><surname>Samari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaleem</forename><surname>Siddiqi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="52" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cascaded refinement network for point cloud completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcelo</forename><forename type="middle">H</forename><surname>Ang</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim Hee</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Point cloud completion by skip-attention network with hierarchical folding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Shen</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1939" to="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Synsin: End-to-end view synthesis from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivia</forename><surname>Wiles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7467" to="7477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Grnet: Gridding residual network for dense point cloud completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxun</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangchen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiageng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Spidercnn: Deep learning on point sets with parameterized convolutional filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingye</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="87" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Foldingnet: Point cloud auto-encoder via deep grid deformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiru</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018-06-01" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Differentiable surface splatting for point-based geometry processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Yifan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felice</forename><surname>Serena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cengizöztireli</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pcn: Point completion network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Mertz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Siamak Ravanbakhsh, Barnabas Poczos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Russ R Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3391" to="3401" />
		</imprint>
	</monogr>
	<note>Deep sets</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Detail preserved point cloud completion via separated feature aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxia</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
