<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pose Refinement Graph Convolutional Network for Skeleton-based Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Yi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazan</forename><forename type="middle">Abu</forename><surname>Farha</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
						</author>
						<title level="a" type="main">Pose Refinement Graph Convolutional Network for Skeleton-based Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the advances in capturing 2D or 3D skeleton data, skeleton-based action recognition has received an increasing interest over the last years. As skeleton data is commonly represented by graphs, graph convolutional networks have been proposed for this task. While current graph convolutional networks accurately recognize actions, they are too expensive for robotics applications where limited computational resources are available. In this paper, we therefore propose a highly efficient graph convolutional network that addresses the limitations of previous works. This is achieved by a parallel structure that gradually fuses motion and spatial information and by reducing the temporal resolution as early as possible. Furthermore, we explicitly address the issue that human poses can contain errors. To this end, the network first refines the poses before they are further processed to recognize the action. We therefore call the network Pose Refinement Graph Convolutional Network. Compared to other graph convolutional networks, our network requires 86%-93% less parameters and reduces the floating point operations by 89%-96% while achieving a comparable accuracy. It therefore provides a much better trade-off between accuracy, memory footprint and processing time, which makes it suitable for robotics applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Action recognition has received an increasing interest in recent years due to its importance for a broad range of applications such as video surveillance, gesture recognition and human-robot interaction. Although deep learning models are very popular for recognizing activities in videos <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>, these approaches are computationally expensive and cannot be used on mobile systems with limited computational resources. To alleviate this problem, skeleton data can be used for action recognition <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. In contrast to video-based approaches, skeleton-based action recognition models require much less computational resources and several approaches based on convolutional neural networks and recurrent neural networks have been proposed <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b7">[8]</ref>.</p><p>Despite the success of the previous approaches, their performance was limited as they did not consider the intrinsic differences between video data and skeleton data. While video frames have a grid structure where standard 2D or 3D convolutional neural networks can be applied, skeleton data is commonly represented by graphs. The spatial-temporal graph convolutional network (ST-GCN) <ref type="bibr" target="#b8">[9]</ref> therefore models skeleton sequences as spatial-temporal graphs and uses graph convolutions. Inspired by this work, further improvements have been proposed to increase the accuracy <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. While the variants of graph convolutional networks achieve very good results in terms of accuracy, the improvements come at a high increase of memory consumption and computational *These authors contributed equally to this work.</p><p>cost. Indeed, <ref type="bibr" target="#b10">[11]</ref> has more than twice the number of parameters than ST-GCN and increases the number of floating point operations (FLOP) for inference by a factor of around 2.5. This prevents the application of these networks within the robotics domain.</p><p>In this work, we therefore propose a graph convolutional network that achieves a higher action recognition accuracy compared to ST-GCN, but that is much more efficient. Instead of increasing the size of the model, we present a new architecture that requires 7 times less parameters than ST-GCN and reduces the FLOP by a factor of 9. This is achieved by treating the temporal and spatial relations in a different way. To this end, we combine temporal and graph convolutions. While the graph convolutions focus on the spatial relations, the temporal convolutions aggregate the temporal information. Whereas in a sequential model the temporal and graph convolutions follow each other, we propose to separate them in a parallel structure and gradually fuse them as part of the gradual fusion module (GFM) as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. In contrast to a spatial-temporal graph, the temporal dimension gets early reduced within the network and not only at the end, which results in a very efficient and compact network.</p><p>As a second contribution, we address the problem that the estimated 2D or 3D human poses can be inaccurate. Although many approaches for human pose estimation from RGB or depth sensors exist and some of them can be deployed on robotics platforms, pose estimation errors occur due to the limited view or occlusions. Previous works on skeleton-based action recognition did not address this issue explicitly, assuming that the networks deal with pose estimation errors implicitly. In this work, we propose to add a module that refines the human poses by taking the spatial and temporal information into account. The so-called pose refinement module (PRM) estimates offsets for each joint and refines the poses by the offsets. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, the pose refinement module first refines the poses and the gradual fusion module with an additional temporal aggregation estimates then the action class probabilities. The entire network is trained only with an action classification loss such that no additional supervision is required. We therefore call the approach pose refinement graph convolutional network (PR-GCN).</p><p>Our contribution is thus two folded:</p><p>• We propose the pose refinement graph convolutional network (PR-GCN), which is a compact model for skeleton-based action recognition that gradually fuses position and motion information and provides a better trade-off between effectiveness and efficiency compared to the state-of-the-art. • We introduce the pose refinement module that reduces the impact of pose estimation errors and further improves the accuracy at a very small increase of computational cost. We evaluate the proposed approach 1 on two very challenging action recognition datasets, namely Kinetics <ref type="bibr" target="#b11">[12]</ref> with estimated 2D human poses <ref type="bibr" target="#b8">[9]</ref> and NTU RGB+D <ref type="bibr" target="#b12">[13]</ref> with estimated 3D human poses. Compared to other variants of graph convolutional networks, our method achieves a competitive accuracy but at a small fraction of the required computational resources. Compared to the state-of-the-art approach <ref type="bibr" target="#b10">[11]</ref>, the number of parameters are reduced by factor 14 and the computational cost by factor 22, which makes it suitable for robotics applications <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Skeleton-based action recognition</head><p>Skeleton-based action recognition is an important research area that has received an increasing attention recently. While earlier approaches use hand-crafted features for action recognition <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, more recent works use datadriven methods based on convolutional neural networks (CNNs) or recurrent neural networks (RNNs). As CNNs are applied on images with a regular grid structure, CNN-based methods represent the skeleton data as pseudo-images <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. On the contrary, RNN-based methods are usually proposed for sequential data. Hence, RNN-based methods model the skeleton data as a sequence of vectors, each of them representing the coordinates of the body joints <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b20">[21]</ref>- <ref type="bibr" target="#b22">[23]</ref>. Despite the success of both RNNs and CNNs, their performance is limited as they do not explicitly capture the dependencies between the body joints. Some approaches <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b10">[11]</ref> therefore model the skeleton data as spatio-temporal graph structures and use graph convolutions instead of 2D convolutions. Graphs have also been used to model other relations for action recognition. For instance, <ref type="bibr" target="#b0">1</ref> The source code is available at https://github.com/sj-li/ PR-GCN.</p><p>spatio-temporal graphs <ref type="bibr" target="#b23">[24]</ref> or scene graphs <ref type="bibr" target="#b24">[25]</ref> are used to model relations between objects. In <ref type="bibr" target="#b25">[26]</ref>, the problem of recognizing activities that occur at the same time is addressed by learning spatio-temporal correlations of activities. There are also a few works that proposed lightweight networks for action recognition <ref type="bibr" target="#b26">[27]</ref>- <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Graph convolutional neural networks</head><p>There are many works that focus on graph convolutional networks (GCNs). The graphs are usually constructed in the spatial or spectral domain <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b36">[37]</ref>. In the spatial domain <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b37">[38]</ref>- <ref type="bibr" target="#b39">[40]</ref>, the convolution operation is directly applied on the graph vertices and their neighbors. Whereas spectral methods perform the graph convolutions in the frequency domain with the help of the graph Fourier transform <ref type="bibr" target="#b29">[30]</ref>. The spectral methods do not require any extra effort to extract locally connected regions from graphs at each convolutional step <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b36">[37]</ref>. Over the years, several improvements have been proposed. For instance, an attention mechanism has been integrated into graph convolutional networks in <ref type="bibr" target="#b40">[41]</ref>. In <ref type="bibr" target="#b41">[42]</ref>, the distance metrics are parameterized so that the graph Laplacian itself becomes trainable. The work <ref type="bibr" target="#b42">[43]</ref> proposes sub-graph training in order to deal with very large graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. POSE REFINEMENT GRAPH CONVOLUTIONAL NETWORK</head><p>The proposed pose refinement graph convolutional network (PR-GCN) is a very compact and efficient network for skeleton-based action recognition which makes it suitable for robotics applications <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>. The architecture of the proposed network is illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. Pose estimation errors of the input sequence are first corrected by the pose refinement module. Examples of refined human poses are shown in <ref type="figure" target="#fig_7">Fig. 8</ref>. The refined skeleton sequence is then processed by a motion-flow-branch and a position-flow-branch that are gradually fused. Finally, the temporal aggregation module generates the final prediction. In the following, we describe the network in detail.  <ref type="bibr" target="#b8">[9]</ref> for the 2D skeleton that is provided for Kinetics. The right figure shows the spatial neighbors of joint 5, which are grouped based on the vertex itself (red), the vertices that are closer to the center of gravity (green) and the vertices that are more far from the center of gravity (yellow).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Graph Construction</head><p>Similar to ST-GCN <ref type="bibr" target="#b8">[9]</ref>, each skeleton sequence is modeled as a spatial-temporal graph G(V, E) as shown in <ref type="figure" target="#fig_1">Fig. 2</ref> (left). The vertices V denote body joints represented by their 2D or 3D joint coordinates. The edges E comprise spatial and temporal edges. Spatial edges E s represent connections between vertices at each frame whereas temporal edges E t connect the same body joint between two adjacent frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Basic Operations</head><p>Since PR-GCN includes graph convolution and temporal convolution layers, we briefly describe them first.</p><p>1) Graph Convolution Layer: The graph convolutions [9] operate on a pre-defined graph structure. In the spatial dimension, the graph convolution for each vertex v i is formulated as:</p><formula xml:id="formula_0">f out (v i ) = vj ∈Bi 1 Z ij f in (v j ) · w(l i (v j )),<label>(1)</label></formula><p>where f (v) denotes the feature for vertex v and w is a weight function. v i is the target vertex and B i is the set of neighbor vertices including v i . In our implementation, B i contains all 1-distance neighbors as it is illustrated in <ref type="figure" target="#fig_1">Fig. 2 (right)</ref>. Since the number of vertices in B i varies based on the selection of v i , <ref type="bibr" target="#b8">[9]</ref> introduced a mapping function l i that maps neighbor vertices into a set of predefined groups: the vertex itself (G i1 ), the vertices that are close to the center of gravity (G i2 ), and vertices that are far from the center of gravity (G i3 ) as shown in <ref type="figure" target="#fig_1">Fig. 2 (right)</ref>. This means that the weighting function w generates three different weights, one for each group. Z ij is the cardinality of G ik that contains v j . It is used to balance the contribution of each group. The input skeleton sequence is organized as a tensor of size (C, T, N ), where C is the number of channels, T is the sequence length and N denotes the number of vertices. Hence, (1) is transformed into: ij k + α is the normalized diagonal matrix and α is set as 0.001 to avoid division by zero. W k is a weight vector of a 1×1 convolution operation and corresponds to w in (1). An attention map M k , which indicates the importance of each vertex, is applied on each vertex by an element-wise product . The graph convolution layer with additional batch normalization layers and a skip connection is shown in <ref type="figure">Fig. 3 (left)</ref>.</p><formula xml:id="formula_1">f out = Kv k=1 W k (f in A k ) M k ,<label>(2)</label></formula><p>2) Temporal Convolution Layer: For each vertex, there are only two connected vertices along the temporal dimension. The temporal convolution is a K t × 1 convolution where K t = 3 is the kernel size in the temporal dimension. We also use batch normalization and residual connections for the temporal convolution layers as shown in <ref type="figure">Fig. 3 (right)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Pose Refinement Module</head><p>Due to partial visibility of the human body and inaccurate predictions from human pose estimation algorithms, the input skeleton data can contain errors, which influences the action recognition accuracy. To reduce this influence, we refine each joint position by the pose refinement module as shown in <ref type="figure">Fig. 4</ref>. For pose refinement, we estimate offsets for each joint that are then added to the input poses.</p><p>In case of 3D skeleton data, we estimate the offset (∆x, ∆y, ∆z) for each joint and add it to its 3D position (x, y, z). As shown in <ref type="figure">Fig. 4</ref>, the offset is estimated by a combination of 1×1 convolution layers, graph convolution layers and one temporal convolution layer. While the graph convolutional layers exploit the spatial relations of the joints to refine the poses, the temporal convolutional layer exploits temporal consistency. In case of 2D skeleton data, we use the x and y coordinates as well as the joint estimation confidence as input, which we obtain from the human pose estimation approach. The estimated 2D offset (∆x, ∆y) is then added to the 2D coordinates for each joint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TCL</head><p>GCL Scale concat </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Motion Flow Position Flow</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Gradual Fusion Module</head><p>Previous works like <ref type="bibr" target="#b10">[11]</ref> iterate between spatial and temporal graph convolutions. This is, however, not only very inefficient but we also show in Sec. IV that this can even slightly decrease the accuracy when motion and spatial information are sequentially processed. Furthermore, the temporal edges do not model the full motion of the joints. Instead, we model the motion by temporal differences:</p><formula xml:id="formula_2">M i,t = P i,t − P i,t−1<label>(3)</label></formula><p>where M i,t is the motion of joint i at time t, P i,t and P i,t−1 are the positions of joint i at time t and t − 1, respectively. As shown in <ref type="figure" target="#fig_3">Fig. 5</ref>, the motion flow takes M as input and the position flow P . In order to include the spatial relations between the joints as additional information for the motion flow, we include one graph convolution layer in the motion flow. In order to capture long-term dependencies, we furthermore reduce the temporal resolution within the gradual fusion module by using stride 2 and 3 for the first and second temporal convolutional layer, respectively. The position flow is in parallel to the motion flow, but we gradually fuse the two flows as shown in <ref type="figure" target="#fig_3">Fig. 5</ref>. It is worth noting that the dimensions of the spatial features and the temporal features do not match due to the reduction of the temporal resolution in the motion flow. We therefore perform max-pooling over the spatial features along the temporal dimension such that the dimensions match at each fusion step. This is shown in <ref type="figure" target="#fig_4">Fig. 6</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Temporal Aggregation Module</head><p>Finally, we aggregate the features to obtain the final class probabilities for each action class as shown in <ref type="figure" target="#fig_5">Fig. 7</ref>. The fused features from the gradual fusion module are first fed into an average pooling layer across the temporal dimension. The features are then recalibrated according to the similarity among different channels. To this end, we scale each feature channel f in by a scalar value:</p><formula xml:id="formula_3">f out = f in · σ(Conv o (ReLU(Conv i (Avg(f in ))))).</formula><p>While the average pooling layer (Avg) squeezes the input features f in into a vector, the 1 × 1 convolutions (Conv) learn the weight for each channel. The values are then mapped by the sigmoid function to values between zero and one. After the features are recalibrated, a graph convolution layer is applied to aggregate spatial information for the last time. Finally, the features are processed by an average pooling layer, a convolution layer, and a softmax function for predicting the probabilities of the action classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets &amp; Evaluation Metrics</head><p>We evaluate our approach on two challenging largescale human action datasets, namely Kinetics <ref type="bibr" target="#b11">[12]</ref> and NTU RGB+D <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PRM TAM</head><p>Top- Kinetics <ref type="bibr" target="#b11">[12]</ref> contains video clips of 400 action classes retrieved from YouTube videos. We use the human poses that are provided by <ref type="bibr" target="#b8">[9]</ref> 2 . The 2D human poses have been extracted using OpenPose <ref type="bibr" target="#b45">[46]</ref>. To this end, the videos have been converted such that all videos have a resolution of 340×256 pixels and a frame rate of 30 frames per second (FPS). The structure of the 2D skeleton is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. The extracted skeleton data is split into a training set (240,000 clips) and a validation set (20,000 clips). As in <ref type="bibr" target="#b8">[9]</ref>, we report the top-1 and top-5 accuracy on the validation set.</p><p>NTU RGB+D <ref type="bibr" target="#b12">[13]</ref> contains 56,880 video clips with 60 actions that are performed by 40 persons, which are between 10 and 35 years old. The video clips have been recorded with multiple cameras and the 3D human poses are estimated by the Kinect v2 SDK. There are two evaluation protocols for this dataset <ref type="bibr" target="#b12">[13]</ref>:</p><p>• Cross-subject (X-Sub): The videos are split into a training set (40,320 videos) and validation set <ref type="bibr" target="#b15">(16,</ref><ref type="bibr">560)</ref> according to different actors. • Cross-view (X-View): The videos are split according to different cameras. The training set contains 37,920 videos recorded by cameras 2 and 3, whereas the validation set includes 18,960 videos recorded by camera 1. We report top-1 accuracy for both protocols. All experiments are conducted using PyTorch on a machine equipped with an i7-5820K CPU (3.3 GHz), a GTX 1080TI GPU, and 16GB RAM. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>Our model is trained with stochastic gradient descent with a learning rate of 0.01 and momentum of 0.9. The learning rate decays by 0.1 every 10 epochs apart from the first 10 epochs. The size of the input data of Kinetics is 300 frames. We apply data augmentation during training as <ref type="bibr" target="#b8">[9]</ref>, where 300 frames are randomly chosen from the input skeleton sequences and slightly disturbed with randomly chosen rotations and translations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Study</head><p>We first examine the effectiveness of the pose refinement module (PRM) and the temporal aggregation module (TAM) in Tab. I. If PRM is not included, the entire module is removed, i.e., we do not refine the poses for action recognition. If TAM is not included, we use only the last average pooling layer, convolution layer, and the softmax function of <ref type="figure" target="#fig_5">Fig. 7</ref>.</p><p>First, we evaluate our model without PRM and TAM. We can see from the table that our model achieves a slightly worse accuracy than ST-GCN <ref type="bibr" target="#b8">[9]</ref>, but it has more than 11 times less parameters and it is more than 4 times faster on a GPU and on a CPU. Compared to ST-GCN, the number of giga floating point operations (GFLOP) are reduced by factor 12. One can also see that AS-GCN <ref type="bibr" target="#b9">[10]</ref> and 2s-AGCN <ref type="bibr" target="#b10">[11]</ref> increase the accuracy of ST-GCN by massively increasing the number of parameters and GFLOP.</p><p>By adding the pose refinement module (PRM) and the temporal aggregation module (TAM) in Tab. I, the accuracy increases while the number of parameters or runtime increases only slightly. By adding both PRM and TAM, we can see that both modules complement each other and achieve the best accuracy with an improvement of 4.3% on the top-1 accuracy and top-5 accuracy. It is interesting to note that the pose refinement module only marginally increases the number of parameters but it slightly increases the runtime. Whereas the temporal aggregation module increases the parameters but only marginally the runtime. This is due to the fact that we have at the beginning of the network very few channels but the full temporal resolution and it is the other way around at the end of the network. Depending on the available memory or computational resources, one of the modules can be deactivated if necessary. While our approach outperforms ST-GCN <ref type="bibr" target="#b8">[9]</ref> in terms of accuracy, memory footprint, and runtime, it achieves nearly the same accuracy as <ref type="bibr" target="#b9">[10]</ref> but with only 10% of the parameters and around 6% of the GFLOP. Only the state-of-the-art method <ref type="bibr" target="#b10">[11]</ref> achieves a higher accuracy, but at the cost of more than doubling the memory footprint and runtime of ST-GCN. Since GPUs are not always available for a robot system, we report the frames per seconds that are processed by ST-GCN <ref type="bibr" target="#b8">[9]</ref> and our approach if they run on a GPU or CPU. In both cases, our approach is more than 3 times faster than <ref type="bibr" target="#b8">[9]</ref>. It is also important to note that fast human pose estimation approaches like OpenPose process 15 or less frames per second on a robot platform, which is less than the 15.7 frames per second that are achieved by our approach on a CPU. In <ref type="figure" target="#fig_7">Fig. 8</ref>, we show a few examples that illustrate the effect of the pose refinement module. Besides of refining the estimated human poses, the module also recovers joints or poses that have not been detected either due to occlusion or due to a failure of the pose estimation approach.</p><p>In a second ablation study, we explore the impact of the gradual fusion module. The results are shown in Tab. II. To prove the effectiveness of our design, we tested two settings: a sequential architecture where we stack temporal convolutional layers and graph convolutional layers in a single flow and the parallel architecture shown in <ref type="figure" target="#fig_3">Fig. 5</ref>. For the sequential and parallel architecture, we also consider two cases. In case of using only position information, we use only the joint positions P as input and in case of position and motion information, we use both P and the motion of joints M .</p><p>In the first two columns of Tab. II, we evaluate the impact of using the additional motion information M for a sequential architecture. In this case, we concatenate P and M . The results show that the accuracy even slightly decreases if motion is added to a sequential architecture. When we compare the columns 1 and 3, we see that there is no substantial difference between a sequential and parallel architecture if only P is used. In the parallel case, we use P as input for both branches. Only if we use P and M for the parallel architecture as shown in <ref type="figure" target="#fig_3">Fig. 5</ref>, we observe an increase in accuracy as reported in the last column. This shows on the one hand that the additional motion information improves the accuracy and on the other hand that the proposed fusion module is essential for fusing the position and motion information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison with State-of-the-Art</head><p>We finally compare our method with current state-of-theart methods for skeleton-based action recognition on the Kinetics and NTU RGB+D datasets. The results are shown in Tab. III and Tab. IV. The tables include methods with handcrafted features <ref type="bibr" target="#b4">[5]</ref>, RNN-based methods <ref type="bibr" target="#b12">[13]</ref>, CNNbased methods <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b46">[47]</ref> and GCN-based methods [9]- <ref type="bibr" target="#b10">[11]</ref>. As shown in the tables, our method achieves an accuracy that is better or close to state-of-the-art methods. This shows that the proposed approach works very well for 2D human poses as well as 3D human poses. The approaches that achieve a higher accuracy use adaptive graph convolutions <ref type="bibr" target="#b9">[10]</ref> or adopt auxiliary strategies to further improve the accuracy like an ensemble of networks <ref type="bibr" target="#b10">[11]</ref>. This, however, makes these approaches demanding in terms of computational resources and difficult to deploy on robot platforms. As shown in Tab. I, our network requires only 10% of the parameters and around 6% of the GFLOP compared to <ref type="bibr" target="#b9">[10]</ref> and only 7% of the parameters and around 4% of the GFLOP compared to <ref type="bibr" target="#b10">[11]</ref>.</p><p>Furthermore, we also compare our method with other lightweight methods for action recognition on the NTU RGB-D dataset in Tab. V. While GFNet <ref type="bibr" target="#b26">[27]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>X-Sub (%) X-View (%) Lie Group <ref type="bibr" target="#b3">[4]</ref> 50.1 82.8 HBRNN <ref type="bibr" target="#b47">[48]</ref> 59.1 64.0 Deep LSTM <ref type="bibr" target="#b12">[13]</ref> 60.7 67.3 ST-LSTM <ref type="bibr" target="#b20">[21]</ref> 69.2 77.7 STA-LSTM <ref type="bibr" target="#b48">[49]</ref> 73.4 81.2 VA-LSTM <ref type="bibr" target="#b6">[7]</ref> 79.2 87.7 ARRN-LSTM <ref type="bibr" target="#b21">[22]</ref> 81.8 89.6 TCN <ref type="bibr" target="#b18">[19]</ref> 74.3 83.1 Clips+CNN+MTLN <ref type="bibr" target="#b5">[6]</ref> 79.6 84.8 Synthesized CNN <ref type="bibr" target="#b49">[50]</ref> 80.0 87.2 RGB+Skeleton <ref type="bibr" target="#b50">[51]</ref> 84.2 89.3 FO-GASTM <ref type="bibr" target="#b51">[52]</ref> 82.8 90.1 Bayesian GC-LSTM <ref type="bibr" target="#b52">[53]</ref> 81.8 89.0 GFNet <ref type="bibr" target="#b26">[27]</ref> 82.0 89.9 EleAtt-GRU <ref type="bibr" target="#b27">[28]</ref> 79.8 87.1 ST-GCN <ref type="bibr" target="#b8">[9]</ref> 81.5 88.3 AS-GCN <ref type="bibr" target="#b9">[10]</ref> 86.8 94.2 2s-AGCN <ref type="bibr" target="#b10">[11]</ref> 88.5 95.1 Ours 85.2 91.7 parameters and achieves a higher accuracy than ST-GCN <ref type="bibr" target="#b8">[9]</ref>, it requires more parameters than our approach and performs worse. Only EleAtt-GRU <ref type="bibr" target="#b27">[28]</ref> has less parameters than our approach, but its accuracy is even worse than ST-GCN. Our model achieves therefore a better trade-off between efficiency and accuracy compared to the state-ofthe-art and is suitable for robotics applications with limited computational resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>X-Sub (%) X-View (%) Params (M) GFLOP ST-GCN <ref type="bibr" target="#b8">[9]</ref> 81.5 88.3 3.5 15.6 GFNet <ref type="bibr" target="#b26">[27]</ref> 82.0 89.9 1.6 48.7 EleAtt-GRU <ref type="bibr" target="#b27">[28]</ref> 79. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>In this paper, we proposed a highly efficient model called Pose Refinement Graph Convolutional Network for 2D or 3D skeleton-based action recognition. It refines the human poses and gradually fuses motion and spatial information. Compared to previous graph convolutional networks, the proposed approach is very efficient in terms of memory footprint and runtime. It reduces the number of parameters by 86%-93% and the computational operations by 89%-96% while achieving a comparable accuracy. It therefore provides a much better trade-off between efficiency and accuracy and is thus suitable for robotics applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Pose Refinement Graph Convolutional Network. The input skeleton sequences are first passed through a pose refinement module to reduce the impact of errors in the skeleton data. Then the refined skeleton sequences are fed into the gradual fusion module consisting of a motion-flow-branch and a position-flow-branch for fusing position and motion information. The position flow aggregates spatial information of skeleton joints at each time step whereas the motion flow captures the long-range temporal dependencies. Finally, the temporal aggregation module aggregates the information over time and predicts the action class probabilities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The left figure illustrates the spatio-temporal graph</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Graph convolution layer (left) and temporal convolution layer (right). Pose refinement module.where K v = 3 is the kernel size of the spatial dimension. The matrix A k is defined by A k = Λ the connection of vertices and an elementĀ ij k is nonzero only if vertex v j ∈ B i . Λ ii k = jĀ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Gradual fusion module. Scale concat denotes the fusion operation shown in Fig. 6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fusion of features from the position and motion flow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Temporal aggregation module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>2</head><label></label><figDesc>https://github.com/kenziyuliu/st-gcn</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Visualization of the pose refinement on the Kinetics dataset. Although we show only a single frame, the pose refinement takes the entire pose sequence into account. The examples show from left to right cases where (a) small pose errors are corrected, (b) wrong poses are refined, (c) missing joints are recovered, or (d) a missing pose is recovered.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>ABLATION STUDY ON THE IMPACT OF THE POSE REFINEMENT MODULE (PRM) AND THE TEMPORAL AGGREGATION MODULE (TAM) ON THE KINETICS DATASET. COMPARED TO OTHER GRAPH CONVOLUTIONAL NETWORKS, THE PROPOSED PR-GCN REQUIRES ONLY A FRACTION OF THE NUMBER OF PARAMETERS AND GIGA FLOATING POINT OPERATIONS (GFLOP) FOR INFERENCE. THE PROPOSED APPROACH ALSO PROCESSES MUCH MORE FRAMES PER SECOND (FPS) ON A GPU AS WELL AS ON A CPU.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">1 (%) Top-5 (%)</cell><cell>Params (M)</cell><cell>GFLOP</cell><cell>FPS (GPU) FPS (CPU)</cell></row><row><cell>ST-GCN [9]</cell><cell></cell><cell></cell><cell>30.7</cell><cell>52.8</cell><cell>3.5</cell><cell>15.6</cell><cell>134.1</cell><cell>4.9</cell></row><row><cell>AS-GCN [10]</cell><cell></cell><cell></cell><cell>34.8</cell><cell>56.5</cell><cell>5.0</cell><cell>27.0</cell><cell>-</cell><cell>-</cell></row><row><cell>2s-AGCN [11]</cell><cell></cell><cell></cell><cell>36.1</cell><cell>58.7</cell><cell>7.1</cell><cell>38.5</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>29.3</cell><cell>51.8</cell><cell>0.3</cell><cell>1.3</cell><cell>570.1</cell><cell>20.3</cell></row><row><cell>PR-GCN</cell><cell></cell><cell></cell><cell>30.7 33.2</cell><cell>53.1 55.5</cell><cell>0.3 0.5</cell><cell>1.6 1.4</cell><cell>442.0 560.6</cell><cell>15.8 19.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell>33.6</cell><cell>56.1</cell><cell>0.5</cell><cell>1.7</cell><cell>433.9</cell><cell>15.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>TABLE I</cell><cell></cell><cell></cell></row><row><cell>Position</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Motion</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Sequential</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Parallel</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Top-1 (%)</cell><cell>32.9</cell><cell>32.2</cell><cell>32.7</cell><cell>33.6</cell><cell></cell><cell></cell></row><row><cell>Top-5 (%)</cell><cell>55.1</cell><cell>55.0</cell><cell>55.2</cell><cell>56.1</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">TABLE II</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">ABLATION STUDY FOR THE GRADUAL FUSION MODULE ON THE</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">KINETICS DATASET.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV COMPARISON</head><label>IV</label><figDesc>WITH STATE-OF-THE-ART METHODS ON THE NTU RGB+D DATASET.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? A new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ms-tcn++: Multi-stage temporal convolutional network for action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Abufarha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Human action recognition by representing 3d skeletons as points in a lie group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="588" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Modeling video evolution for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Oramas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5378" to="5387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A new representation of skeleton sequences for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3288" to="3297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">View adaptive recurrent neural networks for high performance human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Skeletonbased action recognition with gated convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Actionalstructural graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3595" to="3603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Two-stream adaptive graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">35</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">NTU RGB+D: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1010" to="1019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Direct line guidance odometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Frost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Prisacariu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE international conference on Robotics and automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Structured skip list: A compact data structure for 3d reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-P</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Prisacariu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Multiscale interaction for real-time lidar data segmentation on an embedded platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.09162</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Coupled action recognition and pose estimation from multiple views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="16" to="37" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Towards understanding action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3192" to="3199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Interpretable 3d human action analysis with temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1623" to="1631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia &amp; Expo Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="597" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Spatio-temporal LSTM with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="816" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Relational network for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="826" to="831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Independently recurrent neural network (IndRNN): Building a longer and deeper RNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5457" to="5466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning dynamic spatio-temporal relations for human activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="130" to="340" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning object-action relations from bimanual human demonstration using graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R G</forename><surname>Dreher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wächter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Asfour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="187" to="194" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Explainable and efficient sequential correlation network for 3d single person concurrent activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">GFNet: A lightweight group frame network for efficient human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2583" to="2587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">EleAtt-RNN: Adding attentiveness to neurons in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1061" to="1073" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Make skeleton-based action recognition model smaller, faster and better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sakti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia Asia</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">I</forename><surname>Shuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="83" to="98" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep convolutional networks on graph-structured data</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1993" to="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5115" to="5124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Neural relational inference for interacting systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="2693" to="2702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Adaptive graph convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Large-scale learnable graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1416" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Refinedbox: Refining for fewer and high-quality object proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Projected-point-based segmentation: A new paradigm for lidar point cloud segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.03928</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7291" to="7299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Chirality nets for human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8161" to="8171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">An end-to-end spatiotemporal attention model for human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Enhanced skeleton visualization for view invariant human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="346" to="362" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Context-aware cross-attention for skeleton-based human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="15" to="280" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning shape-motion representations from geometric algebra spatio-temporal model for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1066" to="1071" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Bayesian graph convolution lstm for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6882" to="6892" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
