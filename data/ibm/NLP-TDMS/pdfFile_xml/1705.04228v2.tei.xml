<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Incremental Learning Through Deep Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Rosenfeld</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">York University</orgName>
								<address>
									<settlement>Toronto</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
							<email>tsotsos@cse.yorku.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">York University</orgName>
								<address>
									<settlement>Toronto</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Incremental Learning Through Deep Adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Incremental Learning</term>
					<term>Transfer Learning</term>
					<term>Domain Adaptation !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Given an existing trained neural network, it is often desirable to learn new capabilities without hindering performance of those already learned. Existing approaches either learn sub-optimal solutions, require joint training, or incur a substantial increment in the number of parameters for each added domain, typically as many as the original network. We propose a method called Deep Adaptation Networks (DAN) that constrains newly learned filters to be linear combinations of existing ones. DANs precisely preserve performance on the original domain, require a fraction (typically 13%, dependent on network architecture) of the number of parameters compared to standard fine-tuning procedures and converge in less cycles of training to a comparable or better level of performance. When coupled with standard network quantization techniques, we further reduce the parameter cost to around 3% of the original with negligible or no loss in accuracy. The learned architecture can be controlled to switch between various learned representations, enabling a single network to solve a task from multiple different domains. We conduct extensive experiments showing the effectiveness of our method on a range of image classification tasks and explore different aspects of its behavior.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>While deep neural networks continue to show remarkable performance gains in various areas such as image classification <ref type="bibr" target="#b0">[1]</ref>, semantic segmentation <ref type="bibr" target="#b1">[2]</ref>, object detection <ref type="bibr" target="#b2">[3]</ref>, speech recognition <ref type="bibr" target="#b3">[4]</ref> medical image analysis <ref type="bibr" target="#b4">[5]</ref> -and many more -it is still the case that typically, a separate model needs to be trained for each new task. Given two tasks of a totally different modality or nature, such as predicting the next word in a sequence of words versus predicting the class of an object in an image, it stands to reason that each would require a different architecture or computation. A more restricted scenario -and the one we aim to tackle in this work -is that of learning a representation which works well on several related domains. Such a scenario was recently coined by Rebuffi et al. <ref type="bibr" target="#b5">[6]</ref> as multipledomain learning (MDL) to set it aside from multi-task learning -where different tasks are to be performed on the same domain. An example of MDL is image-classification where the images may belong to different domains, such as drawings, natural images, etc. In such a setting it is natural to expect that solutions will: 1) Utilize the same computational pipeline 2) Require a modest increment in the number of required parameters for each added domain 3) Retain performance of already learned datasets (avoid "catastrophic forgetting") 4) Be learned incrementally, dropping the requirement for joint training such as in cases where the training data for previously learned tasks is no longer available.</p><p>Our goal is to enable a network to learn a set of related tasks one by one while adhering to the above requirements. We do so by augmenting a network learned for one task with controller modules which utilize already learned representations for another. The parameters of the controller modules are optimized to minimize a loss on a new task. The training data for the original task is not required at this stage. The network's output on the original task data stays exactly as it was; any number of controller modules may be added to each layer so that a single network can simultaneously encode multiple distinct domains, where the transition from one domain to another can be done by setting a binary switching variable or controlled automatically. The resultant architecture is coined DAN, standing for Deep Adaptation Networks. We demonstrate the effectiveness of our method on the recently introduced Visual Decathlon Challenge <ref type="bibr" target="#b5">[6]</ref> whose task is to produce a classifier to work well on ten different image classification datasets. Though adding only 13% of the number of original parameters for each newly learned task (the specific number depends on the network architecture), the average performance surpasses that of fine tuning all parameters -without the negative side effects of doubling the number of parameters and catastrophic forgetting. In this work, we focus on image classification in various datasets, hence in our experiments the word "task" refers to a specific dataset/domain. The proposed method is extensible to multi-task learning, such as a single network that performs both image segmentation and classification, but this paper does not pursue this.</p><p>Our main contribution is the introduction of an improved alternative to transfer learning, which is as effective as fine-tuning all network parameters towards a new task, precisely preserves old task performance, requires a fraction (network dependent, typically 13%) of the cost in terms of new weights and is able to switch between any number of learned tasks. Experimental results verify the applicability of the method to a wide range of image-classification datasets.</p><p>We introduce two variants of the method, a fullyparametrized version, whose merits are described above  <ref type="figure">Fig. 1</ref>: Overview of proposed method. For newly learned domains, controller modules are attached to convolutions of a base network, whose parameter are frozen. A switching variable α allows to switch the behavior of the network between the original behaviour of the convolution and a re-parametrized one for the new domain. α can be determined either manually or via a sub-network ("Dataset Decider") which determines the source domain of the image, switching accordingly between different sets of control parameters. α Also controls which of the classifiers to apply. Other layers (e.g, non-linearities, batch-normalization, skip layers) not shown for presentation purposes. We visualize one added task, though an arbitrary number of tasks can be added. Controller modules: filters of a convolutional layer of a base network are modified by re-combining their weights through a controller module, where a switching α variable can choose between the original filters F l and newly created ones F a l . We show a controller module for a single added task, but any number of controllers can be added, with α then being a vector instead of scalar. and one with far fewer parameters, which significantly outperforms shallow transfer learning (i.e. feature extraction) for a comparable number of parameters. In the next section, we review some related work. Sec. 3 details the proposed method. In Sec. 4 we present various experiments, including comparison to related methods, as well as exploring various strategies on how to make our method more effective, followed by some discussion &amp; concluding remarks.</p><formula xml:id="formula_0">inremental learning.html F l W l F a l Conv( ) ; F l X l − 1 Conv( ) ; F a l X l − 1 ⋅ α ⋅ ( 1 − α ) X l + , α X l − 1 Controller Module( ) W l Original Convolution Controller Module( ) W k Original Convolution Controller Module( ) W l + 1 Original Convolution ...</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Multi-task Learning</head><p>In multi-task learning, the goal is to train one network to perform several tasks simultaneously on the same input. This is usually done by jointly training on all tasks. Such training is advantageous in that a single representation is used for all tasks. In addition, multiple losses are said to act as an additional regularizer. Some examples include facial landmark localization <ref type="bibr" target="#b6">[7]</ref>, semantic segmentation <ref type="bibr" target="#b7">[8]</ref>, 3Dreasoning <ref type="bibr" target="#b8">[9]</ref>, object and part detection <ref type="bibr" target="#b9">[10]</ref> and others. While all of these learn to perform different tasks on the same dataset, the recent work of <ref type="bibr" target="#b10">[11]</ref> explores the ability of a single network to perform tasks on various image classification datasets. We also aim to classify images from multiple datasets but we propose doing so in a manner which learns them one-by-one rather than jointly. Concurrent with our method is that of <ref type="bibr" target="#b5">[6]</ref> which introduces dataset-specific additional residual units. We compare to this work in Sec 4. Our work bears some resemblance to <ref type="bibr" target="#b11">[12]</ref>, where two networks are trained jointly, with additional "cross-stitch" units, allowing each layer from one network to have as additional input linear combinations of outputs from a lower layer in another. However, our method does not require joint training and requires significantly fewer parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Incremental Learning</head><p>Adding a new ability to a neural net often results in so-called "catastrophic forgetting" <ref type="bibr" target="#b12">[13]</ref>, hindering the network's ability to perform well on old tasks. The simplest way to overcome this is by fixing all parameters of the network and using its penultimate layer as a feature extractor, upon which a classifier may be trained <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. While guaranteed to leave the old performance unaltered, it is observed to yield results which are substantially inferior to fine-tuning the entire architecture <ref type="bibr" target="#b2">[3]</ref>. The work of Li and Hoiem <ref type="bibr" target="#b15">[16]</ref> provides a succinct taxonomy of several variants of such methods. In addition, they propose a mechanism of fine-tuning the entire network while making sure to preserve old-task performance by incorporating a loss function which encourages the output of the old features to remain constant on newly introduced data. While their method adds a very small number of parameters for each new task, it does not guarantee that the model retains its full ability on the old task. Rusu et al. <ref type="bibr" target="#b16">[17]</ref> shows new representations can be added alongside old ones while leaving the old task performance unaffected. However, this comes at a cost of duplicating the number of parameters of the original network for each added task. In Kirkpatrick et al. <ref type="bibr" target="#b17">[18]</ref> the learning rate of neurons is lowered if they are found to be important to the old task. Our method fully preserves the old representation while causing a modest increase in the number of parameters for each added task. Recently, Sarwar et al. <ref type="bibr" target="#b18">[19]</ref> have proposed to train a network incrementally by sharing a subset of early layers and splitting later ones. By construction, their method also preserves the old representation. However, as their experiments show, re-learning the parameters of the new network branches, whether initialized randomly with a similar distribution or simply copying the old ones results in worse performance than learning without any network sharing. Our method, while also re-utilizing existing weights, attains results which are on average better than simple transfer learning or learning from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Network Compression</head><p>Multiple works have been published on reducing the number weights of a neural network as a means to represent it compactly <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, gain speedups <ref type="bibr" target="#b21">[22]</ref> or avoid over-fitting <ref type="bibr" target="#b22">[23]</ref>, using combinations of coding, quantization, pruning and tensor decomposition. Such methods can be used in conjunction with ours to further improve results, as we show in Sec. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">APPROACH</head><p>We begin with some notation. Let T be some task to be learned. Specifically, we use a deep convolutional neural net (DCNN) in order to learn a classifier to solve T , which is an image classification task. Most contemporary DCNN's follow a common structure: for each input x, the DCNN computes a representation of the input by passing it through a set of l layers φ i , i ∈ 1 . . . l interleaved with non-linearities. The initial (lower) layers of the network are computational blocks, e.g. convolutions with optional residual units in more recent architectures <ref type="bibr" target="#b23">[24]</ref>. Our method applies equally to networks with or without residual connections. At least one fully connected layer f i , i ∈ 1 . . . c is attached to the output of the last convolutional layer. Let</p><formula xml:id="formula_1">Φ F N = σ(φ l ) • . . . σ(φ 2 ) • σ(φ 1 )</formula><p>be the composition of all of the convolutional layers of the network N , interleaved by non-linearities. We use an architecture where all nonlinearities σ are the same function, with no tunable param-</p><formula xml:id="formula_2">eters. Denote by Φ F N (x) the feature part of N . Similarly, denote by Φ C N = f c • . . . σ(f 2 ) • σ(f 1 ) the classifier part</formula><p>of N , i.e. the composition of all of the fully-connected layers of N . The output of N is then simply defined as:</p><formula xml:id="formula_3">N (x) = Φ C N • Φ F N (x)<label>(1)</label></formula><p>We do not specify batch-normalization layers in the above notation for brevity. It is possible to drop the Φ C N term, if the network is fully convolutional, as in <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Adapting Representations</head><p>Assume that we are given two tasks, T 1 and T 2 , to be learned, and that we have learned a base network N to solve T 1 . We assume that a good solution to T 2 can be obtained by a network with the same architecture as N but with different parameters. We augment N so that it will be able to solve T 2 as well by attaching a controller module to each of its convolutional layers. Each controller module uses the existing weights of the corresponding layer of N to create new convolutional filters adapted to the new task T 2 : for each convolutional layer φ l in N , let F l ∈ R Co×Ci×k×k be the set of filters for that layer, where C o is the number of output features, C l the number of inputs, and k × k the kernel size (assuming a square kernel). Denote by b l ∈ R C the bias. Denote byF l ∈ R Co×D the matrix whose rows are flattened versions of the filters of F l , where D = C i · k · k; let f ∈ R Ci×k×k be a filter from F l whose values are</p><formula xml:id="formula_4">f 1 =    f 1 11 · · · f 1 1k . . . f 1 kk    , · · · , f i =    f i 11 · · · f i 1k . . . f i kk    (2)</formula><p>The flattened version of f is a row vector:</p><formula xml:id="formula_5">f = (f 1 11 , · · · , f 1 kk , · · · , · · · f i 11 , · · · , f i kk ) ∈ R D<label>(3)</label></formula><p>"Unflattening" a row vectorf reverts it to its tensor form f ∈ R Ci×k×k . This way, we can writẽ</p><formula xml:id="formula_6">F a l = W l ·F l<label>(4)</label></formula><p>where W l ∈ R Co×Co is a weight matrix defining linear combinations of the flattened filters of F l , resulting in C o new filters. UnflatteningF a l to its original shape results in F a l ∈ R Co×Ci×k×k , which we call the adapted filters of layer φ l . Using the symbol X ⊗ Y as shorthand for flatten Y →matrix multiply by X→unflatten, we can write:</p><formula xml:id="formula_7">F a l = W l ⊗ F l<label>(5)</label></formula><p>If the convolution contains a bias, we instantiate a new weight vector b a l instead of the original b l . The output of layer φ l is computed as follows: let x l be the input of φ l in the adapted network. For a given switching parameter α ∈ {0, 1}, we set the output of the modified layer to be the application of the switched convolution parameters and biases:</p><formula xml:id="formula_8">x l+1 = [α(W l ⊗ F l ) + (1 − α)F l ] * x l + αb a l + (1 − α)b l (6)</formula><p>The above formulation is for switching between two different behaviours -that of a base network and that of the controller modules learned for a new task.  To allow the network to perform multiple tasks, we turn α to a vector α ∈ {0, 1} n where n is the overall number of tasks, so that α j = 1if we want to perform the j'th task and 0 otherwise. The output of the l layer will be then determined similarly to Equation 6:</p><formula xml:id="formula_9">x l+1 = n i=1 α i (F ai l * x l + b i l )</formula><p>Where F ai l , b i l are the set of adapted filters / bias for the i'th task and we define for F a1 l = F l , b 1 l = b l to include the original task.</p><p>A set of fully connected layers f a i are learned from scratch, attaching a new "head" to the network for each new task. Throughout training &amp; testing, the weights of F (the filters of N ) are kept fixed and serve as basis functions for F a . The weights of the controller modules are learned via back-propagation given the loss function. Weights of any batch normalization (BN) layers are either kept fixed or learned anew. The batch-normalized output is switched between the values of the old and new BN layers, similarly to Eq. 6. A visualization of the resulting DAN can be seen in <ref type="figure">Fig. 1</ref> and a single control module in <ref type="figure">Fig. 2</ref>.</p><p>We denote a network learned for a dataset/task S as N S . A controller learned using N S as a base network will be denoted as DAN S , where DAN stands for Deep Adaptation Network and DAN S→T means using DAN S for a specific task T . While in this work we apply the method to classification tasks it is applicable to other tasks as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Additional Design Choices</head><p>In the following, we mention some additional design choices of possible ways to augment a network with controllers, as well as provide some analysis on the incurred cost of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Weaker parametrization</head><p>A weaker variant of our method is one that forces the matrices W l to be diagonal, e.g, only scaling the output of each filter of the original network. We call this variant "diagonal" (referring only to scaling coefficients, such as by a diagonal matrix) and the full variant of our method "linear" (referring to a linear combination of filters). The diagonal variant can be seen as a form of explicit regularization which limits the expressive power of the learned representation. While requiring significantly fewer parameters, it results in poorer classification accuracy, but as will be shown later, also outperforms regular feature-extraction for transfer learning, especially in network compression regimes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Multiple Controllers</head><p>The above description mentions one base network and one controller network. However, any number of controller networks can be attached to a single base network, regardless of already attached ones. In this case α is extended to onehot vector of values determined by another sub-network, allowing each controller network to be switched on or off as needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Parameter Cost</head><p>The number of new parameters added for each task depends on the number of filters in each layer and the number of parameters in the fully-connected layers. As the latter are not reused, their parameters are fully duplicated. Let M = C o × D be the filter dimensions for some conv. layer</p><formula xml:id="formula_10">φ l where D = C i × k × k. A controller module for φ l requires C 2</formula><p>o coefficients for F a l and an additional C o for b a l . Hence the ratio of new parameters w.r.t to the old for φ l is  number of weights required to adapt the convolutional layers Φ l combined with a new fully-connected layer amounts to about 13% of the original parameters. For VGG-B, this is roughly 21%. For instance, constructing 10 classifiers using one base network and 9 controller networks requires (1 + 0.13 * 9) · P =2.17 · P parameters where P is the number for the base network alone, compared to 10 · P required to train each network independently. The cost is dependent on network architecture, for example it is higher when applied on the VGG-B architecture. While our method can be applied to any network with convolutional layers, if C o ≥ D, i.e., the number of output filters is greater than the dimension of each input filter, it would only increase the number of parameters.</p><formula xml:id="formula_11">Co×(Co+1) Co×(D+1) = Co+1 D+1 ≈ Co D . Example: for C o = C i = 256 input</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Limitations</head><p>An immediate question that arises from our formulation concerns the expressive power of the resulting adapted networks. We provide some analysis here on this issue. In regular fine-tuning or learning schemes, the weights of each layer may take on arbitrary values, depending on the optimization scheme and training data. In contrast, the proposed method constrains each filter to be a linear combination of the original filters in the corresponding layer. This may, in general, become a strong limiting factor on the performance of the network. The filters F = f 1 , . . . , f k of a layer l can form vectors by flattening their tensor form. F defines a basis of a subspace of filters from which the proposed method creates new ones. As our method defines new filters by applying a linear transformation on F , the initial values thereof can have several effects on the resulting networks. Two important scenarios are when (a) network expressibility and (b) network efficiency are affected. We provide examples of when these scenarios can occur and some experiments to demonstrate them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Expressibility</head><p>One can easily construct cases where the expressive power of a network is critically hindered by choosing to adapt it from an inappropriate base network. For example, assume that the inputs of a network are RGB images. Also assume that -for some reason -all first-layer filters have coefficients of exactly 0 in the green and blue channels. If information in these channels is critical for the task to be learned by the proposed method, it will clearly fail, as the information resides in an orthogonal feature space to that represented by the base network's filters. 3.3.1.1 Residual Connections: The above claim does not hold for networks with residual connections <ref type="bibr" target="#b23">[24]</ref>. In Residual connections the output y i of the i th layer is of the following general form:</p><formula xml:id="formula_12">y i = F (x i ; W i ) + x i<label>(7)</label></formula><p>where x i is the input to this layer and W i is the layer's parameters. This allows preserving information from the previous layer, as a possible function permitted by this is the identity function. If enough information from the signal x is preserved then the next layer can still recover from not representing it adequately in this layer. Hence, the expressive power of the network is probably less sensitive to the choice of basis function in a single layer. Nevertheless, its efficiency may be compromised, as we describe next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Efficiency</head><p>In some cases, the expressive power of the network is not immediately affected by the choice of basis function but we claim that the efficiency of the network is. We define the efficiency of a network w.r.t to a given image patch as the minimal number of layers (depth) required to discriminate it from others by a single filter. In other words, the depth of the shallowest layer where there exists some filter whose response correlates strongly with the presence of the patch. An example is a 3x3 gray-scale patch with 1 in all corners and 0 otherwise. In the first layer, a filter v of values:</p><formula xml:id="formula_13">v =   +1 −1 +1 −1 −1 −1 +1 −1 +1  </formula><p>would perfectly match this patch. On the other hand, one could set 9 filters of the first layer to delta functions with 1 in location i, j for each i and j in 1, 2, 3. These filters span all possible patches of 3x3 so they do not effect expressibility. However, keeping the first layer fixed, the second layer would be required to create a filter to match p, whereas v expressed this directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Toy Dataset</head><p>To further demonstrate the above claims, we constructed a toy classification task. We first describe the dataset and experiments and then discuss the results. The task is: given an input image of size 28x28x3, predict the length of a bar in the an image. In this dataset, the images are restricted to be 1-pixel width bars of length 3 · k where k ∈ {3, 4, 5, 6, 7} over a blank background and the class is the corresponding length of the bar. The bar lengths were set to be in increments of 3 as lesser ones caused difficulties for the network to learn, probably due to maxpooling operations.</p><p>The dataset is composed of 1000 examples, with a random 75%/25% train/test split. The network we tested is deliberately quite degenerate: it contains one 5x5 filter on the first layer and 20 filters on the second, followed by two fully connected layers of 320x50 and 50x5.</p><p>We created 3 variants of this dataset, to test different transfer-learning scenarios on it. The dataset variants are (1) only red horizontal bars, (2) only red vertical bars (referred to as "transposed") and (3) only green horizontal bars (referred to as "new channel"). Note that we limited our method to operate only on the first layer, which in this case boils down to multiplying the output of the filter by a scalar and learning a new bias.</p><p>The network was trained for 50 epochs using the Adam <ref type="bibr" target="#b25">[26]</ref> optimizer (SGD did not converge as well in this case) in all scenarios. We tested seven scenarios. These involve different combinations of the images in the dataset, initial-ization of the first layer and which layers may or may not be modified by the optimizer. Each experiment was repeated 20 times to account for variability. We plot the mean accuracy on the validation set over the 50 training epochs. <ref type="figure" target="#fig_2">Fig. 4</ref> summarizes the result of these experiments. The left sub-figure shows the mean performance over the 50 epochs and the right one shows the maximal (of the 20 trials) attained for each scenario. These show that in most cases, we can reach good accuracy, however on the average case there are drastic differences between the obtained performance, although this is a very simple dataset. The original scenario, where the first layer is fixed, attains 100% accuracy. Switching the channel from green to red ("channel switch") but keeping the first filter fixed results in chance accuracy, as excepted: no linear combination of a purely "green" filter can represent information from the red channel. In this case, if we allow all layers to be learned ("channel switch + learn"), indeed the network can finally converge to a good accuracy ( <ref type="figure" target="#fig_2">Fig. 4 (b)</ref>), but the average performance over 20 trials is far from optimal ( <ref type="figure" target="#fig_2">Fig. 4 (a)</ref>) -the network converges to around 50% accuracy (where chance performance is 20%). This confirms that even for very simple cases, bad initialization can be detrimental to the training process.</p><p>As a "sanity test", we make sure that randomly initializing the network and allowing all layers to be learned in the "channel switch" case achieves 100% performance (in both the average and best case). This is indeed the case ("channel switch + clean start"), although we see that the random initialization converges at first much slower than the informed initialization of "original". Next, we initialize the network as in "original" but with the "channel switch" dataset. However, we add random noise to the first layer's filter, then keep it fixed ("channel switch + noise"). The optimization is able to recover from this initialization in both the average and best case, as the initial filter is not orthogonal to the green channel.</p><p>The "original + learn" is initialized as "original" but allows the first layer to be modified. While on average it initially converges slower than the "original" scenario, on average it performs better.</p><p>Finally, the "transposed" case, where the original filter is fixed to be orthogonal to the required shape (horizontal bars), attains average lower performance, and near 100% in the best case. Note that although we cannot generate an horizontal bar by linearly transforming the vertical one, the information is not lost and can be recovered by filters of the second layer, due to their convolutional nature. This is done by a weighted summation of the shifted responses of the horizontal filter with proper coefficients. This is an example of the network "efficiency" being reduced, i.e, delegating to the second layer computations which could be done in the first.</p><p>To summarize, we conclude from the experiments on this toy dataset that: 1) Setting some network weights using strong domainspecific knowledge and fixing their values, learning only other layers can lead to fast convergence to a strong solution. 2) An even stronger solution can emerge if all layers are initialized randomly, then allowed to be updated -though convergence will be slower. 3) Initializing some weights to "bad" values can cause on-average dramatically inferior performance to the previous two cases, with occasional but rare instances of good performance. 4) Initializing some weights to "bad" values, fixing them and learning the rest of the weights will totally and irreparably prevent the network from learning.</p><p>In the next section, we test our method on richer and more diverse datasets, less degenerate networks (e.g., more than one filter in the first layer), and in some cases residual networks, which all are likely to diminish effects shown on artificial examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS ON REAL DATASETS</head><p>We conduct several experiments to test our method and explore different aspects of its behavior. The experimental section is split into two parts, the first being of a more exploratory nature and the second geared toward results in a recent multi-task learning benchmark. We use two different basic network architectures on two (somewhat overlapping) sets of classification benchmarks for the respective parts. The first is VGG-B <ref type="bibr" target="#b24">[25]</ref> . We begin by listing the datasets we used (4.0.1), followed by establishing baselines by training a separate model for each using a few initial networks. We proceed to test several variants of our proposed method (4.1) as well as testing different training schemes. Next, we discuss methods of predicting how well a network would fare as a base-network (4.3). We show how to discern the domain of an input image and output a proper classification (4.3.1) without manual choice of the control parameters α.</p><p>In the second part of our experiments we show how our applying our method results in leading scores in the Visual Decathlon Challenge <ref type="bibr" target="#b5">[6]</ref>, using a different, more recent architecture. Before concluding we demonstrate some useful properties of our method.</p><p>All Experiments were performed using the PyTorch 1 framework using a single Titan-X Pascal GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.0.1">Datasets and Evaluation</head><p>The first part of our evaluation protocol resembles that of <ref type="bibr" target="#b10">[11]</ref>: we test our method on the following datasets: Caltech-256 <ref type="bibr" target="#b26">[27]</ref>, CIFAR-10 <ref type="bibr" target="#b27">[28]</ref>, Daimler <ref type="bibr" target="#b28">[29]</ref> (DPed), GTSR <ref type="bibr" target="#b29">[30]</ref>, Omniglot <ref type="bibr" target="#b30">[31]</ref>, Plankton imagery data <ref type="bibr" target="#b31">[32]</ref> (Plnk), Human Sketch dataset <ref type="bibr">[33]</ref> and SVHN <ref type="bibr">[34]</ref>. All images are resized to 64 × 64 pixels, duplicating gray-scale images so that they have 3 channels as do the RGB ones. We whiten all images by subtracting the mean pixel value and dividing by the variance per channel. This is done for each dataset separately. We select 80% for training and 20% for validation in datasets where no fixed split is provided. We use the B architecture described in <ref type="bibr" target="#b24">[25]</ref>, henceforth referred to as VGG-B. It performs quite well on the various datasets when trained from scratch (See Tab. 1. Kindly refer to <ref type="bibr" target="#b10">[11]</ref> for a brief description of each dataset.</p><p>As a baseline, we train networks independently on each of the 8 datasets. All experiments in this part are done with the Adam optimizer <ref type="bibr" target="#b25">[26]</ref>, with an initial learning rate of 1e-3 or 1e-4, dependent on a few epochs of trial on each dataset. The learning rate is halved after each 10 epochs. Most networks converge within the first 10-20 epochs, with mostly negligible improvements afterwards. We chose Adam for this part due to its fast initial convergence with respect to non-adaptive optimization methods (e.g, SGD), at the cost of possibly lower final accuracy <ref type="bibr">[35]</ref>. The top-1 accuracy (%) is summarized in <ref type="table" target="#tab_3">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Controller Networks</head><p>To test our method, we trained a network on each of the 8 datasets in turn to be used as a base network for all <ref type="bibr" target="#b0">1</ref>   <ref type="figure">Fig. 5 (a)</ref>. The parameter cost (3.2.3) of each setting is reported in the last column of the table. This (similarly to <ref type="bibr" target="#b5">[6]</ref> is the total number of parameters required for a set of tasks normalized by that of a single fullyparametrized network. We also check how well a network can perform as a base-network after it has seen ample training examples: DAN imagenet is based on VGG-B pretrained on ImageNet <ref type="bibr">[36]</ref>. This improves the average performance by a significant amount (83.7% to 86.5%). On Caltech-256 we see an improvement from 88.2% (training from scratch). However, for both Sketch and Omniglot the performance is in favor of DAN sketch . Note these are the only two domains of strictly unnatural images. Additionally, DAN imagenet is still slightly inferior to the non-pretrained VGG-B(S) (86.5% vs 87.7%), though the latter is more parameter costly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Multiple Base Networks</head><p>Arguably, a good base network should have features generic enough so that a controller network can use them for a broad range of target tasks. In practice this may not necessarily be the case. We conjecture that using a diverse set of features, such as that learned by training on more than one task, will provide a better basis for transfer learning. To use two base-networks simultaneously, we implemented a dual-controlled network by using both DAN caltech−256 and DAN sketch and attaching to them controller networks. The outputs of the feature parts of the resulting sub-networks were concatenated before the fully-connected layer. This resulted in the exact same performance as DAN sketch alone. However, by using selected controller-modules per group of tasks, we can improve the results: for each dataset the maximally performing network (based on validation) is the basis for the control module, i.e., we used DAN imagenet for all datasets except Omniglot and Sketch. For the latter two we use DAN sketch as a base net. We call this network DAN imagenet+sketch . At the cost of more parameters, it boosts the mean performance to 87.76% -better than using any single base net for controllers or training from scratch. Since it is utilized for 9 tasks (counting ImageNet), its parameter cost (2.76) is still quite good.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Starting from a Randomly Initialized Base Network</head><p>We tested how well our method can perform without any prior knowledge, e.g., building a controller network on a randomly initialized base network. The total number of parameters for this architecture is˜12M. However, as 10M have been randomly initialized and only the controller modules and fully-connected layers have been learned, the effective number is actually 2M. Hence its parameter cost is determined to be 0.22. We summarize the results in Tab. 1. Notably, the results of this initialization worked surprisingly well; the mean top-1 precision attained by this network was 76.3%, slightly worse than of DAN caltech−256 (79.9%). This is better than initializing with DAN daimler , which resulted in a mean accuracy of 75%. This is possible because the random values in the base network can still be linearly combined by our method to create ones that are useful for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Initialization</head><p>One question which arises is how to initialize the weights W of a control-module. We tested several options: (1) Setting W to an identity matrix (diagonal). This is equivalent to the controller module starting with a state which effectively mimics the behavior of the base network (2) Setting W to random noise (random) (3) Training an independent network for the new task from scratch, then set W to best linearly approximate the new weights with the base weights (linear approx). To find the best initialization scheme, we trained DAN sketch→caltech256 for one epoch with each and observed the loss. Each experiment was repeated 5 times and the results averaged. From <ref type="figure">Fig. 5(a)</ref>, it is evident that the diagonal initialization is superior, perhaps counterintuitively, there is no need to train a fully parametrized target network. Simply starting with the behavior of the base network and tuning it via the control modules results in faster convergence. Hence we train controller modules with the diagonal method. Interestingly, the residual adaptation unit in <ref type="bibr" target="#b5">[6]</ref> is initially similar to the diagonal configuration. If all of the filters in their adapter unit are set to 1 (up to normalization), the output of the adapter will be initially the same as that of the controller unit initialized with the identity matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Transferability</head><p>How is one to choose a good network to serve as a basenetwork for others? As an indicator of the representative power of the features of each independently trained network N , we test the performance on other datasets, using N for fine tuning. We define the transferability of a source task S w.r.t a target task T as the top-1 accuracy attained by finetuning N trained on S to perform on T . We test 3 different scenarios, as follows: (1) Fine-tuning only the last layer (a.k.a feature extraction) (ft-last); (2) Fine-tuning all layers of N (ftfull); (3) same as ft-full, but freezing the parameters of the batch-normalization layers -this has proven beneficial in some cases -we call this option ft-full-bn-off. The results in <ref type="figure" target="#fig_1">Fig 3</ref> show some interesting phenomena. First, as expected, feature extraction (ft last) is inferior to fine-tuning the entire network. Second, usually training from scratch is the most beneficial option. Third, we see a distinction between natural images (Caltech-256, CIFAR-10, SVHN, GTSR, Daimler) and unnatural ones (Sketch, Omniglot, Plankton); Plankton images are essentially natural but seem to exhibit different behavior than the rest. It is evident that features from the natural images are less beneficial for the unnatural images. Interestingly, the converse is not true: training a network starting from Sketch or Omniglot works quite well for most datasets, both natural and unnatural. This is further shown in Tab. 2: we calculate the mean transferability of each dataset by the mean value of each rows of the transferability matrix from <ref type="figure" target="#fig_1">Fig. 3</ref>. DAN Caltech−256 works best for feature extraction. However, for full fine-tuning using DAN P lankton works as the best starting point, closely followed by DAN Caltech−256 . For controller networks, the best mean accuracy attained for a single base net trained from scratch is attained using DAN sketch (83.7%). This is close to the performance attained by full transfer learning from the same network (84.2%, see Tab. 2) at a fraction of the number of parameters. This is consistent with our transferability measure. To further test the correlation between the transferability and the performance given a specific base network, we used each dataset as a base for control networks for all others and measured the mean overall accuracy. The results can be seen in <ref type="figure">Fig. 5</ref> (b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">A Unified Network</head><p>Finally, we test the possibility of a single network which can both determine the domain of an image and classify it. We train a classifier to predict from which dataset an image originates, using the training images from the 8 datasets. This is learned easily by the network (also VGG-B) which rapidly converges to 99% accuracy. With this "datasetdecider", named N dc we augment DAN sketch to set for each input image I from any of the datasets D i the controller value α i of DAN sketch→Di to 1 if and only if N dc deemed (b) <ref type="figure">Fig. 7</ref>: (a) Accuracy vs. learning method. Using only the last layer (feature extractor) performs worst. finetune: vanilla finetuning. Diagonal : our controller modules with a diagonal combination matrix. Linear: our full method. On average, our full method outperforms vanilla fine tuning. (b) Accuracy vs. quantization: with as low as 8 bits, we see no significant effect of network quantization on our method, showing they can be applied together.</p><p>I to originate from D i and to 0 otherwise. This produces a network which applies to each input image the correct controllers, classifying it within its own domain. While the predicted values of α i are real-valued, we set the highest one to 1 and the rest to zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Visual Decathlon Challenge</head><p>We now show results on the recent Visual Decathlon Challenge of <ref type="bibr" target="#b5">[6]</ref>. The challenge introduces involves 10 different image classification datasets: ImageNet <ref type="bibr">[36]</ref>; Aircraft <ref type="bibr">[37]</ref>; Cifar-100 <ref type="bibr" target="#b27">[28]</ref>; Daimler Pedestrians <ref type="bibr" target="#b28">[29]</ref>; Dynamic Textures <ref type="bibr">[38]</ref>; GTSR <ref type="bibr" target="#b29">[30]</ref>; Flowers <ref type="bibr">[39]</ref>; Omniglot <ref type="bibr">[40]</ref>; SVHN <ref type="bibr">[34]</ref> and UCF-101 <ref type="bibr">[41]</ref>. The goal is to achieve accurate classification on each dataset while retaining a small model size, using the train/val/test splits fixed by the authors. All images are resized so the smaller side of each image is 72 pixels. The classifier is expected to operate on images of 64x64 pixels. Each entry in the challenge is assigned a decathlon score, which is a function designed to highlight methods which do better than the baseline on all 10 datasets. Please refer to the challenge website for details about the scoring and datasets: http://www.robots.ox.ac. uk/ ∼ vgg/decathlon/. Similarly to <ref type="bibr" target="#b5">[6]</ref>, we chose to use a wide residual network <ref type="bibr">[42]</ref> with an overall depth of 28 and a widening factor of 4, with a stride of 2 in the convolution at the beginning of each basic block. In what follows we describe the challenge results, followed by some additional experiments showing the added value of our method in various settings. In this section we used the recent YellowFin optimizer <ref type="bibr">[43]</ref> as it required less tuning than SGD. We use an initial learning rate factor of 0.1 and reduce it to 0.01 after 25 epochs. This is for all datasets with the exception of ImageNet which we train for 150 epochs with SGD with an initial learning rate of 0.1 which is reduced every 35 epochs by a factor of 10. This is the configuration we determined using the available validation data which was then used to train on the validation set as well (as did the authors of the challenge) and obtain results from the evaluation server.</p><p>Here we trained on the reduced resolution ImageNet from scratch and used the resulting net as a base for all other tasks. Tab. 3 summarizes our results as well as baseline methods and the those of <ref type="bibr" target="#b5">[6]</ref>, all using a base architecture of similar capacity. By using a significantly stronger base architecture they obtained higher results (mean of 79.43%) but with a parameter cost of 12, i.e., requiring 12 times the amount of original parameters. All of the rows are copied from <ref type="bibr" target="#b5">[6]</ref>, including their re-implementation of LWF, except the last which shows our results. The final column of the table shows the decathlon score. A score of 2500 reflects the baseline resulting from finetuning the network learned on ImageNet to each dataset independently. For the same architecture, the best results obtained by the Residual Adapters method is slightly below ours in terms of decathlon score and slightly above them in terms of mean performance. However, unlike them, we avoid joint training over all of the datasets and using dataset-dependent weight decay.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Compression and Convergence</head><p>In this section, we highlight some additional useful properties of our method. All experiments in the following were done using the same architecture as in the last section but training was performed only on the training sets of the Visual Decathlon Challenge and tested on the validation sets. First, we check whether the effects of network compression are complementary to ours or can hinder them. Despite the recent trend of sophisticated network compression techniques (for example <ref type="bibr" target="#b19">[20]</ref>) we use only a simple method of compression as a proof-of-concept, noting that using recent compression methods will likely produce better results. We   <ref type="figure">Fig. 8</ref>: (a) Our method (linear) converges to a high accuracy faster than fine-tuning. The weaker variant of our method converges as fast as feature-extraction but reaches an overall higher accuracy (7 (a)). (b) zoom in on top-right of (a).</p><p>apply a simple linear quantization on the network weights, using either 4, 6, 8, 16 or 32 bits to represent each weight, where 32 means no quantization. We do not quantize batchnormalization coefficients. <ref type="figure">Fig. 7 (b)</ref> shows how accuracy is affected by quantizing the coefficients of each network. Using 8 bits results in only a marginal loss of accuracy. This effectively means our method can be used to learn new tasks while requiring an addition of 3.25% of the original amount of parameters. Many maintain performance even at 6 bits (DPed, Flowers, GTSR, Omniglot, SVHN. Next, we compare the effect of quantization on different transfer methods: feature extraction, fine-tuning and our method (both diagonal and linear variants). For each dataset we record the normalized (divided by the max.) accuracy for each method/quantization level (which is transformed into the percentage of required parameters). This is plotted in <ref type="figure" target="#fig_5">Fig.  9</ref>. Our method requires significantly fewer parameters to reach the same accuracy as fine-tuning. If parameter usage is limited, the diagonal variant of our method significantly outperforms feature extraction. Finally, we show that the number of epochs until nearing the maximal performance is markedly lower for our method. This can be seen in <ref type="figure">Fig. 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Discussion</head><p>We have observed that the proposed method converges to a reasonably good solution faster than vanilla fine-tuning and eventually attains slightly better performance. The increase in classification accuracy is despite the network's expressive power, which is limited by our construction. We conjecture that constraining each layer to be expressed as a linear combination of the corresponding layer in the original network serves to regularize the space of solutions and is beneficial when the tasks are sufficiently related to each other. One could come up with simple examples where the proposed method would likely fail. This might occur if tasks are not of the same kind. For example, one task requires counting of horizontal lines and the other requires counting of vertical ones, and such examples are all that appear in the training sets, then the proposed method will likely work far worse than vanilla fine-tuning or training from scratch. We experimented with such scenarios in Sec. 3.3. Empirically, we did not see this happen in any of our experiments on "real" datasets. This may simply stem from some inherent similarity in image classification tasks which makes some basic set of features useful for most of them. We leave the investigation of this issue, as well as finding ways between striking a balance between reusing features and learning new ones as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>We have presented a method for transfer learning thats adapts an existing network to new tasks while fully preserving the existing representation. Our method matches or outperforms vanilla fine-tuning, though requiring a fraction of the parameters, which when combined with net compression reaches 3% of the original parameters with no loss of accuracy. The method converges quickly to high accuracy while being on par or outperforming other methods with the same goal. Built into our method is the ability to easily switch the representation between the various learned tasks, enabling a single network to perform seamlessly on various domains. The control parameter α can be cast as a real-valued vector, allowing a smooth transition between representations of different tasks. An example of the effect of such a smooth transition can be seen in <ref type="figure" target="#fig_4">Fig. 6</ref> where α is used to linearly interpolate between the representation of differently learned pairs tasks, allowing one to smoothly control transitions between different behaviors. Allowing each added task to use a convex combination of already existing controllers will potentially utilize controllers more efficiently and decouple the number of controllers from the number of tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Fig. 2: Controller modules: filters of a convolutional layer of a base network are modified by re-combining their weights through a controller module, where a switching α variable can choose between the original filters F l and newly created ones F a l . We show a controller module for a single added task, but any number of controllers can be added, with α then being a vector instead of scalar.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Transferability of various datasets to each other (ft-last) fine tuning only the last layer (full) fine-tuning all layers (ft-full-bn-off ) fine tuning all layers while disallowing batch-normalization layers' weights to be updated. Overall, networks tend to be more easily transferable to problems from related domains (e.g., natural / drawing). Zoom in to see numbers. It is recommended to view this figure in color on-line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Demonstration of different possible limits of possible method on a toy example. Transferring from a network where the first layer contains features from an orthogonal sub-space to that required for a task can result in chance accuracy. Please see text for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>•</head><label></label><figDesc>original: horizontal dataset, first filter fixed to a horizontal green bar, learn other layers • original+learn : horizontal dataset, learn all layers • transposed : transposed dataset, first filter fixed to a horizontal bar, learn other layers • channel switch: new channel dataset, first filter fixed to horizontal green bar, learn other layers • channel switch + noise: new channel dataset, first filter fixed to horizontal green bar+ Gaussian noise, learn other layers • channel switch + clean start: new channel dataset, learn all layers • channel switch + learn: new channel dataset, first filter initialized to a horizontal green bar, learn all layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Shifting Representations. Using a single base network N sketch , we check the method's sensitivity to varying values of α by varying it in the range [0, 1]. Increasing α shifts the network away from the base representation and towards learned tasks -gradually lowering performance on the base task (diamonds) and improving on the learned ones (full circles). The relatively slow decrease of the performance on sketch (blue diamonds) and increase in that of Plankton (blue circles) indicates a similarity between the learned representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 :</head><label>9</label><figDesc>Mean classification accuracy (normalized, averaged over datasets) w.r.t no. parameters. Our method achieve better performance over baselines for a large range of parameter budgets. For very few parameters diagonal (ours) outperforms features extraction. To obtain maximal accuracy our full method requires far fewer parameters (see linear vs finetune).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>and output units and a kernel size k = 5 this equals 256+1 256·5 2 +1 ≈ 0.04. In the final architecture we use the total</figDesc><table><row><cell>Net</cell><cell>C-10</cell><cell>GTSR</cell><cell>SVHN</cell><cell>Caltech</cell><cell>Dped</cell><cell>Oglt</cell><cell>Plnk</cell><cell>Sketch</cell><cell>Perf.</cell><cell>#par</cell></row><row><cell>VGG-B(S)</cell><cell>92.5</cell><cell>98.2</cell><cell>96.2</cell><cell>88.2</cell><cell>92.9</cell><cell>86.9</cell><cell>74.5</cell><cell>69.2</cell><cell>87.32</cell><cell>8</cell></row><row><cell>VGG-B(P)</cell><cell>93.2</cell><cell>99.0</cell><cell>95.8</cell><cell>92.6</cell><cell>98.7</cell><cell>83.8</cell><cell>73.2</cell><cell>65.4</cell><cell>87.71</cell><cell>8</cell></row><row><cell>DAN caltech−256</cell><cell>77.9</cell><cell>93.6</cell><cell>91.8</cell><cell>88.2</cell><cell>93.8</cell><cell>81.0</cell><cell>63.6</cell><cell>49.4</cell><cell>79.91</cell><cell>2.54</cell></row><row><cell>DAN sketch</cell><cell>77.9</cell><cell>93.3</cell><cell>93.2</cell><cell>86.9</cell><cell>94.0</cell><cell>85.4</cell><cell>69.6</cell><cell>69.2</cell><cell>83.7</cell><cell>2.54</cell></row><row><cell>DANnoise</cell><cell>68.1</cell><cell>90.9</cell><cell>90.4</cell><cell>84.6</cell><cell>91.3</cell><cell>80.6</cell><cell>61.7</cell><cell>42.7</cell><cell>76.29</cell><cell>1.76</cell></row><row><cell>DANimagenet</cell><cell>91.6</cell><cell>97.6</cell><cell>94.6</cell><cell>92.2</cell><cell>98.7</cell><cell>81.3</cell><cell>72.5</cell><cell>63.2</cell><cell>86.46</cell><cell>2.76</cell></row><row><cell>DAN imagenet+sketch</cell><cell>91.6</cell><cell>97.6</cell><cell>94.6</cell><cell>92.2</cell><cell>98.7</cell><cell>85.4</cell><cell>72.5</cell><cell>69.2</cell><cell>87.76</cell><cell>3.32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 1 :</head><label>1</label><figDesc>Perf: top-1 accuracy (%, higher is better) on various datasets and parameter cost (#par., lower is better) for a few baselines and several variants of our method. Rows 1,2: independent baseline performance. VGG-B: VGG [25] architecture B. (S) -trained from scratch. (P) -pre-trained on ImageNet. Rows 3-7: (ours) controller network performance; DAN sketch as a base network outperforms DAN caltech−256 on most datasets. A controller network based on random weights (DAN noise ) works quite well given that its number of learned parameters is a fifth of the other methods. DAN imagenet : controller networks initialized from VGG-B model pretrained on ImageNet. DAN imagenet+sketch : selective control network based on both VGG-B(P) &amp; Sketch. We color code the first, second and third highest values in each column (lowest for #par). #par: amortized number of weights learned to achieve said performance for all tasks divided by number of tasks addressed (lower is better).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>. pytorch.org</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>8</cell></row><row><cell>logloss</cell><cell>5 6 7 8</cell><cell cols="5">Effect of Controller Initialization linear_approx random diagonal</cell><cell>Mean Accuracy</cell><cell>0.78 0.80 0.82 0.84</cell><cell>SVHN</cell><cell>GTSR Transferability vs. Mean Accuracy CIFAR10</cell><cell>caltech256 omniglot plankton sketch</cell></row><row><cell></cell><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.76</cell></row><row><cell></cell><cell>0</cell><cell>50</cell><cell cols="2">100 minibatch number 150</cell><cell>200</cell><cell></cell><cell></cell><cell></cell><cell>0.70 daimler</cell><cell>0.75</cell><cell>0.80 Transferability 0.85</cell><cell>0.90</cell><cell>0.95</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b)</cell></row><row><cell cols="10">Fig. 5: (a) Controller initialization schemes. Mean loss averaged over 5 experiments for different ways of initializing</cell></row><row><cell cols="10">controller modules, overlaid with minimal and maximal values. Random initialization performs the worst (random).</cell></row><row><cell cols="10">Approximating the behavior of a fine-tuned network is slightly better (linear approx) and initializing by mimicking the</cell></row><row><cell cols="10">base network (diagonal) performs the best (b) Predictability of a control network's overall accuracy average over all</cell></row><row><cell cols="5">datasets, given its transferability measure.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Dataset</cell><cell>DPed</cell><cell>SVHN</cell><cell cols="2">GTSR</cell><cell>C-10</cell><cell>Oglt</cell><cell>Plnk Sketch</cell><cell>Caltech</cell></row><row><cell></cell><cell></cell><cell cols="2">ft-full</cell><cell>60.1</cell><cell>60</cell><cell>65.8</cell><cell></cell><cell>70.9</cell><cell>80.4</cell><cell>81.6</cell><cell>84.2</cell><cell>82.3</cell></row><row><cell></cell><cell></cell><cell cols="2">ft-full-bn-off</cell><cell>61.8</cell><cell>64.6</cell><cell>66.2</cell><cell></cell><cell>72.5</cell><cell>78</cell><cell>80.2</cell><cell>82.5</cell><cell>81</cell></row><row><cell></cell><cell></cell><cell cols="2">ft-last</cell><cell>24.4</cell><cell>33.9</cell><cell>42.5</cell><cell></cell><cell>44</cell><cell>44.1</cell><cell>47</cell><cell>50.3</cell><cell>55.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 2 :</head><label>2</label><figDesc></figDesc><table /><note>Mean transfer learning performance. We show the mean top-1 accuracy (%) attained by fine-tuning a network from each domain to all domains. Out of the datasets above, starting with Caltech-256 proves most generic as a feature extractor (ft-last). However, fine tuning is best when initially training on the Sketch dataset (ft-full).others. We compare this to the baseline of training on each dataset from scratch (VGG-B(S)) or pretrained (VGG-B(P)) networks. Tab. 1 summarizes performance on all datasets for two representative base nets: DAN caltech−256 (79.9%) and DAN sketch (83.7%). Mean performance for other base nets are shown in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 3 :</head><label>3</label><figDesc>Results on Visual Decathlon Challenge. Scratch: training on each task independently. Feature: using a pre-trained network as a feature extractor. Finetune : vanilla fine tuning. Performs well but requires many parameters. Learningwithout-forgetting (LWF,<ref type="bibr" target="#b15">[16]</ref> slightly outperforms it but with a large parameter cost. Residual adapt.<ref type="bibr" target="#b5">[6]</ref> significantly reduce the number of parameters. Results improve when training jointly on all task (Res.Adapt(Joint)). The proposed method (DAN) outperforms residual adapters despite adding each task independently of the others. S is the decathlon challenge score.</figDesc><table><row><cell>Mean Normalized Accuracy %</cell><cell>0.2 0.4 0.6 0.8 1.0</cell><cell></cell><cell cols="3">Convergence Speed</cell><cell>mode linear finetune diagonal feature extractor</cell><cell>Mean Normalized Accuracy</cell><cell>0.9825 0.9850 0.9875 0.9900 0.9925 0.9950 0.9975 1.0000</cell><cell>mode feature extractor diagonal finetune linear</cell><cell cols="3">Convergence Speed</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.9800</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>10</cell><cell>20</cell><cell>Epoch</cell><cell>30</cell><cell>40</cell><cell></cell><cell>25</cell><cell>30</cell><cell>35</cell><cell>Epoch</cell><cell>40</cell><cell>45</cell><cell>50</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b)</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This research was funded by the Canada Research Chairs program and by the Air Force Office for Scientific Research (USA) for which the authors are grateful.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A practical outlet for this theory embodies elements of the theory into the vision systems of mobile robots.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deep speech: Scaling up end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.5567</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A survey on deep learning in medical image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Litjens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kooi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Bejnordi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A A</forename><surname>Setio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ciompi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghafoorian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Van Der Laak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">I</forename><surname>Sánchez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05747</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning multiple visual domains with residual adapters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-A</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08045</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Face alignment by coarseto-fine shape searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4998" to="5006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06870</idno>
		<title level="m">Mask R-CNN</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2650" to="2658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Integrated perception with recurrent multi-task neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>Advances in Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Universal representations:The missing link between faces, text, planktons, and cat breeds</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cross-stitch Networks for Multi-task Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<idno>abs/1604.03539</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Catastrophic forgetting in connectionist networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>French</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="128" to="135" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Icml</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="647" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">CNN features off-the-shelf: an astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="806" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning without Forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<idno>abs/1606.09282</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04671</idno>
	</analytic>
	<monogr>
		<title level="j">Progressive neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="page">201611835</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Incremental Learning in Deep Convolutional Neural Networks Using Partial Network Sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Sarwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ankit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Roy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.00149</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1135" to="1143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Exploiting linear structure within convolutional networks for efficient evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1269" to="1277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Comparing biases for minimal network construction with back-propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Y</forename><surname>Pratt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st International Conference on Neural Information Processing Systems</title>
		<meeting>the 1st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1988" />
			<biblScope unit="page" from="177" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Caltech-256 object category dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An experimental study on pedestrian classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Munder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Gavrila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1863" to="1868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stallkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schlipsing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="323" to="332" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Planktonset 1.0: Plankton imagery data collected from fg walton smith in straits of florida from 2014-06-03 to 2014-06-06 and used in the 2015 national data science bowl (ncei accession 0127422)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Cowen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sponaugle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<imprint/>
		<respStmt>
			<orgName>Oregon State University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">How do humans sketch objects?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alexa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="44" to="45" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop on deep learning and unsupervised feature learning</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2011</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The Marginal Value of Adaptive Gradient Methods in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<idno>abs/1705.08292</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Fine-grained visual classification of aircraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.5151</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Describing Textures in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno>abs/1311.3618</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Automated Flower Classification over a Large Number of Classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICVGIP</title>
		<imprint>
			<biblScope unit="page" from="722" to="729" />
			<date type="published" when="2008" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Humanlevel concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Wide Residual Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno>abs/1605.07146</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">YellowFin and the Art of Momentum Tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03471</idno>
	</analytic>
	<monogr>
		<title level="m">US Patent US 9 196 164B1</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="11" to="24" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Pedestrian behavior analysis using 110-car naturalistic driving data in usa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Luzetski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sherony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Takahashi</surname></persName>
		</author>
		<idno>2017-06- 3</idno>
		<ptr target="https://www-nrd.nhtsa.dot.gov/pdf/Esv/esv23/23ESV-000291.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Pedestrians at the kerb-recognising the action intentions of humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Färber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transportation research part F: traffic psychology and behaviour</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="300" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Behavioural issues in pedestrian speed choice and street crossing behaviour: a review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Ishaque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Noland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transport Reviews</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="85" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Study of pedestrians&apos; gap acceptance behavior when they jaywalk outside crossing facilities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Transportation Systems (ITSC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1295" to="1300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Modeling of motorist-pedestrian interaction at uncontrolled mid-block crosswalks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ukkusuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>Benekohal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Waller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Urbana</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page">61801</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A critical assessment of pedestrian behaviour models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Papadimitriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Golias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transportation research part F: traffic psychology and behaviour</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="242" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">To cross or not to cross: The effect of locomotion on street-crossing behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Oudejans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Michaels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Dort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Frissen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecological psychology</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="259" to="267" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Requirements for the design of advanced driver assistance systems-the differences between Swedish and Chinese drivers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lindgren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Design</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Trajectory analysis and prediction for improved pedestrian safety: Integrated framework and evaluations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="330" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Analysis of pedestrian dynamics from a vehicle perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Kooij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Gavrila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1445" to="1450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">A data-driven approach for pedestrian intention estimation,&quot; and Attentive Vision, York University. Her research interests include visual attention, computer vision, cognitive systems and autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Völz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Behrendt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mielenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gilitschenski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nieto</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
