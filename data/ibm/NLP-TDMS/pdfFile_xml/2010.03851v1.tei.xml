<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Two are Better than One: Joint Entity and Relation Extraction with Table-Sequence Encoders</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
							<email>zjuwangjue@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
							<email>luwei@sutd.edu.sg</email>
							<affiliation key="aff1">
								<orgName type="laboratory">StatNLP Research Group</orgName>
								<orgName type="institution">Singapore University of Technology and Design</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Two are Better than One: Joint Entity and Relation Extraction with Table-Sequence Encoders</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Named entity recognition and relation extraction are two important fundamental problems. Joint learning algorithms have been proposed to solve both tasks simultaneously, and many of them cast the joint task as a table-filling problem. However, they typically focused on learning a single encoder (usually learning representation in the form of a table) to capture information required for both tasks within the same space. We argue that it can be beneficial to design two distinct encoders to capture such two different types of information in the learning process. In this work, we propose the novel table-sequence encoders where two different encoders -a table encoder and a sequence encoder are designed to help each other in the representation learning process. Our experiments confirm the advantages of having two encoders over one encoder. On several standard datasets, our model shows significant improvements over existing approaches. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Named Entity Recognition <ref type="bibr">(NER, Florian et al. 2006</ref><ref type="bibr" target="#b14">, 2010</ref> and Relation Extraction <ref type="bibr">(RE, Zhao and Grishman 2005;</ref><ref type="bibr" target="#b21">Jiang and Zhai 2007;</ref><ref type="bibr" target="#b44">Sun et al. 2011;</ref><ref type="bibr" target="#b38">Plank and Moschitti 2013)</ref> are two fundamental tasks in Information Extraction (IE). Both tasks aim to extract structured information from unstructured texts. One typical approach is to first identify entity mentions, and next perform classification between every two mentions to extract relations, forming a pipeline <ref type="bibr">(Zelenko et al., 2002;</ref><ref type="bibr" target="#b5">Chan and Roth, 2011)</ref>. An alternative and more recent approach is to perform these two tasks jointly <ref type="bibr" target="#b27">(Li and Ji, 2014;</ref><ref type="bibr" target="#b32">Miwa and Sasaki, 2014;</ref><ref type="bibr" target="#b31">Miwa and Bansal, 2016)</ref>, which mitigates the error propagation issue associated with the pipeline ap-1 Our code is available at https://github.com/ LorrinWWW/two-are-better-than-one. proach and leverages the interaction between tasks, resulting in improved performance. Among several joint approaches, one popular idea is to cast NER and RE as a table filling problem <ref type="bibr" target="#b32">(Miwa and Sasaki, 2014;</ref><ref type="bibr" target="#b18">Gupta et al., 2016;</ref><ref type="bibr" target="#b25">Zhang et al., 2017)</ref>. Typically, a two-dimensional (2D) table is formed where each entry captures the interaction between two individual words within a sentence. NER is then regarded as a sequence labeling problem where tags are assigned along the diagonal entries of the table. RE is regarded as the problem of labeling other entries within the table. Such an approach allows NER and RE to be performed using a single model, enabling the potentially useful interaction between these two tasks. One example 2 is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>Unfortunately, there are limitations with the existing joint methods. First, these methods typically suffer from feature confusion as they use a single representation for the two tasks -NER and RE. As a result, features extracted for one task may <ref type="bibr">2</ref> The exact settings for table filling may be different for different papers. Here we fill the entire table (rather than the lower half of the table), and assign relation tags to cells involving two complete entity spans (rather than part of such spans). We also preserve the direction of the relations. coincide or conflict with those for the other, thus confusing the learning model. Second, these methods underutilize the table structure as they usually convert it to a sequence and then use a sequence labeling approach to fill the table. However, crucial structural information (e.g., the 4 entries at the bottom-left corner of <ref type="figure" target="#fig_0">Figure 1</ref> share the same label) in the 2D table might be lost during such conversions.</p><p>In this paper, we present a novel approach to address the above limitations. Instead of predicting entities and relations with a single representation, we focus on learning two types of representations, namely sequence representations and table representations, for NER and RE respectively. On one hand, the two separate representations can be used to capture task-specific information. On the other hand, we design a mechanism to allow them to interact with each other, in order to take advantage of the inherent association underlying the NER and RE tasks. In addition, we employ neural network architectures that can better capture the structural information within the 2D table representation. As we will see, such structural information (in particular the context of neighboring entries in the table) is essential in achieving better performance.</p><p>The recent prevalence of BERT <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref> has led to great performance gains on various NLP tasks. However, we believe that the previous use of BERT, i.e., employing the contextualized word embeddings, does not fully exploit its potential. One important observation here is that the pairwise self-attention weights maintained by BERT carry knowledge of word-word interactions. Our model can effectively use such knowledge, which helps to better learn table representations. To the best of our knowledge, this is the first work to use the attention weights of BERT for learning table representations.</p><p>We summarize our contributions as follows:</p><p>• We propose to learn two separate encoders -a table encoder and a sequence encoder. They interact with each other, and can capture taskspecific information for the NER and RE tasks; • We propose to use multidimensional recurrent neural networks to better exploit the structural information of the table representation; • We effectively leverage the word-word interaction information carried in the attention weights from BERT, which further improves the performance.</p><p>Our proposed method achieves the state-of-theart performance on four datasets, namely ACE04, ACE05, CoNLL04, and ADE. We also conduct further experiments to confirm the effectiveness of our proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>NER and RE can be tackled by using separate models. By assuming gold entity mentions are given as inputs, RE can be regarded as a classification task. Such models include kernel methods <ref type="bibr">(Zelenko et al., 2002)</ref>, <ref type="bibr">RNNs (Zhang and Wang, 2015)</ref>, recursive neural networks <ref type="bibr" target="#b42">(Socher et al., 2012</ref><ref type="bibr">), CNNs (Zeng et al., 2014</ref>, and Transformer models <ref type="bibr" target="#b49">(Verga et al., 2018;</ref><ref type="bibr" target="#b53">Wang et al., 2019)</ref>. Another branch is to detect cross-sentence level relations <ref type="bibr" target="#b35">(Peng et al., 2017;</ref><ref type="bibr" target="#b17">Gupta et al., 2019)</ref>, and even document-level relations <ref type="bibr">(Yao et al., 2019;</ref><ref type="bibr" target="#b33">Nan et al., 2020)</ref>. However, entities are usually not directly available in practice, so these approaches may require an additional entity recognizer to form a pipeline.</p><p>Joint learning has been shown effective since it can alleviate the error propagation issue and benefit from exploiting the interrelation between NER and RE. Many studies address the joint problem through a cascade approach, i.e., performing NER first followed by RE. <ref type="bibr" target="#b31">Miwa and Bansal (2016)</ref> use bi-LSTM <ref type="bibr" target="#b16">(Graves et al., 2013)</ref> and tree-LSTM <ref type="bibr" target="#b46">(Tai et al., 2015)</ref> for the joint task. <ref type="bibr">Bekoulis et al. (2018a,b)</ref> formulate it as a head selection problem. Nguyen and Verspoor (2019) apply biaffine attention <ref type="bibr" target="#b11">(Dozat and Manning, 2017)</ref> for RE. , <ref type="bibr" target="#b9">Dixit and Al (2019)</ref>, and  use span representations to predict relations. <ref type="bibr" target="#b32">Miwa and Sasaki (2014)</ref> tackle joint NER and RE as from a table filling perspective, where the entry at row i and column j of the table corresponds to the pair of i-th and j-th word of the input sentence. The diagonal of the table is filled with the entity tags and the rest with the relation tags indicating possible relations between word pairs. Similarly, <ref type="bibr" target="#b18">Gupta et al. (2016)</ref> employ a bi-RNN structure to label each word pair. <ref type="bibr" target="#b25">Zhang et al. (2017)</ref> propose a global optimization method to fill the table. <ref type="bibr" target="#b47">Tran and Kavuluru (2019)</ref> investigate CNNs on this task.</p><p>Recent work <ref type="bibr" target="#b9">Dixit and Al, 2019;</ref><ref type="bibr" target="#b28">Li et al., 2019;</ref><ref type="bibr" target="#b12">Eberts and Ulges, 2019)</ref> usually leverages pre-trained language models such as ELMo <ref type="bibr" target="#b37">(Peters et al., 2018)</ref>, BERT <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref>, RoBERTa (Liu et al.,   <ref type="bibr" target="#b24">(Lan et al., 2019)</ref>. However, none of them use pre-trained attention weights, which convey rich relational information between words. We believe it can be useful for learning better table representations for RE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2019), and ALBERT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Formulation</head><p>In this section, we formally formulate the NER and RE tasks. We regard NER as a sequence labeling problem, where the gold entity tags y NER are in the standard BIO (Begin, Inside, Outside) scheme <ref type="bibr" target="#b41">(Sang and Veenstra, 1999;</ref><ref type="bibr" target="#b39">Ratinov and Roth, 2009</ref>). For the RE task, we mainly follow the work of <ref type="bibr" target="#b32">Miwa and Sasaki (2014)</ref> to formulate it as a table filling problem. Formally, given an input sentence x = [x i ] 1≤i≤N , we maintain a tag table y RE = [y RE i,j ] 1≤i,j≤N . Suppose there is a relation with type r pointing from mention x i b , .., x i e to mention x j b , .., x j e , we have y RE i,j = − → r and</p><formula xml:id="formula_0">y RE j,i = ← − r for all i ∈ [i b , i e ] ∧ j ∈ [j b , j e ].</formula><p>We use ⊥ for word pairs with no relation. An example was given earlier in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Model</head><p>We describe the model in this section. The model consists of two types of interconnected encoders, a table encoder for table representation and a sequence encoder for sequence representation, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Collectively, we call them tablesequence encoders. <ref type="figure" target="#fig_2">Figure 3</ref> presents the details of each layer of the two encoders, and how they interact with each other. In each layer, the table encoder uses the sequence representation to construct the  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Text Embedder</head><p>For a sentence containing N words x = [x i ] 1≤i≤N , we define the word embeddings x w ∈ R N ×d 1 , as well as character embeddings x c ∈ R N ×d 2 computed by an LSTM <ref type="bibr" target="#b23">(Lample et al., 2016)</ref>. We also consider the contextualized word embeddings x ∈ R N ×d 3 , which can be produced from language models such as BERT.</p><p>We concatenate those embeddings for each word and use a linear projection to form the initial sequence representation S 0 ∈ R N ×H :</p><formula xml:id="formula_1">S 0 = Linear([x c ; x w ; x ])<label>(1)</label></formula><p>where each word is represented as an H dimensional vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Table Encoder</head><p>The table encoder, shown in the left part of <ref type="figure" target="#fig_2">Figure  3</ref>, is a neural network used to learn a table representation, an N × N table of vectors, where the vector at row i and column j corresponds to the i-th and j-th word of the input sentence. We first construct a non-contextualized table by concatenating every two vectors of the sequence representation followed by a fully-connected layer to halve the hidden size. Formally, for the l-th layer, we have X l ∈ R N ×N ×H , where:</p><formula xml:id="formula_2">X l,i,j = ReLU(Linear([S l−1,i ; S l−1,j ])) (2)</formula><p>Next, we use the Multi-Dimensional Recurrent Neural Networks (MD-RNN, <ref type="bibr" target="#b15">Graves et al. 2007)</ref> Figure 4: How the hidden states are computed in MD-RNN with 4 directions. We use D + or D − to indicate the direction that the hidden states flow between cells at the D dimension (where D can be layer, row or col). For brevity, we omit the input and the layer dimension for cases (b), (c) and (d), as they are the same as (a).</p><p>with Gated Recurrent Unit (GRU, <ref type="bibr" target="#b6">Cho et al. 2014)</ref> to contextualize X l . We iteratively compute the hidden states of each cell to form the contextualized table representation T l , where:</p><formula xml:id="formula_3">T l,i,j = GRU(X l,i,j , T l−1,i,j , T l,i−1,j , T l,i,j−1 ) (3)</formula><p>We provide the multi-dimensional adaptations of GRU in Appendix A to avoid excessive formulas here.</p><p>Generally, it exploits the context along layer, row, and column dimensions. That is, it does not consider only the cells at neighbouring rows and columns, but also those of the previous layer.</p><p>The time complexity of the naive implementation (i.e., two for-loops) for each layer is O(N ×N ) for a sentence with length N . However, antidiagonal entries 3 can be calculated at the same time as they do not depend on each other. Therefore, we can optimize it through parallelization and reduce the effective time complexity to O(N ).</p><p>The above illustration describes a unidirectional RNN, corresponding to Figure 4(a). Intuitively, we would prefer the network to have access to the surrounding context in all directions. However, this could not be done by one single RNN. For the case of 1D sequence modeling, this problem is resolved by introducing bidirectional RNNs. <ref type="bibr" target="#b15">Graves et al. (2007)</ref> discussed quaddirectional RNNs to access the context from four directions for modeling 2D data. Therefore, similar to 2D-RNN, we also need to consider RNNs in four directions 4 . We visualize them in <ref type="figure">Figure 4</ref>.</p><p>Empirically, we found the setting only considering cases (a) and (c) in <ref type="figure">Figure 4</ref> achieves no worse performance than considering four cases altogether. Therefore, to reduce the amount of computation, we use such a setting as default. The final table representation is then the concatenation of the hidden states of the two RNNs:</p><formula xml:id="formula_4">T (a) l,i,j = GRU (a) (X l,i,j , T (a) l−1,i,j , T (a) l,i−1,j , T (a) l,i,j−1 ) (4) T (c) l,i,j = GRU (c) (X l,i,j , T (c) l−1,i,j , T (c) l,i+1,j , T (c) l,i,j+1 ) (5) T l,i,j = [T (a) l,i,j ; T (c) l,i,j ]<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Sequence Encoder</head><p>The sequence encoder is used to learn the sequence representation -a sequence of vectors, where the i-th vector corresponds to the i-th word of the input sentence. The architecture is similar to Transformer <ref type="bibr" target="#b48">(Vaswani et al., 2017)</ref>, shown in the right portion of <ref type="figure" target="#fig_2">Figure 3</ref>. However, we replace the scaled dotproduct attention with our proposed table-guided attention. Here, we mainly illustrate why and how the table representation can be used to compute attention weights. First of all, given Q (queries), K (keys) and V (values), a generalized form of attention is defined in <ref type="figure" target="#fig_3">Figure 5</ref>. For each query, the output is a weighted sum of the values, where the weight assigned to each value is determined by the relevance (given by score function f ) of the query with all the keys.</p><p>For each query Q i and key K j , <ref type="bibr" target="#b2">Bahdanau et al. (2015)</ref> define f in the form of:</p><formula xml:id="formula_5">f (Q i , K j ) = U · g(Q i , K j )<label>(7)</label></formula><p>where U is a learnable vector and g is the function to map each query-key pair to a vector. Specifically,</p><formula xml:id="formula_6">they define g(Q i , K j ) = tanh(Q i W 0 + K j W 1 ), where W 0 , W 1 are learnable parameters.</formula><p>Our attention mechanism is essentially a selfattention mechanism, where the queries, keys and values are exactly the same. In our case, they are essentially sequence representation S l−1 of the previous layer (i.e., Q = K = V = S l−1 ). The attention weights (i.e., the output from the function f in <ref type="figure" target="#fig_3">Figure 5</ref>) are essentially constructed from both queries and keys (which are the same in our case). On the other hand, we also notice the table representation T l is also constructed from S l−1 . So we can consider T l to be a function of queries and keys, such that T l,i,j = g(S l−1,i , S l−1,j ) = g(Q i , K j ). Then we put back this g function to Equation 7, and get the proposed table-guided attention, whose score function is:</p><formula xml:id="formula_7">f (Q i , K j ) = U · T l,i,j<label>(8)</label></formula><p>We show the advantages of using this tableguided attention: (1) we do not have to calculate g function since T l is already obtained from the table encoder; (2) T l is contextualized along the row, column, and layer dimensions, which corresponds to queries, keys, and queries and keys in the previous layer, respectively. Such contextual information allows the network to better capture more difficult word-word dependencies; (3) it allows the table encoder to participate in the sequence representation learning process, thereby forming the bidirectional interaction between the two encoders.</p><p>The table-guided attention can be extended to have multiple heads <ref type="bibr" target="#b48">(Vaswani et al., 2017)</ref>, where each head is an attention with independent parameters. We concatenate their outputs and use a fullyconnected layer to get the final attention outputs.</p><p>The remaining parts are similar to Transformer. For layer l, we use position-wise feedforward neural networks (FFNN) after self-attention, and wrap attention and FFNN with a residual connection <ref type="bibr" target="#b20">(He et al., 2016)</ref> and layer normalization <ref type="bibr">(Ba et al. 2016)</ref>, to get the output sequence representation:</p><formula xml:id="formula_8">S l = LayerNorm(S l−1 + SelfAttn(S l−1 )) (9) S l = LayerNorm(S l + FFNN(S l ))<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Exploit Pre-trained Attention Weights</head><p>In this section, we describe the dashed lines in <ref type="figure" target="#fig_1">Figures 2 and 3</ref>, which we ignored in the previous discussions. Essentially, they exploit information in the form of attention weights from a pre-trained language model such as BERT.</p><p>We stack the attention weights of all heads and all layers to form T ∈ R N ×N ×(L ×A ) , where L is the number of stacked Transformer layers, and A is the number of heads in each layer. We leverage T to form the inputs of MD-RNNs in the table encoder. Equation 2 is now replaced with:</p><formula xml:id="formula_9">X l,i,j = ReLU(Linear([S l−1,i ; S l−1,j ; T i,j ])) (11)</formula><p>We keep the rest unchanged. We believe this simple yet novel use of the attention weights allows us to effectively incorporate the useful word-word interaction information captured by pre-trained models such as BERT into our table-sequence encoders for improved performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Training and Evaluation</head><p>We use S L and T L to predict the probability distribution of the entity and relation tags:</p><formula xml:id="formula_10">P θ (Y NER ) = softmax(Linear(S L )) (12) P θ (Y RE ) = softmax(Linear(T L ))<label>(13)</label></formula><p>where Y NER and Y RE are random variables of the predicted tags, and P θ is the estimated probability function with θ being our model parameters.</p><p>For training, both NER and RE adopt the prevalent cross-entropy loss. Given the input text x and its gold tag sequence y NER and tag table y RE , we then calculate the following two losses:</p><formula xml:id="formula_11">L NER = i∈[1,N ] − log P θ (Y NER i = y NER i ) (14) L RE = i,j∈[1,N ];i =j − log P θ (Y RE i,j = y RE i,j )<label>(15)</label></formula><p>The goal is to minimize both losses L NER + L RE . During evaluation, the prediction of relations relies on the prediction of entities, so we first predict the entities, and then look up the relation probability table P θ (Y RE ) to see if there exists a valid relation between predicted entities.</p><p>Specifically, we predict the entity tag of each word by choosing the class with the highest probability: argmax</p><formula xml:id="formula_12">e P θ (Y NER i = e)<label>(16)</label></formula><p>The whole tag sequence can be transformed into entities with their boundaries and types.</p><p>Relations on entities are mapped to relation classes with highest probabilities on words of the entities. We also consider the two directed tags for each relation. Therefore, for two entity spans (i b , i e ) and (j b , j e ), their relation is given by:</p><formula xml:id="formula_13">argmax − → r i∈[i b ,i e ],j∈[j b ,j e ] P θ (Y RE i,j = − → r ) + P θ (Y RE j,i = ← − r ) (17)</formula><p>where the no-relation type ⊥ has no direction, so if − → r = ⊥, we have ← − r = ⊥ as well.</p><p>6 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Data</head><p>We evaluate our model on four datasets, namely ACE04 <ref type="bibr" target="#b10">(Doddington et al., 2004)</ref>, ACE05 <ref type="bibr" target="#b52">(Walker et al., 2006)</ref>, CoNLL04 <ref type="bibr" target="#b40">(Roth and tau Yih, 2004)</ref> and ADE <ref type="bibr" target="#b19">(Gurulingappa et al., 2012)</ref>. More details could be found in Appendix B.</p><p>Following the established line of work, we use the F1 measure to evaluate the performance of NER and RE. For NER, an entity prediction is correct if and only if its type and boundaries both match with those of a gold entity. 5 For RE, a relation prediction is considered correct if its relation type and the boundaries of the two entities match with those in the gold data. We also report the strict relation F1 (denoted RE+), where a relation prediction is considered correct if its relation type as well as the boundaries and types of the two entities all match with those in the gold data. Relations are asymmetric, so the order of the two entities in a relation matters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Model Setup</head><p>We tune hyperparameters based on results on the development set of ACE05 and use the same setting for other datasets. GloVe vectors <ref type="bibr" target="#b36">(Pennington et al., 2014)</ref> are used to initialize word embeddings. We also use the BERT variant -ALBERT as the default pre-trained language model. Both pre-trained word embeddings and language model are fixed without fine-tuning. In addition, we stack three encoding layers (L = 3) with independent parameters including the GRU cell in each layer. For the table encoder, we use two separate MD-RNNs with the directions of "layer + row + col + " and "layer + row − col − " respectively. For the sequence encoder, we use eight attention heads to attend to different representation subspaces. We report the averaged F1 scores of 5 runs for our models. For each run, we keep the model that achieves  the highest averaged entity F1 and relation F1 on the development set, and evaluate and report its score on the test set. Other hyperparameters could be found in Appendix C.  for jointly extracting entities and their relations. Since our reported numbers are the average of 5 runs, we can consider our model to be achieving new state-of-the-art results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Comparison with Other Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Comparison of Pre-trained Models</head><p>In this section, we evaluate our method with different pre-trained language models, including ELMo, BERT, RoBERTa and ALBERT, with and without attention weights, to see their individual contribution to the final performance. <ref type="table" target="#tab_5">Table 2</ref> shows that, even using the relatively earlier contextualized embeddings without attention weights (ELMo +x ), our system is still comparable to the state-of-the-art approach , which was based on BERT and achieved F1 scores of 88.6 and 63.4 for NER and RE respectively. It is important to note that the model of  was trained on the additional coreference annotations from OntoNotes <ref type="bibr" target="#b54">(Weischedel et al., 2011)</ref> before fine-tuning on ACE05. Nevertheless, our system still achieves comparable results, showing the effectiveness of the table-sequence encoding architecture.</p><p>The overall results reported in <ref type="table" target="#tab_5">Table 2</ref> confirm the importance of leveraging the attention weights, which bring improvements for both NER and RE tasks. This allows the system using vanilla BERT to obtain results no worse than RoBERTa and AL-BERT in relation extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Ablation Study</head><p>We design several additional experiments to understand the effectiveness of components in our system. The experiments are conducted on ACE05.</p><p>We also compare different table filling settings, which are included in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.1">Bidirectional Interaction</head><p>We first focus on the understanding of the necessity of modeling the bidirectional interaction between  the two encoders. Results are presented in <ref type="table" target="#tab_7">Table  3</ref>. "RE (gold)" is presented so as to compare with settings that do not predict entities, where the gold entity spans are used in the evaluation. We first try optimizing the NER and RE objectives separately, corresponding to "w/o Relation Loss" and "w/o Entity Loss". Compared with learning with a joint objective, the results of these two settings are slightly worse, which indicates that learning better representations for one task not only is helpful for the corresponding task, but also can be beneficial for the other task.</p><p>Next, we investigate the individual sequence and table encoder, corresponding to "w/o Table Encoder" and "w/o Sequence Encoder". We also try jointly training the two encoders but cut off the interaction between them, which is "w/o Bi-Interaction". Since no interaction is allowed in the above three settings, the table-guided attention is changed to conventional multi-head scaled dotproduct attention, and the table encoding layer always uses the initial sequence representation S 0 to enrich the table representation. The results of these settings are all significantly worse than the default one, which indicates the importance of the bidirectional interaction between sequence and table representation in our table-sequence encoders.</p><p>We also experiment the use of the main diagonal entries of the table representation to tag entities, with results reported under "NER on diagonal". This setup attempts to address NER and RE in the same encoding space, in line with the original intention of <ref type="bibr" target="#b32">Miwa and Sasaki (2014)</ref>. By exploiting the interrelation between NER and RE, it achieves better performance compared with models without such information. However, it is worse than our default setting. We ascribe this to the potential incompatibility of the desired encoding space of entities and relations. Finally, although it does not  directly use the sequence representation, removing the sequence encoder will lead to performance drop for NER, which indicates the sequence encoder can help improve the table encoder by better capturing the structured information within the sequence. <ref type="table" target="#tab_9">Table 4</ref> shows the effect of the number of encoding layers, which is also the number of bidirectional interactions involved. We conduct one set of experiments with shared parameters for the encoding layers and another set with independent parameters. In general, the performance increases when we gradually enlarge the number of layers L. Specifically, since the shared model does not introduce more parameters when tuning L, we consider that our model benefits from the mutual interaction inside table-sequence encoders. Typically, under the same value L, the non-shared model employs more parameters than the shared one to enhance its modeling capability, leading to better performance. However, when L &gt; 3, there is no significant improvement by using non-shared model. We believe that increasing the number of layers may bring the risk of over-fitting, which limits the performance of the network. We choose to adopt the non-shared model with L = 3 as our default setting.  information is beneficial. Since the bidirectional model is almost as good as the quaddirectional one, we leave the former as the default setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.2">Encoding Layers</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.3">Settings of MD-RNN</head><p>In addition, we are also curious about the contribution of layer, row, and column dimensions for MD-RNNs. We separately removed the layer, row, and column dimension. As we can see, the results are all lower than the original model without removal of any dimension. "Layer-wise only" removed row and col dimensions, and is worse than others as it does not exploit the sentential context.</p><p>More experiments with more settings are presented in Appendix D. Specifically, all unidirectional RNNs are consistently worse than others, while bidirectional RNNs are usually on-par with quaddirectional RNNs. Besides, we also tried to use CNNs to implement the table encoder. However, since it is usually difficult for CNNs to learn long-range dependencies, we found the performance was worse than the RNN-based models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Attention Visualization</head><p>We visualize the table-guided attention with bertviz <ref type="bibr" target="#b50">(Vig, 2019)</ref>  <ref type="bibr">6</ref> for a better understanding of how the network works. We compare it with pre-trained Transformers (ALBERT) and humandefined ground truth, as presented in <ref type="figure" target="#fig_4">Figure 6</ref>.</p><p>Our discovery is similar to <ref type="bibr" target="#b7">Clark et al. (2019)</ref>. Most attention heads in the table-guided attention and ALBERT show simple patterns. As shown in the left part of <ref type="figure" target="#fig_4">Figure 6</ref>, these patterns include attending to the word itself, the next word, the last word, and the punctuation.</p><p>The right part of <ref type="figure" target="#fig_4">Figure 6</ref> also shows task-related patterns, i.e., entities and relations. For a relation, we connect words from the head entity to the tail entity; For an entity, we connect every two words inside this entity mention. We can find that our pro-    -guided attention has learned more taskrelated knowledge compared to ALBERT. In fact, not only does it capture the entities and their relations that ALBERT failed to capture, but it also has higher confidence. This indicates that our model has a stronger ability to capture complex patterns other than simple ones. <ref type="figure" target="#fig_5">Figure 7</ref> presents an example picked from the development set of ACE05. The prediction layer after training (a linear layer) is used as a probe to display the intermediate state of the model, so we can interpret how the model improves both representations from stacking multiple layers and thus from the bidirectional interaction. Such probing is valid since we use skip connection between two adjacent encoding layers, so the encoding spaces of the outputs of different encoding layers are consistent and therefore compatible with the prediction layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7">Probing Intermediate States</head><p>In <ref type="figure" target="#fig_5">Figure 7</ref>, the model made many wrong predictions in the first layer, which were gradually corrected in the next layers. Therefore, we can see that more layers allow more interaction and thus make the model better at capturing entities or relations, especially difficult ones. More cases are presented in Appendix F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we introduce the novel tablesequence encoders architecture for joint extraction of entities and their relations. It learns two separate encoders rather than one -a sequence encoder and a table encoder where explicit interactions exist between the two encoders. We also introduce a new method to effectively employ useful information captured by the pre-trained language models for such a joint learning task where a table representation is involved. We achieved state-of-the-art F1 scores for both NER and RE tasks across four standard datasets, which confirm the effectiveness of our approach. In the future, we would like to investigate how the table representation may be applied to other tasks. Another direction is to generalize the way in which the table and sequence interact to other types of representations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A MD-RNN</head><p>In this section we present the detailed implementation of MD-RNN with GRU. Formally, at the time-step layer l, row i, and column j, with the input X l,i,j , the cell at layer l, row i and column j calculates the gates as follows:</p><formula xml:id="formula_14">T prev l,i,j = [T l−1,i,j ; T l,i−1,j ; T l,i,j−1 ], ∈ R 3H (18) r l,i,j = σ([X l,i,j ; T prev l,i,j ]W r + b r )), ∈ R H (19) z l,i,j = σ([X l,i,j ; T prev l,i,j ]W z + b z )), ∈ R H<label>(20)</label></formula><p>λ l,i,j,m = [X l,i,j ; T prev l,i,j ]W λ m + b λ m , ∈ R H (21) λ l,i,j,0 ,λ l,i,j,1 , λ l,i,j,2 = softmax(λ l,i,j,0 ,λ l,i,j,1 ,λ l,i,j,2 ) And then calculate the hidden states:</p><formula xml:id="formula_16">T l,i,j = tanh(X l,i,j W x + r l,i,j (T prev l,i,j W p ) + b h ), ∈ R H (23) T prev l,i,j = λ l,i,j,0 T l−1,i,j + λ l,i,j,1 T l,i−1,j + λ l,i,j,2 T l,i,j−1 , ∈ R H (24) T l,i,j = z l,i,j T l,i,j + (1 − z l,i,j ) T prev l,i,j , ∈ R H<label>(25)</label></formula><p>where W and b are trainable parameters and please note that they share parameters in different rows and columns but not necessarily in different layers. Besides, is the element-wise product, and σ is the sigmoid function. As in GRU, r is the reset gate controlling whether to forget previous hidden states, and z is the update gate, selecting whether the hidden states are to be updated with new hidden states. In addition, we employ a lambda gate λ, which is used to weight the predecessor cells before passing them through the update gate.</p><p>There are two slightly different ways to compute the candidate activationT l,i,j , namelỹ</p><formula xml:id="formula_17">T l,i,j = tanh(X l,i,j W x + r l,i,j (T prev l,i,j W p ) + b h l )<label>(26)</label></formula><p>andT</p><formula xml:id="formula_18">l,i,j = tanh(W x l X l,i,j + (r l,i,j T prev l,i,j )W p + b h l )<label>(27)</label></formula><p>And we found in our preliminary experiments that both of them performed as well as each other, and we choose the former, which saves some computation.</p><p>The time complexity of the naive implementation (i.e., two for-loops in each layer) is O(L×N ×  N ) for a sentence with length N and the number of encoding layer L. However, antidiagonal entries can be calculated at the same time because their values do not depend on each other, shown in the same color in <ref type="figure" target="#fig_6">Figure 8</ref>. Therefore, we can optimize it through parallelization and reduce the effective time complexity to O(L × N ). <ref type="table" target="#tab_16">Table 6</ref> shows the dataset statistics after preprocessing. We keep the same pre-processing and evaluation standards used by most previous works. The ACE04 and ACE05 corpora are collected from a variety of domains, such as newswire and online forums. We use the same entity and relation types, data splits, and pre-processing as <ref type="bibr" target="#b27">Li and Ji (2014)</ref> and <ref type="bibr" target="#b31">Miwa and Bansal (2016)</ref>  <ref type="bibr">7</ref> . Specifically, they use head spans for entities but not use the full mention boundary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Data</head><p>The CoNLL04 dataset provides entity and relation labels. We use the same train-test split as <ref type="bibr" target="#b18">Gupta et al. (2016)</ref>  <ref type="bibr">8</ref> , and we use the same 20% train set as development set as <ref type="bibr" target="#b12">Eberts and Ulges (2019)</ref>  <ref type="bibr">9</ref> . Both micro and macro average F1 are used in previous work, so we will specify this while comparing with other systems.</p><p>The ADE dataset is constructed from medical reports that describe the adverse effects arising from drug use. It contains a single relation type "Adverse-Effect" and the two entity types "Adverse-Effect" and "Drug". Similar to previous work, we filter out instances containing overlapping entities, only accounting for 2.8% of total.</p><p>Following prior work, we perform 5-fold crossvalidation for ACE04 and 10-fold for ADE. Besides, we use 15% of the training set as the development set. We report the average score of 5 runs <ref type="bibr">7</ref> We use the prepocess script provided by   for every dataset. For each run, we use the model that achieves the best performance (averaged entity metric score and relation metric score) on the development set, and evaluate and report its score on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Hyperparameters and Pre-trained Language Models</head><p>The detailed hyperparameters are present in <ref type="table" target="#tab_18">Table 7</ref>. For the word embeddings, we use 100-dimensional GloVe word embeddings trained on 6B tokens 10 as initialization. We disable updating the word embeddings during training. We set the hidden size to 200, and since we use bidirectional MD-RNNs, the hidden size for each MD-RNN is 100. We use inverse time learning rate decay:lr = lr/(1 + decay rate × steps/decay steps), with decay rate 0.05 and decay steps 1000. Besides, the tested pre-trained language models are shown as follows: <ref type="bibr" target="#b37">(Peters et al., 2018)</ref>: Characterbased pre-trained language model. We use the large checkpoint, with embeddings of dimension 3072.    <ref type="table" target="#tab_1">context when learning the  table representation</ref>, instead, only layer-wise operations are used. As a result, it performs much worse than the ones exploiting the context, confirming the importance to leverage the context information.</p><formula xml:id="formula_19">• [ELMo]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Ways to Leverage the Table Context</head><p>Context along row and column Neighbors along both the row and column dimensions are important. setting "layer + row + col ; layer + row − col" and "layer + row col + ; layer + row col − " remove the row and column dimensions respectively, and their performance is though better than "layer + row col", but worse than setting "layer + row + col + ; layer + row − col − ".</p><p>Multiple dimensions Since in setting "layer + row + col + ", the cell at row i and column j only knows the information before the i-th and j-th word, causing worse performance than bidirectional ("layer + row + col + ; layer + row − col − " and "layer + row + col − ; layer + row − col + ") and quaddirectional ("layer + row + col + ; layer + row − col − ; layer + row + col − ; layer + row − col + ") settings.</p><p>Besides, the quaddirectional model does not show superior performance than bidirectional ones, so we use the latter by default.</p><p>Layer dimension Different from the row and column dimensions, the layer dimension does not carry more sentential context information. Instead, it carries the information from previous layers, so the model can reason high-level relations based on low-level dependencies captured by predecessor layers, which may help recognize syntactically and semantically complex relations. Moreover, recurring along the layer dimension can also be viewed as a layer-wise short-cut, serving similarly to high way <ref type="bibr">(Srivastava et al., 2015)</ref> and residual connection <ref type="bibr" target="#b20">(He et al., 2016)</ref> and making it possible for the networks to be very deep. By removing it (results under "layer row + col + ; layer row − col − "), the performance is harmed.</p><p>Other network Our model architecture can be adapted to other table encoders. We try CNN to encode the table representation. For each layer l, given inputs X l , we have:</p><p>T 0 l = ReLU(Linear([X l ; T l−1 ]))</p><p>T 1 l = ReLU(LayerNorm(CNN(T 0 l )))</p><p>T l = ReLU(T l−1 + LayerNorm(CNN(T 1 l )))</p><p>We also try different kernel sizes for CNN. How-entire  ever, despite its advantages in training time, its performance is worse than the MD-RNN based ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Table Filling Formulations</head><p>Our table filling formulation does not exactly follow <ref type="bibr" target="#b32">Miwa and Sasaki (2014)</ref>. Specifically, we fill the entire table instead of only the lower (or higger) triangular part, and we assign relation tags to cells where entity spans intersect instead of where last words intersect. To maintain the ratio of positive instances to negative instances, although the entire table can express directed relations by undirected tags, we still keep the directed relation tags. I.e, if y RE i,j = − → r then y RE j,i = ← − r , and vice versa. <ref type="table" target="#tab_23">Table 9</ref> ablates our formulation (last row), and compares it with the original one <ref type="bibr" target="#b32">(Miwa and Sasaki, 2014)</ref> (first row). <ref type="figure">Figure 9</ref> presents examples picked from the development set of ACE05. The prediction layer (a linear layer) after training is used as a probe to display the intermediate state of the model, so we can interpret how the model improves both representations from stacking multiple layers and thus from the bidirectional interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Probing Intermediate States</head><p>Such probing is valid since for the table encoder, the encoding spaces of different cells are consistent as they are connected through gate mechanism, including cells in different encoding layers; for the sequence encoder, we used residual connection so the encoding spaces of the inputs and outputs are consistent. Therefore, they are all compatible with the prediction layer. Empirically, the intermediate layers did give valid predictions, although they are not directly trained for prediction.</p><p>In <ref type="figure">Figure 9a</ref>, the model made a wrong prediction</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An example of table filling for NER and RE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Overview of the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>A layer in the table-sequence encoders.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>The generalized form of attention. The softmax function is used to normalize the weights of values V for each query Q i .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Comparison between ground truth and selected heads of ALBERT and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Probing intermediate states posed</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>For 2D-RNNs, cells in the same color can be computed in parallel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>table -</head><label>-</label><figDesc></figDesc><table><row><cell>sequence encoders.</cell></row><row><cell>Dashed lines are for optional components (T ).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>table representation</head><label>representation</label><figDesc></figDesc><table><row><cell>; and then the sequence encoder</cell></row><row><cell>uses the table representation to contextualize the</cell></row><row><cell>sequence representation. With multiple layers, we</cell></row><row><cell>incrementally improve the quality of both represen-</cell></row><row><cell>tations.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Main results. : micro-averaged F1; : macro- averaged F1.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1</head><label>1</label><figDesc>RoBERTa 88.9 66.2 89.3 67.6 ALBERT 89.4 66.0 89.5 67.6</figDesc><table><row><cell>presents the comparison of our model with</cell></row><row><cell>previous methods on four datasets. Our NER per-</cell></row><row><cell>formance is increased by 1.2, 0.9, 1.2/0.6 and 0.4</cell></row></table><note>absolute F1 points over the previous best results. Besides, we observe even stronger performance gains in the RE task, which are 3.6, 4.2, 2.1/2.5 (RE+) and 0.9 (RE+) absolute F1 points, respec- tively. This indicates the effectiveness of our model</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Using different pre-trained language models on ACE05. +x uses the contextualized word embeddings; +T uses the attention weights.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Ablation of the two encoders on ACE05. Gold entity spans are given in RE (gold).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: The performance on ACE05 with different</cell></row><row><cell>number of layers. Pre-trained word embeddings and</cell></row><row><cell>language models are not counted to the number of pa-</cell></row><row><cell>rameters. The underlined ones are from our default set-</cell></row><row><cell>ting.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>presents the comparisons of using different</cell></row><row><cell>dimensions and directions to learn the table repre-</cell></row><row><cell>sentation, based on MD-RNN. Among those set-</cell></row><row><cell>tings, "Unidirectional" refers to an MD-RNN with</cell></row><row><cell>direction "layer</cell></row></table><note>+ row + col + "; "Bidirectional" uses two MD-RNNs with directions "layer + row + col + " and "layer + row − col − " respectively; "Quaddirec- tional" uses MD-RNNs in four directions, illus- trated in Figure 4. Their results are improved when adding more directions, showing richer contextual</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5 :</head><label>5</label><figDesc>The effect of the dimensions and directions of MD-RNNs. Experiments are conducted on ACE05. The underlined ones are from our default setting.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>table-guided attention. The sentence is randomly selected from the development set of ACE05.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>ART</cell><cell cols="3">PART-WHOLE</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Labels</cell><cell>An</cell><cell cols="2">Abu Dhabi TV</cell><cell>office</cell><cell>in</cell><cell>Baghdad</cell><cell>also</cell><cell>was</cell><cell>hit</cell></row><row><cell></cell><cell></cell><cell>ORG</cell><cell></cell><cell>FAC</cell><cell></cell><cell>GPE</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>ART</cell><cell cols="3">PART-WHOLE</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Prediction Layer 3</cell><cell>An</cell><cell cols="2">Abu Dhabi TV</cell><cell>office</cell><cell>in</cell><cell>Baghdad</cell><cell>also</cell><cell>was</cell><cell>hit</cell></row><row><cell></cell><cell></cell><cell>ORG</cell><cell></cell><cell>FAC</cell><cell></cell><cell>GPE</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">PART-WHOLE</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Prediction Layer 2</cell><cell>An</cell><cell cols="2">Abu Dhabi TV</cell><cell>office</cell><cell>in</cell><cell>Baghdad</cell><cell>also</cell><cell>was</cell><cell>hit</cell></row><row><cell></cell><cell></cell><cell>ORG</cell><cell></cell><cell>FAC</cell><cell></cell><cell>GPE</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">ORG-AFF</cell><cell></cell><cell></cell><cell>PHYS</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Prediction Layer 1</cell><cell>An</cell><cell>Abu Dhabi</cell><cell>TV</cell><cell>office</cell><cell>in</cell><cell>Baghdad</cell><cell>also</cell><cell>was</cell><cell>hit</cell></row><row><cell></cell><cell>FAC</cell><cell>ORG</cell><cell>ORG</cell><cell>FAC</cell><cell></cell><cell>GPE</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>table</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, and Jamie Brew. 2019. Huggingface's transformers: State-of-the-art natural language processing. arXiv preprint.</figDesc><table><row><cell>Yuan Yao, Deming Ye, Peng Li, Xu Han, Yankai Lin,</cell></row><row><cell>Zhenghao Liu, Zhiyuan Liu, Lixin Huang, Jie Zhou,</cell></row><row><cell>and Maosong Sun. 2019. Docred: A large-scale</cell></row><row><cell>document-level relation extraction dataset. In Proc.</cell></row><row><cell>of ACL.</cell></row><row><cell>Dmitry Zelenko, Chinatsu Aone, and Anthony</cell></row><row><cell>Richardella. 2002. Kernel methods for relation ex-</cell></row><row><cell>traction. In Proc. of EMNLP.</cell></row><row><cell>Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,</cell></row><row><cell>and Jun Zhao. 2014. Relation classification via con-</cell></row><row><cell>volutional deep neural network. In Proc. of COL-</cell></row><row><cell>ING.</cell></row><row><cell>Dongxu Zhang and Dong Wang. 2015. Relation</cell></row><row><cell>classification via recurrent neural network. arXiv</cell></row><row><cell>preprint.</cell></row><row><cell>Meishan Zhang, Yue Zhang, and Guohong Fu. 2017.</cell></row><row><cell>End-to-end neural relation extraction with global op-</cell></row><row><cell>timization. In Proc. of EMNLP.</cell></row><row><cell>Shubin Zhao and Ralph Grishman. 2005. Extracting</cell></row><row><cell>relations with integrated information using kernel</cell></row><row><cell>methods. In Proc. of ACL.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 6 :</head><label>6</label><figDesc>Dataset statistics</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 7 :</head><label>7</label><figDesc>Hyperparameters used in our experiments.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head></head><label></label><figDesc>row + col + 89.6 66.9 layer + row + col − 89.4 66.3 layer + row − col − 89.6 66.9 layer + row − col + 89.4 66.7 layer + row + col ; layer + row − col 89.5 67.2 layer + row col + ; layer + row col − 89.3 67.4 layer row + col + ; layer row − col − 89.3 66.7 layer + row + col + ; layer + row − col − 89.5 67.6 layer + row + col − ; layer + row − col + 89.7 67.4 layer + row + col + ; layer + row − col − ; layer + row + col − ; layer + row − col +</figDesc><table><row><cell>Setting</cell><cell>NER RE</cell></row><row><cell>MD-RNN</cell><cell></cell></row><row><cell>layer + row col</cell><cell>89.3 63.9</cell></row><row><cell cols="2">layer 89.7 67.6</cell></row><row><cell>CNN</cell><cell></cell></row><row><cell>kernel size 1 × 1</cell><cell>89.3 64.7</cell></row><row><cell>kernel size 3 × 3</cell><cell>89.3 66.2</cell></row><row><cell>kernel size 5 × 5</cell><cell>89.3 65.8</cell></row><row><cell></cell><cell cols="2">• [BERT] (Devlin et al., 2019):</cell><cell>Pre-</cell></row><row><cell></cell><cell>trained Transformer.</cell><cell>We use the</cell></row><row><cell></cell><cell>bert-large-uncased</cell><cell>checkpoint,</cell></row><row><cell></cell><cell cols="2">with embeddings of dimension 1024 and</cell></row><row><cell></cell><cell cols="2">attention weight feature of dimension 384 (24</cell></row><row><cell></cell><cell>layers × 16 heads).</cell></row><row><cell></cell><cell cols="2">• [RoBERTa] (Liu et al., 2019): Pre-trained</cell></row><row><cell></cell><cell cols="2">Transformer. We use the roberta-large</cell></row><row><cell></cell><cell cols="2">checkpoint, with embeddings of dimension</cell></row></table><note>+</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 8 :</head><label>8</label><figDesc>Comparisons with different methods to learn the table representation. For MD-RNN, D + , D − and D are indicators representing the direction, in which the hidden state flows forward, backward, or unable to flow at dimension D (D could be layer, row, or col).We use the implementation provided byWolf  et al. (2019)  11  <ref type="bibr" target="#b0">and Akbik et al. (2019)</ref> 12  to generate contextualized embeddings and attention weights. Specifically, we generate the contextualized word embedding by averaging all sub-word embeddings in the last four layers; we generate the attention weight feature (if available) by summing all subword attention weights for each word, which are then concatenated for all layers and all heads. Both of them are fixed without fine-tuning.</figDesc><table><row><cell>When using multiple MD-RNNs, we separate the indi-</cell></row><row><cell>cators by ";".</cell></row><row><cell>1024 and attention weight feature of dimen-</cell></row><row><cell>sion 384 (24 layers × 16 heads).</cell></row><row><cell>• [ALBERT] (Lan et al., 2019): A lite version</cell></row><row><cell>of BERT with shared layer parameters. We</cell></row><row><cell>use the albert-xxlarge-v1 checkpoint,</cell></row><row><cell>with embeddings of dimension 4096 and at-</cell></row><row><cell>tention weight feature of dimension 768 (12</cell></row><row><cell>layers × 64 heads). We by default use this</cell></row><row><cell>pre-trained model.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 8</head><label>8</label><figDesc>presents the comparisons of different ways to learn the table representation. 11 https://github.com/huggingface/ Transformers 12 https://github.com/flairNLP/flair Importance of context Setting "layer + row col" does not exploit the table</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 9 :</head><label>9</label><figDesc>Comparisons of different table filling formulations. When not filling the entire table, L only fills the lower-triangular part, and U fills the upper-triangular part.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We define antidiagonal entries to be entries at position(i, j) such that i+j = N +1+∆, where ∆ ∈ [−N +1, N −1]is the offset to the main antidiagonal entries.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">In our scenario, there is an additional layer dimension. However, as the model always traverses from the first layer to the last layer, only one direction shall be considered for the layer dimension.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5"> Follow Li and Ji (2014);<ref type="bibr" target="#b31">Miwa and Bansal (2016)</ref>, we use head spans for entities in ACE. And we keep the full mention boundary for other corpora.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://github.com/jessevig/bertviz</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">https://nlp.stanford.edu/projects/ glove/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank the anonymous reviewers for their helpful comments and Lidan Shou for his suggestions and support on this work. This work was done during the first author's remote internship with the StatNLP Group in Singapore University of Technology and Design. This research is supported by Ministry of Education, Singapore, under its Academic Research Fund (AcRF) Tier 2 Programme (MOE AcRF Tier 2 Award No: MOE2017-T2-1-156). Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not reflect the views of the Ministry of Education, Singapore.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PHYS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Labels</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prediction</head><p>Layer <ref type="formula">1</ref>  with the representation learned by the first encoding layer. But after the second encoding layer, this mistake has been corrected by the model. This is also the case that happens most frequently, indicating that two encoding layers are already good enough for most situations. For some more compli-cated cases, the model needs three encoding layers to determine the final decision, shown in <ref type="figure">Figure 9b</ref>. Nevertheless, more layers do not always push the prediction towards the correct direction, and <ref type="figure">Figure 9c</ref> shows a negative example, where the model made a correct prediction in the second encoding layer, but in the end it decided not to output one relation, resulting in a false-negative error. But we must note that such errors rarely occur, and the more common errors are that entities or relationships are not properly captured at all encoding layers.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Flair: An easy-to-use framework for state-of-the-art nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanja</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duncan</forename><surname>Blythe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Schweter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
		<meeting>of NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Kiros</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Hinton. 2016. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adversarial training for multi-context joint entity and relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giannis</forename><surname>Bekoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Develder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Joint entity recognition and relation extraction as a multi-head selection problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giannis</forename><surname>Bekoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Develder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Exploiting syntactico-semantic structures for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><surname>Seng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
		<meeting>of NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">What does bert look at? an analysis of bert&apos;s attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
		<meeting>of NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Span-level model for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalpit</forename><surname>Dixit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The automatic content extraction (ace) program-tasks, data, and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">R</forename><surname>Doddington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">A</forename><surname>Przybocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lance</forename><forename type="middle">A</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><forename type="middle">M</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><forename type="middle">M</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of LREC</title>
		<meeting>of LREC</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep biaffine attention for neural dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Span-based joint entity and relation extraction with transformer pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Eberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Ulges</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Factorizing complex models: A case study in mention detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyan</forename><surname>Jing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>Nanda Kambhatla, and Imed Zitouni</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improving mention detection robustness to noisy input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">F</forename><surname>Pitrelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imed</forename><surname>Zitouni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-dimensional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICANN</title>
		<meeting>of ICANN</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Abdel-Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural relation extraction within and across sentence boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pankaj</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subburam</forename><surname>Rajaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Runkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Table filling multi-task recurrent neural network for joint entity and relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pankaj</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Andrassy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING</title>
		<meeting>of COLING</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Development of a benchmark corpus to support the automatic extraction of drug-related adverse effects from medical case reports</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsha</forename><surname>Gurulingappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdul</forename><forename type="middle">Mateen</forename><surname>Rajput</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angus</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juliane</forename><surname>Fluck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Hofmann-Apitius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Toldo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of biomedical informatics</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A systematic exploration of the feature space for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of HLT-NAACL</title>
		<meeting>of HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Going out on a limb: Joint extraction of entity mentions and relations without dependency trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arzoo</forename><surname>Katiyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of HLT-NAACL</title>
		<meeting>of HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A neural joint model for entity and relation extraction from biomedical text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Joint models for extracting adverse drug events from biomedical text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IJCAI</title>
		<meeting>of IJCAI</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Incremental joint extraction of entity mentions and relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Entity-relation extraction as multi-turn question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiayu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duo</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A general framework for information extraction using dynamic span graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
		<meeting>of NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">End-to-end relation extraction using lstms on sequences and tree structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Modeling joint entity and relation extraction with table representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Sasaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Reasoning with latent structure refinement for document-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoshun</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Sekulic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">End-toend neural relation extraction using deep biaffine attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karin</forename><surname>Dat Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Verspoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ECIR</title>
		<meeting>of ECIR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Cross-sentence n-ary relation extraction with graph lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Embedding semantic similarity in tree kernels for domain adaptation of relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Design challenges and misconceptions in named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev-</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arie</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoNLL</title>
		<meeting>of CoNLL</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A linear programming formulation for global inference in natural language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Tau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yih</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoNLL</title>
		<meeting>of CoNLL</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Representing text chunks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorn</forename><surname>Veenstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EACL</title>
		<meeting>of EACL</meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh Kumar</forename><surname>Srivastava</surname></persName>
		</author>
		<title level="m">Klaus Greff, and Jürgen Schmidhuber. 2015. Highway networks. arXiv preprint</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Semi-supervised relation extraction with large-scale word clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Sekine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
		<meeting>of NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Extracting entities and relations with joint minimum risk training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changzhi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuang-Chih</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewen</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Neural metric learning for fast end-to-end relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tung</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakanth</forename><surname>Kavuluru</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Simultaneously self-attending to all mentions for full-abstract biological relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
		<meeting>of NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A multiscale visualization of attention in the transformer model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Vig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Entity, relation, and event extraction with contextualized span representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulme</forename><surname>Wennberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP/IJCNLP</title>
		<meeting>of EMNLP/IJCNLP</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Medero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuaki</forename><surname>Maeda</surname></persName>
		</author>
		<title level="m">Ace 2005 multilingual training corpus. Linguistic Data Consortium</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Extracting multiple-relations in one-pass with pre-trained transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dakuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saloni</forename><surname>Potdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Ontonotes: A large training corpus for enhanced processing. Handbook of Natural Language Processing and Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Belvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
