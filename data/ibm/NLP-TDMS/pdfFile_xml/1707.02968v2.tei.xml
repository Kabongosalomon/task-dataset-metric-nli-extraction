<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Revisiting Unreasonable Effectiveness of Data in Deep Learning Era</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Revisiting Unreasonable Effectiveness of Data in Deep Learning Era</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The success of deep learning in vision can be attributed to: (a) models with high capacity; (b) increased computational power; and (c) availability of large-scale labeled data. Since 2012, there have been significant advances in representation capabilities of the models and computational capabilities of GPUs. But the size of the biggest dataset has surprisingly remained constant. What will happen if we increase the dataset size by 10× or 100×? This paper takes a step towards clearing the clouds of mystery surrounding the relationship between 'enormous data' and visual deep learning. By exploiting the JFT-300M dataset which has more than 375M noisy labels for 300M images, we investigate how the performance of current vision tasks would change if this data was used for representation learning. Our paper delivers some surprising (and some expected) findings. First, we find that the performance on vision tasks increases logarithmically based on volume of training data size. Second, we show that representation learning (or pretraining) still holds a lot of promise. One can improve performance on many vision tasks by just training a better base model. Finally, as expected, we present new state-of-theart results for different vision tasks including image classification, object detection, semantic segmentation and human pose estimation. Our sincere hope is that this inspires vision community to not undervalue the data and develop collective efforts in building larger datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>There is unanimous agreement that the current ConvNet revolution is a product of big labeled datasets (specifically, 1M labeled images from ImageNet <ref type="bibr" target="#b34">[35]</ref>) and large computational power (thanks to GPUs). Every year we get further increase in computational power (a newer and faster GPU) but our datasets have not been so fortunate. ImageNet, a dataset of 1M labeled images based on 1000 categories, was used to train AlexNet <ref type="bibr" target="#b24">[25]</ref> more than five years ago. Curi- ously, while both GPUs and model capacity have continued to grow, datasets to train these models have remained stagnant. Even a 101-layer ResNet with significantly more capacity and depth is still trained with 1M images from Im-ageNet circa 2011. Why is that? Have we once again belittled the importance of data in front of deeper models and computational power? What will happen if we scale up the amount of training data 10× or 100×, will the performance double? This paper takes the first steps towards clearing the clouds of mystery surrounding the relationship between 'enormous data' and deep learning. We exploit the al-ready existing JFT-image dataset, first introduced by Hinton et al. <ref type="bibr" target="#b16">[17]</ref> and expanded by <ref type="bibr" target="#b6">[7]</ref>. The JFT dataset has more than 300M images that are labeled with 18291 categories. The annotations have been automatically obtained and, therefore, are noisy and not exhaustive. These annotations have been cleaned using complex algorithms to increase the precision of labels; however there is still approximately 20% error in precision. We will use this data to investigate the nature of relationship between amount of data and performance on vision tasks. Specifically, we will look into the power of data for visual representation learning (pre-training). We evaluate our learned representation on a variety of vision tasks: image classification, object detection, semantic segmentation and human pose estimation. Our experiments yield some surprising (and some expected) findings:</p><p>• Better Representation Learning Helps! Our first observation is that large-scale data helps in representation learning as evidenced by improvement in performance on each and every vision task we study.</p><p>This suggests that collection of a larger-scale dataset to study visual pretraining may greatly benefit the field. Our findings also suggest a bright future for unsupervised or self-supervised <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b42">43]</ref> representation learning approaches. It seems the scale of data can overpower noise in the label space.</p><p>• Performance increases logarithmically based on volume of training data. We find there is a logarithmic relationship between performance on vision tasks and the amount of training data used for representation learning. Note that previous papers on large-scale learning <ref type="bibr" target="#b22">[23]</ref> have shown diminishing returns even on log-scale.</p><p>• Capacity is Crucial: We also observe that to fully exploit 300M images, one needs higher capacity models. For example, in case of ResNet-50 the gain on COCO object detection is much smaller (1.87%) compared to (3%) when using ResNet-152.</p><p>• Training with Long-tail: Our data has quite a long tail and yet the representation learning seems to work. This long-tail does not seem to adversely affect the stochastic training of ConvNets (training still converges).</p><p>• New state of the art results: Finally, our paper presents new state-of-the-art results on several benchmarks using the models learned from JFT-300M. For example, a single model (without any bells and whistles) can now achieve 37.4 AP as compared to 34.3 AP on the COCO detection benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Ever since the seminal work by Krizhevsky et al. <ref type="bibr" target="#b24">[25]</ref> showcased the power of Convolutional Neural Networks (ConvNets) on large-scale image recognition task, a lot of work has been done to make them more accurate. A common approach is to increase the complexity of these networks by increasing the width or depth of these networks. For example, Simonyan and Zisserman <ref type="bibr" target="#b36">[37]</ref> proposed the VGG-19 model which uses smaller convolutional filters and has depth of 19 layers. Since then the representational power and depth of these models have continued to grow every year. GoogleNet <ref type="bibr" target="#b38">[39]</ref> was a 22-layer network. In this paper, we perform all our experiments with the ResNet models proposed by He et al. <ref type="bibr" target="#b15">[16]</ref>. The core idea is to add residual connections between layers which helps in optimization of very-deep models. This results in new stateof-the-art performances on a number of recognition tasks.</p><p>Convolutional neural networks learn a hierarchy of visual representations. These visual representations have been shown to be effective on a wide range of computer vision tasks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36]</ref>. Learning these visual representations require large-scale training data. However, the biggest detection and segmentation datasets are still on the order of hundreds of thousands of images. Therefore, most of these approaches employ pre-training. The original model is learning using million labeled images in Ima-geNet and then further trained on target tasks (fine-tuning) to yield better performance <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b32">33]</ref>. Huang et al. <ref type="bibr" target="#b17">[18]</ref> thoroughly evaluated the influence of multiple ConvNet architectures on object detection performance, and found that it is closely correlated with the models' capacity and classification performances on ImageNet.</p><p>While there has been significant work on increasing the representational capacity of ConvNets, the amount of training data for pre-training has remain kind of fixed over years. The prime reason behind this is the lack of human verified image datasets larger than ImageNet. In order to overcome the bottleneck, there have been recent efforts on visual representation learning using web-supervision <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27]</ref> or unsupervised <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref> paradigms. However, most of these efforts are still are still exploratory in nature and far lower in performance compared to fully-supervised learning.</p><p>In this paper, we aim to shift the discussion from models to data. Our paper is inspired from several papers which have time and again paid closer look to impact and properties of data rather than models. In 2009, Pereira et al. <ref type="bibr" target="#b29">[30]</ref> presented a survey paper to look into impact of data in fields such as natural language processing and computer vision. They argued unlike physics, areas in AI are more likely to see an impact using more data-driven approaches. Another related work is the empirical study by Torralba and Efros <ref type="bibr" target="#b40">[41]</ref> that highlighted the dataset biases in current com-puter vision approaches and how it impacts future research.</p><p>Specifically, we focus on understanding the relationship between data and visual deep learning. There have been some efforts to understand this relationship. For example, Oquab et al. <ref type="bibr" target="#b27">[28]</ref> showed that expanding the training data to cover 1512 labels from ImageNet-14M further improves the object detection performance. Similarly, Huh et al. <ref type="bibr" target="#b18">[19]</ref> showed that using a smaller subset of images for training from ImageNet hurts performance. Both these studies also show that selection of categories for training is important and random addition of categories tends to hurt the performance. But what happens when the number of categories are increased 10x? Do we still need manual selection of categories? Similarly, neither of these efforts demonstrated data effects at significantly larger scale.</p><p>Some recent work <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b43">44]</ref> have looked at training Con-vNets with significantly larger data. While <ref type="bibr" target="#b43">[44]</ref> looked at geo-localization, <ref type="bibr" target="#b22">[23]</ref> utilized the YFCC-100M dataset <ref type="bibr" target="#b39">[40]</ref> for representation learning. However, unlike ours, <ref type="bibr" target="#b22">[23]</ref> showed plateauing of detection performance when trained on 100M images. Why is that? We believe there could be two possible reasons: a) YFCC-100M images come only from Flickr. JFT includes images all over the web, and has better visual diversity. The usage of user feedback signals in JFT further reduces label noise. YFCC-100M has a much bigger vocabulary size and noisier annotations. b) But more importantly, they did not see real effect of data due to use of smaller AlexNet of VGG models. In our experiments, we see more gain with larger model sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The JFT-300M Dataset</head><p>We now introduce the JFT-300M dataset used throughout this paper. JFT-300M is a follow up version of the dataset introduced by <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17]</ref>. The JFT-300M dataset is closely related and derived from the data which powers the Image Search. In this version, the dataset has 300M images and 375M labels, on average each image has 1.26 labels. These images are labeled with 18291 categories: e.g., 1165 type of animals and 5720 types of vehicles are labeled in the dataset. These categories form a rich hierarchy with the maximum depth of hierarchy being 12 and maximum number of child for parent node being 2876.</p><p>The images are labeled using an algorithm that uses complex mixture of raw web signals, connections between webpages and user feedback. The algorithm starts from over one billion image label pairs, and ends up with 375M labels for 300M images with the aim to select labeled images with high precision. However, there is still some noise in the labels: approximately 20% of the labels in this dataset are noisy. Since there is no exhaustive annotation, we have no way to estimate the recall of the labels. <ref type="figure">Figure 2</ref> shows the kind of noise that exists in the dataset. Because the labels are generated automatically, there is a problem of 'tortoise'  <ref type="figure">Figure 2</ref>. JFT-300M dataset can be noisy in terms of label confusion and incorrect labels. This is because labels are generated via a complex mixture of web signals, and not annotated or cleaned by humans. x-axis corresponds to the quantized distances to K-Means centroids, which are computed based on visual features. being confused with 'tortoise-shell glasses'.</p><p>Finally, it is important to discuss the data distribution of JFT-300M. The distribution is heavily long-tailed: e.g., there are more than 2M 'flowers', 3250 'subarau360' but only 131 images of 'train conductors'. In fact, the tail is so heavy that we have more than 3K categories with less than 100 images each and approximately 2K categories with less than 20 images per category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Training and Evaluation Framework</head><p>We now describe our training and evaluation framework for the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Training on JFT-300M Data</head><p>Although there are several novel ConvNet architectures recently proposed, we decide to use a standard Residual Network architecture <ref type="bibr" target="#b15">[16]</ref> with 101 layers (ResNet-101) for its state-of-the-art performance and the ease of comparison with previous work. To train a ResNet-101 model on JFT-300M, We add a fully-connected layer with 18291 outputs at the end of the network for classification. As the image labels are not mutually exclusive, we compute per-label logistic loss, and treat all non-present labels as negatives. To alleviate the issue of missing labels, we use a hand-designed label hierarchy and fill in the missing labels accordingly. For example, an image with label 'apple' is also considered as a correct example for 'fruit'.</p><p>During training, all input images are resized to 340×340 pixels, and then randomly cropped to 299 × 299. The image pixels are normalized to the range of [−1, 1] independently per channel, and we use random reflection for data augmentation. We set weight decay to 10 −4 and use batch normalization <ref type="bibr" target="#b19">[20]</ref> after all the convolutional layers. RMSProp optimizer is used with momentum of 0.9, and the batch size is set to 32. The learning rate is 10 −3 initially and we decay it by 0.9 every 3M steps. We use asynchronous gradient descent training on 50 NVIDIA K80 GPUs. The model is implemented in TensorFlow.</p><p>To allow asynchrounous training of models on 50 GPUs, we adopt the Downpour SGD training scheme <ref type="bibr" target="#b7">[8]</ref>, where we use 17 parameter servers to store and update the model weights. The final classification fully-connected layer with 2048 input units and over 18K output units has over 36M parameters. To handle this in our parameter servers, we split it vertically into 50 equal sized sub-fc layers, and distribute them around different parameter servers.</p><p>ImageNet baseline: As observed by <ref type="bibr" target="#b6">[7]</ref>, hyperparameters that are selected to train with JFT-300M data yield sub-optimal performance when training on ImageNet (IVSVRC 2012 image classification dataset with 1.2M images). Therefore, for ImageNet, we use a momentum optimizer with the momentum of 0.9, and set the initial learning rate to 5 × 10 −2 and batch size to 32. Learning rate is reduced by a factor of 10 every 30 epochs ( 1.2M steps), and we train the model for a total of 5M steps. Similar to JFT-300M training, we use asynchronous gradient descent training on 50 NVIDIA K80 GPUs and 17 parameter servers.</p><p>Our baseline ResNet-101 performs 1% better than the open-sourced ResNet-101 checkpoint from the authors of <ref type="bibr" target="#b15">[16]</ref>, using the same evaluation protocol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Monitoring Training Progress</head><p>For monitoring the training progress on JFT-300M, we use the validation set from Chollet <ref type="bibr" target="#b6">[7]</ref>: 'FastEval14k'. FastEval14k consists of 14000 images with labels from 6000 classes (subset of 18291 classes from JFT-300M). Unlike labels in JFT-300M, the images in FastEval14k are densely annotated and there are around 37 labels per image on average. We use the same mAP@100 metric as in <ref type="bibr" target="#b6">[7]</ref>, which is computed as the mean average precision (mAP) for top-100 predictions. Note that the class AP is weighted by how common the class is among social media images.</p><p>We tried two strategies to initialize the model weights for training: random initialization and initializing from an Ima-geNet checkpoint. In both settings, we used the same training schedule (e.g., learning rates). We found that on FastE-val14k benchmark, model trained from ImageNet initialization performs better at the first 15M iterations, but then becomes on par with random initialization. <ref type="figure" target="#fig_2">Figure 3</ref> shows the training progress for these two settings. On FastEval14k benchmark, model trained from ImageNet initialization performs better at the first 15M iterations, but then becomes on par with random initialization.</p><p>Please note that the full training schedule takes 90M iterations or around 10 epochs. However, due to the time constraints, we train the models for 36M iterations or 4 epochs, which takes approximately 2 months. We will study the impact of training iterations in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluating the Visual Representations</head><p>We use two approaches to evaluate the quality of visual representations learned from 300M training data. The first approach is to freeze the model weights and use these models as pure feature extractors. The second approach is to use the model weights as initialization and fine-tune the weights for other tasks. For evaluating visual representations, we select three representative computer vision tasks: object detection, semantic segmentation and human pose estimation.</p><p>We will perform a more rigorous ablative analysis to observe the effect of dataset size, vocabulary size, etc. on the object detection task. For the other tasks, we will just show how JFT-300M provides significant improvement compared to baseline ImageNet ResNet.</p><p>De-duplication One concern with using large-scale sets such as JFT-300M is the possible overlap between training and test sets. Such duplication exist in current frameworks as well: e.g. 890 out of 50K validation images in ImageNet have near-duplicate images training set. However, to ensure such duplication does not affect our results, we performed all experiments by removing near-duplicate images from test sets. We found the difference in performance to be insignificant for all the experiments. We therefore report de-duplicated test-set results in Appendix A. Object Detection. We use the Faster RCNN framework <ref type="bibr" target="#b32">[33]</ref> for its state-of-the-art performance. Faster RCNN is a two-stage model. The first stage is called region proposal network (RPN), which aims at generating classagnostic object proposals. The second stage is a box classifier, it takes the boxes predicted by RPN and crops feature maps to generate classification predictions and refined bounding box predictions. These two stages share a common feature map generated by a ConvNet, and box classifier has additional convolutional layers before its final classification and regression layers. To use the ResNet-101 model pre-trained on JFT-300M data, we split the model into two parts: the first part starts from conv1 block and ends at conv4 block, it is used for feature extraction and is shared by both RPN and box classifier; the second part consists of the conv5 block, it is used by box classifier. Semantic Segmentation. We use the DeepLab framework <ref type="bibr" target="#b3">[4]</ref> with ResNet-101 base architecture for the task of semantic segmentation. In particular, we use a variant which adds four branches after the conv5 block of ResNet-101 architecture. Each branch is an atrous convolutional Initialization Top-1 Acc. Top-5 Acc. MSRA checkpoint <ref type="bibr" target="#b15">[16]</ref> 76.4 92.9 Random initialization 77.5 93.9 Fine-tune from JFT-300M 79.2 94.7 layer that predicts a sub-sampled pixel-wise class probabilities. Predictions from all branches are fused together to produce the final segmentation output. Please refer to the DeepLab-ASPP-L model (Atrous Spatial Pyramid Pooling, with Large atrous rates) from <ref type="bibr" target="#b3">[4]</ref> for details. Pose Estimation. We follow the framework proposed by Papandreou et al. <ref type="bibr" target="#b28">[29]</ref>. It uses person bounding boxes detected by Faster RCNN, then applies a ResNet <ref type="bibr" target="#b15">[16]</ref> fully convolutionally to produce heatmaps and offsets for all keypoints. A novel scoring and non-maximum suppression (NMS) scheme is used to suppress duplicate detections and improve performance. We simply replace the base models used in their framework by our trained ResNet-101 models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We present results of fine-tuning JFT-300M ResNet-101 checkpoints on four tasks: image classification, object detection, semantic segmentation and human pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Image Classification</head><p>We fine-tune the JFT-300M pre-trained ResNet101 using ImageNet classification data and compare it with a ResNet101 model trained from scratch. For this experiment, we use the standard ILSVRC 2012 'train' and 'val' sets for training and evaluation. There are 1.2M training images and 50K validation images, over 1000 classes.</p><p>We use the same ImageNet training setup as described in Section 4.1 for the ImageNet baseline, but lowered the initial learning rate to 10 −3 (standard for fine-tuning). We initialize the model weights from the JFT-300M checkpoint trained for 36M iterations and fine-tune on ImageNet for 4M iterations. <ref type="table" target="#tab_0">Table 1</ref> compares the fine-tuning results with models trained from the scratch. For reference, we show the random initialization performance for the open-sourced checkpoint from the authors of <ref type="bibr" target="#b15">[16]</ref>. We report top-1 and top-5 accuracies with a single crop being evaluated. We can see that fine-tuning on JFT-300M gives considerable performance boost for both top-1 and top-5 accuracies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Object Detection</head><p>We next evaluate the JFT-300M checkpoints on object detection tasks. We evaluate on the two most popular datasets: COCO <ref type="bibr" target="#b25">[26]</ref> and PASCAL VOC <ref type="bibr" target="#b12">[13]</ref>. Instead of just showing state-of-the-art performance, we will also perform a rigorous ablative analysis to gain insights into the relationship between data and representation learning.</p><p>Specifically, we use object detection experiments to answer the following questions:</p><p>• How does the performance of trained representations vary with iterations and epochs? • Does the performance of learned visual representations saturate after certain amount of data? Do we see any plateauing effect with more and more data? • How important is representational capacity?</p><p>• Is the number of classes a key factor in learning visual representation? • How could clean data (e.g., ImageNet) help improve the visual representations?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Setup</head><p>For COCO <ref type="bibr" target="#b25">[26]</ref>, we use a held-out 8000 images from the standard 'val' set as our validation set, we refer to it as 'minival * ', the same set of images was used by <ref type="bibr" target="#b17">[18]</ref>. We use a combination of the standard training set and the remaining validation images for training. Unless otherwise specified, all COCO results are reported on the minival * set. In particular, we are interested in mean average precision at 50% IOU threshold (mAP@.5), and the average of mAP at IOU thresholds 50% to 95% (mAP@[.5, .95]). For our best ResNet101 models, we also evaluate on the COCO 'test-dev' split (evaluated by the official result server). For PASCAL VOC, we use the 16551 'trainval' images from PASCAL VOC 2007 and 2012 for training, and report performance on the PASCAL VOC 2007 Test, which has 4952 images using mAP@.5 metric. We use the TensorFlow Faster RCNN implementation <ref type="bibr" target="#b17">[18]</ref> and adopt their default training hyperparameters except for learning rate schedules. We use asynchronous training with 9 GPU workers and 11 parameter servers, momentum optimizer is used with the momentum of 0.9. Each worker takes a single input image per step, the batch size for RPN and box classifier training are 64 and 256 respectively. Input images are resized to have 600 minimum pixels and 1024 maximum pixels while maintaining the aspect ratio. The only data augmentation used is random flipping.</p><p>For COCO, we set the initial learning rate to be 4×10 −4 , and decay the learning rate by a factor of 10 after 2.5M steps, the total number of steps is 3M. For PASCAL VOC, we set the initial learning rate to be 3 × 10 −4 , and decay the learning rate by 0.1 after 500K steps, and the model is trained for 700K steps. The training schedules were selected on held-out validation images using the open-source ResNet-101 model (pre-trained on ImageNet). We found the same training schedules work well on other checkpoints, and keep them fixed throughout for fairer comparison. ing inference, we use 300 RPN proposals. Our vanilla FasterRCNN implementation does not use the multi-scale inference, context or box-refinement as described in <ref type="bibr" target="#b32">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with ImageNet Models</head><p>We first present the performance comparison with Ima-geNet checkpoints. <ref type="table">Table 2</ref> shows the detection performance on COCO 'test-dev' split. To show that our Faster RCNN baseline is competitive, we also report results from the Faster RCNN paper <ref type="bibr" target="#b15">[16]</ref>, which uses both box refinement and context information. We can see that our Ima-geNet baseline performs competitively. We evaluate JFT-300M trained from scratch ('300M') and from ImageNet initialization ('ImageNet+300M'). Both models outperforms the ImageNet baseline by large margins, with 3.3% and 4.4% boost in mAP@.5, 2.4% and 3.1% in mAP@[.5,.95] respectively. As a reference, we also show the performance of ImageNet trained InceptionRes-Netv2 in <ref type="table">Table 2</ref>. We would like to point out that the gain is even more significant than recently achieved by doubling the number of layers on Inception ResNet <ref type="bibr" target="#b17">[18]</ref>. This clearly indicates that while there are indications of a plateauing effect on model representation capacity; in terms of data there is still a lot that can be easily gained. <ref type="table">Table 3</ref> shows the performance on the PASCAL VOC 2007 'test' set. Again, both JFT-300M checkpoints outperforms the ImageNet baseline significantly, by 5.1% and 5.0% mAP@.5 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of Epochs</head><p>We study how the number of training epochs affects the object detection performance. For this experiment we report results on COCO minival * set. <ref type="table">Table 4</ref> shows the performance comparison when the JFT-300M model has been trained for 1.3, 2.6 and 4 epochs respectively. We can see that as the number of training steps increases, the perfor-  mance also improves. As a comparison, in <ref type="table">Table 5</ref> we show the ImageNet counterpart when trained for 3, 6, 12 and 150 epochs, we can see that the performance of ImageNet checkpoints improves faster than JFT-300M with respect to the number of epochs. We would also like to point out that our learning schedules have been developed using the experience from smaller datasets. One can envision better learning schedules which provide more improvement as more epochs are used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of Data Size</head><p>For this experiment, we randomly sample a subset of 10M, 30M and 100M images from the JFT-300M training data. We use the same training schedule as the JFT-300M model training. We pick the checkpoints corresponding to the 4th epoch for each subset. To study the impact of learned visual representations, we also conduct an experiments to freeze the model weights for all layers before the conv5 block. For this set of experiments we change the learning rate decay to happen at 900K steps, and the total number of training steps to 1.5M, as we find they tend to converge earlier.</p><p>In <ref type="figure" target="#fig_4">Figure 4</ref>, we show the mAP@[.5,.95] with checkpoints trained on different JFT-300M subsets, the blue curve corresponds to the regular faster RCNN training (with finetuning), while the red curve corresponds to freezing feature extractors. Not surprisingly, fine-tuning offers significantly better performance on all data sizes. Most interestingly, we can see that the performance grows logarithmically as pretraining data expands, this is particularly true when feature extraction layers are frozen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of Classes</head><p>JFT-300M has 18K labels in total. To understand what the large number of classes brings us, we select a subset of 941 labels which have direct correspondence to the 1000 Ima-geNet labels, and sample JFT-300M images which contain   at least one of such labels. This results in a subset of 30M images. We then train on this dataset for 4 epochs using the same training scheme. <ref type="table">Table 6</ref> shows the performance comparison on COCO minival * set. We see that the two models perform on par with each other. This indicates that the performance benefit comes from more training images instead of more labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of Model Capacity</head><p>Finally, we study the impact of model capacity when 300M images are available for training. We conduct the experiments on the 50-layer, 101-layer and 152-layer ResNet models. Each model is trained from scratch on the JFT-300M data, with the same hyper parameters used for ResNet-101 experiments. For comparison, we also train the models on ImageNet data till convergence, using the same hyper parameters for ResNet-101. <ref type="figure" target="#fig_5">Figure 5</ref> shows the performance of fine-tuning different pre-trained models on COCO minival * set. We observe that higher capacity models are better at utilizing 300M data. For example, in case of ResNet-50 the gain is smaller compared to when using ResNet-152.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Semantic Segmentation</head><p>We use the PASCAL VOC 2012 semantic segmentation benchmark <ref type="bibr" target="#b11">[12]</ref> which has pixel-wise labels for 20 foreground classes and one background class. As is standard practice, all models are trained on an augmented PASCAL VOC <ref type="bibr" target="#b11">[12]</ref> 2012 'trainaug' set with 10582 images (extra annotations from <ref type="bibr" target="#b14">[15]</ref>). We report quantitative results on the PASCAL VOC 2012 'val' set (1449 images) using the standard mean intersection-over-union (mIOU) metric.</p><p>Implementation details.</p><p>The DeepLab-ASPP-L model <ref type="bibr" target="#b3">[4]</ref> has four parallel branches after conv5 block of ResNet101 architecture. Each branch is a (3 × 3) convolutional layer, with a different atrous rate r (r ∈ {6, 12, 8, 24}). Different atrous rates enable the model to capture objects and context at different scales. Output of each branch is pixel-wise scores for 21 classes with the same resolution output map (subsampled by factor of 8 compared to the original image). These scores are added together and normalized for the final pixel-wise class probabilities.</p><p>For training, we use mini-batch SGD with momentum. Our model is trained for 30k SGD iterations using a minibatch of 6 images, momentum of 0.9, an initialize learning rate (LR) of 10 −3 and "polynomial" learning rate policy <ref type="bibr" target="#b3">[4]</ref>. All layers are trained with L2-regularization (weight decay of 5 × 10 −4 ). We do not use any data-augmentation, multiscale training/testing or post-processing using CRFs for this task. To initialize the DeepLab-ASPP-L model using Ima-geNet or JFT-300M trained checkpoints, the final classification layer from these checkpoints is replaced with four convolutional branches (initialized using Xavier). All in- AP AP@.5 AR AR@.5 CMU Pose <ref type="bibr" target="#b2">[3]</ref> 61  <ref type="table">Table 7</ref>. Human pose estimation performance on COCO 'test-dev' split. We follow the implementation of G-RMI Pose <ref type="bibr" target="#b28">[29]</ref>, but change the ResNet-101 initial checkpoints from ImageNet pretrained to JFT-300M pre-trained.</p><p>put images are resized to (513 × 513), which results in a (65 × 65) conv5 block from the ResNet101 network as well as (65 × 65 × 21) predictions from the entire model.</p><p>Comparison with ImageNet Models. We present quantitative comparison of JFT-300M checkpoints with ImageNet checkpoints in <ref type="figure" target="#fig_6">Figure 6</ref> (left). We see that the JFT-300M checkpoint outperforms ImageNet by 1.7% points. We further observe that the JFT-300M model trained from the ImageNet checkpoint provides 2.9% points boost over the vanilla ImageNet checkpoint.</p><p>Impact of Data Size. In <ref type="figure" target="#fig_6">Figure 6</ref> (right), we further present analysis of impact of training data size by randomly sampling a subset of 10M, 30M and 100M images from the JFT-300M for training base checkpoints (same as Section 5.2). Once again we observe that the performance increases logarithmically as the pre-training dataset increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Human Pose Estimation</head><p>We train the fully-convolutional pose detector <ref type="bibr" target="#b28">[29]</ref> by initializing the base ResNet model with our checkpoints and fine-tuning. The model is trained with SGD+Momentum for 450K steps. The learning rate was dropped by a factor of 10 after 250K steps, starting with a base learning rate. Best hyper parameter combination for each model was then selected independently and used in further experimentation.</p><p>In <ref type="table">Table 7</ref>, we present the end to end pose estimation results evaluated on COCO 'test-dev' set. G-RMI Pose uses the ImageNet pre-trained checkpoint for fine-tuning, and we can see that our models with JFT-300M initialization perform much better. Note that to have a fair comparison with G-RMI Pose, we show their performance when only COCO images are used for training (fine-tuning) and no ensembling is performed. We use the person detection results provided by the authors and apply our trained pose detectors on the same set of person boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussions</head><p>Is it to be expected that performance of computer vision algorithms would always improve with more and more data? In our personal correspondences with several researchers, the general consensus seems to be that everyone expects some gain in performance numbers if the dataset size is increased dramatically, with decreasing marginal performance as the dataset grows. Yet, while a tremendous amount of time is spent on engineering and parameter sweeps; little to no time has been spent collectively on data.</p><p>Our paper is an attempt to put the focus back on the data. The models seem to be plateauing but when it comes to the performance with respect to data -but modest performance improvements are still possible for exponential increases of the data. Another major finding of our paper is that having better models is not leading to substantial gains because ImageNet is no more sufficient to use all the parameters or their representational power.</p><p>Representation learning: One of the underlying debates is that should we spend more time collecting data for individual tasks such as detection and segmentation. Our findings show there is still a lot to be gained from representation learning. Improved base models or base features can lead to significant gains in performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Disclaimer -Large scale learning:</head><p>We would like to highlight that the training regime, learning schedules and parameters used in this paper are based on our understanding of training ConvNets with 1M images. Searching the right set of hyper-parameters requires significant more effort: even training a JFT model for 4 epochs needed 2 months on 50 K-80 GPUs. Therefore, in some sense the quantitative performance reported in this paper underestimates the impact of data for all reported image volumes.</p><p>A dataset with 300M images is almost guaranteed to contain images that overlap with the validation set of target tasks. In fact, we find that even for ImageNet, there are 890 out of 50K validation images have near-duplicate images in the training.</p><p>We use visual embeddings to measure similarities and identify duplicate or near-duplicate images. The embeddings are based on deep learning features. We find there are 5536 out of 50K images in ImageNet validation set, 1648 out of 8K images in COCO minival * , 201 out of 4952 images in Pascal VOC 2007 test set, and 84 out of 1449 images in Pascal VOC 2012 validation set that have near duplicates in JFT-300M. We rerun several experiments by removing near-duplicate images from validation sets and then comparing performance between baselines and learned models. We observe no significant differences in trends. We do not conduct de-duplication experiments of COCO testdev dataset for object detection and pose estimation as their groundtruth annotations are not publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B Detailed and Per-category Results: Object Detection</head><p>In this section, we present detailed and per-category object detection results for Table 2 (Section 5.2) from the main submission, evaluated on the COCO test-dev split. In <ref type="table" target="#tab_0">Table 11</ref>, we report detailed AP and AR results using different initializations. In <ref type="table" target="#tab_0">Table 14,</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Per-category Results: Semantic Segmentation</head><p>In <ref type="table" target="#tab_0">Table 12</ref>, we report quantitative results on the VOC 2012 segmentation validation set for all classes (refer to <ref type="figure" target="#fig_5">Figure 5</ref> (left), Section 5.3 in the main submission). Results are reported for different initializations. We observe more than 7 point improvement for categories like boat and horse. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detailed Results: Human Pose Estimation</head><p>In <ref type="table" target="#tab_0">Table 13</ref>, we present all AP and AR results for the performance reported in <ref type="table">Table 7</ref> (Section 5.4) in the main submission. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The Curious Case of Vision Datasets: While GPU computation power and model sizes have continued to increase over the last five years, size of the largest training dataset has surprisingly remained constant. Why is that? What would have happened if we have used our resources to increase dataset size as well? This paper provides a sneak-peek into what could be if the dataset sizes are increased dramatically.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Comparison of training progress with random initialization (blue) and ImageNet initialization (yellow) on JFT-300M data. x-axis is the number of training steps, and y-axis shows the mAP@100 metric computed on FastEval14k.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Object detection performance when initial checkpoints are pre-trained on different subsets of JFT-300M from scratch. x-axis is the data size in log-scale, y-axis is the detection performance in mAP@[.5,.95] on COCO minival * (left), and in mAP@.5 on PASCAL VOC 2007 test (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Object detection performance on COCO minival * on ResNet models with different number of layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Semantic segmentation performance on Pascal VOC 2012 val set. (left) Quantitative performance of different initializations; (right) Impact of data size on performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>we provide per-category AP and AP@.5 results. AP AP@.5 AP@.75 AP(S) AP(M) AP(L) AR AR@.5 AR@.75 AR(S) AR(M) AR(</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Top-1 and top-5 classification accuracy on the ImageNet 'val' set (single model and single crop inference are used).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>method airplane bicycle bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train TV mean ImageNet 79.7 80.6 77.1 65.9 64.2 85.3 81.0 88.4 60.5 83.1 70.8 86.7 86.2 79.7 79.5 49.5 78.3 80.2 79.2 69.7 76.3 300M 87.2 88.8 79.6 75.2 67.9 88.2 89.3 88.6 64.3 86.1 73.6 88.7 89.1 86.5 86.4 57.7 84.2 82.1 86.7 78.6 81.4 ImageNet+300M 86.9 88.0 80.1 74.7 68.8 88.9 89.6 88.0 69.7 86.9 71.9 88.5 89.6 86.9 86.8 53.7 78.2 82.3 87.7 77.9 81.3</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .Table 6 .</head><label>36</label><figDesc>Average Precision @ IOU threshold of 0.5 on PASCAL VOC 2007 'test' set. The 'trainval' set of PASCAL VOC 2007 and 2012 are used for training. Object detection performance in mean AP@[.5,.95] on COCO minival * set. We compare checkpoints pre-trained on 30M JFT images where labels are limited to the 1K ImageNet classes, and 30M JFT images covering all 18K JFT classes.</figDesc><table><row><cell cols="4">#Iters on JFT-300M #Epochs mAP@[0.5,0.95]</cell></row><row><cell>12M</cell><cell></cell><cell>1.3</cell><cell>35.0</cell></row><row><cell>24M</cell><cell></cell><cell>2.6</cell><cell>36.1</cell></row><row><cell>36M</cell><cell></cell><cell>4</cell><cell>36.8</cell></row><row><cell cols="4">Table 4. mAP@[.5,.95] on COCO minival *  with JFT-300M check-</cell></row><row><cell cols="4">point trained from scratch for different number of epochs.</cell></row><row><cell cols="4">#Iters on ImageNet #Epochs mAP@[0.5,0.95]</cell></row><row><cell>100K</cell><cell></cell><cell>3</cell><cell>22.2</cell></row><row><cell>200K</cell><cell></cell><cell>6</cell><cell>25.9</cell></row><row><cell>400K</cell><cell></cell><cell>12</cell><cell>27.4</cell></row><row><cell>5M</cell><cell></cell><cell>150</cell><cell>34.5</cell></row><row><cell cols="4">Table 5. mAP@[.5,.95] on COCO minival *  with ImageNet check-</cell></row><row><cell cols="3">point trained for different number of epochs.</cell></row><row><cell></cell><cell cols="3">Number of classes mAP@[.5,.95]</cell></row><row><cell></cell><cell>1K ImageNet</cell><cell></cell><cell>31.2</cell></row><row><cell></cell><cell>18K JFT</cell><cell></cell><cell>31.9</cell></row><row><cell cols="3">#Layers ImageNet 300M</cell></row><row><cell>50</cell><cell>31.6</cell><cell>33.5</cell></row><row><cell>101</cell><cell>34.5</cell><cell>36.8</cell></row><row><cell>152</cell><cell>34.7</cell><cell>37.7</cell></row><row><cell></cell><cell></cell><cell>50</cell><cell>101</cell><cell>152</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Number of layers →</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 8 Table 10 .</head><label>810</label><figDesc>, 9 and 10 show that the duplicate images have minimal impact on performance for all experiments.Table 9. mAP@0.5 and mAP@[0.5,0.95] for object detection performance on COCO minival * , before and after de-duplication. Object detection and semantic segmentation performance on Pascal VOC, before and after deduplication. (Left) Object detection mAP@0.5 on Pascal VOC 2007 test set. (Right) Semantic segmentation mIOU on Pascal VOC 2012 validation set.</figDesc><table><row><cell></cell><cell></cell><cell>Original</cell><cell></cell><cell cols="2">De-duplication</cell></row><row><cell></cell><cell cols="5">Top-1 Acc. Top-5 Acc. Top-1 Acc. Top-5 Acc.</cell></row><row><cell>MSRA checkpoint</cell><cell></cell><cell>76.4</cell><cell>92.9</cell><cell>76.4</cell><cell>92.9</cell></row><row><cell cols="2">Random initialization</cell><cell>77.5</cell><cell>93.9</cell><cell>77.5</cell><cell>93.8</cell></row><row><cell cols="2">Fine-tune from JFT-300M</cell><cell>79.2</cell><cell>94.7</cell><cell>79.3</cell><cell>94.7</cell></row><row><cell cols="6">Table 8. Top-1 and top-5 classification accuracy on ImageNet validation set, before and after de-duplication. Single model and single crop</cell></row><row><cell>are used.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Original</cell><cell cols="3">De-duplication</cell></row><row><cell cols="6">mAP@0.5 mAP@[0.5,0.95] mAP@0.5 mAP@[0.5,0.95]</cell></row><row><cell>ImageNet</cell><cell>54.0</cell><cell>34.5</cell><cell>54.0</cell><cell cols="2">34.6</cell></row><row><cell>300M</cell><cell>57.1</cell><cell>36.8</cell><cell>56.8</cell><cell cols="2">36.7</cell></row><row><cell>ImageNet+300M</cell><cell>58.2</cell><cell>37.8</cell><cell>58.2</cell><cell cols="2">37.7</cell></row><row><cell></cell><cell cols="2">VOC07 Detection</cell><cell cols="3">VOC12 Segmentation</cell></row><row><cell></cell><cell cols="5">Original De-duplication Original De-duplication</cell></row><row><cell>ImageNet</cell><cell>76.3</cell><cell>76.5</cell><cell>73.6</cell><cell>73.3</cell></row><row><cell>300M</cell><cell>81.4</cell><cell>81.5</cell><cell>75.3</cell><cell>75.1</cell></row><row><cell>ImageNet+300M</cell><cell>81.3</cell><cell>81.2</cell><cell>76.5</cell><cell>76.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 11 .</head><label>11</label><figDesc>Object detection performance on COCO test-dev split using different model initializations.</figDesc><table><row><cell>L)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 12 .</head><label>12</label><figDesc>Initialization mIOU bg aero bike bird boat bottle bus car cat chair cow table dog horse mbike persn plant sheep sofa train tv ImageNet 73.6 93.2 88.9 40.1 87.3 65.0 78.8 89.9 84.3 88.8 37.2 81.6 49.3 84.1 78.9 79.3 83.3 57.7 82.0 41.7 80.3 73.1 300M 75.3 93.7 89.8 40.1 89.8 70.6 78.5 89.9 86.1 92.0 36.9 80.9 52.8 87.6 82.4 80.8 84.3 61.7 84.4 44.8 80.9 72.6 ImageNet+300M 76.5 94.8 90.4 41.6 89.1 73.1 80.4 92.3 86.7 92.0 39.6 82.7 52.7 86.2 86.1 83.6 85.7 61.5 83.9 45.3 84.6 73.6 Per-class semantic segmentation performance on PASCAL VOC 2012 validation set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 14 .</head><label>14</label><figDesc>Per-class object detection performance on COCO test-dev split using different model initializations.</figDesc><table><row><cell>Initialization →</cell><cell cols="2">ImageNet</cell><cell cols="2">300M</cell><cell cols="2">ImageNet+300M</cell><cell>Initialization →</cell><cell cols="2">ImageNet</cell><cell>300M</cell><cell>ImageNet+300M</cell></row><row><cell></cell><cell cols="5">AP@.5 AP AP@.5 AP AP@.5</cell><cell>AP</cell><cell></cell><cell cols="3">AP@.5 AP AP@.5 AP AP@.5</cell><cell>AP</cell></row><row><cell>person</cell><cell cols="5">71.5 47.7 73.1 49.8 72.7</cell><cell>49.9</cell><cell>wine glass</cell><cell cols="3">53.8 30.2 56.3 33.3 58.7</cell><cell>34.7</cell></row><row><cell>bicycle</cell><cell cols="5">48.9 26.4 54.9 30.0 52.7</cell><cell>29.9</cell><cell>cup</cell><cell cols="3">64.7 32.5 67.5 35.6 68.4</cell><cell>35.9</cell></row><row><cell>car</cell><cell cols="5">55.7 34.7 58.3 36.9 59.3</cell><cell>37.1</cell><cell>fork</cell><cell cols="3">45.7 23.2 45.1 26.5 50.1</cell><cell>27.8</cell></row><row><cell>motorcycle</cell><cell cols="5">56.5 36.7 61.6 40.5 59.9</cell><cell>39.6</cell><cell>knife</cell><cell cols="3">29.9 12.8 37.1 15.7 37.2</cell><cell>16.4</cell></row><row><cell>airplane</cell><cell cols="5">67.9 52.0 70.1 55.0 70.4</cell><cell>54.7</cell><cell>spoon</cell><cell cols="3">13.0 10.0 11.4 11.7 11.6</cell><cell>13.3</cell></row><row><cell>bus</cell><cell cols="5">77.7 62.5 79.5 64.6 79.0</cell><cell>64.2</cell><cell>bowl</cell><cell cols="3">49.4 32.1 53.6 35.4 52.2</cell><cell>35.4</cell></row><row><cell>train</cell><cell cols="5">66.8 59.2 69.7 62.8 69.7</cell><cell>62.1</cell><cell>banana</cell><cell cols="3">38.1 18.7 39.8 20.4 40.0</cell><cell>21.1</cell></row><row><cell>truck</cell><cell cols="5">46.3 29.9 49.7 33.0 52.2</cell><cell>34.5</cell><cell>apple</cell><cell cols="3">49.4 19.1 50.1 20.8 51.5</cell><cell>21.7</cell></row><row><cell>boat</cell><cell cols="5">30.6 19.4 32.5 22.1 32.1</cell><cell>22.3</cell><cell>sandwich</cell><cell cols="3">44.0 29.6 45.2 31.3 47.8</cell><cell>34.1</cell></row><row><cell>traffic light</cell><cell cols="5">48.9 22.7 49.8 24.3 49.1</cell><cell>24.6</cell><cell>orange</cell><cell cols="3">48.7 25.0 50.7 26.2 49.0</cell><cell>26.1</cell></row><row><cell>fire hydrant</cell><cell cols="5">75.3 59.1 74.4 59.3 74.9</cell><cell>59.5</cell><cell>broccoli</cell><cell cols="3">30.6 22.9 32.5 24.8 31.9</cell><cell>24.6</cell></row><row><cell>stop sign</cell><cell cols="5">83.2 63.6 84.4 63.8 85.6</cell><cell>66.4</cell><cell>carrot</cell><cell cols="3">25.9 14.0 28.6 16.1 21.5</cell><cell>16.4</cell></row><row><cell>parking meter</cell><cell cols="5">62.2 37.5 64.9 38.5 64.5</cell><cell>37.6</cell><cell>hot dog</cell><cell cols="3">43.7 21.8 46.5 24.8 48.2</cell><cell>25.8</cell></row><row><cell>bench</cell><cell cols="5">38.1 19.6 39.3 20.1 40.6</cell><cell>21.4</cell><cell>pizza</cell><cell cols="3">67.9 51.1 69.0 52.3 68.7</cell><cell>52.8</cell></row><row><cell>bird</cell><cell cols="5">60.2 29.4 61.9 33.0 63.3</cell><cell>34.2</cell><cell>donut</cell><cell cols="3">60.2 40.1 64.8 43.9 66.8</cell><cell>46.4</cell></row><row><cell>cat</cell><cell cols="5">64.2 58.1 68.0 61.9 67.9</cell><cell>62.4</cell><cell>cake</cell><cell cols="3">42.7 25.5 46.4 28.1 46.5</cell><cell>29.1</cell></row><row><cell>dog</cell><cell cols="5">62.6 52.9 66.1 56.2 66.9</cell><cell>57.3</cell><cell>chair</cell><cell cols="3">33.0 21.1 36.7 24.0 35.9</cell><cell>24.4</cell></row><row><cell>horse</cell><cell cols="5">67.2 53.5 70.8 57.0 71.3</cell><cell>57.0</cell><cell>couch</cell><cell cols="3">41.3 36.2 44.5 38.9 44.9</cell><cell>39.4</cell></row><row><cell>sheep</cell><cell cols="5">64.4 43.6 64.8 45.4 66.7</cell><cell>46.3</cell><cell>potted plant</cell><cell cols="3">25.6 20.1 27.3 21.9 30.0</cell><cell>23.4</cell></row><row><cell>cow</cell><cell cols="5">70.7 45.4 71.9 47.4 73.3</cell><cell>48.9</cell><cell>bed</cell><cell cols="3">44.5 40.6 45.6 41.7 47.2</cell><cell>43.4</cell></row><row><cell>elephant</cell><cell cols="5">75.1 64.1 77.3 66.4 76.1</cell><cell>65.5</cell><cell>dining table</cell><cell cols="3">33.9 25.3 36.3 27.5 36.8</cell><cell>27.6</cell></row><row><cell>bear</cell><cell cols="5">70.5 66.9 74.5 69.8 72.7</cell><cell>70.0</cell><cell>toilet</cell><cell cols="3">61.1 54.8 61.8 56.1 63.3</cell><cell>57.4</cell></row><row><cell>zebra</cell><cell cols="5">71.0 59.3 71.5 60.4 71.3</cell><cell>61.0</cell><cell>tv</cell><cell cols="3">61.8 50.0 63.0 51.9 63.7</cell><cell>52.7</cell></row><row><cell>giraffe</cell><cell cols="5">75.3 67.4 75.9 69.0 75.9</cell><cell>69.3</cell><cell>laptop</cell><cell cols="3">65.8 54.5 68.3 56.6 68.9</cell><cell>57.5</cell></row><row><cell>backpack</cell><cell cols="5">19.6 12.8 19.5 14.7 18.5</cell><cell>15.1</cell><cell>mouse</cell><cell cols="3">72.1 44.4 72.0 47.6 75.6</cell><cell>47.3</cell></row><row><cell>umbrella</cell><cell cols="5">46.2 28.9 50.7 32.3 50.4</cell><cell>32.8</cell><cell>remote</cell><cell cols="3">56.4 22.1 55.8 24.4 59.1</cell><cell>26.0</cell></row><row><cell>handbag</cell><cell>14.7</cell><cell>9.7</cell><cell cols="3">13.7 10.9 16.1</cell><cell>12.0</cell><cell>keyboard</cell><cell cols="3">57.1 45.4 57.5 45.9 61.4</cell><cell>48.3</cell></row><row><cell>tie</cell><cell cols="5">50.8 26.3 53.2 27.9 51.5</cell><cell>28.4</cell><cell>cell phone</cell><cell cols="3">54.0 23.4 58.5 26.1 57.5</cell><cell>26.7</cell></row><row><cell>suitcase</cell><cell cols="5">40.4 26.7 44.4 30.3 46.9</cell><cell>32.5</cell><cell>microwave</cell><cell cols="3">53.9 50.3 53.7 50.5 58.7</cell><cell>53.1</cell></row><row><cell>frisbee</cell><cell cols="5">53.4 43.8 55.3 48.6 58.6</cell><cell>48.3</cell><cell>oven</cell><cell cols="3">40.9 31.7 41.9 33.5 43.2</cell><cell>34.6</cell></row><row><cell>skis</cell><cell>1.5</cell><cell>18.1</cell><cell>3.0</cell><cell>20.0</cell><cell>2.3</cell><cell>20.7</cell><cell>toaster</cell><cell cols="3">32.6 14.7 39.9 20.5 32.9</cell><cell>20.1</cell></row><row><cell>snowboard</cell><cell cols="5">45.7 29.3 47.0 33.3 43.9</cell><cell>32.1</cell><cell>sink</cell><cell cols="3">43.2 31.0 44.8 34.4 44.0</cell><cell>33.9</cell></row><row><cell>sports ball</cell><cell cols="5">41.8 35.6 48.7 37.6 42.3</cell><cell>38.6</cell><cell>refrigerator</cell><cell cols="3">48.6 42.3 51.7 44.6 52.4</cell><cell>46.1</cell></row><row><cell>kite</cell><cell cols="5">39.4 37.5 33.9 38.9 35.9</cell><cell>40.0</cell><cell>book</cell><cell>15.2</cell><cell>7.4</cell><cell>18.7</cell><cell>8.8</cell><cell>21.3</cell><cell>9.8</cell></row><row><cell>baseball bat</cell><cell>8.3</cell><cell>23.4</cell><cell>6.7</cell><cell>25.1</cell><cell>9.9</cell><cell>27.5</cell><cell>clock</cell><cell cols="3">56.7 43.7 56.7 45.3 55.8</cell><cell>45.1</cell></row><row><cell>baseball glove</cell><cell cols="5">35.6 27.4 33.7 31.2 41.9</cell><cell>31.8</cell><cell>vase</cell><cell cols="3">57.1 32.3 61.5 35.9 61.4</cell><cell>36.5</cell></row><row><cell>skateboard</cell><cell cols="5">42.2 40.0 48.6 44.7 49.2</cell><cell>44.4</cell><cell>scissors</cell><cell cols="3">31.1 20.8 38.9 25.2 34.8</cell><cell>25.9</cell></row><row><cell>surfboard</cell><cell cols="5">48.5 31.1 51.7 32.8 52.4</cell><cell>33.9</cell><cell>teddy bear</cell><cell cols="3">50.4 35.4 54.7 40.2 54.7</cell><cell>40.4</cell></row><row><cell>tennis racket</cell><cell cols="5">53.1 42.6 55.1 44.1 55.4</cell><cell>45.1</cell><cell>hair drier</cell><cell>2.3</cell><cell>1.0</cell><cell>4.8</cell><cell>1.8</cell><cell>4.0</cell><cell>1.9</cell></row><row><cell>bottle</cell><cell cols="5">61.2 28.6 61.8 30.5 61.6</cell><cell>30.8</cell><cell>toothbrush</cell><cell cols="3">48.5 34.3 50.7 36.7 51.2</cell><cell>37.4</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: This work would not have been possible without the heroic efforts of Image Understanding and Expander teams at Google who built the massive JFT dataset. We would specifically like to thank Tom Duerig, Neil Alldrin, Howard Zhou, Lu Chen, David Cai, Gal Chechik, Zheyun Feng, Xiangxin Zhu and Rahul Sukthankar for their help. Also big thanks to the VALE team for APIs and specifically, Jonathan Huang, George Papandreou, Liang-Chieh Chen and Kevin Murphy for helpful discussions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A De-duplication Experiments</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Analyzing the performance of multilayer neural networks for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Exploiting weakly-labeled web images to improve object classification: a domain adaptation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bergamo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08050</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Webly supervised learning of convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neil: Extracting visual knowledge from web data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02357</idno>
		<title level="m">Xception: Deep learning with depthwise separable convolutions</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Large scale distributed deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning everything about anything: Webly-supervised visual concept learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09782</idno>
		<title level="m">Adversarial feature learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">The Pascal Visual Object Classes (VOC) Challenge. IJCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The pascal visual object classes (voc) challenge. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1311.2524</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Speed/accuracy trade-offs for modern convolutional object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Korattikara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">What makes imagenet good for transfer learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.08614</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep classifiers from image tags in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Izadinia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">What do 15,000 object categories tell us about classifying and localizing actions?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning visual features from large weakly supervised data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasilache</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02251</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The unreasonable effectiveness of noisy data for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06789</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Pearce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Boakye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V</forename><surname>Essen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03409</idno>
		<title level="m">Large-scale deep learning on the YFCC100M dataset</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Towards accurate multiperson pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.01779</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Norvig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Halev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.01360</idno>
		<title level="m">The curious robot: Learning visual representations via physical interactions</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.06825</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Unsupervised joint object discovery and segmentation in internet images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0575</idno>
		<title level="m">Imagenet large scale visual recognition challenge</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2199</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07261</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Poland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.01817</idno>
		<title level="m">The new data and new challenges in multimedia research</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Unbiased look at dataset bias. CVPR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Generating videos with scene dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00687</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Planetphoto geolocation with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kostrikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.05314</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<title level="m">AP AP@.5 AP@.75 AP(M) AP(L) AR AR@.5 AR@.75 AR(M) AR(L)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Human pose estimation performance on the COCO test-dev split</title>
	</analytic>
	<monogr>
		<title level="m">Table 13</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
