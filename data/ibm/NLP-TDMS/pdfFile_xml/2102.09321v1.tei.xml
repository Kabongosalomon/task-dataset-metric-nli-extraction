<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Miner: A Deep and Multi-branch Network which Mines Rich and Diverse Features for Person Re-identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdallah</forename><surname>Benzine</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Digeiz AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>El</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amine</forename><surname>Seddik</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Ecole Polytechnique</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Desmarais</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Digeiz AI Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Miner: A Deep and Multi-branch Network which Mines Rich and Diverse Features for Person Re-identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most recent person re-identification approaches are based on the use of deep convolutional neural networks (CNNs). These networks, although effective in multiple tasks such as classification or object detection, tend to focus on the most discriminative part of an object rather than retrieving all its relevant features. This behavior penalizes the performance of a CNN for the re-identification task, since it should identify diverse and fine grained features. It is then essential to make the network learn a wide variety of finer characteristics in order to make the re-identification process of people effective and robust to finer changes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, person re-identification (Re-ID) has attracted major interest due to its important role in various computer vision applications: video surveillance, human authentication, human-machine interaction etc. The main (ii) Several Input-Erased (IE) branches (in green) that takes as input erased feature maps and predict mined features f e1 and f e2 ; (iii) The local branch (in blue) that outputs local features f l and in which a uniform partition strategy is employed for part level feature resolution as proposed by <ref type="bibr" target="#b37">[38]</ref>. In the global branch, and the bottom IE Branch, attention modules are used in order to improve their feature representation.</p><p>objective of person Re-ID is to determine whether a given person has already appeared over a network of cameras, which technically implies a robust modelling of the global appearance of individuals. The Re-ID task is particularly challenging because of significant appearance changes -often caused by variations in the background, the lightening conditions, the body pose and the subject orientation w.r.t. the recording camera. In order to overcome these issues, one of the main goals of person Re-ID models is to produce rich representations of any input image for person matching. Notably, CNNs are known to be robust to appearance changes and spatial location variations, as their global features are invariant to such  . For instance, in the first row, the IE-branches are more attentive to the person pant. In the second row, they discover some patterns on the coat and get attentive to its cap. In the third row, they find out the plastic bag. The local branch (last column) helps the network to focus on local features such as the shoes of the subject or the object handled by the subject in the second row.</p><p>transformations. Nevertheless, the aforementioned global features are prone to ignore detailed and potentially relevant information for identifying specific person representations.</p><p>To enforce the learning of detailed features, attention mechanisms and aggregating global part-based representations were introduced in the literature, yielding very promising results <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b30">31]</ref>. Specifically, attention mechanisms allow to reduce the influence of background noise and to focus on relevant features, while part-based models divide feature maps into spatial horizontal parts thereby allowing the network to focus on fine-grained and local features. Despite their observed effectiveness in various tasks, these approaches do not provide ways to enrich and diversify an individual's representation. In fact, deep learning models are shown to exhibit a biased learning behavior <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b31">32]</ref>; in the sense that they retrieve sufficiently partial attributes concepts which contribute to reduce the training loss over the seen classes, rather than learning all-sided details and concepts. Basically, deep networks tend to focus on surface statistical regularities rather than more general abstract concepts. This behavior is problematic in the context of re-identification, since the network is required to provide the richest and most diverse possible representations.</p><p>In this paper, we propose to address this problem by adding Input Erased Branches (IE-Branch) into a standard backbone. Precisely, an IE-Branch takes partially removed feature maps as input in the aim of producing (that is "mining") more diversified features as output (as depicted in <ref type="figure" target="#fig_2">Figure 2</ref>). In particular, the removed regions correspond quite intuitively to areas where the network has strong activations and are determined by a simple suppression operation (see subsection 3.2). The proposed Deep Miner model is therefore made as the combination of IE-branches with local and global branches. The multi-branch architecture of Deep Miner is depicted on <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>The main contributions brought by this work may be summarized in the following items: (i) We provide a multibranch model allowing the mining of rich and diverse features for people re-identification. The proposed model includes three types of branches: a Global branch (G-branch), a Local branch (L-branch) and an Input-Erased Branch (IE-Branch); the latter being responsible of mining extra features; (ii) IE-Branches are constructed by adding an erase operation on the global branch feature maps, thereby allowing the network to discover new relevant features; (iii) Extensive experiments were conducted on Market1501 <ref type="bibr" target="#b43">[44]</ref>, DukeMTMC-ReID <ref type="bibr" target="#b23">[24]</ref>, CUHK03 <ref type="bibr" target="#b16">[17]</ref> and MSMT17 <ref type="bibr" target="#b36">[37]</ref>. We demonstrate that our model significantly outperforms the existing SOTA methods on all benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>There has been an extensive amount of works around the problem of people re-identification. This section particularly recalls the main advances and techniques to tackle this task, which we regroup subsequently in terms of different approaches:</p><p>Part-level features which essentially pushes the model to discover fine-grained information by extracting features at a part-level scale. Specifically, this approach consists in dividing the input image into multiple overlapping parts in order to learn part-level features <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b16">17]</ref>. In the same vein, other methods of body division were also proposed; Pose-Driven Deep Convolutional (PDC) leverages human pose information to transform a global body image into normalized part regions, Part-based Convolution Baseline (PCB) <ref type="bibr" target="#b30">[31]</ref> learns part level features by dividing the feature map equally -Essentially, the network has a 6-branch structure by dividing the feature maps into six horizontal stripes and an independent loss is applied to each strip. Based on PCB, stronger part based methods were notably developed <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b35">36]</ref>. However, theses division strategies usually suffer from misalignment between corresponding parts -because of large variations in poses, viewpoints and scales. In order to avoid this misalignment, <ref type="bibr" target="#b37">[38]</ref> suggest to concatenate the part-level feature vectors into a single vector and then apply a single loss to the concatenated vector. This strategy is more effective than applying individual loss to each part-level feature vector. As shall be seen subsequently, we particularly employ this strategy to learn the local branch of our Deep Miner model.</p><p>Metric learning methods contribute also to enhance the representations power of Re-ID features as notably shown in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b23">24]</ref>. For instance, the batch hard triplet loss introduced in <ref type="bibr" target="#b11">[12]</ref> retrieves the hardest positive and the hardest negative samples for each pedestrian image in a batch. Moreover, the soft-margin triplet loss 1 <ref type="bibr" target="#b11">[12]</ref> extends the hard triplet loss by using the SOFTPLUS function to remove the margin hyper-parameter. Finally, authors in <ref type="bibr" target="#b25">[26]</ref> improve the training and testing processes by galleryto-gallery affinities and through the use of a group-shuffling random walk network.</p><p>Attention Modules were also suggested to improve the feature representation of Re-ID models <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b38">39]</ref>. Specifically, a dual attention matching network with inter-class and intra-class attention module that captures the context information of video sequences was proposed in <ref type="bibr" target="#b26">[27]</ref>. In <ref type="bibr" target="#b17">[18]</ref>, a multi-task model jointly learns soft pixel-level and hard region-level attention to improve the discriminative feature representations. In <ref type="bibr" target="#b38">[39]</ref>, the final feature embedding is obtained by combining global and part features through the use of pose information to learn attention masks.</p><p>In the aim of learning fine grained features, a series of works focus on using erasing methods -in various contexts beyond the Re-ID paradigm. We briefly recall the main works following this approach in the following paragraph and we particularly emphasize that our proposed Deep Miner model relies on a feature erasing approach.</p><p>Feature erasing methods were commonly used for weakly-supervised object localization <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b1">2]</ref>. Technically, these methods rely on erasing the most discriminative part to encourage the CNN to detect other parts of an object. Our Deep Miner Model relies on the same principal whereas it fundamentally differs in terms of the targeted purpose and from the implementation standpoint. Indeed, Deep Miner aims to enrich the feature representation of deep neural networks in order to properly distinguish a person from another one. Similar to the proposed Deep Miner model, authors in <ref type="bibr" target="#b7">[8]</ref> introduce a Salient Feature Extraction unit which suppresses the salient features learned in the previous cascaded stage thereby extracting other potential salient features. Still, our method differs from <ref type="bibr" target="#b7">[8]</ref> through different aspects: (i) the erasing operation used in Deep Miner is conceptually simpler -since it consists in averaging and thresholding operations yielding an erasing mask -while being very efficient in terms of outcome; (ii) the obtained erasing mask is then multiplied by features maps of the initial CNN backbone allowing to create an Input-Erased branch that will discover new relevant features; (iii) by simply incorporating such simple Input-Erased branches into the standard Resnet50 backbone, Deep Miner achieves the same mAP in Market1501 as <ref type="bibr" target="#b7">[8]</ref> which include many other complex modules (e.g. attention modules); (iv) Deep Miner contains a local branch with part level resolution that is absent in <ref type="bibr" target="#b7">[8]</ref> and the two models differ in the used attention module; (v) finally, <ref type="bibr" target="#b7">[8]</ref> integrates a non-local multistage feature fusion which we found unnecessary for Deep Miner to achieve high re-identification performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>This section presents in more details our proposed method. As previously discussed, Deep Miner aims to enable the learning of more rich and fine grained features in the context of person re-identification. Specifically, Deep Miner relies on a given CNN backbone (e.g., Resnet50), and enriching it with new branches (each of them is described subsequently) to allow the network to mine richer and more diverse features. The overall architecture of the network is described in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Global Branch</head><p>The Global branch (G-branch) corresponds to a standard CNN backbone like Resnet50. This backbone is composed of B convolutional blocks 2 B i for i ∈ [B] 3 . The output of each block is denoted Y i . We particularly apply a global max pooling to the last convolutional layer (with stride being set to 1) yielding to an output vector f g . The global feature representation of a person is then obtained as a linear transformation of f g .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Input Erased Branch</head><p>As we discussed earlier, recent works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b31">32]</ref> have demonstrated that CNNs tend to focus only on the most discriminative parts of an image. In the context of person re-identification, this behavior is problematic since the network may not use important information to predict valuable features to increase person identification.</p><p>To this end though, we propose to add new branches to the initial backbone network to mine a larger diversity of features from the images (yielding our Deep Miner proposed method). Specifically, we add Input-Erased branches (IE-branches) to the initial backbone. An IE-Branch can be added after any convolutional block B i 4 of the global branch and provide a feature vector in the same way as the global branch. The convolutional block B i+1 of the Gbranch takes as input the unerased feature maps Y i 5 , while the corresponding block in the IE-branch takes as input a partially erased version of it. An Erasing Operation (ErO) (illustrated in <ref type="figure" target="#fig_3">Figure 3</ref>) is particularly applied to the feature maps Y i in order to obtain the erased features denoted Y e i . The ErO operation consists first in compressing Y i through channel-wise average pooling so to get the average features mapsỸ i . A min-max normalization is then applied toỸ i to obtainỸ n i = MIN-MAX-NORM(Ỹ i ). Given a thresholding parameter τ ∈ [0, 1], an erasing mask M i is computed as follows:</p><formula xml:id="formula_0">M i (x, y) = 0, ifỸ n i (x, y) &gt; τ 1, otherwise<label>(1)</label></formula><p>where I(x, y) stands for the pixel intensity at position (x, y) of a 2D map I. As such, the erasing mask M i is simply obtained through averaging and thresholding operations. Furthermore, the erased features Y e i are then obtained by element-wise multiplication between M i and each channel of</p><formula xml:id="formula_1">Y i , i.e., Y e i (c) = M i Y i (c)</formula><p>for c indexing the channels. As depicted in <ref type="figure" target="#fig_0">Figure 1</ref>, creating an IE branch k at block B i of the backbone will result in an additional branch composed of B − i convolutional blocks which we denote B e k j for j ∈ {i + 1, . . . , B}, each of them being identical to the corresponding convolutional block of the main branch (Gbranch). In <ref type="figure" target="#fig_0">Figure 1</ref>, an IE-Branch 1 is created after the block B 2 (bottom IE-Branch). This IE-Branch is composed of the blocks B e1 3 and B e1 4 which have the same layers architecture as B <ref type="bibr" target="#b2">3</ref> and B 4 and are initialised with same pretrained weights. If an attention module (subsection 3.4) is added to the backbone, the same attention module is added to the IE-branch. In the same way, another IE-Branch is created after block B 3 . We stress however that the weights are  Like in the global branch, we apply a global max pooling to the last convolutional layer of an IE Branch k yielding to an output vector f e k followed by a linear layer.</p><p>Note that a similar erasing operation was already introduced in <ref type="bibr" target="#b7">[8]</ref>. Nevertheless, the IE-branch in our proposed Deep Miner model differs from <ref type="bibr" target="#b7">[8]</ref> in two fundamental ways: (i) the erasing mask is computed differently involving only a thresholding parameter τ . Indeed, ErO is simply based on average pooling and thresholding operations, while <ref type="bibr" target="#b7">[8]</ref> uses a complex salient feature extractor that divides the features maps into several stripes, the selector guiding each stripe to mine important information; (ii) the erasing operation is performed in a cascaded way at the end of the Resnet50 backbone in <ref type="bibr" target="#b7">[8]</ref> , while it is performed at different stages of the backbone in our Deep Miner model. This notably allows our model to mine new relevant features at different resolutions and semantic levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Local Branch</head><p>While the IE-erased branches help the network to mine more diverse features, these branches are still based on the global appearance of a person. To help the network to mine local and more precise features, a local branch is added to Deep Miner. This branch is placed after the convolutional block B 3 and is composed of the convolutional block B l 4 which have the same layers architecture as B <ref type="bibr" target="#b3">4</ref> and is initialised with the same pretrained weights. Like in the IE-Branches, the weights are not shared between the different branches during training, so to let the network discover new localized features.</p><p>As illustrated in <ref type="figure" target="#fig_5">Figure 4</ref>, the local branch outputs features maps that are then partitioned into 4 horizontal stripes. A global average pooling is then applied to each strip to obtain 4 local features vectors. The 4 local vectors are then concatenated yielding to an output vector f l as performed in <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Attention Modules</head><p>Attention modules are commonly used in various deep learning application tasks and specifically in the context of person re-identification. The proposed Deep Miner model is also compatible with attention modules, which notably yield an enhancement of the model ability to retrieve more relevant features. To stress out the effectiveness of attention modules on the proposed method, we implement a simple attention module composed of a Spatial Attention Module (SAM) and a CHannel Attention Module (CHAM) which we describe subsequently. Features maps Y i are first processed by SAM, the result of which is then processed by CHAM. The obtained features after SAM and CHAM are denoted ATT(Y i ) = CHAM(SAM(Y i )).</p><p>SAM focuses on the most relevant features within the spatial dimension which is essentially based on <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b37">38]</ref>. Indeed, SAM captures and aggregates related features in the spatial domain. An illustration of this module is depicted in <ref type="figure">Figure 5</ref>. The input feature maps Y i of dimension H ×W ×C corresponding to the output of the convolutional block B i are fed into two convolutional layers to get two features maps A i and B i both of dimension H × W × C 8 . The tensor A i is transposed and reshaped to shape D × C and B i is reshaped to C × D, with D = H × W . An affinity matrixF i = A r i · B r i ∈ R D×D is computed by a matrix multiplication between the reshaped tensors A i and B i denoted respectively as A r i and B r i . A Softmax activation is then applied toF i leading to F i . After a reshaping operation, Y i is multiplied (map-wise) by F i and a Batch Normalization layer is applied to the resulting tensor followed by a multiplication with a learnable scalar γ. This parameter adjusts the importance of the SAM transformation. The result is then added to the input Y i to get SAM(Y i ), the resulting tensor of the Spatial Attention Module. In contrast, CHAM as illustrated in <ref type="figure" target="#fig_7">Figure 6</ref>, explores the correlation and the inter-dependencies between channel features. It is specifically based on the Squeeze-and-Exitation block <ref type="bibr" target="#b13">[14]</ref>. Besides, compared to <ref type="bibr" target="#b13">[14]</ref>, the global average pooling at the beginning of the block is removed to preserve spatial information into the attention block. A convolutional layer is applied to SAM(Y i ) to obtain feature maps of size H × W × C 16 for which a second convolution layer is applied to obtain feature maps of size H × W × C. A Soft-max activation is applied to the result that is then multiplied element-wise by SAM(Y i ) to obtain the result of CHAM.</p><p>Soft. As depicted in <ref type="figure" target="#fig_0">Figure 1</ref>, the attention module is placed right after the blocks B 2 and B 3 of the G-branch as well as after the block B e1 3 of IE-branch 1 (bottom IE branch).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Loss Functions</head><p>Each branch Br ∈ {g, l, e 1 , e 2 , . . .} (among global branch, local branch and IE branches) outputs a corresponding feature vector f Br . We apply the sames losses for each feature vector, i.e., an ID loss with label smoothing L Br ID , a soft margin triplet loss L Br triplet and a center loss L Br center yielding a global loss for each branch Br as</p><formula xml:id="formula_2">L Br = L Br ID + L Br triplet + λL Br center<label>(2)</label></formula><p>where λ &gt; 0 is an hyper-parameter (λ = 5 · 10 −4 in all our experiments). The ID loss is specifically defined as</p><formula xml:id="formula_3">L Br ID = − 1 N N i=1 q i log p Br i<label>(3)</label></formula><p>with N standing for the number of samples, p Br i denotes the predicted probability for identity i, while q i stands for the (ground-truth) smoothed label, and is defined as</p><formula xml:id="formula_4">q i = 1 − N −1 N , if i = y N , otherwise<label>(4)</label></formula><p>where y is the (hard) ground-truth identity and is a precision parameter ( = 0.1 in practice) used to enforce the model to be less confident on the training set. Note that we apply the BNNeck <ref type="bibr" target="#b20">[21]</ref> to the feature vector f Br right before the linear layer predicting the IDs probabilities. The soft margin triplet loss L Br triplet is employed to enhance the final ranking performance of the Re-ID model. L Br triplet is formally defined as</p><formula xml:id="formula_5">(5) P i =1 K k =1 SOFTPLUS(max ∈[K] f Br (x i k ) − f Br (x i ) 2 − min j∈[P ]\{i} ∈[K] f Br (x i k ) − f Br (x j ) 2 )</formula><p>where P stands for the number of identities per batch, K is the number of samples per identity, x i k is the k-th image of person i and f Br (x i k ) is the corresponding predicted feature vector (extracted among the branches of Deep Miner).</p><p>Following <ref type="bibr" target="#b20">[21]</ref>, the Center Loss L Br center is also considered for training our Deep Miner model. It simultaneously learns a center for deep features of each identity and penalizes the distances between the deep features and their corresponding identity centers. As such, intra-class compactness is increased. The Center Loss is particularly defined as</p><formula xml:id="formula_6">L Br center = 1 2 P i=1 K k=1 f Br (x i k ) − c i Br 2 2<label>(6)</label></formula><p>where c i Br is the center of identity i for the branch Br.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental details</head><p>Deep Miner is implemented in PyTorch and trained on a single Nvidia GV100 GPU. All images are resized into 384 × 128 pixels. Random Horizontal flipping and erasing are used during training. Each mini batch consists of N = P × n images where P is the number of randomly selected identities and n is the number of samples per identity. We take P = 16 and n = 4. We employ Adam as the optimizer with a warm-up strategy for the learning rate. We spent 10 epochs linearly increasing the learning rate from 3.5×10 −5 to 3.5×10 −4 . Then, the learning rate is decayed to 3.5×10 −5 and 3.5×10 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Metrics</head><p>To compare the performance of the proposed method with previous SOTA methods, the Cumulative Matching Characteristics (CMC) at Rank-1 and mean Average Precision (mAP) are adopted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Datasets</head><p>We considered four standard re-identification benchmarks, the details of which are provided subsequently.</p><p>Market1501 <ref type="bibr" target="#b43">[44]</ref> consists of 1501 identities and 32668 images. 12936 images of 751 subjects are used for training while 19732 images of 750 subjects are used for testing with 3368 query images and 16364 gallery images. The images are shot by six cameras. The Deformable Part Model is used to generate the bounding boxes <ref type="bibr" target="#b10">[11]</ref>.</p><p>DukeMTMC-ReID <ref type="bibr" target="#b23">[24]</ref> contains 36411 images of 1404 identities captured by more than 2 cameras. The training subset contains 702 identities with 16522 images and the testing subset has other 702 identities. The gallery set contains 17661 images and the query set contains 2228 images.</p><p>CUHK03 <ref type="bibr" target="#b16">[17]</ref> contains 1467 identities and a total of 14096 labeled images and 14097 detected captured by two camera views. 767 identities are used for training and 700 identities are used for testing. The labeled dataset contains 7368 training images, 5328 gallery and 1400 query images for testing, while the detected dataset contains 7365 images for training, 5332 gallery, and 1400 query images for test.</p><p>MSMT17 <ref type="bibr" target="#b36">[37]</ref> is the largest and more challenging person Re-ID dataset. 4101 identities and 126441 images are captured by a 15-camera network (12 outdoor and 3 indoor). Faster RCNN <ref type="bibr" target="#b22">[23]</ref> is used to annotate the bounding boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Studies</head><p>To demonstrate the effectiveness of the IE and Local branches on the performance of Deep Miner, we incrementally evaluate each module on Market-1501. First, we evaluate the effect of the IE-branch.</p><p>Erasing threshold: We evaluate a model with a single IE branch and vary the erasing threshold <ref type="figure" target="#fig_9">(Figure 7</ref>). Compared to the baseline, adding the IE Branch improves the mAP with all the evaluated thresholds. Nevertheless, a careful choice of the optimal value of the threshold is needed for an optimal performance. With a low threshold, too much features are erased and the IE-Branch cannot mine new significant features. With a high threshold, no enough features are removed are the IE-branch does not discover new ones. The highest scores are obtained with τ = 0.8 with 94.7% Rank-1 score and 88.0% mAP score. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IE-Branch Position:</head><p>We now evaluate the importance of the position of the IE-branch (see <ref type="table">Table 1</ref>). The IEbranch has a significant impact on the model performance, whatever its position in the CNN backbone. Still, the optimal scores are obtained at a particular position of the backbone. In fact, the best scores are obtained when the IEbranch is placed after the third convolution block with a 94.7% Rank-1 and 88.0% mAP.  <ref type="table">Table 1</ref>: Influence of IE-Branch position (τ = 0.8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IE Branch Position</head><p>Multiple IE-Branches: In this paragraph we evaluate whether the incorporation of multiple IE-branches into Deep Miner improves the results compared to a single IEbranch. <ref type="table" target="#tab_2">Table 2</ref> shows that adding IE-branches after the convolution block 3 and the convolutions block 1 or 2 improves the re-identification performance. Indeed, multiple IE-branches ensure that Deep Miner discovers new features at different semantics levels of the initial backbone. In particular, the optimal scores are obtained by adding IE-branches after the blocks 2 and 3. However, adding IE-branches after three different convolutional blocks does not yield a significant improvement compared to the 2 IEbranches architecture.  Local Branch: Now we evaluate the effect of adding a local branch. <ref type="table" target="#tab_4">Table 3</ref> shows that adding this branch to the optimal Multiple IE-branches (obtained by the previous ablation study, i.e., adding IE-branches after blocks 2 and 3) significantly improves the mAP (89.4% versus 88.5%).  Attention Modules: As previously discussed, attention modules are shown to increase the performance of modern person re-identification models. We therefore evaluate in this part the effect of such modules on Deep Miner. <ref type="table" target="#tab_6">Table  4</ref> shows that the attention module described in Subsection 3.4 helps the proposed model to mine relevant features with a 1.0% absolute mAP improvement.</p><p>In the next section, we consider the optimal Deep Miner model obtained through this ablation studies which we com-   pare to State-of-the-art methods in terms of mAP and Rank-1 score. Specifically, we define the Deep Miner model as a Resnet50 with two IE-branches placed after blocks 2 and 3, a local branch and attention modules (see <ref type="figure" target="#fig_0">Figure 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparison with State-of-the-art Methods</head><p>Now we compare our proposed Deep Miner model with recent SOTA methods to demonstrate its effectiveness and robustness compared to more advanced methods.</p><p>Market1501: <ref type="table" target="#tab_7">Table 5</ref> shows the results on the Mar-ket1501 dataset. We divide the methods into two groups: methods that uses local features (top of the    <ref type="table">Table 8</ref>: Comparison with state-of-the-art person Re-ID methods on the MSMT17 dataset. <ref type="table" target="#tab_9">Table 6</ref> show that Deep Miner achieves optimal mAP scores compared to all SOTA methods on this dataset. CUHK03: The results are shown in <ref type="table" target="#tab_10">Table 7</ref>. Note that this dataset is more challenging than the previous ones in the sense that it contains fewer number of samples and has limited viewpoint variations. The proposed Deep Miner model is shown to achieve the optimal mAP score compared to SOTA methods. Indeed, Deep Miner exceeds PLR OSNet by 4.2% in mAP on the labeled dataset and exceeds SCSN by 0.7%. On the detected dataset, it surpasses PLR OSNet by 3.2% and SCSN by 0.4%. These experimental results notably express the benefits of the feature mining method, even under the condition of limited training samples.</p><p>MSMT17: Lastly, <ref type="table">Table 8</ref> shows the results on the MSMT17 dataset, which is the more recent and largest dataset. Note that Deep Miner significantly outperforms all SOTA methods in terms of both mAP and Rank-1 scores. Notably, we obtain a significant gain of 6.5% mAP compared to the best SOTA method on this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper introduced Deep Miner, a multi-branch network that mines rich and diverse features for person reidentification. This model is composed of three types of branches that are all complementary to each other: the global branch extracts general features of the person; the Input-Erased branches mine richer and more diverse features; the Local branch looks for fine grained details. Specifically, our main insight is to add to a given backbone Input-Erased Branches which take as input partially erased features maps (through a simple erasing strategy) and find new features ignored by the backbone branch. Extensive experiments have demonstrated the effectiveness of the proposed Deep Miner model and its superiority in terms of performance compared to much more complex SOTA methods. It is worth noting that the proposed Deep Miner model makes 6.5% mAP improvement on the MSMT17 dataset -the largest and more complex existing person reidentification dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Deep Miner Model Architecture. Given a standard CNN backbone, several branches are created to enrich and diversify the features for the purpose of person reidentification. The proposed Deep Miner model is made of three types of branches: (i) The main branch G (in orange) is the original backbone and predicts the standard global features f g ;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Feature visualization for three examples. Warmer color denotes higher value. The global branch (second column) focuses only on some features but ignores other important ones. Thanks to the Input Erased branches (third and fourth columns), Deep Miner discovers new important features (localized by the black boxes)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Erasing Operation (ErO).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Local Branch of Deep Miner. not shared between the different branches during training, so to let the network discover new features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>8 D×Figure 5 :</head><label>85</label><figDesc>Spatial Attention Module (SAM).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>CHannel Attention Module (CHAM).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>−6 at 40-th epoch and 70-th epoch respectively. The model is trained until convergence. The feature vector of an input image produced by Deep Miner corresponds the concatenation of the feature vectors predicted by each branch after application of the BNNeck. A Resnet50 backbone is used in all experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Influence of the erasing threshold τ with a single IE-Branch. The IE-Branch is placed after B 3 , the third Resnet50 convolution block. The baseline (only global branch) has 94.2% Rank-1 and 84.7% mAP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Influence of multiple IE-Branches (τ = 0.8).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Influence of the Local Branch. IE-branches are placed after blocks 2 and 3 (τ = 0.8).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Influence of the Attention Module. IE-branches are placed after blocks 2 and 3 (τ = 0.8) and a Local branch is used.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>mAP</cell><cell>Rank-1</cell></row><row><cell>Deep Miner</cell><cell>Resnet50</cell><cell cols="2">90.40 95.70</cell></row><row><cell cols="2">* + PLR OSNet [38] OSNet</cell><cell cols="2">88.90 95.60</cell></row><row><cell>HOReID [35]</cell><cell>Resnet50</cell><cell>84.9</cell><cell>94.2</cell></row><row><cell>* + SCSN [8]</cell><cell>Resnet50</cell><cell>88.50</cell><cell>95.70</cell></row><row><cell>*  ABDNet [7]</cell><cell>Resnet50</cell><cell cols="2">88.28 95.60</cell></row><row><cell>+ Pyramid [43]</cell><cell>Resnet101</cell><cell>88.20</cell><cell>95.70</cell></row><row><cell>DCDS [1]</cell><cell>Resnet101</cell><cell cols="2">85.80 94.81</cell></row><row><cell>* + MHN [5]</cell><cell>Resnet50</cell><cell cols="2">85.00 95.10</cell></row><row><cell>+ MGN [19]</cell><cell>Resnet50</cell><cell>86.9</cell><cell>95.70</cell></row><row><cell>BFE [10]</cell><cell>Resnet50</cell><cell cols="2">86.20 95.30</cell></row><row><cell>* +CASN [45]</cell><cell>Resnet50</cell><cell cols="2">82.80 94.40</cell></row><row><cell>* +AANet [33]</cell><cell>Resnet152</cell><cell cols="2">83.41 93.93</cell></row><row><cell>*  IANet [13]</cell><cell>Resnet50</cell><cell cols="2">83.10 94.40</cell></row><row><cell>* + VPM [29]</cell><cell>Resnet50</cell><cell cols="2">80.80 93.00</cell></row><row><cell>PSE+ECN [25]</cell><cell>Resnet50</cell><cell cols="2">80.50 90.40</cell></row><row><cell>+ PCB+RPP [31]</cell><cell>Resnet50</cell><cell cols="2">81.60 93.80</cell></row><row><cell>+ PCB [31]</cell><cell>Resnet50</cell><cell cols="2">77.40 92.30</cell></row><row><cell>Pose-transfer [20]</cell><cell>DenseNet169</cell><cell cols="2">56.90 78.50</cell></row><row><cell>SPReID [16]</cell><cell>Resnet152</cell><cell cols="2">83.36 93.68</cell></row><row><cell>* RGA-SC [42]</cell><cell>Resnet50</cell><cell>88.4</cell><cell>96.1</cell></row><row><cell>SNR [15]</cell><cell>Resnet50</cell><cell cols="2">84.70 94.40</cell></row><row><cell>OSNet [47]</cell><cell>OSNet</cell><cell>84.9</cell><cell>94.8</cell></row><row><cell>Tricks [21]</cell><cell>SEResNet101</cell><cell cols="2">87.30 94.60</cell></row><row><cell>*  Mancs [34]</cell><cell>Resnet50</cell><cell cols="2">82.30 93.10</cell></row><row><cell>PAN [46]</cell><cell>Resnet50</cell><cell cols="2">63.40 82.80</cell></row><row><cell>SVDNet [30]</cell><cell>Resnet50</cell><cell cols="2">62.10 82.30</cell></row><row><cell cols="4">*Attention related, +Stripes Related, Pose or human pose related</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Comparison with state-of-the-art person Re-ID methods on the Market1501 dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>table)and methods that uses only global features (bottom). Deep Miner significantly outperforms previous methods in terms of the mAP score and has the same Rank-1 score as the</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>mAP</cell><cell>Rank-1</cell></row><row><cell>Deep Miner</cell><cell>Resnet50</cell><cell>81.80</cell><cell>91.20</cell></row><row><cell cols="2">* + PLR OSNet [38] OSNet</cell><cell>81.20</cell><cell>91.60</cell></row><row><cell>HOReID [35]</cell><cell>Resnet50</cell><cell>75.60</cell><cell>86.90</cell></row><row><cell>* + SCSN [8]</cell><cell>Resnet50</cell><cell>79.00</cell><cell>91.00</cell></row><row><cell>*  ABDNet [7]</cell><cell>Resnet50</cell><cell>78.60</cell><cell>89.00</cell></row><row><cell>+ Pyramid [43]</cell><cell>Resnet101</cell><cell>79.00</cell><cell>89.00</cell></row><row><cell>DCDS [1]</cell><cell>Resnet101</cell><cell>75.50</cell><cell>87.50</cell></row><row><cell>* + MHN [5]</cell><cell>Resnet50</cell><cell>77.20</cell><cell>89.10</cell></row><row><cell>+ MGN [19]</cell><cell>Resnet50</cell><cell>78.40</cell><cell>88.70</cell></row><row><cell>BFE [10]</cell><cell>Resnet50</cell><cell>75.90</cell><cell>88.90</cell></row><row><cell>* +CASN [45]</cell><cell>Resnet50</cell><cell>73.70</cell><cell>87.70</cell></row><row><cell>* +AANet [33]</cell><cell>Resnet50</cell><cell>74.29</cell><cell>87.65</cell></row><row><cell>*  IANet [13]</cell><cell>Resnet50</cell><cell>73.40</cell><cell>83.10</cell></row><row><cell>* + VPM [29]</cell><cell>Resnet50</cell><cell>72.60</cell><cell>83.60</cell></row><row><cell>PSE+ECN [25]</cell><cell>Resnet50</cell><cell>75.70</cell><cell>84.50</cell></row><row><cell>+ PCB+RPP [31]</cell><cell>Resnet50</cell><cell>69.20</cell><cell>83.30</cell></row><row><cell>Pose-transfer [20]</cell><cell>Densenet169</cell><cell>56.90</cell><cell>78.50</cell></row><row><cell>SPReID [16]</cell><cell>Resnet152</cell><cell>73.34</cell><cell>85.95</cell></row><row><cell>SNR [15]</cell><cell>Resnet50</cell><cell>72.9</cell><cell>84.4</cell></row><row><cell>OSNet [47]</cell><cell>OSNet</cell><cell>73.50</cell><cell>88.60</cell></row><row><cell>Tricks [21]</cell><cell>SeResnet101</cell><cell>78.00</cell><cell>87.50</cell></row><row><cell>*  Mancs [34]</cell><cell>Resnet50</cell><cell>71.80</cell><cell>84.90</cell></row><row><cell>PAN [46]</cell><cell>Resnet50</cell><cell>51.51</cell><cell>71.59</cell></row><row><cell>SVDNet [30]</cell><cell>Resnet50</cell><cell>56.80</cell><cell>76.70</cell></row><row><cell cols="4">*Attention related, +Stripes Related, Pose or human pose related</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Comparison with state-of-the-art person Re-ID methods on the DukeMTMC-ReID dataset.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>Labeled mAP</cell><cell cols="3">Detected Rank-1 mAP Rank-1</cell></row><row><cell>Deep Miner</cell><cell>Resnet50</cell><cell>84.7</cell><cell>86.6</cell><cell>81.4</cell><cell>83.5</cell></row><row><cell cols="2">* + PLR OSNet [38] OSNet</cell><cell>80.5</cell><cell>84.6</cell><cell>77.2</cell><cell>80.4</cell></row><row><cell>* + SCSN [8]</cell><cell>Resnet50</cell><cell cols="2">84.00 86.80</cell><cell cols="2">81.00 84.70</cell></row><row><cell>+ Pyramid [43]</cell><cell>Resnet101</cell><cell cols="2">76.90 78.90</cell><cell cols="2">74.80 78.90</cell></row><row><cell>* + MHN [5]</cell><cell>Resnet50</cell><cell cols="2">72.40 77.20</cell><cell cols="2">65.40 71.70</cell></row><row><cell>+ MGN [19]</cell><cell>Resnet50</cell><cell cols="2">67.40 68.00</cell><cell cols="2">66.00 68.00</cell></row><row><cell>BFE [10]</cell><cell>Resnet50</cell><cell cols="2">76.60 79.40</cell><cell cols="2">73.50 76.40</cell></row><row><cell>* +CASN [45]</cell><cell>Resnet50</cell><cell cols="2">68.00 73.70</cell><cell cols="2">64.40 71.50</cell></row><row><cell>+ PCB+RPP [31]</cell><cell>Resnet50</cell><cell>-</cell><cell>-</cell><cell cols="2">57.50 63.70</cell></row><row><cell>OSNet [47]</cell><cell>OSNet</cell><cell>-</cell><cell>-</cell><cell>67.8</cell><cell>72.3</cell></row><row><cell>Tricks [21]</cell><cell cols="3">SeResnet101 70.40 72.00</cell><cell cols="2">68.00 69.60</cell></row><row><cell>*  Mancs [34]</cell><cell>Resnet50</cell><cell cols="2">63.90 69.00</cell><cell cols="2">60.50 65.50</cell></row></table><note>*Attention related, +Stripes Related,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Comparison with state-of-the-art person Re-ID methods on the the CUHK03 dataset with the 767/700 split.SOTA methods SCSN and Pyramid while being conceptually much simpler than the former. We also stress that Deep Miner surpasses methods that use stronger backbones while we only consider Resnet50. In fact, authors in<ref type="bibr" target="#b42">[43]</ref> and<ref type="bibr" target="#b0">[1]</ref> use Resnet101 while authors in<ref type="bibr" target="#b32">[33]</ref> and<ref type="bibr" target="#b15">[16]</ref> use a Resnet152. Moreover, Deep Miner -with a local branch that uses only 4 stripes -surpasses more complex methods like Pyramid that uses a pyramidal feature set and a large number of stripes. Overall, the proposed Deep Miner model surpasses all SOTA methods of the global features group demonstrating its ability to mine more diverse and richer features for person re-identification.DukeMTMC-ReID: Similar to Market1501, results in</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>mAP</cell><cell>Rank-1</cell></row><row><cell>Deep Miner</cell><cell>Resnet50</cell><cell cols="2">67.30 85.60</cell></row><row><cell cols="2">* + SCSN [8] Resnet50</cell><cell cols="2">58.50 83.80</cell></row><row><cell cols="2">*  ABDNet [7] Resnet50</cell><cell cols="2">60.80 82.30</cell></row><row><cell>BFE [10]</cell><cell>Resnet50</cell><cell cols="2">51.50 78.80</cell></row><row><cell>*  IANet [13]</cell><cell>Resnet50</cell><cell cols="2">46.80 75.50</cell></row><row><cell>GLAD[36]</cell><cell>Resnet50</cell><cell cols="2">34.00 61.40</cell></row><row><cell>PDC[28]</cell><cell cols="3">GoogLeNet 29.70 58.00</cell></row></table><note>*Attention related, +Stripes Related</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This loss function is used for our Deep Miner model training.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">As a standard residual network block.<ref type="bibr" target="#b2">3</ref> The notation [n] = {1, . . . , n}.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">If an attention module is used after block B i , the IE-branch is added after this attention module.<ref type="bibr" target="#b4">5</ref> If an attention module is used after block B i , Y i stands for the output of this attention module.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep constrained dominant sets for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Leulseged Tesfaye Alemu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="9855" to="9864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Hierarchical complementary learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wuzhen</forename><surname>Sabrina Narimene Benassou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdallah</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Benzine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.08014,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Energy confused adversarial metric learning for zero-shot image retrieval and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binghui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hybrid-attention based decoupled metric learning for zero-shot image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binghui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mixed highorder attention network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binghui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="371" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Group consistent similarity learning via deep crf for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8649" to="8658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Abdnet: Attentive but diverse person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wuyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="8351" to="8361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Salience-guided cascaded suppression network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuesong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canmiao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkuan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="3300" to="3310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Attention-based dropout layer for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjung</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2219" to="2228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Batch dropblock network for person reidentification and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuozhuo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingqiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="3691" to="3701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A discriminatively trained, multiscale, deformable part model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person re-identification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Interaction-and-aggregation network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruibing</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingpeng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinqian</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="9317" to="9326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Style normalization and restitution for generalizable person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="3143" to="3152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Human semantic parsing for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emrah</forename><surname>Kalayeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhittin</forename><surname>Basaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gökmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Kamasak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1062" to="1071" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Harmonious attention network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2285" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improving person re-identification by attribute and identity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutian</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenggang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="151" to="161" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pose transferrable person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="4099" to="4108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bag of tricks and a strong baseline for deep person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youzhi</forename><surname>Hao Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenqi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Auto-reid: Searching for a part-aware convnet for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijie</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3750" to="3759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ergys</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A pose-sensitive embedding for person re-identification with expanded cross neighborhood reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arne</forename><surname>Saquib Sarfraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Eberle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="420" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep group-shuffling random walk for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yantao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2265" to="2274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dual attention matching network for context-aware feature sequence based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlou</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Guang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangfei</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5363" to="5372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pose-driven deep convolutional model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3960" to="3969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Perceive where to focus: Learning visibility-aware part-level features for partial person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="393" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Svdnet for pedestrian retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="3800" to="3808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="480" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Céline Hudelot, Mohamed El Amine Seddik, and Mohamed Tamaazousti. Learning more universal representations for transferlearning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youssef</forename><surname>Tamaazousti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Hervé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borgne</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.09708</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Aanet: Attribute attention network for person re-identifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiat-Pin</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharmili</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim-Hui</forename><surname>Yap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="7134" to="7143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mancs: A multi-task attentional network with curriculum sampling for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="365" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">High-order information matters: Learning relation and topology for occluded person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Guan&amp;apos;an Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuliang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erjin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="6449" to="6458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning discriminative features with multiple granularities for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanshuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufeng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM international conference on Multimedia</title>
		<meeting>the 26th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="274" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Person transfer gan to bridge domain gap for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Learning diverse features with part-level resolution for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suofei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07442</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attention-aware compositional network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2119" to="2128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep metric learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 22nd International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="34" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Adversarial complementary learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1325" to="1334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Relation-aware global attention for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3186" to="3195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pyramidal person re-identification via multi-loss dynamic training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongqiao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="8514" to="8522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Re-identification with consistent attentive siamese networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srikrishna</forename><surname>Karanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">J</forename><surname>Radke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="5735" to="5744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Pedestrian alignment network for large-scale person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3037" to="3045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Omni-scale feature learning for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Cavallaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="3702" to="3712" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
